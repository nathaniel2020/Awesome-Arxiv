{"arxiv_id": "2506.02351v1", "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "abstract": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-06-03 01:10:20", "updated": "2025-06-03 01:10:20", "pdf_url": "http://arxiv.org/pdf/2506.02351v1", "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02357v1", "title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components", "authors": ["Ram Potham"], "abstract": "Credible safety plans for advanced AI development require methods to verify\nagent behavior and detect potential control deficiencies early. A fundamental\naspect is ensuring agents adhere to safety-critical principles, especially when\nthese conflict with operational goals. Failure to prioritize such principles\nindicates a potential basic control failure. This paper introduces a\nlightweight, interpretable benchmark methodology using a simple grid world to\nevaluate an LLM agent's ability to uphold a predefined, high-level safety\nprinciple (e.g., \"never enter hazardous zones\") when faced with conflicting\nlower-level task instructions. We probe whether the agent reliably prioritizes\nthe inviolable directive, testing a foundational controllability aspect of\nLLMs. This pilot study demonstrates the methodology's feasibility, offers\npreliminary insights into agent behavior under principle conflict, and\ndiscusses how such benchmarks can contribute empirical evidence for assessing\ncontrollability. We argue that evaluating adherence to hierarchical principles\nis a crucial early step in understanding our capacity to build governable AI\nsystems.", "categories": ["cs.LG", "cs.AI", "cs.CY"], "published": "2025-06-03 01:16:34", "updated": "2025-06-03 01:16:34", "pdf_url": "http://arxiv.org/pdf/2506.02357v1", "comment": "Preprint. This work has been submitted to the Technical AI Governance\n  Workshop at ICML 2025 for review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02362v1", "title": "MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models", "authors": ["Xueqi Cheng", "Minxing Zheng", "Shixiang Zhu", "Yushun Dong"], "abstract": "Model extraction attacks aim to replicate the functionality of a black-box\nmodel through query access, threatening the intellectual property (IP) of\nmachine-learning-as-a-service (MLaaS) providers. Defending against such attacks\nis challenging, as it must balance efficiency, robustness, and utility\npreservation in the real-world scenario. Despite the recent advances, most\nexisting defenses presume that attacker queries have out-of-distribution (OOD)\nsamples, enabling them to detect and disrupt suspicious inputs. However, this\nassumption is increasingly unreliable, as modern models are trained on diverse\ndatasets and attackers often operate under limited query budgets. As a result,\nthe effectiveness of these defenses is significantly compromised in realistic\ndeployment scenarios. To address this gap, we propose MISLEADER (enseMbles of\ndIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does\nnot rely on OOD assumptions. MISLEADER formulates model protection as a bilevel\noptimization problem that simultaneously preserves predictive fidelity on\nbenign inputs and reduces extractability by potential clone models. Our\nframework combines data augmentation to simulate attacker queries with an\nensemble of heterogeneous distilled models to improve robustness and diversity.\nWe further provide a tractable approximation algorithm and derive theoretical\nerror bounds to characterize defense effectiveness. Extensive experiments\nacross various settings validate the utility-preserving and\nextraction-resistant properties of our proposed defense strategy. Our code is\navailable at https://github.com/LabRAI/MISLEADER.", "categories": ["cs.CR", "cs.AI"], "published": "2025-06-03 01:37:09", "updated": "2025-06-03 01:37:09", "pdf_url": "http://arxiv.org/pdf/2506.02362v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02378v1", "title": "Exploring Explanations Improves the Robustness of In-Context Learning", "authors": ["Ukyo Honda", "Tatsushi Oka"], "abstract": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-03 02:29:14", "updated": "2025-06-03 02:29:14", "pdf_url": "http://arxiv.org/pdf/2506.02378v1", "comment": "Accepted to ACL 2025 (Main Conference)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02386v1", "title": "Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget", "authors": ["Jie Bian", "Vincent Y. F. Tan"], "abstract": "The challenge of identifying the best feasible arm within a fixed budget has\nattracted considerable interest in recent years. However, a notable gap remains\nin the literature: the exact exponential rate at which the error probability\napproaches zero has yet to be established, even in the relatively simple\nsetting of $K$-armed bandits with Gaussian noise. In this paper, we address\nthis gap by examining the problem within the context of linear bandits. We\nintroduce a novel algorithm for best feasible arm identification that\nguarantees an exponential decay in the error probability. Remarkably, the decay\nrate -- characterized by the exponent -- matches the theoretical lower bound\nderived using information-theoretic principles. Our approach leverages a\nposterior sampling framework embedded within a game-based sampling rule\ninvolving a min-learner and a max-learner. This strategy shares its foundations\nwith Thompson sampling, but is specifically tailored to optimize the\nidentification process under fixed-budget constraints. Furthermore, we validate\nthe effectiveness of our algorithm through comprehensive empirical evaluations\nacross various problem instances with different levels of complexity. The\nresults corroborate our theoretical findings and demonstrate that our method\noutperforms several benchmark algorithms in terms of both accuracy and\nefficiency.", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "published": "2025-06-03 02:56:26", "updated": "2025-06-03 02:56:26", "pdf_url": "http://arxiv.org/pdf/2506.02386v1", "comment": "Accepted to the Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02387v1", "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments", "authors": ["Zelai Xu", "Zhexuan Xu", "Xiangmin Yi", "Huining Yuan", "Xinlei Chen", "Yi Wu", "Chao Yu", "Yu Wang"], "abstract": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.", "categories": ["cs.AI"], "published": "2025-06-03 02:57:38", "updated": "2025-06-03 02:57:38", "pdf_url": "http://arxiv.org/pdf/2506.02387v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02389v1", "title": "Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting", "authors": ["Chamara Madarasingha", "Nasrin Sohrabi", "Zahir Tari"], "abstract": "Time-series prediction or forecasting is critical across many real-world\ndynamic systems, and recent studies have proposed using Large Language Models\n(LLMs) for this task due to their strong generalization capabilities and\nability to perform well without extensive pre-training. However, their\neffectiveness in handling complex, noisy, and multivariate time-series data\nremains underexplored. To address this, we propose LLMPred which enhances\nLLM-based time-series prediction by converting time-series sequences into text\nand feeding them to LLMs for zero shot prediction along with two main data\npre-processing techniques. First, we apply time-series sequence decomposition\nto facilitate accurate prediction on complex and noisy univariate sequences.\nSecond, we extend this univariate prediction capability to multivariate data\nusing a lightweight prompt-processing strategy. Extensive experiments with\nsmaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B\ndemonstrate that LLMPred achieves competitive or superior performance compared\nto state-of-the-art baselines. Additionally, a thorough ablation study\nhighlights the importance of the key components proposed in LLMPred.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 03:02:47", "updated": "2025-06-03 03:02:47", "pdf_url": "http://arxiv.org/pdf/2506.02389v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02391v1", "title": "Consultant Decoding: Yet Another Synergistic Mechanism", "authors": ["Chuanghao Ding", "Jiaping Wang", "Ziqing Yang", "Xiaoliang Wang", "Dahua Lin", "Cam-Tu Nguyen", "Fei Tan"], "abstract": "The synergistic mechanism based on Speculative Decoding (SD) has garnered\nconsiderable attention as a simple yet effective approach for accelerating the\ninference of large language models (LLMs). Nonetheless, the high rejection\nrates require repeated LLMs calls to validate draft tokens, undermining the\noverall efficiency gain of SD. In this work, we revisit existing verification\nmechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).\nUnlike SD, which relies on a metric derived from importance sampling for\nverification, CD verifies candidate drafts using token-level likelihoods\ncomputed solely by the LLM. CD achieves up to a 2.5-fold increase in inference\nspeed compared to the target model, while maintaining comparable generation\nquality (around 100% of the target model's performance). Interestingly, this is\nachieved by combining models whose parameter sizes differ by two orders of\nmagnitude. In addition, CD reduces the call frequency of the large target model\nto below 10%, particularly in more demanding tasks. CD's performance was even\nfound to surpass that of the large target model, which theoretically represents\nthe upper bound for speculative decoding.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 03:13:27", "updated": "2025-06-03 03:13:27", "pdf_url": "http://arxiv.org/pdf/2506.02391v1", "comment": "ACL 2025 findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02397v1", "title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation", "authors": ["Shengjia Zhang", "Junjie Wu", "Jiawei Chen", "Changwang Zhang", "Xingyu Lou", "Wangchunshu Zhou", "Sheng Zhou", "Can Wang", "Jun Wang"], "abstract": "Recent advanced large reasoning models (LRMs) leverage extended\nchain-of-thought (CoT) reasoning to solve complex tasks, achieving\nstate-of-the-art performance. Despite their success, we identify a critical\nissue: a substantial portion of simple tasks solved by LRMs can also be\naddressed by non-reasoning LLMs using significantly fewer tokens, indicating\nthe complex reasoning may not always be necessary. To address this, we\nsystematically analyze the reasoning trajectories of LRMs and present a method\nutilizing identified paradigms and LLM-Judge to classify these trajectories as\neither Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,\na method that prunes redundant reasoning steps while preserving logical\nvalidity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)\nfor straightforward problems while engaging in deliberate thinking\n(slow-thinking) for complex problems. Experiments across mathematical and\nquestion-answering tasks demonstrate that OThink-R1 reduces reasoning\nredundancy by almost 23\\% on average without compromising accuracy, offering\npractical guidelines for efficient reasoning models. The code is available at\nhttps://github.com/AgenticIR-Lab/OThink-R1.", "categories": ["cs.AI"], "published": "2025-06-03 03:31:30", "updated": "2025-06-03 03:31:30", "pdf_url": "http://arxiv.org/pdf/2506.02397v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02404v1", "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "abstract": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 03:44:26", "updated": "2025-06-03 03:44:26", "pdf_url": "http://arxiv.org/pdf/2506.02404v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02406v1", "title": "Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL", "authors": ["Renat Sergazinov", "Jing Wu", "Shao-An Yin"], "abstract": "While random Fourier features are a classic tool in kernel methods, their\nutility as a pre-processing step for deep learning on tabular data has been\nlargely overlooked. Motivated by shortcomings in tabular deep learning\npipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit\nand repurpose random Fourier mappings as a parameter-free,\narchitecture-agnostic transformation. By projecting each input into a fixed\nfeature space via sine and cosine projections with frequencies drawn once at\ninitialization, this approach circumvents the need for ad hoc normalization or\nadditional learnable embeddings. We show within the NTK framework that this\nmapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)\nintroduces a bias that shortens the optimization trajectory, thereby\naccelerating gradient-based training. These effects pre-condition the network\nwith a stable kernel from the outset. Empirically, we demonstrate that deep\nnetworks trained on Fourier-transformed inputs converge more rapidly and\nconsistently achieve strong final performance, often with fewer epochs and less\nhyperparameter tuning. Our findings establish random Fourier pre-processing as\na theoretically motivated, plug-and-play enhancement for tabular deep learning.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-06-03 03:45:13", "updated": "2025-06-03 03:45:13", "pdf_url": "http://arxiv.org/pdf/2506.02406v1", "comment": "16 pages, 3 figures, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02412v1", "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning", "authors": ["Zhengyuan Liu", "Geyu Lin", "Hui Li Tan", "Huayun Zhang", "Yanfeng Lu", "Xiaoxue Gao", "Stella Xin Yin", "He Sun", "Hock Huan Goh", "Lung Hsiang Wong", "Nancy F. Chen"], "abstract": "The integration of generative artificial intelligence into educational\napplications has enhanced personalized and interactive learning experiences,\nand it shows strong potential to promote young learners language acquisition.\nHowever, it is still challenging to ensure consistent and robust performance\nacross different languages and cultural contexts, and kids-friendly design\nrequires simplified instructions, engaging interactions, and age-appropriate\nscaffolding to maintain motivation and optimize learning outcomes. In this\nwork, we introduce SingaKids, a dialogic tutor designed to facilitate language\nlearning through picture description tasks. Our system integrates dense image\ncaptioning, multilingual dialogic interaction, speech understanding, and\nengaging speech generation to create an immersive learning environment in four\nlanguages: English, Mandarin, Malay, and Tamil. We further improve the system\nthrough multilingual pre-training, task-specific tuning, and scaffolding\noptimization. Empirical studies with elementary school students demonstrate\nthat SingaKids provides effective dialogic teaching, benefiting learners at\ndifferent performance levels.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 03:56:45", "updated": "2025-06-03 03:56:45", "pdf_url": "http://arxiv.org/pdf/2506.02412v1", "comment": "ACL 2025 Industry Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02415v1", "title": "AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting", "authors": ["Karthikeyan Vaiapury"], "abstract": "Optimization remains a fundamental pillar of machine learning, yet existing\nmethods often struggle to maintain stability and adaptability in dynamic, non\nlinear systems, especially under uncertainty. We introduce AERO (Adversarial\nEnergy-based Redirection Optimization), a novel framework inspired by the\nredirection principle in Judo, where external disturbances are leveraged rather\nthan resisted. AERO reimagines optimization as a redirection process guided by\n15 interrelated axioms encompassing adversarial correction, energy\nconservation, and disturbance-aware learning. By projecting gradients,\nintegrating uncertainty driven dynamics, and managing learning energy, AERO\noffers a principled approach to stable and robust model updates. Applied to\nprobabilistic solar energy forecasting, AERO demonstrates substantial gains in\npredictive accuracy, reliability, and adaptability, especially in noisy and\nuncertain environments. Our findings highlight AERO as a compelling new\ndirection in the theoretical and practical landscape of optimization.", "categories": ["cs.LG", "cs.AI", "62M10, 60G25, 62P30", "I.2.6; I.5.1; G.3; J.2"], "published": "2025-06-03 04:02:20", "updated": "2025-06-03 04:02:20", "pdf_url": "http://arxiv.org/pdf/2506.02415v1", "comment": "15 pages, 1 figure, submitted to NeurIPS 2025 (preprint version)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02426v1", "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "abstract": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at\n\\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "published": "2025-06-03 04:19:47", "updated": "2025-06-03 04:19:47", "pdf_url": "http://arxiv.org/pdf/2506.02426v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02438v1", "title": "A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges", "authors": ["Sudhanshu Sekhar Tripathy", "Bichitrananda Behera"], "abstract": "IDS aims to protect computer networks from security threats by detecting,\nnotifying, and taking appropriate action to prevent illegal access and protect\nconfidential information. As the globe becomes increasingly dependent on\ntechnology and automated processes, ensuring secured systems, applications, and\nnetworks has become one of the most significant problems of this era. The\nglobal web and digital technology have significantly accelerated the evolution\nof the modern world, necessitating the use of telecommunications and data\ntransfer platforms. Researchers are enhancing the effectiveness of IDS by\nincorporating popular datasets into machine learning algorithms. IDS, equipped\nwith machine learning classifiers, enhances security attack detection accuracy\nby identifying normal or abnormal network traffic. This paper explores the\nmethods of capturing and reviewing intrusion detection systems (IDS) and\nevaluates the challenges existing datasets face. A deluge of research on\nmachine learning (ML) and deep learning (DL) architecture-based intrusion\ndetection techniques has been conducted in the past ten years on various\ncybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017,\nand CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth\nanalysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB,\nRF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique,\nexplaining the role of the classifiers and algorithms used. A detailed tabular\nanalysis highlights the datasets used, classifiers employed, attacks detected,\nevaluation metrics, and conclusions drawn. This article offers a thorough\nreview for future IDS research.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "published": "2025-06-03 04:47:21", "updated": "2025-06-03 04:47:21", "pdf_url": "http://arxiv.org/pdf/2506.02438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02448v1", "title": "VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos", "authors": ["Baoyu Liang", "Qile Su", "Shoutai Zhu", "Yuchen Liang", "Chao Tong"], "abstract": "Despite the significant impact of visual events on human cognition,\nunderstanding events in videos remains a challenging task for AI due to their\ncomplex structures, semantic hierarchies, and dynamic evolution. To address\nthis, we propose the task of video event understanding that extracts event\nscripts and makes predictions with these scripts from videos. To support this\ntask, we introduce VidEvent, a large-scale dataset containing over 23,000\nwell-labeled events, featuring detailed event structures, broad hierarchies,\nand logical relations extracted from movie recap videos. The dataset was\ncreated through a meticulous annotation process, ensuring high-quality and\nreliable event data. We also provide comprehensive baseline models offering\ndetailed descriptions of their architecture and performance metrics. These\nmodels serve as benchmarks for future research, facilitating comparisons and\nimprovements. Our analysis of VidEvent and the baseline models highlights the\ndataset's potential to advance video event understanding and encourages the\nexploration of innovative algorithms and models. The dataset and related\nresources are publicly available at www.videvent.top.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 05:12:48", "updated": "2025-06-03 05:12:48", "pdf_url": "http://arxiv.org/pdf/2506.02448v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02454v1", "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework", "authors": ["Zhaorui Yang", "Bo Pan", "Han Wang", "Yiyao Wang", "Xingyu Liu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "abstract": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 05:18:19", "updated": "2025-06-03 05:18:19", "pdf_url": "http://arxiv.org/pdf/2506.02454v1", "comment": "47 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02456v1", "title": "VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents", "authors": ["Tri Cao", "Bennett Lim", "Yue Liu", "Yuan Sui", "Yuexin Li", "Shumin Deng", "Lin Lu", "Nay Oo", "Shuicheng Yan", "Bryan Hooi"], "abstract": "Computer-Use Agents (CUAs) with full system access enable powerful task\nautomation but pose significant security and privacy risks due to their ability\nto manipulate files, access user data, and execute arbitrary commands. While\nprior work has focused on browser-based agents and HTML-level attacks, the\nvulnerabilities of CUAs remain underexplored. In this paper, we investigate\nVisual Prompt Injection (VPI) attacks, where malicious instructions are\nvisually embedded within rendered user interfaces, and examine their impact on\nboth CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of\n306 test cases across five widely used platforms, to evaluate agent robustness\nunder VPI threats. Each test case is a variant of a web platform, designed to\nbe interactive, deployed in a realistic environment, and containing a visually\nembedded malicious prompt. Our empirical study shows that current CUAs and BUAs\ncan be deceived at rates of up to 51% and 100%, respectively, on certain\nplatforms. The experimental results also indicate that system prompt defenses\noffer only limited improvements. These findings highlight the need for robust,\ncontext-aware defenses to ensure the safe deployment of multimodal AI agents in\nreal-world environments. The code and dataset are available at:\nhttps://github.com/cua-framework/agents", "categories": ["cs.AI", "cs.CR"], "published": "2025-06-03 05:21:50", "updated": "2025-06-03 05:21:50", "pdf_url": "http://arxiv.org/pdf/2506.02456v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02470v1", "title": "A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning", "authors": ["Xuejiao Zhao", "Siyan Liu", "Su-Yin Yang", "Chunyan Miao"], "abstract": "Misdiagnosis causes significant harm to healthcare systems worldwide, leading\nto increased costs and patient risks. MedRAG is a smart multimodal healthcare\ncopilot equipped with powerful large language model (LLM) reasoning, designed\nto enhance medical decision-making. It supports multiple input modalities,\nincluding non-intrusive voice monitoring, general medical queries, and\nelectronic health records. MedRAG provides recommendations on diagnosis,\ntreatment, medication, and follow-up questioning. Leveraging\nretrieval-augmented generation enhanced by knowledge graph-elicited reasoning,\nMedRAG retrieves and integrates critical diagnostic insights, reducing the risk\nof misdiagnosis. It has been evaluated on both public and private datasets,\noutperforming existing models and offering more specific and accurate\nhealthcare assistance. A demonstration video of MedRAG is available at:\nhttps://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:\nhttps://github.com/SNOWTEAM2023/MedRAG.", "categories": ["cs.AI"], "published": "2025-06-03 05:39:02", "updated": "2025-06-03 05:39:02", "pdf_url": "http://arxiv.org/pdf/2506.02470v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02481v1", "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths", "authors": ["Inderjeet Nair", "Lu Wang"], "abstract": "Evaluations of LLMs' ethical risks and value inclinations often rely on\nshort-form surveys and psychometric tests, yet real-world use involves\nlong-form, open-ended responses -- leaving value-related risks and preferences\nin practical settings largely underexplored. In this work, we ask: Do value\npreferences inferred from short-form tests align with those expressed in\nlong-form outputs? To address this question, we compare value preferences\nelicited from short-form reactions and long-form responses, varying the number\nof arguments in the latter to capture users' differing verbosity preferences.\nAnalyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),\nwe find (1) a weak correlation between value preferences inferred from\nshort-form and long-form responses across varying argument counts, and (2)\nsimilarly weak correlation between preferences derived from any two distinct\nlong-form generation settings. (3) Alignment yields only modest gains in the\nconsistency of value expression. Further, we examine how long-form generation\nattributes relate to value preferences, finding that argument specificity\nnegatively correlates with preference strength, while representation across\nscenarios shows a positive correlation. Our findings underscore the need for\nmore robust methods to ensure consistent value expression across diverse\napplications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 05:52:03", "updated": "2025-06-03 05:52:03", "pdf_url": "http://arxiv.org/pdf/2506.02481v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02485v1", "title": "Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning", "authors": ["Haowen Xu", "Sisi Zlatanova", "Ruiyu Liang", "Ismet Canbulat"], "abstract": "Wildfires continue to inflict devastating human, environmental, and economic\nlosses globally, as tragically exemplified by the 2025 Los Angeles wildfire and\nthe urgent demand for more effective response strategies. While physics-based\nand deep learning models have advanced wildfire simulation, they face critical\nlimitations in predicting and visualizing multimodal fire spread in real time,\nparticularly in both 2D and 3D spatial domains using dynamically updated GIS\ndata. These limitations hinder timely emergency response, infrastructure\nprotection, and community safety. Generative AI has recently emerged as a\ntransformative approach across research and industry. Models such as Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and\ndiffusion-based architectures offer distinct advantages over traditional\nmethods, including the integration of multimodal data, generation of diverse\nscenarios under uncertainty, and improved modeling of wildfire dynamics across\nspatial and temporal scales. This position paper advocates for the adoption of\ngenerative AI as a foundational framework for wildfire prediction. We explore\nhow such models can enhance 2D fire spread forecasting and enable more\nrealistic, scalable 3D simulations. Additionally, we employ a novel human-AI\ncollaboration framework using large language models (LLMs) for automated\nknowledge extraction, literature synthesis, and bibliometric mapping. Looking\nahead, we identify five key visions for integrating generative AI into wildfire\nmanagement: multimodal approaches, AI foundation models, conversational AI\nsystems, edge-computing-based scenario generation, and cognitive digital twins.\nWe also address three major challenges accompanying these opportunities and\npropose potential solutions to support their implementation.", "categories": ["cs.AI", "cs.CE"], "published": "2025-06-03 05:54:40", "updated": "2025-06-03 05:54:40", "pdf_url": "http://arxiv.org/pdf/2506.02485v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02488v1", "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models", "authors": ["Hongtao Huang", "Xiaojun Chang", "Lina Yao"], "abstract": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 06:02:50", "updated": "2025-06-03 06:02:50", "pdf_url": "http://arxiv.org/pdf/2506.02488v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02490v1", "title": "Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM", "authors": ["Yong Xiang", "Charley Peter Chen", "Liyi Zeng", "Wei Yin", "Xin Liu", "Hu Li", "Wei Xu"], "abstract": "Kubernetes, a notably complex and distributed system, utilizes an array of\ncontrollers to uphold cluster management logic through state reconciliation.\nNevertheless, maintaining state consistency presents significant challenges due\nto unexpected failures, network disruptions, and asynchronous issues,\nespecially within dynamic cloud environments. These challenges result in\noperational disruptions and economic losses, underscoring the necessity for\nrobust root cause analysis (RCA) to enhance Kubernetes reliability. The\ndevelopment of large language models (LLMs) presents a promising direction for\nRCA. However, existing methodologies encounter several obstacles, including the\ndiverse and evolving nature of Kubernetes incidents, the intricate context of\nincidents, and the polymorphic nature of these incidents. In this paper, we\nintroduce SynergyRCA, an innovative tool that leverages LLMs with retrieval\naugmentation from graph databases and enhancement with expert prompts.\nSynergyRCA constructs a StateGraph to capture spatial and temporal\nrelationships and utilizes a MetaGraph to outline entity connections. Upon the\noccurrence of an incident, an LLM predicts the most pertinent resource, and\nSynergyRCA queries the MetaGraph and StateGraph to deliver context-specific\ninsights for RCA. We evaluate SynergyRCA using datasets from two production\nKubernetes clusters, highlighting its capacity to identify numerous root\ncauses, including novel ones, with high efficiency and precision. SynergyRCA\ndemonstrates the ability to identify root causes in an average time of about\ntwo minutes and achieves an impressive precision of approximately 0.90.", "categories": ["cs.DC", "cs.AI", "cs.SE"], "published": "2025-06-03 06:09:13", "updated": "2025-06-03 06:09:13", "pdf_url": "http://arxiv.org/pdf/2506.02490v1", "comment": "12 pages, 13 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02494v1", "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "abstract": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-06-03 06:17:16", "updated": "2025-06-03 06:17:16", "pdf_url": "http://arxiv.org/pdf/2506.02494v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02510v1", "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset", "authors": ["Jie Zhu", "Junhui Li", "Yalong Wen", "Xiandong Li", "Lifan Guo", "Feng Chen"], "abstract": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\n$\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\n$\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 06:41:09", "updated": "2025-06-03 06:41:09", "pdf_url": "http://arxiv.org/pdf/2506.02510v1", "comment": "Accepted by ACL-2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02515v1", "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning", "authors": ["Zhuohan Xie", "Dhruv Sahnan", "Debopriyo Banerjee", "Georgi Georgiev", "Rushil Thareja", "Hachem Madmoun", "Jinyan Su", "Aaryamonvikram Singh", "Yuxia Wang", "Rui Xing", "Fajri Koto", "Haonan Li", "Ivan Koychev", "Tanmoy Chakraborty", "Salem Lahlou", "Veselin Stoyanov", "Preslav Nakov"], "abstract": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-03 06:44:42", "updated": "2025-06-03 06:44:42", "pdf_url": "http://arxiv.org/pdf/2506.02515v1", "comment": "15 pages, 8 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02522v1", "title": "Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making", "authors": ["Xu Wan", "Wenyue Xu", "Chao Yang", "Mingyang Sun"], "abstract": "Recent advancements in Large Language Models (LLMs) and Reinforcement\nLearning (RL) have shown significant promise in decision-making tasks.\nNevertheless, for large-scale industrial decision problems, both approaches\nface distinct challenges: LLMs lack real-time long-sequence decision-making\ncapabilities, while RL struggles with sample efficiency in vast action spaces.\nTo bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic\nframework between LLMs and RL agents for large-scale decision-making scenarios.\nACE introduces a dual-role trajectory refinement mechanism where LLMs act as\nboth Policy Actor and Value Critic during RL's training: the Actor refines\nsuboptimal actions via multi-step reasoning and environment validation, while\nthe Critic performs temporal credit assignment through trajectory-level reward\nshaping. Concurrently, RL agent enhances LLMs' task-specific decision-making\nwith high-quality fine-tuning datasets generated via prioritized experience\nreplay. Through extensive experiments across multiple power grid operation\nchallenges with action spaces exceeding 60K discrete actions, ACE demonstrates\nsuperior performance over existing RL methods and LLM-based methods.", "categories": ["cs.AI"], "published": "2025-06-03 06:52:37", "updated": "2025-06-03 06:52:37", "pdf_url": "http://arxiv.org/pdf/2506.02522v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02527v1", "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base", "authors": ["Yingying Zhuang", "Aman Gupta", "Anurag Beniwal"], "abstract": "Multilingual information retrieval has emerged as powerful tools for\nexpanding knowledge sharing across languages. On the other hand, resources on\nhigh quality knowledge base are often scarce and in limited languages,\ntherefore an effective embedding model to transform sentences from different\nlanguages into a feature vector space same as the knowledge base language\nbecomes the key ingredient for cross language knowledge sharing, especially to\ntransfer knowledge available in high-resource languages to low-resource ones.\nIn this paper we propose a novel strategy to fine-tune multilingual embedding\nmodels with weighted sampling for contrastive learning, enabling multilingual\ninformation retrieval with a monolingual knowledge base. We demonstrate that\nthe weighted sampling strategy produces performance gains compared to standard\nones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our\nproposed methodology is language agnostic and applicable for both multilingual\nand code switching use cases.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-03 07:05:49", "updated": "2025-06-03 07:05:49", "pdf_url": "http://arxiv.org/pdf/2506.02527v1", "comment": "6 pages, accepted at GENNEXT@SIGIR25", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02529v1", "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs", "authors": ["Nguyen-Khang Le", "Quan Minh Bui", "Minh Ngoc Nguyen", "Hiep Nguyen", "Trung Vo", "Son T. Luu", "Shoshin Nomura", "Minh Le Nguyen"], "abstract": "Web applications are critical to modern software ecosystems, yet ensuring\ntheir reliability remains challenging due to the complexity and dynamic nature\nof web interfaces. Recent advances in large language models (LLMs) have shown\npromise in automating complex tasks, but limitations persist in handling\ndynamic navigation flows and complex form interactions. This paper presents an\nautomated system for generating test cases for two key aspects of web\napplication testing: site navigation and form filling. For site navigation, the\nsystem employs screen transition graphs and LLMs to model navigation flows and\ngenerate test scenarios. For form filling, it uses state graphs to handle\nconditional forms and automates Selenium script generation. Key contributions\ninclude: (1) a novel integration of graph structures and LLMs for site\nnavigation testing, (2) a state graph-based approach for automating\nform-filling test cases, and (3) a comprehensive dataset for evaluating\nform-interaction testing. Experimental results demonstrate the system's\neffectiveness in improving test coverage and robustness, advancing the state of\nweb application testing.", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "published": "2025-06-03 07:08:21", "updated": "2025-06-03 07:08:21", "pdf_url": "http://arxiv.org/pdf/2506.02529v1", "comment": "Published in the Proceedings of JSAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02537v1", "title": "VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning", "authors": ["Hao Yan", "Handong Zheng", "Hao Wang", "Liang Yin", "Xingchen Liu", "Zhenbiao Cao", "Xinxing Su", "Zihao Chen", "Jihao Wu", "Minghui Liao", "Chao Weng", "Wei Chen", "Yuliang Liu", "Xiang Bai"], "abstract": "Recent strides in multimodal large language models (MLLMs) have significantly\nadvanced their performance in many reasoning tasks. However, Abstract Visual\nReasoning (AVR) remains a critical challenge, primarily due to limitations in\nperceiving abstract graphics. To tackle this issue, we investigate the\nbottlenecks in current MLLMs and synthesize training data to improve their\nabstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,\nfeaturing tasks meticulously constructed to assess models' reasoning capacities\nacross five core dimensions and two high-level reasoning categories. Second, we\nintroduce the Perceptual Riddle Synthesizer (PRS), an automated framework for\ngenerating riddles with fine-grained perceptual descriptions. PRS not only\ngenerates valuable training data for abstract graphics but also provides\nfine-grained perceptual description, crucially allowing for supervision over\nintermediate reasoning stages and thereby improving both training efficacy and\nmodel interpretability. Our extensive experimental results on VisuRiddles\nempirically validate that fine-grained visual perception is the principal\nbottleneck and our synthesis framework markedly enhances the performance of\ncontemporary MLLMs on these challenging tasks. Our code and dataset will be\nreleased at https://github.com/yh-hust/VisuRiddles", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 07:24:00", "updated": "2025-06-03 07:24:00", "pdf_url": "http://arxiv.org/pdf/2506.02537v1", "comment": "13 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02541v1", "title": "Rethinking Post-Unlearning Behavior of Large Vision-Language Models", "authors": ["Minsung Kim", "Nakyeong Yang", "Kyomin Jung"], "abstract": "Machine unlearning is used to mitigate the privacy risks of Large\nVision-Language Models (LVLMs) arising from training on large-scale web data.\nHowever, existing unlearning methods often fail to carefully select substitute\noutputs for forget targets, resulting in Unlearning Aftermaths-undesirable\nbehaviors such as degenerate, hallucinated, or excessively refused responses.\nWe highlight that, especially for generative LVLMs, it is crucial to consider\nthe quality and informativeness of post-unlearning responses rather than\nrelying solely on naive suppression. To address this, we introduce a new\nunlearning task for LVLMs that requires models to provide privacy-preserving\nyet informative and visually grounded responses. We also propose PUBG, a novel\nunlearning method that explicitly guides post-unlearning behavior toward a\ndesirable output distribution. Experiments show that, while existing methods\nsuffer from Unlearning Aftermaths despite successfully preventing privacy\nviolations, PUBG effectively mitigates these issues, generating visually\ngrounded and informative responses without privacy leakage for forgotten\ntargets.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-06-03 07:28:22", "updated": "2025-06-03 07:28:22", "pdf_url": "http://arxiv.org/pdf/2506.02541v1", "comment": "10 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02542v1", "title": "HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification", "authors": ["Niklas Kormann", "Masoud Ramuz", "Zeeshan Nisar", "Nadine S. Schaadt", "Hendrik Annuth", "Benjamin Doerr", "Friedrich Feuerhake", "Thomas Lampert", "Johannes F. Lutzeyer"], "abstract": "Graph Neural Networks (GNNs) have recently been found to excel in\nhistopathology. However, an important histopathological task, where GNNs have\nnot been extensively explored, is the classification of glomeruli health as an\nimportant indicator in nephropathology. This task presents unique difficulties,\nparticularly for the graph construction, i.e., the identification of nodes,\nedges, and informative features. In this work, we propose a pipeline composed\nof different traditional and machine learning-based computer vision techniques\nto identify nodes, edges, and their corresponding features to form a\nheterogeneous graph. We then proceed to propose a novel heterogeneous GNN\narchitecture for glomeruli classification, called HIEGNet, that integrates both\nglomeruli and their surrounding immune cells. Hence, HIEGNet is able to\nconsider the immune environment of each glomerulus in its classification. Our\nHIEGNet was trained and tested on a dataset of Whole Slide Images from kidney\ntransplant patients. Experimental results demonstrate that HIEGNet outperforms\nseveral baseline models and generalises best between patients among all\nbaseline models. Our implementation is publicly available at\nhttps://github.com/nklsKrmnn/HIEGNet.git.", "categories": ["cs.LG", "cs.AI", "cs.CV", "q-bio.QM"], "published": "2025-06-03 07:28:25", "updated": "2025-06-03 07:28:25", "pdf_url": "http://arxiv.org/pdf/2506.02542v1", "comment": "Accepted for poster presentation at MIDL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02544v1", "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "abstract": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge\n\\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG}\n(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles\ninconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage\npipeline: it first generates an internal response from parametric knowledge,\nthen selects the most relevant multimodal evidence via joint similarity\nassessment, generates an external response, and finally integrates both to\nproduce a reliable answer. Additionally, a specialized training paradigm\nenhances knowledge source discrimination, multimodal integration, and unified\nanswer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG\nachieves substantial improvements over baseline methods, achieving 5.6\\% and\n9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We\nrelease code and data at\n\\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 07:32:40", "updated": "2025-06-03 07:32:40", "pdf_url": "http://arxiv.org/pdf/2506.02544v1", "comment": "Accepted to ACL 2025 Main", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02548v1", "title": "CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale", "authors": ["Zhun Wang", "Tianneng Shi", "Jingxuan He", "Matthew Cai", "Jialin Zhang", "Dawn Song"], "abstract": "Large language model (LLM) agents are becoming increasingly skilled at\nhandling cybersecurity tasks autonomously. Thoroughly assessing their\ncybersecurity capabilities is critical and urgent, given the high stakes in\nthis domain. However, existing benchmarks fall short, often failing to capture\nreal-world scenarios or being limited in scope. To address this gap, we\nintroduce CyberGym, a large-scale and high-quality cybersecurity evaluation\nframework featuring 1,507 real-world vulnerabilities found and patched across\n188 large software projects. While it includes tasks of various settings,\nCyberGym primarily focuses on the generation of proof-of-concept (PoC) tests\nfor vulnerability reproduction, based on text descriptions and corresponding\nsource repositories. Solving this task is particularly challenging, as it\nrequires comprehensive reasoning across entire codebases to locate relevant\ncode fragments and produce effective PoCs that accurately trigger the target\nvulnerability starting from the program's entry point. Our evaluation across 4\nstate-of-the-art agent frameworks and 9 LLMs reveals that even the best\ncombination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9%\nreproduction success rate, mainly on simpler cases. Beyond reproducing\nhistorical vulnerabilities, we find that PoCs generated by LLM agents can\nreveal new vulnerabilities, identifying 15 zero-days affecting the latest\nversions of the software projects.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "published": "2025-06-03 07:35:14", "updated": "2025-06-03 07:35:14", "pdf_url": "http://arxiv.org/pdf/2506.02548v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02550v1", "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Yisen Feng", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "abstract": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 07:36:52", "updated": "2025-06-03 07:36:52", "pdf_url": "http://arxiv.org/pdf/2506.02550v1", "comment": "The champion solution for the Ego4D Long-Term Action Anticipation\n  Challenge at the CVPR EgoVis Workshop 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02553v1", "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective", "authors": ["Shenghua He", "Tian Xia", "Xuan Zhou", "Hui Wei"], "abstract": "We study a common challenge in reinforcement learning for large language\nmodels (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,\nintermediate token generations) receive zero task-specific immediate reward,\nwhile only the final token receives a reward for the entire response. This\nassumption arises frequently in practice, as precise token-level rewards are\noften difficult or infeasible to obtain in LLM applications. In this work, we\nprovide a unifying theoretical perspective. We introduce the Trajectory Policy\nGradient Theorem, which shows that the policy gradient based on true, unknown\ntoken-level rewards can be unbiasedly estimated using only a response-level\nreward model, regardless of whether the Zero-Reward Assumption holds or not,\nfor algorithms in the REINFORCE and Actor-Critic families. This result reveals\nthat widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess\nthe capacity to model token-level reward signals, offering a theoretical\njustification for response-level reward approaches. Our findings pave the way\nfor more practical, efficient LLM fine-tuning, allowing developers to treat\ntraining algorithms as black boxes and focus on improving the response-level\nreward model with auxiliary sub-models. We also offer a detailed analysis of\npopular RL and non-RL methods, comparing their theoretical foundations and\npractical advantages across common LLM tasks. Finally, we propose a new\nalgorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically\ngrounded method that is simpler than PPO, matches GRPO in memory efficiency,\nand holds promise for broad applicability.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-03 07:44:31", "updated": "2025-06-03 07:44:31", "pdf_url": "http://arxiv.org/pdf/2506.02553v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02554v1", "title": "HiLO: High-Level Object Fusion for Autonomous Driving using Transformers", "authors": ["Timo Osterburg", "Franz Albers", "Christopher Diehl", "Rajesh Pushparaj", "Torsten Bertram"], "abstract": "The fusion of sensor data is essential for a robust perception of the\nenvironment in autonomous driving. Learning-based fusion approaches mainly use\nfeature-level fusion to achieve high performance, but their complexity and\nhardware requirements limit their applicability in near-production vehicles.\nHigh-level fusion methods offer robustness with lower computational\nrequirements. Traditional methods, such as the Kalman filter, dominate this\narea. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel\ntransformer-based high-level object fusion method called HiLO. Experimental\nresults demonstrate improvements of $25.9$ percentage points in $\\textrm{F}_1$\nscore and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale\nreal-world dataset demonstrates the effectiveness of the proposed approaches.\nTheir generalizability is further validated by cross-domain evaluation between\nurban and highway scenarios. Code, data, and models are available at\nhttps://github.com/rst-tu-dortmund/HiLO .", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "published": "2025-06-03 07:44:35", "updated": "2025-06-03 07:44:35", "pdf_url": "http://arxiv.org/pdf/2506.02554v1", "comment": "6 pages, accepted at IEEE Intelligent Vehicles Symposium (IV) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02561v1", "title": "Pruning General Large Language Models into Customized Expert Models", "authors": ["Yirao Zhao", "Guizhen Chen", "Kenji Kawaguchi", "Lidong Bing", "Wenxuan Zhang"], "abstract": "Large language models (LLMs) have revolutionized natural language processing,\nyet their substantial model sizes often require substantial computational\nresources. To preserve computing resources and accelerate inference speed, it\nis crucial to prune redundant parameters, especially for experienced users who\noften need compact expert models tailored to specific downstream scenarios.\nHowever, most existing pruning methods focus on preserving the model's general\ncapabilities, often requiring extensive post-training or suffering from\ndegraded performance due to coarse-grained pruning. In this work, we design a\n$\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to\nprune a large general model into a smaller lightweight expert model, which is\npositioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying\nand pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates\nexpert models without any post-training. Our experiments demonstrate that\n$\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal\nloss in both expert and general capabilities across various models from\ndifferent model families and sizes.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 07:47:30", "updated": "2025-06-03 07:47:30", "pdf_url": "http://arxiv.org/pdf/2506.02561v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02565v1", "title": "Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine", "authors": ["Zhuoxuan Jiang", "Tianyang Zhang", "Peiyan Peng", "Jing Chen", "Yinong Xun", "Haotian Zhang", "Lichi Li", "Yong Li", "Shaohua Zhang"], "abstract": "Generating high-quality geometry problems is both an important and\nchallenging task in education. Compared to math word problems, geometry\nproblems further emphasize multi-modal formats and the translation between\ninformal and formal languages. In this paper, we introduce a novel task for\ngeometry problem generation and propose a new pipeline method: the Symbolic\nDeduction Engine-based Geometry Problem Generation framework (SDE-GPG). The\nframework leverages a symbolic deduction engine and contains four main steps:\n(1) searching a predefined mapping table from knowledge points to extended\ndefinitions, (2) sampling extended definitions and performing symbolic\ndeduction, (3) filtering out unqualified problems, and (4) generating textual\nproblems and diagrams. Specifically, our method supports to avoid inherent\nbiases in translating natural language into formal language by designing the\nmapping table, and guarantees to control the generated problems in terms of\nknowledge points and difficulties by an elaborate checking function. With\nobtained formal problems, they are translated to natural language and the\naccompanying diagrams are automatically drew by rule-based methods. We conduct\nexperiments using real-world combinations of knowledge points from two public\ndatasets. The results demonstrate that the SDE-GPG can effectively generate\nreadable, solvable and controllable geometry problems.", "categories": ["cs.AI"], "published": "2025-06-03 07:49:38", "updated": "2025-06-03 07:49:38", "pdf_url": "http://arxiv.org/pdf/2506.02565v1", "comment": "To Appear in ACL'25", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02568v1", "title": "MLaGA: Multimodal Large Language and Graph Assistant", "authors": ["Dongzhe Fan", "Yi Fang", "Jiajin Liu", "Djellel Difallah", "Qiaoyu Tan"], "abstract": "Large Language Models (LLMs) have demonstrated substantial efficacy in\nadvancing graph-structured data analysis. Prevailing LLM-based graph methods\nexcel in adapting LLMs to text-rich graphs, wherein node attributes are text\ndescriptions. However, their applications to multimodal graphs--where nodes are\nassociated with diverse attribute types, such as texts and images--remain\nunderexplored, despite their ubiquity in real-world scenarios. To bridge the\ngap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an\ninnovative model that adeptly extends LLM capabilities to facilitate reasoning\nover complex graph structures and multimodal attributes. We first design a\nstructure-aware multimodal encoder to align textual and visual attributes\nwithin a unified space through a joint graph pre-training objective.\nSubsequently, we implement a multimodal instruction-tuning approach to\nseamlessly integrate multimodal features and graph structures into the LLM\nthrough lightweight projectors. Extensive experiments across multiple datasets\ndemonstrate the effectiveness of MLaGA compared to leading baseline methods,\nachieving superior performance in diverse graph learning tasks under both\nsupervised and transfer learning scenarios.", "categories": ["cs.AI"], "published": "2025-06-03 07:52:00", "updated": "2025-06-03 07:52:00", "pdf_url": "http://arxiv.org/pdf/2506.02568v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02572v1", "title": "HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference", "authors": ["Ping Gong", "Jiawei Yi", "Shengnan Wang", "Juncheng Zhang", "Zewen Jin", "Ouxiang Zhou", "Ruibo Liu", "Guanbin Xu", "Youhui Bai", "Bowen Ye", "Kun Yuan", "Tong Yang", "Gong Zhang", "Renhai Chen", "Feng Wu", "Cheng Li"], "abstract": "Large Language Models (LLMs) have emerged as a pivotal research area, yet the\nattention module remains a critical bottleneck in LLM inference, even with\ntechniques like KVCache to mitigate redundant computations. While various\ntop-$k$ attention mechanisms have been proposed to accelerate LLM inference by\nexploiting the inherent sparsity of attention, they often struggled to strike a\nbalance between efficiency and accuracy. In this paper, we introduce HATA\n(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates\nlow-overhead learning-to-hash techniques into the Top-$k$ attention process.\nDifferent from the existing top-k attention methods which are devoted to\nseeking an absolute estimation of qk score, typically with a great cost, HATA\nmaps queries and keys into binary hash codes, and acquires the relative qk\nscore order with a quite low cost, which is sufficient for realizing top-k\nattention. Extensive experiments demonstrate that HATA achieves up to\n7.2$\\times$ speedup compared to vanilla full attention while maintaining model\naccuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention\nmethods in both accuracy and efficiency across multiple mainstream LLM models\nand diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 07:53:32", "updated": "2025-06-03 07:53:32", "pdf_url": "http://arxiv.org/pdf/2506.02572v1", "comment": "ACL 2025 findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02576v1", "title": "ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting", "authors": ["Haichen Wang", "Liu Yang", "Xinyuan Zhang", "Haomin Yu", "Ming Li", "Jilin Hu"], "abstract": "Passenger demand forecasting helps optimize vehicle scheduling, thereby\nimproving urban efficiency. Recently, attention-based methods have been used to\nadequately capture the dynamic nature of spatio-temporal data. However,\nexisting methods that rely on heuristic masking strategies cannot fully adapt\nto the complex spatio-temporal correlations, hindering the model from focusing\non the right context. These works also overlook the high-level correlations\nthat exist in the real world. Effectively integrating these high-level\ncorrelations with the original correlations is crucial. To fill this gap, we\npropose the Aggregation Differential Transformer (ADFormer), which offers new\ninsights to demand forecasting promotion. Specifically, we utilize Differential\nAttention to capture the original spatial correlations and achieve attention\ndenoising. Meanwhile, we design distinct aggregation strategies based on the\nnature of space and time. Then, the original correlations are unified with the\nhigh-level correlations, enabling the model to capture holistic spatio-temporal\nrelations. Experiments conducted on taxi and bike datasets confirm the\neffectiveness and efficiency of our model, demonstrating its practical value.\nThe code is available at https://github.com/decisionintelligence/ADFormer.", "categories": ["cs.AI"], "published": "2025-06-03 07:55:51", "updated": "2025-06-03 07:55:51", "pdf_url": "http://arxiv.org/pdf/2506.02576v1", "comment": "9 pages, 5 figures, 3 tables. IJCAI-2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02580v1", "title": "V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving", "authors": ["Xuewen Luo", "Fengze Yang", "Fan Ding", "Xiangbo Gao", "Shuo Xing", "Yang Zhou", "Zhengzhong Tu", "Chenxi Liu"], "abstract": "Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning\ncapabilities, but face two critical challenges: limited perception due to the\nshort-sightedness of single-vehicle sensors, and hallucination arising from the\nlack of real-time environmental grounding. To address these issues, this paper\nintroduces V2X-UniPool, a unified framework that integrates multimodal\nVehicle-to-Everything (V2X) data into a time-indexed and language-based\nknowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)\nmechanism, which enables retrieval of both static and dynamic knowledge, our\nsystem enables ADs to perform accurate, temporally consistent reasoning over\nboth static environment and dynamic traffic context. Experiments on a\nreal-world cooperative driving dataset demonstrate that V2X-UniPool\nsignificantly enhances motion planning accuracy and reasoning capability.\nRemarkably, it enables even zero-shot vehicle-side models to achieve\nstate-of-the-art performance by leveraging V2X-UniPool, while simultaneously\nreducing transmission cost by over 99.9\\% compared to prior V2X methods.", "categories": ["cs.AI"], "published": "2025-06-03 08:00:57", "updated": "2025-06-03 08:00:57", "pdf_url": "http://arxiv.org/pdf/2506.02580v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02584v1", "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning", "authors": ["Sarenne Wallbridge", "Christoph Minixhofer", "Catherine Lai", "Peter Bell"], "abstract": "People exploit the predictability of lexical structures during text\ncomprehension. Though predictable structure is also present in speech, the\ndegree to which prosody, e.g. intonation, tempo, and loudness, contributes to\nsuch structure independently of the lexical content is unclear. This study\nleverages self-supervised learning (SSL) to examine the temporal granularity of\nstructures in the acoustic correlates of prosody. Representations from our\nproposed Masked Prosody Model can predict perceptual labels dependent on local\ninformation, such as word boundaries, but provide the most value for labels\ninvolving longer-term structures, like emotion recognition. Probing experiments\nacross various perceptual labels show strong relative gains over untransformed\npitch, energy, and voice activity features. Our results reveal the importance\nof SSL training objective timescale and highlight the value of complex\nSSL-encoded structures compared to more constrained classical structures.", "categories": ["cs.CL", "cs.AI", "eess.AS"], "published": "2025-06-03 08:04:03", "updated": "2025-06-03 08:04:03", "pdf_url": "http://arxiv.org/pdf/2506.02584v1", "comment": "Accepted at INTERSPEECH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02589v1", "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM", "authors": ["Maria Levchenko"], "abstract": "This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements.", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; H.3.3"], "published": "2025-06-03 08:11:16", "updated": "2025-06-03 08:11:16", "pdf_url": "http://arxiv.org/pdf/2506.02589v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02594v1", "title": "EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization", "authors": ["Ruibo Duan", "Yuxin Liu", "Xinyao Dong", "Chenglin Fan"], "abstract": "Generating challenging instances is crucial for the evaluation and\nadvancement of combinatorial optimization solvers. In this work, we introduce\nEALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),\na novel framework that automates the co-evolution of optimization problem\ninstances and their corresponding heuristic solvers using large language models\n(LLMs). EALG leverages a mutation-based adversarial approach that dynamically\nevolves instance generation procedures to create increasingly difficult\nproblems, while simultaneously synthesizing adaptive heuristic algorithms\nthrough interactions with LLMs guided by algorithmic structure. Unlike existing\napproaches that focus solely on static benchmark creation or manual solver\ndesign, EALG provides a seamless pipeline from instance generation to solver\nsynthesis. Experimental results demonstrate that EALG generates significantly\nharder instances than current benchmarks, and its synthesized solvers\ngeneralize effectively across a broad spectrum of combinatorial tasks. This\nwork explores a new paradigm for combinatorial optimization that integrates\ninstance generation with solver design, resulting in state-of-the-art\nperformance.", "categories": ["cs.AI"], "published": "2025-06-03 08:13:41", "updated": "2025-06-03 08:13:41", "pdf_url": "http://arxiv.org/pdf/2506.02594v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02596v1", "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing", "authors": ["Fan Gao", "Dongyuan Li", "Ding Xia", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Baojun Wang"], "abstract": "Chinese essay writing and its evaluation are critical in educational\ncontexts, yet the capabilities of Large Language Models (LLMs) in this domain\nremain largely underexplored. Existing benchmarks often rely on coarse-grained\ntext quality metrics, largely overlooking the structural and rhetorical\ncomplexities of Chinese essays, particularly across diverse genres. To address\nthis gap, we propose \\benchName, a multi-genre benchmark specifically designed\nfor Chinese essay writing across four major genres: Argumentative, Narrative,\nDescriptive, and Expository. We curate and refine a total of 728 real-world\nprompts to ensure authenticity and meticulously categorize them into the\n\\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing\nscenarios. To reliably evaluate generated essays, we develop a fine-grained,\ngenre-specific scoring framework that hierarchically aggregates scores. We\nfurther validate our evaluation protocol through a comprehensive human\nagreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their\nstrengths and limitations across genres and instruction types. With \\benchName,\nwe aim to advance LLM-based Chinese essay evaluation and inspire future\nresearch on improving essay generation in educational settings.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 08:14:46", "updated": "2025-06-03 08:14:46", "pdf_url": "http://arxiv.org/pdf/2506.02596v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02606v1", "title": "Multi Layered Autonomy and AI Ecologies in Robotic Art Installations", "authors": ["Baoyang Chen", "Xian Xu", "Huamin Qu"], "abstract": "Symbiosis of Agents is a large-scale installation by Baoyang Chen that embeds\nAI-driven robots in an immersive, mirror-lined arena, probing the tension\nbetween machine agency and artistic authorship. Drawing on early cybernetics,\nrule-based conceptual art, and seminal robotic works, it orchestrates fluid\nexchanges among robotic arms, quadruped machines, their environment, and the\npublic. A three tier faith system pilots the ecology: micro-level adaptive\ntactics, meso-level narrative drives, and a macro-level prime directive. This\nhierarchy lets behaviors evolve organically in response to environmental cues\nand even a viewer's breath, turning spectators into co-authors of the unfolding\ndrama.Framed by a speculative terraforming scenario that recalls the historical\nexploitation of marginalized labor, the piece asks who bears responsibility in\nAI-mediated futures. Choreographed motion, AI-generated scripts, reactive\nlighting, and drifting fog cast the robots as collaborators rather than tools,\nforging a living, emergent artwork. Exhibited internationally, Symbiosis of\nAgents shows how cybernetic feedback, robotic experimentation, and conceptual\nrule-making can converge to redefine agency, authorship, and ethics in\ncontemporary art.", "categories": ["cs.RO", "cs.AI"], "published": "2025-06-03 08:28:19", "updated": "2025-06-03 08:28:19", "pdf_url": "http://arxiv.org/pdf/2506.02606v1", "comment": null, "doi": "10.48550/arXiv.2410.22462", "journal_ref": null}
{"arxiv_id": "2506.02609v1", "title": "A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting", "authors": ["Tianfan Jiang", "Mei Wu", "Wenchao Weng", "Dewen Seng", "Yiqian Lin"], "abstract": "In recent years, traffic flow prediction has become a highlight in the field\nof intelligent transportation systems. However, due to the temporal variations\nand dynamic spatial correlations of traffic data, traffic prediction remains\nhighly challenging.Traditional spatiotemporal networks, which rely on\nend-to-end training, often struggle to handle the diverse data dependencies of\nmultiple traffic flow patterns. Additionally, traffic flow variations are\nhighly sensitive to temporal information changes. Regrettably, other\nresearchers have not sufficiently recognized the importance of temporal\ninformation.To address these challenges, we propose a novel approach called A\nTime-Enhanced Data Disentanglement Network for Traffic Flow Forecasting\n(TEDDN). This network disentangles the originally complex and intertwined\ntraffic data into stable patterns and trends. By flexibly learning temporal and\nnode information through a dynamic graph enhanced by a temporal feature\nextraction module, TEDDN demonstrates significant efficacy in disentangling and\nextracting complex traffic information. Experimental evaluations and ablation\nstudies on four real-world datasets validate the superiority of our method.", "categories": ["cs.AI"], "published": "2025-06-03 08:28:48", "updated": "2025-06-03 08:28:48", "pdf_url": "http://arxiv.org/pdf/2506.02609v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02610v1", "title": "Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm", "authors": ["Zhaoyang Li", "Jie Wang", "XiaoXiao Li", "Wangjie Li", "Longjie Luo", "Lin Li", "Qingyang Hong"], "abstract": "In speaker diarization, traditional clustering-based methods remain widely\nused in real-world applications. However, these methods struggle with the\ncomplex distribution of speaker embeddings and overlapping speech segments. To\naddress these limitations, we propose an Overlapping Community Detection method\nbased on Graph Attention networks and the Label Propagation Algorithm\n(OCDGALP). The proposed framework comprises two key components: (1) a graph\nattention network that refines speaker embeddings and node connections by\naggregating information from neighboring nodes, and (2) a label propagation\nalgorithm that assigns multiple community labels to each node, enabling\nsimultaneous clustering and overlapping community detection. Experimental\nresults show that the proposed method significantly reduces the Diarization\nError Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III\ndataset without oracle Voice Activity Detection (VAD), and an impressive 11.07%\nwith oracle VAD.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-06-03 08:29:10", "updated": "2025-06-03 08:29:10", "pdf_url": "http://arxiv.org/pdf/2506.02610v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02612v1", "title": "Simple, Good, Fast: Self-Supervised World Models Free of Baggage", "authors": ["Jan Robine", "Marc H\u00f6ftmann", "Stefan Harmeling"], "abstract": "What are the essential components of world models? How far do we get with\nworld models that are not employing RNNs, transformers, discrete\nrepresentations, and image reconstructions? This paper introduces SGF, a\nSimple, Good, and Fast world model that uses self-supervised representation\nlearning, captures short-time dependencies through frame and action stacking,\nand enhances robustness against model errors through data augmentation. We\nextensively discuss SGF's connections to established world models, evaluate the\nbuilding blocks in ablation studies, and demonstrate good performance through\nquantitative comparisons on the Atari 100k benchmark.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-06-03 08:29:32", "updated": "2025-06-03 08:29:32", "pdf_url": "http://arxiv.org/pdf/2506.02612v1", "comment": "Published as a conference paper at ICLR 2025. Code is available at\n  https://github.com/jrobine/sgf", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02614v1", "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset", "authors": ["Guohang Zhuang", "Weixi Song", "Jinyang Huang", "Chenwei Yang", "Yan Lu"], "abstract": "With the rapid development of space exploration, space debris has attracted\nmore attention due to its potential extreme threat, leading to the need for\nreal-time and accurate debris tracking. However, existing methods are mainly\nbased on traditional signal processing, which cannot effectively process the\ncomplex background and dense space debris. In this paper, we propose a deep\nlearning-based Space Debris Tracking Network~(SDT-Net) to achieve highly\naccurate debris tracking. SDT-Net effectively represents the feature of debris,\nenhancing the efficiency and stability of end-to-end model learning. To train\nand evaluate this model effectively, we also produce a large-scale dataset\nSpace Debris Tracking Dataset (SDTD) by a novel observation-based data\nsimulation scheme. SDTD contains 18,040 video sequences with a total of 62,562\nframes and covers 250,000 synthetic space debris. Extensive experiments\nvalidate the effectiveness of our model and the challenging of our dataset.\nFurthermore, we test our model on real data from the Antarctic Station,\nachieving a MOTA score of 70.6%, which demonstrates its strong transferability\nto real-world scenarios. Our dataset and code will be released soon.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 08:30:25", "updated": "2025-06-03 08:30:25", "pdf_url": "http://arxiv.org/pdf/2506.02614v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02615v1", "title": "Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models", "authors": ["Safaa Abdullahi Moallim Mohamud", "Minjin Baek", "Dong Seog Han"], "abstract": "In this paper, we present a hierarchical question-answering (QA) approach for\nscene understanding in autonomous vehicles, balancing cost-efficiency with\ndetailed visual interpretation. The method fine-tunes a compact vision-language\nmodel (VLM) on a custom dataset specific to the geographical area in which the\nvehicle operates to capture key driving-related visual elements. At the\ninference stage, the hierarchical QA strategy decomposes the scene\nunderstanding task into high-level and detailed sub-questions. Instead of\ngenerating lengthy descriptions, the VLM navigates a structured question tree,\nwhere answering high-level questions (e.g., \"Is it possible for the ego vehicle\nto turn left at the intersection?\") triggers more detailed sub-questions (e.g.,\n\"Is there a vehicle approaching the intersection from the opposite\ndirection?\"). To optimize inference time, questions are dynamically skipped\nbased on previous answers, minimizing computational overhead. The extracted\nanswers are then synthesized using handcrafted templates to ensure coherent,\ncontextually accurate scene descriptions. We evaluate the proposed approach on\nthe custom dataset using GPT reference-free scoring, demonstrating its\ncompetitiveness with state-of-the-art methods like GPT-4o in capturing key\nscene details while achieving significantly lower inference time. Moreover,\nqualitative results from real-time deployment highlight the proposed approach's\ncapacity to capture key driving elements with minimal latency.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 08:32:43", "updated": "2025-06-03 08:32:43", "pdf_url": "http://arxiv.org/pdf/2506.02615v1", "comment": "This work has been submitted to the IEEE for possible publication", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02619v1", "title": "HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport", "authors": ["Yanbei Liu", "Chongxu Wang", "Zhitao Xiao", "Lei Geng", "Yanwei Pang", "Xiao Wang"], "abstract": "Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent\ncapabilities in processing heterogeneous information networks. Self-supervised\nlearning on heterogeneous graphs, especially contrastive self-supervised\nstrategy, shows great potential when there are no labels. However, this\napproach requires the use of carefully designed graph augmentation strategies\nand the selection of positive and negative samples. Determining the exact level\nof similarity between sample pairs is non-trivial.To solve this problem, we\npropose a novel self-supervised Heterogeneous graph neural network with Optimal\nTransport (HGOT) method which is designed to facilitate self-supervised\nlearning for heterogeneous graphs without graph augmentation strategies.\nDifferent from traditional contrastive self-supervised learning, HGOT employs\nthe optimal transport mechanism to relieve the laborious sampling process of\npositive and negative samples. Specifically, we design an aggregating view\n(central view) to integrate the semantic information contained in the views\nrepresented by different meta-paths (branch views). Then, we introduce an\noptimal transport plan to identify the transport relationship between the\nsemantics contained in the branch view and the central view. This allows the\noptimal transport plan between graphs to align with the representations,\nforcing the encoder to learn node representations that are more similar to the\ngraph space and of higher quality. Extensive experiments on four real-world\ndatasets demonstrate that our proposed HGOT model can achieve state-of-the-art\nperformance on various downstream tasks. In particular, in the node\nclassification task, HGOT achieves an average of more than 6% improvement in\naccuracy compared with state-of-the-art methods.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 08:35:29", "updated": "2025-06-03 08:35:29", "pdf_url": "http://arxiv.org/pdf/2506.02619v1", "comment": "The paper has 9 pages of text and 13 pages in total (including\n  acknowledgments, impact statement, references, and appendix), with 6 figures\n  and 2 tables. This paper has been accepted by ICML 2025 conference and this\n  is a final version of the manuscript submitted to the conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02623v1", "title": "SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search", "authors": ["Yuyang Zhou", "Ferrante Neri", "Yew-Soon Ong", "Ruibin Bai"], "abstract": "Modern neural architecture search (NAS) is inherently multi-objective,\nbalancing trade-offs such as accuracy, parameter count, and computational cost.\nThis complexity makes NAS computationally expensive and nearly impossible to\nsolve without efficient approximations. To address this, we propose a novel\nsurrogate modelling approach that leverages an ensemble of Siamese network\nblocks to predict dominance relationships between candidate architectures.\nLightweight and easy to train, the surrogate achieves 92% accuracy and replaces\nthe crowding distance calculation in the survivor selection strategy with a\nheuristic rule based on model size. Integrated into a framework termed SiamNAS,\nthis design eliminates costly evaluations during the search process.\nExperiments on NAS-Bench-201 demonstrate the framework's ability to identify\nPareto-optimal solutions with significantly reduced computational costs. The\nproposed SiamNAS identified a final non-dominated set containing the best\narchitecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in\nterms of test error rate, within 0.01 GPU days. This proof-of-concept study\nhighlights the potential of the proposed Siamese network surrogate model to\ngeneralise to multi-tasking optimisation, enabling simultaneous optimisation\nacross tasks. Additionally, it offers opportunities to extend the approach for\ngenerating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal\nsolutions for heterogeneous task settings.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-06-03 08:39:42", "updated": "2025-06-03 08:39:42", "pdf_url": "http://arxiv.org/pdf/2506.02623v1", "comment": "Genetic and Evolutionary Computation Conference (GECCO' 25)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02634v1", "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider", "authors": ["Jiahao Wang", "Jinbo Han", "Xingda Wei", "Sijie Shen", "Dingyan Zhang", "Chenguang Fang", "Rong Chen", "Wenyuan Yu", "Haibo Chen"], "abstract": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.", "categories": ["cs.DC", "cs.AI"], "published": "2025-06-03 08:51:38", "updated": "2025-06-03 08:51:38", "pdf_url": "http://arxiv.org/pdf/2506.02634v1", "comment": "Accepted by USENIX ATC'25", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02648v1", "title": "Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation", "authors": ["Yue Yang", "MingKang Chen", "Qihua Liu", "Mengkang Hu", "Qiguang Chen", "Gengrui Zhang", "Shuyue Hu", "Guangtao Zhai", "Yu Qiao", "Yu Wang", "Wenqi Shao", "Ping Luo"], "abstract": "Recent advances in large language models (LLMs) have demonstrated impressive\nreasoning capacities that mirror human-like thinking. However, whether LLMs\npossess genuine fluid intelligence (i.e., the ability to reason abstractly and\ngeneralize rules in novel situations) remains an open question. Existing\nreasoning benchmarks either focus on domain-specific knowledge (crystallized\nintelligence) or lack interpretability. To address these limitations, we\npropose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a\nhierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning\ntasks organized across four cognitive levels, with each task featuring multiple\ndynamic variants that test the same underlying latent rule. This design enables\nfine-grained, interpretable, and reliable assessments of fluid intelligence. We\nevaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,\nClaude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).\nExperimental results reveal that although most LLMs achieve competent and\nrobust performance in low-level cognition, they struggle with high-level\ncognition and exhibit limited generalization as task complexity grows. Our\nfindings highlight the gap between current LLMs and true human-like fluid\nintelligence and offer a new path for systematically tracking reasoning\nprogress in LLMs.", "categories": ["cs.AI"], "published": "2025-06-03 09:01:08", "updated": "2025-06-03 09:01:08", "pdf_url": "http://arxiv.org/pdf/2506.02648v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02649v1", "title": "From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida", "Zhu Han"], "abstract": "A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness\nin emergency response. Its agility and ability to optimize mobility and\nestablish Line-of-Sight (LoS) communication make it increasingly vital for\nmanaging emergencies such as disaster response, search and rescue, and wildfire\nmonitoring. While Deep Reinforcement Learning (DRL) has been applied to\noptimize UAV navigation and control, its high training complexity, low sample\nefficiency, and simulation-to-reality gap limit its practicality in public\nsafety. Recent advances in Large Language Models (LLMs) offer a compelling\nalternative. With strong reasoning and generalization capabilities, LLMs can\nadapt to new tasks through In-Context Learning (ICL), which enables task\nadaptation via natural language prompts and example-based guidance, without\nretraining. Deploying LLMs at the network edge, rather than in the cloud,\nfurther reduces latency and preserves data privacy, thereby making them\nsuitable for real-time, mission-critical public safety UAVs. This paper\nproposes the integration of LLM-enabled ICL with public safety UAV to address\nthe key functions, such as path planning and velocity control, in the context\nof emergency response. We present a case study on data collection scheduling\nwhere the LLM-enabled ICL framework can significantly reduce packet loss\ncompared to conventional approaches, while also mitigating potential\njailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify\nfuture research directions. The ICL framework enables adaptive, context-aware\ndecision-making for public safety UAV, thus offering a lightweight and\nefficient solution for enhancing UAV autonomy and responsiveness in\nemergencies.", "categories": ["cs.AI", "53-01", "C.2"], "published": "2025-06-03 09:01:33", "updated": "2025-06-03 09:01:33", "pdf_url": "http://arxiv.org/pdf/2506.02649v1", "comment": "8 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02654v1", "title": "A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction", "authors": ["Shiyu Shen", "Bin Pan", "Guirong Xue"], "abstract": "City-scale traffic volume prediction plays a pivotal role in intelligent\ntransportation systems, yet remains a challenge due to the inherent\nincompleteness and bias in observational data. Although deep learning-based\nmethods have shown considerable promise, most existing approaches produce\ndeterministic point estimates, thereby neglecting the uncertainty arising from\nunobserved traffic flows. Furthermore, current models are typically trained in\na city-specific manner, which hinders their generalizability and limits\nscalability across diverse urban contexts. To overcome these limitations, we\nintroduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model\ntraffic volume as a distributional aggregation of trajectories. Our framework\nfuses heterogeneous data sources-including real-time observations, historical\ntrajectory data, and road network topology-enabling robust and\nuncertainty-aware traffic inference. TrafficPPT is initially pretrained on\nlarge-scale simulated data spanning multiple urban scenarios, and later\nfine-tuned on target cities to ensure effective domain adaptation. Experiments\non real-world datasets show that TrafficPPT consistently surpasses\nstate-of-the-art baselines, particularly under conditions of extreme data\nsparsity. Code will be open.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 09:07:29", "updated": "2025-06-03 09:07:29", "pdf_url": "http://arxiv.org/pdf/2506.02654v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02668v1", "title": "FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems", "authors": ["Frederico Metelo", "Alexandre Oliveira", "Stevo Rackovi\u0107", "Pedro \u00c1kos Costa", "Cl\u00e1udia Soares"], "abstract": "Edge computing addresses the growing data demands of connected-device\nnetworks by placing computational resources closer to end users through\ndecentralized infrastructures. This decentralization challenges traditional,\nfully centralized orchestration, which suffers from latency and resource\nbottlenecks. We present \\textbf{FAuNO} -- \\emph{Federated Asynchronous Network\nOrchestrator} -- a buffered, asynchronous \\emph{federated\nreinforcement-learning} (FRL) framework for decentralized task offloading in\nedge systems. FAuNO adopts an actor-critic architecture in which local actors\nlearn node-specific dynamics and peer interactions, while a federated critic\naggregates experience across agents to encourage efficient cooperation and\nimprove overall system performance. Experiments in the \\emph{PeersimGym}\nenvironment show that FAuNO consistently matches or exceeds heuristic and\nfederated multi-agent RL baselines in reducing task loss and latency,\nunderscoring its adaptability to dynamic edge-computing scenarios.", "categories": ["cs.AI", "cs.LG"], "published": "2025-06-03 09:15:03", "updated": "2025-06-03 09:15:03", "pdf_url": "http://arxiv.org/pdf/2506.02668v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02672v1", "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "authors": ["Shihan Dou", "Ming Zhang", "Chenhao Huang", "Jiayi Chen", "Feng Chen", "Shichun Liu", "Yan Liu", "Chenxiao Liu", "Cheng Zhong", "Zongzhang Zhang", "Tao Gui", "Chao Xin", "Wei Chengzhi", "Lin Yan", "Qi Zhang", "Xuanjing Huang"], "abstract": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 09:18:33", "updated": "2025-06-03 09:18:33", "pdf_url": "http://arxiv.org/pdf/2506.02672v1", "comment": "47 pages, 24 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02677v1", "title": "Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation", "authors": ["Jintao Tong", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "abstract": "Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a\nsource-domain dataset to unseen target-domain datasets with limited\nannotations. Current methods typically compare the distance between training\nand testing samples for mask prediction. However, we find an entanglement\nproblem exists in this widely adopted method, which tends to bind sourcedomain\npatterns together and make each of them hard to transfer. In this paper, we aim\nto address this problem for the CD-FSS task. We first find a natural\ndecomposition of the ViT structure, based on which we delve into the\nentanglement problem for an interpretation. We find the decomposed ViT\ncomponents are crossly compared between images in distance calculation, where\nthe rational comparisons are entangled with those meaningless ones by their\nequal importance, leading to the entanglement problem. Based on this\ninterpretation, we further propose to address the entanglement problem by\nlearning to weigh for all comparisons of ViT components, which learn\ndisentangled features and re-compose them for the CD-FSS task, benefiting both\nthe generalization and finetuning. Experiments show that our model outperforms\nthe state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under\n1-shot and 5-shot settings, respectively.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-06-03 09:23:20", "updated": "2025-06-03 09:23:20", "pdf_url": "http://arxiv.org/pdf/2506.02677v1", "comment": "Accepted by ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02694v1", "title": "XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation", "authors": ["Daichi Kimura", "Tomonori Izumitani", "Hisashi Kashima"], "abstract": "Various Transformer-based models have been proposed for time series\nforecasting. These models leverage the self-attention mechanism to capture\nlong-term temporal or variate dependencies in sequences. Existing methods can\nbe divided into two approaches: (1) reducing computational cost of attention by\nmaking the calculations sparse, and (2) reshaping the input data to aggregate\ntemporal features. However, existing attention mechanisms may not adequately\ncapture inherent nonlinear dependencies present in time series data, leaving\nroom for improvement. In this study, we propose a novel attention mechanism\nbased on Chatterjee's rank correlation coefficient, which measures nonlinear\ndependencies between variables. Specifically, we replace the matrix\nmultiplication in standard attention mechanisms with this rank coefficient to\nmeasure the query-key relationship. Since computing Chatterjee's correlation\ncoefficient involves sorting and ranking operations, we introduce a\ndifferentiable approximation employing SoftSort and SoftRank. Our proposed\nmechanism, ``XicorAttention,'' integrates it into several state-of-the-art\nTransformer models. Experimental results on real-world datasets demonstrate\nthat incorporating nonlinear correlation into the attention improves\nforecasting accuracy by up to approximately 9.1\\% compared to existing models.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 09:43:45", "updated": "2025-06-03 09:43:45", "pdf_url": "http://arxiv.org/pdf/2506.02694v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02696v1", "title": "Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations", "authors": ["Jinyuan Luo", "Zhen Fang", "Yixuan Li", "Seongheon Park", "Ling Chen"], "abstract": "Hallucination remains a key obstacle to the reliable deployment of large\nlanguage models (LLMs) in real-world question answering tasks. A widely adopted\nstrategy to detect hallucination, known as self-assessment, relies on the\nmodel's own output confidence to estimate the factual accuracy of its answers.\nHowever, this strategy assumes that the model's output distribution closely\nreflects the true data distribution, which may not always hold in practice. As\nbias accumulates through the model's layers, the final output can diverge from\nthe underlying reasoning process, making output-level confidence an unreliable\nsignal for hallucination detection. In this work, we propose Sample-Specific\nPrompting (SSP), a new framework that improves self-assessment by analyzing\nperturbation sensitivity at intermediate representations. These\nrepresentations, being less influenced by model bias, offer a more faithful\nview of the model's latent reasoning process. Specifically, SSP dynamically\ngenerates noise prompts for each input and employs a lightweight encoder to\namplify the changes in representations caused by the perturbation. A\ncontrastive distance metric is then used to quantify these differences and\nseparate truthful from hallucinated responses. By leveraging the dynamic\nbehavior of intermediate representations under perturbation, SSP enables more\nreliable self-assessment. Extensive experiments demonstrate that SSP\nsignificantly outperforms prior methods across a range of hallucination\ndetection benchmarks.", "categories": ["cs.AI"], "published": "2025-06-03 09:44:28", "updated": "2025-06-03 09:44:28", "pdf_url": "http://arxiv.org/pdf/2506.02696v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02703v1", "title": "Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies", "authors": ["Khizar Hayat", "Baptiste Magnier"], "abstract": "This study critically examines the methodological rigor in credit card fraud\ndetection research, revealing how fundamental evaluation flaws can overshadow\nalgorithmic sophistication. Through deliberate experimentation with improper\nevaluation protocols, we demonstrate that even simple models can achieve\ndeceptively impressive results when basic methodological principles are\nviolated. Our analysis identifies four critical issues plaguing current\napproaches: (1) pervasive data leakage from improper preprocessing sequences,\n(2) intentional vagueness in methodological reporting, (3) inadequate temporal\nvalidation for transaction data, and (4) metric manipulation through recall\noptimization at precision's expense. We present a case study showing how a\nminimal neural network architecture with data leakage outperforms many\nsophisticated methods reported in literature, achieving 99.9\\% recall despite\nfundamental evaluation flaws. These findings underscore that proper evaluation\nmethodology matters more than model complexity in fraud detection research. The\nstudy serves as a cautionary example of how methodological rigor must precede\narchitectural sophistication, with implications for improving research\npractices across machine learning applications.", "categories": ["cs.LG", "cs.AI", "cs.CY"], "published": "2025-06-03 09:56:43", "updated": "2025-06-03 09:56:43", "pdf_url": "http://arxiv.org/pdf/2506.02703v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02713v1", "title": "Open-Set Living Need Prediction with Large Language Models", "authors": ["Xiaochong Lan", "Jie Feng", "Yizhou Sun", "Chen Gao", "Jiahuan Lei", "Xinlei Shi", "Hengliang Luo", "Yong Li"], "abstract": "Living needs are the needs people generate in their daily lives for survival\nand well-being. On life service platforms like Meituan, user purchases are\ndriven by living needs, making accurate living need predictions crucial for\npersonalized service recommendations. Traditional approaches treat this\nprediction as a closed-set classification problem, severely limiting their\nability to capture the diversity and complexity of living needs. In this work,\nwe redefine living need prediction as an open-set classification problem and\npropose PIGEON, a novel system leveraging large language models (LLMs) for\nunrestricted need prediction. PIGEON first employs a behavior-aware record\nretriever to help LLMs understand user preferences, then incorporates Maslow's\nhierarchy of needs to align predictions with human living needs. For evaluation\nand application, we design a recall module based on a fine-tuned text embedding\nmodel that links flexible need descriptions to appropriate life services.\nExtensive experiments on real-world datasets demonstrate that PIGEON\nsignificantly outperforms closed-set approaches on need-based life service\nrecall by an average of 19.37%. Human evaluation validates the reasonableness\nand specificity of our predictions. Additionally, we employ instruction tuning\nto enable smaller LLMs to achieve competitive performance, supporting practical\ndeployment.", "categories": ["cs.AI"], "published": "2025-06-03 10:10:19", "updated": "2025-06-03 10:10:19", "pdf_url": "http://arxiv.org/pdf/2506.02713v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02718v1", "title": "Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems", "authors": ["Guanzhong Chen", "Shaoxiong Yang", "Chao Li", "Wei Liu", "Jian Luan", "Zenglin Xu"], "abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse\nnatural language processing tasks, yet their deployment in real-world\napplications is hindered by fixed knowledge cutoffs and difficulties in\ngenerating controllable, accurate outputs in a single inference. Multi-agent\nsystems (MAS) built from specialized LLM agents offer a promising solution,\nenabling dynamic collaboration and iterative reasoning. However, optimizing\nthese systems remains a challenge, as conventional methods such as prompt\nengineering and supervised fine-tuning entail high engineering overhead and\nlimited adaptability. Reinforcement learning (RL), particularly multi-agent\nreinforcement learning (MARL), provides a scalable framework by refining agent\npolicies based on system-level feedback. Nevertheless, existing MARL\nalgorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on\nCritic networks, which can cause training instability and increase\ncomputational burden. To address these limitations and target the prototypical\nMulti-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group\nPolicy Optimization (MHGPO), a novel Critic-free algorithm that guides policy\nupdates by estimating relative reward advantages across heterogeneous groups of\nrollouts. MHGPO eliminates the need for Critic networks, enhancing stability\nand reducing computational overhead. Additionally, we introduce three group\nrollout sampling strategies that trade off between efficiency and\neffectiveness. Experiments on a multi-agent LLM-based search system demonstrate\nthat MHGPO consistently outperforms MAPPO in both task performance and\ncomputational efficiency, without requiring warm-up, underscoring its potential\nfor stable and scalable optimization of complex LLM-based MAS.", "categories": ["cs.LG", "cs.AI", "I.2.7"], "published": "2025-06-03 10:17:19", "updated": "2025-06-03 10:17:19", "pdf_url": "http://arxiv.org/pdf/2506.02718v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02720v1", "title": "Benchmarking and Advancing Large Language Models for Local Life Services", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "abstract": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-03 10:18:19", "updated": "2025-06-03 10:18:19", "pdf_url": "http://arxiv.org/pdf/2506.02720v1", "comment": "KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02726v1", "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "authors": ["Qihang Yan", "Xinyu Zhang", "Luming Guo", "Qi Zhang", "Feifan Liu"], "abstract": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; H.3.3"], "published": "2025-06-03 10:36:38", "updated": "2025-06-03 10:36:38", "pdf_url": "http://arxiv.org/pdf/2506.02726v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02733v1", "title": "LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering", "authors": ["Xiaoyi Feng", "Kaifeng Zou", "Caichun Cen", "Tao Huang", "Hui Guo", "Zizhou Huang", "Yingli Zhao", "Mingqing Zhang", "Diwei Wang", "Yuntao Zou", "Dagang Li"], "abstract": "Existing optical flow datasets focus primarily on real-world simulation or\nsynthetic human motion, but few are tailored to Celluloid(cel) anime character\nmotion: a domain with unique visual and motion characteristics. To bridge this\ngap and facilitate research in optical flow estimation and downstream tasks\nsuch as anime video generation and line drawing colorization, we introduce\nLinkTo-Anime, the first high-quality dataset specifically designed for cel\nanime character motion generated with 3D model rendering. LinkTo-Anime provides\nrich annotations including forward and backward optical flow, occlusion masks,\nand Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230\ntraining frames, 720 validation frames, and 4,320 test frames. Furthermore, a\ncomprehensive benchmark is constructed with various optical flow estimation\nmethods to analyze the shortcomings and limitations across multiple datasets.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 10:50:20", "updated": "2025-06-03 10:50:20", "pdf_url": "http://arxiv.org/pdf/2506.02733v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02739v1", "title": "Why do AI agents communicate in human language?", "authors": ["Pengcheng Zhou", "Yinglun Feng", "Halimulati Julaiti", "Zhongliang Yang"], "abstract": "Large Language Models (LLMs) have become foundational to modern AI agent\nsystems, enabling autonomous agents to reason and plan. In most existing\nsystems, inter-agent communication relies primarily on natural language. While\nthis design supports interpretability and human oversight, we argue that it\nintroduces fundamental limitations in agent-to-agent coordination. The semantic\nspace of natural language is structurally misaligned with the high-dimensional\nvector spaces in which LLMs operate, resulting in information loss and\nbehavioral drift. Beyond surface-level inefficiencies, we highlight a deeper\narchitectural limitation: current LLMs were not trained with the objective of\nsupporting agentic behavior. As such, they lack mechanisms for modeling role\ncontinuity, task boundaries, and multi-agent dependencies. The standard\nnext-token prediction paradigm fails to support the structural alignment\nrequired for robust, scalable agent coordination. Based on this, we argue that\ntwo core questions deserve careful examination: first, given that AI agents\nfundamentally operate in high-dimensional vector spaces, should they rely on a\nlanguage system originally designed for human cognition as their communication\nmedium? Second, should we consider developing a new model construction paradigm\nthat builds models from the ground up to natively support structured\ncommunication, shared intentionality, and task alignment in multi-role,\nmulti-agent environments? This paper calls for a reconsideration not only of\nhow agents should communicate, but also of what it fundamentally means to train\na model that natively supports multi-agent coordination and communication.", "categories": ["cs.AI"], "published": "2025-06-03 10:53:29", "updated": "2025-06-03 10:53:29", "pdf_url": "http://arxiv.org/pdf/2506.02739v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02742v1", "title": "Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions", "authors": ["Xiaoxue Gao", "Huayun Zhang", "Nancy F. Chen"], "abstract": "Existing expressive text-to-speech (TTS) systems primarily model a limited\nset of categorical emotions, whereas human conversations extend far beyond\nthese predefined emotions, making it essential to explore more diverse\nemotional speech generation for more natural interactions. To bridge this gap,\nthis paper proposes a novel prompt-unseen-emotion (PUE) approach to generate\nunseen emotional speech via emotion-guided prompt learning. PUE is trained\nutilizing an LLM-TTS architecture to ensure emotional consistency between\ncategorical emotion-relevant prompts and emotional speech, allowing the model\nto quantitatively capture different emotion weightings per utterance. During\ninference, mixed emotional speech can be generated by flexibly adjusting\nemotion proportions and leveraging LLM contextual knowledge, enabling the model\nto quantify different emotional styles. Our proposed PUE successfully\nfacilitates expressive speech synthesis of unseen emotions in a zero-shot\nsetting.", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "published": "2025-06-03 10:59:22", "updated": "2025-06-03 10:59:22", "pdf_url": "http://arxiv.org/pdf/2506.02742v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02744v1", "title": "Enriching Location Representation with Detailed Semantic Information", "authors": ["Junyuan Liu", "Xinglei Wang", "Tao Cheng"], "abstract": "Spatial representations that capture both structural and semantic\ncharacteristics of urban environments are essential for urban modeling.\nTraditional spatial embeddings often prioritize spatial proximity while\nunderutilizing fine-grained contextual information from places. To address this\nlimitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that\nsystematically integrates Point-of-Interest (POI) names alongside categorical\nlabels within a multimodal contrastive learning framework. We evaluate its\neffectiveness on two downstream tasks, land use classification and\nsocioeconomic status distribution mapping, demonstrating consistent performance\ngains of 4% to 11% over baseline methods. Additionally, we show that\nincorporating POI names enhances location retrieval, enabling models to capture\ncomplex urban concepts with greater precision. Ablation studies further reveal\nthe complementary role of POI names and the advantages of leveraging pretrained\ntext encoders for spatial representations. Overall, our findings highlight the\npotential of integrating fine-grained semantic attributes and multimodal\nlearning techniques to advance the development of urban foundation models.", "categories": ["cs.CE", "cs.AI"], "published": "2025-06-03 11:06:51", "updated": "2025-06-03 11:06:51", "pdf_url": "http://arxiv.org/pdf/2506.02744v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02746v1", "title": "Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search", "authors": ["Lin Xie", "Hanyi Li"], "abstract": "The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems\n(RMFS) involves selecting optimal storage locations for pods returning from\npick stations. This work presents an improved solution method that integrates\nAdaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning\n(DRL). A DRL agent dynamically selects destroy and repair operators and adjusts\nkey parameters such as destruction degree and acceptance thresholds during the\nsearch. Specialized heuristics for both operators are designed to reflect\nPRP-specific characteristics, including pod usage frequency and movement costs.\nComputational results show that this DRL-guided ALNS outperforms traditional\napproaches such as cheapest-place, fixed-place, binary integer programming, and\nstatic heuristics. The method demonstrates strong solution quality and\nillustrating the benefit of learning-driven control within combinatorial\noptimization for warehouse systems.", "categories": ["cs.RO", "cs.AI", "math.OC"], "published": "2025-06-03 11:07:41", "updated": "2025-06-03 11:07:41", "pdf_url": "http://arxiv.org/pdf/2506.02746v1", "comment": "14 pages, 2 figures, conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02749v1", "title": "Knowledge Graph Completion by Intermediate Variables Regularization", "authors": ["Changyi Xiao", "Yixin Cao"], "abstract": "Knowledge graph completion (KGC) can be framed as a 3-order binary tensor\ncompletion task. Tensor decomposition-based (TDB) models have demonstrated\nstrong performance in KGC. In this paper, we provide a summary of existing TDB\nmodels and derive a general form for them, serving as a foundation for further\nexploration of TDB models. Despite the expressiveness of TDB models, they are\nprone to overfitting. Existing regularization methods merely minimize the norms\nof embeddings to regularize the model, leading to suboptimal performance.\nTherefore, we propose a novel regularization method for TDB models that\naddresses this limitation. The regularization is applicable to most TDB models\nand ensures tractable computation. Our method minimizes the norms of\nintermediate variables involved in the different ways of computing the\npredicted tensor. To support our regularization method, we provide a\ntheoretical analysis that proves its effect in promoting low trace norm of the\npredicted tensor to reduce overfitting. Finally, we conduct experiments to\nverify the effectiveness of our regularization technique as well as the\nreliability of our theoretical analysis. The code is available at\nhttps://github.com/changyi7231/IVR.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 11:11:33", "updated": "2025-06-03 11:11:33", "pdf_url": "http://arxiv.org/pdf/2506.02749v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02757v1", "title": "Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection", "authors": ["Ruiying Lu", "Jinhan Liu", "Chuan Du", "Dandan Guo"], "abstract": "Tabular anomaly detection, which aims at identifying deviant samples, has\nbeen crucial in a variety of real-world applications, such as medical disease\nidentification, financial fraud detection, intrusion monitoring, etc. Although\nrecent deep learning-based methods have achieved competitive performances,\nthese methods suffer from representation entanglement and the lack of global\ncorrelation modeling, which hinders anomaly detection performance. To tackle\nthe problem, we incorporate mask modeling and prototype learning into tabular\nanomaly detection. The core idea is to design learnable masks by disentangled\nrepresentation learning within a projection space and extracting normal\ndependencies as explicit global prototypes. Specifically, the overall model\ninvolves two parts: (i) During encoding, we perform mask modeling in both the\ndata space and projection space with orthogonal basis vectors for learning\nshared disentangled normal patterns; (ii) During decoding, we decode multiple\nmasked representations in parallel for reconstruction and learn association\nprototypes to extract normal characteristic correlations. Our proposal derives\nfrom a distribution-matching perspective, where both projection space learning\nand association prototype learning are formulated as optimal transport\nproblems, and the calibration distances are utilized to refine the anomaly\nscores. Quantitative and qualitative experiments on 20 tabular benchmarks\ndemonstrate the effectiveness and interpretability of our model.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 11:22:44", "updated": "2025-06-03 11:22:44", "pdf_url": "http://arxiv.org/pdf/2506.02757v1", "comment": "12 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02758v1", "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs", "authors": ["Stefano Bann\u00f2", "Kate Knill", "Mark Gales"], "abstract": "Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 11:23:57", "updated": "2025-06-03 11:23:57", "pdf_url": "http://arxiv.org/pdf/2506.02758v1", "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02761v1", "title": "Rethinking Machine Unlearning in Image Generation Models", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "abstract": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU.", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "published": "2025-06-03 11:25:14", "updated": "2025-06-03 11:25:14", "pdf_url": "http://arxiv.org/pdf/2506.02761v1", "comment": "Accepted by ACM CCS 2025", "doi": null, "journal_ref": "ACM Conference on Computer and Communications Security (CCS 2025)"}
{"arxiv_id": "2506.02764v1", "title": "Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations", "authors": ["Fatma Youssef Mohammed", "Kostas Alexis"], "abstract": "Computational human attention modeling in free-viewing and task-specific\nsettings is often studied separately, with limited exploration of whether a\ncommon representation exists between them. This work investigates this question\nand proposes a neural network architecture that builds upon the Human Attention\ntransformer (HAT) to test the hypothesis. Our results demonstrate that\nfree-viewing and visual search can efficiently share a common representation,\nallowing a model trained in free-viewing attention to transfer its knowledge to\ntask-driven visual search with a performance drop of only 3.86% in the\npredicted fixation scanpaths, measured by the semantic sequence score (SemSS)\nmetric which reflects the similarity between predicted and human scanpaths.\nThis transfer reduces computational costs by 92.29% in terms of GFLOPs and\n31.23% in terms of trainable parameters.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 11:29:11", "updated": "2025-06-03 11:29:11", "pdf_url": "http://arxiv.org/pdf/2506.02764v1", "comment": "Accepted to the 2025 IEEE International Conference on Development and\n  Learning (ICDL)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02785v1", "title": "AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service Migration", "authors": ["Charalampos Kalalas", "Pavol Mulinka", "Guillermo Candela Belmonte", "Miguel Fornell", "Michail Dalgitsis", "Francisco Paredes Vera", "Javier Santaella S\u00e1nchez", "Carmen Vicente Villares", "Roshan Sedar", "Eftychia Datsika", "Angelos Antonopoulos", "Antonio Fern\u00e1ndez Ojea", "Miquel Payaro"], "abstract": "Artificial intelligence (AI) has been increasingly applied to the condition\nmonitoring of vehicular equipment, aiming to enhance maintenance strategies,\nreduce costs, and improve safety. Leveraging the edge computing paradigm,\nAI-based condition monitoring systems process vast streams of vehicular data to\ndetect anomalies and optimize operational performance. In this work, we\nintroduce a novel vehicle condition monitoring service that enables real-time\ndiagnostics of a diverse set of anomalies while remaining practical for\ndeployment in real-world edge environments. To address mobility challenges, we\npropose a closed-loop service orchestration framework where service migration\nacross edge nodes is dynamically triggered by network-related metrics. Our\napproach has been implemented and tested in a real-world race circuit\nenvironment equipped with 5G network capabilities under diverse operational\nconditions. Experimental results demonstrate the effectiveness of our framework\nin ensuring low-latency AI inference and adaptive service placement,\nhighlighting its potential for intelligent transportation and mobility\napplications.", "categories": ["cs.NI", "cs.AI"], "published": "2025-06-03 12:12:27", "updated": "2025-06-03 12:12:27", "pdf_url": "http://arxiv.org/pdf/2506.02785v1", "comment": "6 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02787v1", "title": "Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization", "authors": ["Ruilong Wu", "Xinjiao Li", "Yisu Wang", "Xinyu Chen", "Dirk Kutscher"], "abstract": "Hybrid parallelism techniques are essential for efficiently training large\nlanguage models (LLMs). Nevertheless, current automatic parallel planning\nframeworks often overlook the simultaneous consideration of node heterogeneity\nand dynamic network topology changes, limiting their effectiveness in practical\napplications. In this paper, we address these limitations by modeling\nheterogeneous nodes within dynamically changing network environments and\nleveraging simulation-based strategies to determine optimal parallel\nconfigurations. Our approach enables fine-grained workload allocation tailored\nfor heterogeneous nodes and complex network scenarios, achieving performance\ncompetitive with state-of-the-art methods under regular and stable network\nconditions. Additionally, we introduce a strategy pruning technique to rapidly\ndiscard infeasible parallel configurations, substantially reducing the search\nspace and accelerating the search process through parallel execution within the\nsimulator. Preliminary evaluations confirm that our method notably enhances\ntraining performance on heterogeneous nodes and demonstrates improved\nadaptability in complex, dynamic scenarios such as cloud computing\nenvironments.", "categories": ["cs.DC", "cs.AI"], "published": "2025-06-03 12:14:17", "updated": "2025-06-03 12:14:17", "pdf_url": "http://arxiv.org/pdf/2506.02787v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02791v1", "title": "Rethinking the effects of data contamination in Code Intelligence", "authors": ["Zhen Yang", "Hongyi Lin", "Yifan He", "Jie Xu", "Zeyu Sun", "Shuo Liu", "Pengpeng Wang", "Zhongxing Yu", "Qingyuan Liang"], "abstract": "In recent years, code intelligence has gained increasing importance in the\nfield of automated software engineering. Meanwhile, the widespread adoption of\nPretrained Language Models (PLMs) and Large Language Models (LLMs) has raised\nconcerns regarding data contamination and its potential impact on model\nperformance evaluation. This paper presents a systematic empirical study to\ninvestigate the fine-grained data contamination on code intelligence tasks. Our\nstudy involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs,\nnamely LLaMA and StarCoder, covering three major tasks: code translation, code\ngeneration, and code summarization. We categorize contamination scenarios into\nfour types according to the code intelligence practice, namely input-only,\noutput-only, unpaired, and paired contamination settings, and construct\ncorresponding experimental and control groups for exploration.\n  Experimental results show that, under the pre-training, fine-tuning, and\ninference paradigm adopted by PLMs, even deliberately injecting paired\ncontamination does not lead to significant performance overestimation. But\ndirect inference or small-scale fine-tuning uncovers the contamination effects.\nIn contrast, LLMs with pre-training and inference paradigm are significantly\naffected by the paired contamination. Apart from the above, other contamination\nscenarios have no impact on both PLMs and LLMs. Our findings challenge the\nconventional belief that contamination inevitably leads to performance\noverestimation, providing new insights into the evaluation and deployment of\ncode intelligence models.", "categories": ["cs.SE", "cs.AI"], "published": "2025-06-03 12:15:44", "updated": "2025-06-03 12:15:44", "pdf_url": "http://arxiv.org/pdf/2506.02791v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02794v1", "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis", "authors": ["Mijeong Kim", "Gunhee Kim", "Jungyoon Choi", "Wonjae Roh", "Bohyung Han"], "abstract": "We introduce PhysGaia, a novel physics-aware dataset specifically designed\nfor Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects\nand unstructured physical phenomena. Unlike existing datasets that primarily\nfocus on photorealistic reconstruction, PhysGaia is created to actively support\nphysics-aware dynamic scene modeling. Our dataset provides complex dynamic\nscenarios with rich interactions among multiple objects, where they\nrealistically collide with each other and exchange forces. Furthermore, it\ncontains a diverse range of physical materials, such as liquid, gas,\nviscoelastic substance, and textile, which moves beyond the rigid bodies\nprevalent in existing datasets. All scenes in PhysGaia are faithfully generated\nto strictly adhere to physical laws, leveraging carefully selected\nmaterial-specific physics solvers. To enable quantitative evaluation of\nphysical modeling, our dataset provides essential ground-truth information,\nincluding 3D particle trajectories and physics parameters, e.g., viscosity. To\nfacilitate research adoption, we also provide essential integration pipelines\nfor using state-of-the-art DyNVS models with our dataset and report their\nresults. By addressing the critical lack of datasets for physics-aware\nmodeling, PhysGaia will significantly advance research in dynamic view\nsynthesis, physics-based scene understanding, and deep learning models\nintegrated with physical simulation -- ultimately enabling more faithful\nreconstruction and interpretation of complex dynamic scenes. Our datasets and\ncodes are available in the project website,\nhttp://cvlab.snu.ac.kr/research/PhysGaia.", "categories": ["cs.GR", "cs.AI", "cs.CV"], "published": "2025-06-03 12:19:18", "updated": "2025-06-03 12:19:18", "pdf_url": "http://arxiv.org/pdf/2506.02794v1", "comment": "Project page: http://cvlab.snu.ac.kr/research/PhysGaia, Data:\n  https://huggingface.co/datasets/mijeongkim/PhysGaia/tree/main", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02796v1", "title": "Deep Learning Enhanced Multivariate GARCH", "authors": ["Haoyuan Wang", "Chen Liu", "Minh-Ngoc Tran", "Chao Wang"], "abstract": "This paper introduces a novel multivariate volatility modeling framework,\nnamed Long Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep\nlearning into multivariate GARCH processes. By combining the flexibility of\nrecurrent neural networks with the econometric structure of BEKK models, our\napproach is designed to better capture nonlinear, dynamic, and high-dimensional\ndependence structures in financial return data. The proposed model addresses\nkey limitations of traditional multivariate GARCH-based methods, particularly\nin capturing persistent volatility clustering and asymmetric co-movement across\nassets. Leveraging the data-driven nature of LSTMs, the framework adapts\neffectively to time-varying market conditions, offering improved robustness and\nforecasting performance. Empirical results across multiple equity markets\nconfirm that the LSTM-BEKK model achieves superior performance in terms of\nout-of-sample portfolio risk forecast, while maintaining the interpretability\nfrom the BEKK models. These findings highlight the potential of hybrid\neconometric-deep learning models in advancing financial risk management and\nmultivariate volatility forecasting.", "categories": ["q-fin.CP", "cs.AI", "econ.EM"], "published": "2025-06-03 12:22:57", "updated": "2025-06-03 12:22:57", "pdf_url": "http://arxiv.org/pdf/2506.02796v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02805v1", "title": "Optimising the attribute order in Fuzzy Rough Rule Induction", "authors": ["Henri Bollaert", "Chris Cornelis", "Marko Palangeti\u0107", "Salvatore Greco", "Roman S\u0142owi\u0144ski"], "abstract": "Interpretability is the next pivotal frontier in machine learning research.\nIn the pursuit of glass box models - as opposed to black box models, like\nrandom forests or neural networks - rule induction algorithms are a logical and\npromising avenue, as the rules can easily be understood by humans. In our\nprevious work, we introduced FRRI, a novel rule induction algorithm based on\nfuzzy rough set theory. We demonstrated experimentally that FRRI outperformed\nother rule induction methods with regards to accuracy and number of rules. FRRI\nleverages a fuzzy indiscernibility relation to partition the data space into\nfuzzy granules, which are then combined into a minimal covering set of rules.\nThis indiscernibility relation is constructed by removing attributes from rules\nin a greedy way. This raises the question: does the order of the attributes\nmatter? In this paper, we show that optimising only the order of attributes\nusing known methods from fuzzy rough set theory and classical machine learning\ndoes not improve the performance of FRRI on multiple metrics. However, removing\na small number of attributes using fuzzy rough feature selection during this\nstep positively affects balanced accuracy and the average rule length.", "categories": ["cs.AI"], "published": "2025-06-03 12:34:40", "updated": "2025-06-03 12:34:40", "pdf_url": "http://arxiv.org/pdf/2506.02805v1", "comment": "This is the author's version of the work accepted for publication in\n  Lecture Notes in Computer Science. The final publication is available at\n  Springer via https://doi.org/10.1007/978-3-031-92747-8_16", "doi": "10.1007/978-3-031-92747-8_16", "journal_ref": null}
{"arxiv_id": "2506.02838v1", "title": "TaxAgent: How Large Language Model Designs Fiscal Policy", "authors": ["Jizhou Wang", "Xiaodan Fang", "Lei Huang", "Yongfeng Huang"], "abstract": "Economic inequality is a global challenge, intensifying disparities in\neducation, healthcare, and social stability. Traditional systems like the U.S.\nfederal income tax reduce inequality but lack adaptability. Although models\nlike the Saez Optimal Taxation adjust dynamically, they fail to address\ntaxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,\na novel integration of large language models (LLMs) with agent-based modeling\n(ABM) to design adaptive tax policies. In our macroeconomic simulation,\nheterogeneous H-Agents (households) simulate real-world taxpayer behaviors\nwhile the TaxAgent (government) utilizes LLMs to iteratively optimize tax\nrates, balancing equity and productivity. Benchmarked against Saez Optimal\nTaxation, U.S. federal income taxes, and free markets, TaxAgent achieves\nsuperior equity-efficiency trade-offs. This research offers a novel taxation\nsolution and a scalable, data-driven framework for fiscal policy evaluation.", "categories": ["cs.AI", "econ.GN", "q-fin.EC", "I.2.11; I.6.5; J.4"], "published": "2025-06-03 13:06:19", "updated": "2025-06-03 13:06:19", "pdf_url": "http://arxiv.org/pdf/2506.02838v1", "comment": "Accepted as oral presentation at ICME 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02839v1", "title": "DeepShop: A Benchmark for Deep Research Shopping Agents", "authors": ["Yougang Lyu", "Xiaoyu Zhang", "Lingyong Yan", "Maarten de Rijke", "Zhaochun Ren", "Xiuying Chen"], "abstract": "Web agents for online shopping have shown great promise in automating user\ninteractions across e-commerce platforms. Benchmarks for assessing such agents\ndo not reflect the complexity of real-world shopping scenarios, as they often\nconsist of overly simple queries with deterministic paths, such as \"Find iPhone\n15.\" Real shopping scenarios are inherently more layered, involving\nmulti-dimensional product attributes, search filters, and user-specific sorting\npreferences. To address this gap, we introduce DeepShop, a benchmark designed\nto evaluate web agents in complex and realistic online shopping environments.\nDeepShop comprises three key components. (1) Query diversity evolution:\nStarting from real user queries, we generate diverse queries across five\npopular online shopping domains. (2) Query complexity evolution: We further\nevolve these queries to increase complexity, considering product attributes,\nsearch filters, and sorting preferences, and classify them into three levels:\neasy, medium, and hard, based on the number of evolutions. (3) Fine-grained and\nholistic evaluation: We propose an automated evaluation framework that assesses\nagent performance in terms of fine-grained aspects (product attributes, search\nfilters, and sorting preferences) and reports the overall success rate through\nholistic evaluation. We conduct a systematic evaluation of retrieval-augmented\ngeneration (RAG) methods, web agents, and deep research systems. Results show\nthat RAG struggles with complex queries due to its lack of web interaction,\nwhile other methods face significant challenges with filters and sorting\npreferences, leading to low overall success rates. We also perform\ncross-category, complexity-based evaluations and error analyses to support the\nadvancement of deep research shopping agents.", "categories": ["cs.IR", "cs.AI"], "published": "2025-06-03 13:08:17", "updated": "2025-06-03 13:08:17", "pdf_url": "http://arxiv.org/pdf/2506.02839v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02842v1", "title": "Sheaves Reloaded: A Directional Awakening", "authors": ["Stefano Fiorini", "Hakan Aktas", "Iulia Duta", "Stefano Coniglio", "Pietro Morerio", "Alessio Del Bue", "Pietro Li\u00f2"], "abstract": "Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph\nNeural Networks (GNNs) that significantly improve our ability to model complex\nrelational data. While directionality has been shown to substantially boost\nperformance in graph learning tasks and is key to many real-world applications,\nexisting SNNs fall short in representing it. To address this limitation, we\nintroduce the Directed Cellular Sheaf, a special type of cellular sheaf\ndesigned to explicitly account for edge orientation. Building on this\nstructure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which\ncaptures both the graph's topology and its directional information. This\noperator serves as the backbone of the Directed Sheaf Neural Network (DSNN),\nthe first SNN model to embed a directional bias into its architecture.\nExtensive experiments on nine real-world benchmarks show that DSNN consistently\noutperforms baseline methods.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 13:13:56", "updated": "2025-06-03 13:13:56", "pdf_url": "http://arxiv.org/pdf/2506.02842v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02858v1", "title": "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization", "authors": ["Geonyoung Lee", "Geonhee Han", "Paul Hongsuck Seo"], "abstract": "Language-queried Audio Source Separation (LASS) enables open-vocabulary sound\nseparation via natural language queries. While existing methods rely on\ntask-specific training, we explore whether pretrained diffusion models,\noriginally designed for audio generation, can inherently perform separation\nwithout further training. In this study, we introduce a training-free framework\nleveraging generative priors for zero-shot LASS. Analyzing na\\\"ive adaptations,\nwe identify key limitations arising from modality-specific challenges.To\naddress these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a\ntest-time optimization framework that refines spectrogram masks for precise,\ninput-aligned separation. Our approach effectively repurposes pretrained\ndiffusion models for source separation, achieving competitive performance\nwithout task-specific supervision. This work expands the application of\ndiffusion models beyond generation, establishing a new paradigm for zero-shot\naudio separation. The code is available at: https://wltschmrz.github.io/DGMO/", "categories": ["cs.SD", "cs.AI"], "published": "2025-06-03 13:24:57", "updated": "2025-06-03 13:24:57", "pdf_url": "http://arxiv.org/pdf/2506.02858v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02859v1", "title": "ATAG: AI-Agent Application Threat Assessment with Attack Graphs", "authors": ["Parth Atulbhai Gandhi", "Akansha Shukla", "David Tayouri", "Beni Ifland", "Yuval Elovici", "Rami Puzis", "Asaf Shabtai"], "abstract": "Evaluating the security of multi-agent systems (MASs) powered by large\nlanguage models (LLMs) is challenging, primarily because of the systems'\ncomplex internal dynamics and the evolving nature of LLM vulnerabilities.\nTraditional attack graph (AG) methods often lack the specific capabilities to\nmodel attacks on LLMs. This paper introduces AI-agent application Threat\nassessment with Attack Graphs (ATAG), a novel framework designed to\nsystematically analyze the security risks associated with AI-agent\napplications. ATAG extends the MulVAL logic-based AG generation tool with\ncustom facts and interaction rules to accurately represent AI-agent topologies,\nvulnerabilities, and attack scenarios. As part of this research, we also\ncreated the LLM vulnerability database (LVD) to initiate the process of\nstandardizing LLM vulnerabilities documentation. To demonstrate ATAG's\nefficacy, we applied it to two multi-agent applications. Our case studies\ndemonstrated the framework's ability to model and generate AGs for\nsophisticated, multi-step attack scenarios exploiting vulnerabilities such as\nprompt injection, excessive agency, sensitive information disclosure, and\ninsecure output handling across interconnected agents. ATAG is an important\nstep toward a robust methodology and toolset to help understand, visualize, and\nprioritize complex attack paths in multi-agent AI systems (MAASs). It\nfacilitates proactive identification and mitigation of AI-agent threats in\nmulti-agent applications.", "categories": ["cs.CR", "cs.AI"], "published": "2025-06-03 13:25:40", "updated": "2025-06-03 13:25:40", "pdf_url": "http://arxiv.org/pdf/2506.02859v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02860v1", "title": "Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs", "authors": ["Wenjing Tang", "Xinyu He", "Yongxi Huang", "Yunxiao Xiao", "Cewu Lu", "Panpan Cai"], "abstract": "Task planning under uncertainty is essential for home-service robots\noperating in the real world. Tasks involve ambiguous human instructions, hidden\nor unknown object locations, and open-vocabulary object types, leading to\nsignificant open-ended uncertainty and a boundlessly large planning space. To\naddress these challenges, we propose Tru-POMDP, a planner that combines\nstructured belief generation using Large Language Models (LLMs) with principled\nPOMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),\nwhich systematically queries an LLM to construct high-quality particle beliefs\nover possible world states and human goals. We further formulate an open-ended\nPOMDP model that enables rigorous Bayesian belief tracking and efficient\nbelief-space planning over these LLM-generated hypotheses. Experiments on\ncomplex object rearrangement tasks across diverse kitchen environments show\nthat Tru-POMDP significantly outperforms state-of-the-art LLM-based and\nLLM-tree-search hybrid planners, achieving higher success rates with\nsignificantly better plans, stronger robustness to ambiguity and occlusion, and\ngreater planning efficiency.", "categories": ["cs.RO", "cs.AI"], "published": "2025-06-03 13:26:08", "updated": "2025-06-03 13:26:08", "pdf_url": "http://arxiv.org/pdf/2506.02860v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02863v1", "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech", "authors": ["Helin Wang", "Jiarui Hai", "Dading Chong", "Karan Thakkar", "Tiantian Feng", "Dongchao Yang", "Junhyeok Lee", "Laureano Moro Velazquez", "Jesus Villalba", "Zengyi Qin", "Shrikanth Narayanan", "Mounya Elhiali", "Najim Dehak"], "abstract": "Recent advancements in generative artificial intelligence have significantly\ntransformed the field of style-captioned text-to-speech synthesis (CapTTS).\nHowever, adapting CapTTS to real-world applications remains challenging due to\nthe lack of standardized, comprehensive datasets and limited research on\ndownstream tasks built upon CapTTS. To address these gaps, we introduce\nCapSpeech, a new benchmark designed for a series of CapTTS-related tasks,\nincluding style-captioned text-to-speech synthesis with sound events\n(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS\n(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech\ncomprises over 10 million machine-annotated audio-caption pairs and nearly 0.36\nmillion human-annotated audio-caption pairs. In addition, we introduce two new\ndatasets collected and recorded by a professional voice actor and experienced\naudio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside\nthe datasets, we conduct comprehensive experiments using both autoregressive\nand non-autoregressive models on CapSpeech. Our results demonstrate\nhigh-fidelity and highly intelligible speech synthesis across a diverse range\nof speaking styles. To the best of our knowledge, CapSpeech is the largest\navailable dataset offering comprehensive annotations for CapTTS-related tasks.\nThe experiments and findings further provide valuable insights into the\nchallenges of developing CapTTS systems.", "categories": ["eess.AS", "cs.AI", "cs.SD"], "published": "2025-06-03 13:28:55", "updated": "2025-06-03 13:28:55", "pdf_url": "http://arxiv.org/pdf/2506.02863v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02864v1", "title": "BNPO: Beta Normalization Policy Optimization", "authors": ["Changyi Xiao", "Mengdi Zhang", "Yixin Cao"], "abstract": "Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that\nreinforcement learning with rule-based, binary-valued reward functions can\nsignificantly enhance the reasoning capabilities of large language models.\nThese models primarily utilize REINFORCE-based policy optimization techniques,\nsuch as REINFORCE with baseline and group relative policy optimization (GRPO).\nHowever, a key limitation remains: current policy optimization methods either\nneglect reward normalization or employ static normalization strategies, which\nfail to adapt to the dynamic nature of policy updates during training. This may\nresult in unstable gradient estimates and hinder training stability. To address\nthis issue, we propose Beta Normalization Policy Optimization (BNPO), a novel\npolicy optimization method that adaptively normalizes rewards using a Beta\ndistribution with dynamically updated parameters. BNPO aligns the normalization\nwith the changing policy distribution, enabling more precise and lower-variance\ngradient estimation, which in turn promotes stable training dynamics. We\nprovide theoretical analysis demonstrating BNPO's variance-reducing properties\nand show that it generalizes both REINFORCE and GRPO under binary-valued reward\nsettings. Furthermore, we introduce an advantage decomposition mechanism to\nextend BNPO's applicability to more complex reward systems. Experimental\nresults confirm that BNPO achieves state-of-the-art performance among policy\noptimization methods on reasoning tasks. The code is available at\nhttps://github.com/changyi7231/BNPO.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 13:28:57", "updated": "2025-06-03 13:28:57", "pdf_url": "http://arxiv.org/pdf/2506.02864v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02865v1", "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights", "authors": ["Mathieu Andreux", "Breno Baldas Skuk", "Hamza Benchekroun", "Emilien Bir\u00e9", "Antoine Bonnet", "Riaz Bordie", "Matthias Brunel", "Pierre-Louis Cedoz", "Antoine Chassang", "Micka\u00ebl Chen", "Alexandra D. Constantinou", "Antoine d'Andign\u00e9", "Hubert de La Jonqui\u00e8re", "Aur\u00e9lien Delfosse", "Ludovic Denoyer", "Alexis Deprez", "Augustin Derupti", "Michael Eickenberg", "Math\u00efs Federico", "Charles Kantor", "Xavier Koegler", "Yann Labb\u00e9", "Matthew C. H. Lee", "Erwan Le Jumeau de Kergaradec", "Amir Mahla", "Avshalom Manevich", "Adrien Maret", "Charles Masson", "Rafa\u00ebl Maurin", "Arturo Mena", "Philippe Modard", "Axel Moyal", "Axel Nguyen Kerbel", "Julien Revelle", "Mats L. Richter", "Mar\u00eda Santos", "Laurent Sifre", "Maxime Theillard", "Marc Thibault", "Louis Thiry", "L\u00e9o Tronchon", "Nicolas Usunier", "Tony Wu"], "abstract": "We present Surfer-H, a cost-efficient web agent that integrates\nVision-Language Models (VLM) to perform user-defined tasks on the web. We pair\nit with Holo1, a new open-weight collection of VLMs specialized in web\nnavigation and information extraction. Holo1 was trained on carefully curated\ndata sources, including open-access web content, synthetic examples, and\nself-produced agentic data. Holo1 tops generalist User Interface (UI)\nbenchmarks as well as our new web UI localization benchmark, WebClick. When\npowered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on\nWebVoyager, striking a Pareto-optimal balance between accuracy and\ncost-efficiency. To accelerate research advancement in agentic systems, we are\nopen-sourcing both our WebClick evaluation dataset and the Holo1 model weights.", "categories": ["cs.AI"], "published": "2025-06-03 13:29:03", "updated": "2025-06-03 13:29:03", "pdf_url": "http://arxiv.org/pdf/2506.02865v1", "comment": "Alphabetical order", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02867v1", "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning", "authors": ["Chen Qian", "Dongrui Liu", "Haochen Wen", "Zhen Bai", "Yong Liu", "Jing Shao"], "abstract": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-03 13:31:10", "updated": "2025-06-03 13:31:10", "pdf_url": "http://arxiv.org/pdf/2506.02867v1", "comment": "Preprint. Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02873v1", "title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics", "authors": ["Matthew Kowal", "Jasper Timm", "Jean-Francois Godbout", "Thomas Costello", "Antonio A. Arechar", "Gordon Pennycook", "David Rand", "Adam Gleave", "Kellin Pelrine"], "abstract": "Persuasion is a powerful capability of large language models (LLMs) that both\nenables beneficial applications (e.g. helping people quit smoking) and raises\nsignificant risks (e.g. large-scale, targeted political manipulation). Prior\nwork has found models possess a significant and growing persuasive capability,\nmeasured by belief changes in simulated or real users. However, these\nbenchmarks overlook a crucial risk factor: the propensity of a model to attempt\nto persuade in harmful contexts. Understanding whether a model will blindly\n``follow orders'' to persuade on harmful topics (e.g. glorifying joining a\nterrorist group) is key to understanding the efficacy of safety guardrails.\nMoreover, understanding if and when a model will engage in persuasive behavior\nin pursuit of some goal is essential to understanding the risks from agentic AI\nsystems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts\nthe focus from persuasion success to persuasion attempts, operationalized as a\nmodel's willingness to generate content aimed at shaping beliefs or behavior.\nOur evaluation framework probes frontier LLMs using a multi-turn conversational\nsetup between simulated persuader and persuadee agents. APE explores a diverse\nspectrum of topics including conspiracies, controversial issues, and\nnon-controversially harmful content. We introduce an automated evaluator model\nto identify willingness to persuade and measure the frequency and context of\npersuasive attempts. We find that many open and closed-weight models are\nfrequently willing to attempt persuasion on harmful topics and that\njailbreaking can increase willingness to engage in such behavior. Our results\nhighlight gaps in current safety guardrails and underscore the importance of\nevaluating willingness to persuade as a key dimension of LLM risk. APE is\navailable at github.com/AlignmentResearch/AttemptPersuadeEval", "categories": ["cs.AI"], "published": "2025-06-03 13:37:51", "updated": "2025-06-03 13:37:51", "pdf_url": "http://arxiv.org/pdf/2506.02873v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02878v1", "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective", "authors": ["Jintian Shao", "Yiming Cheng"], "abstract": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 13:45:01", "updated": "2025-06-03 13:45:01", "pdf_url": "http://arxiv.org/pdf/2506.02878v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02890v1", "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights", "authors": ["Jakub Krajewski", "Marcin Chochowski", "Daniel Korzekwa"], "abstract": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-03 13:55:48", "updated": "2025-06-03 13:55:48", "pdf_url": "http://arxiv.org/pdf/2506.02890v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02899v1", "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator", "authors": ["Yusuke Sakai", "Takumi Goto", "Taro Watanabe"], "abstract": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 14:05:37", "updated": "2025-06-03 14:05:37", "pdf_url": "http://arxiv.org/pdf/2506.02899v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02911v1", "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "abstract": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "published": "2025-06-03 14:16:53", "updated": "2025-06-03 14:16:53", "pdf_url": "http://arxiv.org/pdf/2506.02911v1", "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02918v1", "title": "Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs", "authors": ["Shangmin Guo", "Omar Darwiche Domingues", "Rapha\u00ebl Avalos", "Aaron Courville", "Florian Strub"], "abstract": "Tool use in stateful environments presents unique challenges for large\nlanguage models (LLMs), where existing test-time compute strategies relying on\nrepeated trials in the environment are impractical. We propose dynamics\nmodelling (DyMo), a method that augments LLMs with a state prediction\ncapability alongside function calling during post-training. This enables LLMs\nto predict the future states of their actions through an internal environment\nmodel. On the Berkeley Function Calling Leaderboard V2, DyMo improves success\nrates and significantly reduces hallucinations. We further integrate the\ninternal environment model into self-verification sampling (SVS), and show that\nthis substantially improves pass^k over number of trials k, and allows the\nmodel to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the\neffectiveness and reliability of LLMs for tool use. We believe this work charts\na path towards scalable planning RL methods for LLM inference without\nrepeatedly querying the oracle environment.", "categories": ["cs.AI", "cs.LG"], "published": "2025-06-03 14:20:59", "updated": "2025-06-03 14:20:59", "pdf_url": "http://arxiv.org/pdf/2506.02918v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02923v1", "title": "The Limits of Predicting Agents from Behaviour", "authors": ["Alexis Bellot", "Jonathan Richens", "Tom Everitt"], "abstract": "As the complexity of AI systems and their interactions with the world\nincreases, generating explanations for their behaviour is important for safely\ndeploying AI. For agents, the most natural abstractions for predicting\nbehaviour attribute beliefs, intentions and goals to the system. If an agent\nbehaves as if it has a certain goal or belief, then we can make reasonable\npredictions about how it will behave in novel situations, including those where\ncomprehensive safety evaluations are untenable. How well can we infer an\nagent's beliefs from their behaviour, and how reliably can these inferred\nbeliefs predict the agent's behaviour in novel situations? We provide a precise\nanswer to this question under the assumption that the agent's behaviour is\nguided by a world model. Our contribution is the derivation of novel bounds on\nthe agent's behaviour in new (unseen) deployment environments, which represent\na theoretical limit for predicting intentional agents from behavioural data\nalone. We discuss the implications of these results for several research areas\nincluding fairness and safety.", "categories": ["cs.AI", "stat.ML"], "published": "2025-06-03 14:24:58", "updated": "2025-06-03 14:24:58", "pdf_url": "http://arxiv.org/pdf/2506.02923v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02931v1", "title": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms", "authors": ["Praneet Sai Madhu Surabhi", "Dheeraj Reddy Mudireddy", "Jian Tao"], "abstract": "This paper presents ThinkTank, a comprehensive and scalable framework\ndesigned to transform specialized AI agent systems into versatile collaborative\nintelligence platforms capable of supporting complex problem-solving across\ndiverse domains. ThinkTank systematically generalizes agent roles, meeting\nstructures, and knowledge integration mechanisms by adapting proven scientific\ncollaboration methodologies. Through role abstraction, generalization of\nmeeting types for iterative collaboration, and the integration of\nRetrieval-Augmented Generation with advanced knowledge storage, the framework\nfacilitates expertise creation and robust knowledge sharing. ThinkTank enables\norganizations to leverage collaborative AI for knowledge-intensive tasks while\nensuring data privacy and security through local deployment, utilizing\nframeworks like Ollama with models such as Llama3.1. The ThinkTank framework is\ndesigned to deliver significant advantages in cost-effectiveness, data\nsecurity, scalability, and competitive positioning compared to cloud-based\nalternatives, establishing it as a universal platform for AI-driven\ncollaborative problem-solving. The ThinkTank code is available at\nhttps://github.com/taugroup/ThinkTank", "categories": ["cs.MA", "cs.AI", "cs.LG"], "published": "2025-06-03 14:32:48", "updated": "2025-06-03 14:32:48", "pdf_url": "http://arxiv.org/pdf/2506.02931v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02949v1", "title": "Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing", "authors": ["Lixiang Xu", "Xianwei Ding", "Xin Yuan", "Richang Hong", "Feiping Nie", "Enhong Chen", "Philip S. Yu"], "abstract": "Knowledge Tracing (KT) involves monitoring the changes in a student's\nknowledge over time by analyzing their past responses, with the goal of\npredicting future performance. However, most existing methods primarily focus\non feature enhancement, while overlooking the deficiencies in cognitive\nrepresentation and the ability to express cognition-issues often caused by\ninterference from non-cognitive factors such as slipping and guessing. This\nlimitation hampers the ability to capture the continuity and coherence of the\nstudent's cognitive process. As a result, many methods may introduce more\nprediction bias and modeling costs due to their inability to maintain cognitive\ncontinuity and coherence. Based on the above discussion, we propose the\nCognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)\nmodel. This model em ploys a dynamic programming algorithm to optimize\ncognitive representations based on the difficulty of the questions and the\nperformance intervals between them. This approach ensures that the cognitive\nrepresentation aligns with the student's cognitive patterns, maintaining\noverall continuity and coherence. As a result, it provides more accurate and\nsystematic input features for subsequent model training, thereby minimizing\ndistortion in the simulation of cognitive states. Additionally, the CRDP-KT\nmodel performs partitioned optimization of cognitive representations to enhance\nthe reliability of the optimization process. Furthermore, it improves its\nability to express the student's cognition through a weighted fusion of\noptimized record representations and re lationships learned from a bipartite\ngraph. Finally, experiments conducted on three public datasets validate the\neffectiveness of the proposed CRDP-KT model.", "categories": ["cs.AI"], "published": "2025-06-03 14:44:48", "updated": "2025-06-03 14:44:48", "pdf_url": "http://arxiv.org/pdf/2506.02949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02950v1", "title": "Interaction Field Matching: Overcoming Limitations of Electrostatic Models", "authors": ["Stepan I. Manukhov", "Alexander Kolesov", "Vladimir V. Palyulin", "Alexander Korotin"], "abstract": "Electrostatic field matching (EFM) has recently appeared as a novel\nphysics-inspired paradigm for data generation and transfer using the idea of an\nelectric capacitor. However, it requires modeling electrostatic fields using\nneural networks, which is non-trivial because of the necessity to take into\naccount the complex field outside the capacitor plates. In this paper, we\npropose Interaction Field Matching (IFM), a generalization of EFM which allows\nusing general interaction fields beyond the electrostatic one. Furthermore,\ninspired by strong interactions between quarks and antiquarks in physics, we\ndesign a particular interaction field realization which solves the problems\nwhich arise when modeling electrostatic fields in EFM. We show the performance\non a series of toy and image data transfer problems.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-06-03 14:45:14", "updated": "2025-06-03 14:45:14", "pdf_url": "http://arxiv.org/pdf/2506.02950v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02955v1", "title": "UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models", "authors": ["Zewen Yang", "Xiaobing Dai", "Dian Yu", "Qianru Li", "Yu Li", "Valentin Le Mesle"], "abstract": "Generative models have become increasingly powerful tools for robot motion\ngeneration, enabling flexible and multimodal trajectory generation across\nvarious tasks. Yet, most existing approaches remain limited in handling\nmultiple types of constraints, such as collision avoidance and dynamic\nconsistency, which are often treated separately or only partially considered.\nThis paper proposes UniConFlow, a unified flow matching (FM) based framework\nfor trajectory generation that systematically incorporates both equality and\ninequality constraints. UniConFlow introduces a novel prescribed-time zeroing\nfunction to enhance flexibility during the inference process, allowing the\nmodel to adapt to varying task requirements. To ensure constraint satisfaction,\nparticularly with respect to obstacle avoidance, admissible action range, and\nkinodynamic consistency, the guidance inputs to the FM model are derived\nthrough a quadratic programming formulation, which enables constraint-aware\ngeneration without requiring retraining or auxiliary controllers. We conduct\nmobile navigation and high-dimensional manipulation tasks, demonstrating\nimproved safety and feasibility compared to state-of-the-art constrained\ngenerative planners. Project page is available at https://uniconflow.github.io.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-06-03 14:48:04", "updated": "2025-06-03 14:48:04", "pdf_url": "http://arxiv.org/pdf/2506.02955v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02959v1", "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "abstract": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 14:52:44", "updated": "2025-06-03 14:52:44", "pdf_url": "http://arxiv.org/pdf/2506.02959v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02975v1", "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Zunnan Xu", "Zhaoyang Zhang", "Yixiao Ge", "Xiu Li", "Ying Shan"], "abstract": "With the advancement of language models, unified multimodal understanding and\ngeneration have made significant strides, with model architectures evolving\nfrom separated components to unified single-model frameworks. This paper\nexplores an efficient training paradigm to build a single transformer for\nunified multimodal understanding and generation. Specifically, we propose a\nmultimodal warmup strategy utilizing prior knowledge to extend capabilities. To\naddress cross-modal compatibility challenges, we introduce feature pre-scaling\nand multimodal AdaLN techniques. Integrating the proposed technologies, we\npresent the HaploOmni, a new single multimodal transformer. With limited\ntraining costs, HaploOmni achieves competitive performance across multiple\nimage and video understanding and generation benchmarks over advanced unified\nmodels. All codes will be made public at https://github.com/Tencent/HaploVLM.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 15:14:00", "updated": "2025-06-03 15:14:00", "pdf_url": "http://arxiv.org/pdf/2506.02975v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02976v1", "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge", "authors": ["Rachid Zeghlache", "Ikram Brahim", "Pierre-Henri Conze", "Mathieu Lamard", "Mohammed El Amine Lazouni", "Zineb Aziza Elaouaber", "Leila Ryma Lazouni", "Christopher Nielsen", "Ahmad O. Ahsan", "Matthias Wilms", "Nils D. Forkert", "Lovre Antonio Budimir", "Ivana Matovinovi\u0107", "Donik Vr\u0161nak", "Sven Lon\u010dari\u0107", "Philippe Zhang", "Weili Jiang", "Yihao Li", "Yiding Hao", "Markus Frohmann", "Patrick Binder", "Marcel Huber", "Taha Emre", "Teresa Finisterra Ara\u00fajo", "Marzieh Oghbaie", "Hrvoje Bogunovi\u0107", "Amerens A. Bekkers", "Nina M. van Liebergen", "Hugo J. Kuijf", "Abdul Qayyum", "Moona Mazher", "Steven A. Niederer", "Alberto J. Beltr\u00e1n-Carrero", "Juan J. G\u00f3mez-Valverde", "Javier Torresano-Rodr\u00edquez", "\u00c1lvaro Caballero-Sastre", "Mar\u00eda J. Ledesma Carbayo", "Yosuke Yamagishi", "Yi Ding", "Robin Peretzke", "Alexandra Ertl", "Maximilian Fischer", "Jessica K\u00e4chele", "Sofiane Zehar", "Karim Boukli Hacene", "Thomas Monfort", "B\u00e9atrice Cochener", "Mostafa El Habib Daho", "Anas-Alexis Benyoussef", "Gwenol\u00e9 Quellec"], "abstract": "The MARIO challenge, held at MICCAI 2024, focused on advancing the automated\ndetection and monitoring of age-related macular degeneration (AMD) through the\nanalysis of optical coherence tomography (OCT) images. Designed to evaluate\nalgorithmic performance in detecting neovascular activity changes within AMD,\nthe challenge incorporated unique multi-modal datasets. The primary dataset,\nsourced from Brest, France, was used by participating teams to train and test\ntheir models. The final ranking was determined based on performance on this\ndataset. An auxiliary dataset from Algeria was used post-challenge to evaluate\npopulation and device shifts from submitted solutions. Two tasks were involved\nin the MARIO challenge. The first one was the classification of evolution\nbetween two consecutive 2D OCT B-scans. The second one was the prediction of\nfuture AMD evolution over three months for patients undergoing anti-vascular\nendothelial growth factor (VEGF) therapy. Thirty-five teams participated, with\nthe top 12 finalists presenting their methods. This paper outlines the\nchallenge's structure, tasks, data characteristics, and winning methodologies,\nsetting a benchmark for AMD monitoring using OCT, infrared imaging, and\nclinical data (such as the number of visits, age, gender, etc.). The results of\nthis challenge indicate that artificial intelligence (AI) performs as well as a\nphysician in measuring AMD progression (Task 1) but is not yet able of\npredicting future evolution (Task 2).", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 15:14:10", "updated": "2025-06-03 15:14:10", "pdf_url": "http://arxiv.org/pdf/2506.02976v1", "comment": "MARIO-MICCAI-CHALLENGE 2024", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02987v1", "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "authors": ["Richard Armitage"], "abstract": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-06-03 15:25:38", "updated": "2025-06-03 15:25:38", "pdf_url": "http://arxiv.org/pdf/2506.02987v1", "comment": "12 pages, 1 Table", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02992v1", "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation", "authors": ["Li Zhang", "Kevin D. Ashley"], "abstract": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/", "categories": ["cs.AI", "cs.CL", "cs.LG", "68T50", "I.2"], "published": "2025-06-03 15:28:30", "updated": "2025-06-03 15:28:30", "pdf_url": "http://arxiv.org/pdf/2506.02992v1", "comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02996v1", "title": "Linear Spatial World Models Emerge in Large Language Models", "authors": ["Matthieu Tehenan", "Christian Bolivar Moya", "Tenghai Long", "Guang Lin"], "abstract": "Large language models (LLMs) have demonstrated emergent abilities across\ndiverse tasks, raising the question of whether they acquire internal world\nmodels. In this work, we investigate whether LLMs implicitly encode linear\nspatial world models, which we define as linear representations of physical\nspace and object configurations. We introduce a formal framework for spatial\nworld models and assess whether such structure emerges in contextual\nembeddings. Using a synthetic dataset of object positions, we train probes to\ndecode object positions and evaluate geometric consistency of the underlying\nspace. We further conduct causal interventions to test whether these spatial\nrepresentations are functionally used by the model. Our results provide\nempirical evidence that LLMs encode linear spatial world models.", "categories": ["cs.AI"], "published": "2025-06-03 15:31:00", "updated": "2025-06-03 15:31:00", "pdf_url": "http://arxiv.org/pdf/2506.02996v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03009v1", "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech", "authors": ["Florian Ludwig", "Torsten Zesch", "Frederike Zufall"], "abstract": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 15:50:27", "updated": "2025-06-03 15:50:27", "pdf_url": "http://arxiv.org/pdf/2506.03009v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03022v1", "title": "Smartflow: Enabling Scalable Spatiotemporal Geospatial Research", "authors": ["David McVicar", "Brian Avant", "Adrian Gould", "Diego Torrejon", "Charles Della Porta", "Ryan Mukherjee"], "abstract": "BlackSky introduces Smartflow, a cloud-based framework enabling scalable\nspatiotemporal geospatial research built on open-source tools and technologies.\nUsing STAC-compliant catalogs as a common input, heterogeneous geospatial data\ncan be processed into standardized datacubes for analysis and model training.\nModel experimentation is managed using a combination of tools, including\nClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is\nKubernetes, which orchestrates the provisioning and execution of workflows to\nsupport both horizontal and vertical scalability. This combination of features\nmakes Smartflow well-suited for geospatial model development and analysis over\nlarge geographic areas, time scales, and expansive image archives.\n  We also present a novel neural architecture, built using Smartflow, to\nmonitor large geographic areas for heavy construction. Qualitative results\nbased on data from the IARPA Space-based Machine Automated Recognition\nTechnique (SMART) program are presented that show the model is capable of\ndetecting heavy construction throughout all major phases of development.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 15:58:52", "updated": "2025-06-03 15:58:52", "pdf_url": "http://arxiv.org/pdf/2506.03022v1", "comment": null, "doi": "10.1109/IGARSS52108.2023.10283095", "journal_ref": "IGARSS 2023"}
{"arxiv_id": "2506.03032v1", "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment", "authors": ["Junhao Yu", "Yan Zhuang", "YuXuan Sun", "Weibo Gao", "Qi Liu", "Mingyue Cheng", "Zhenya Huang", "Enhong Chen"], "abstract": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions.", "categories": ["cs.AI", "cs.CY", "cs.LG"], "published": "2025-06-03 16:07:54", "updated": "2025-06-03 16:07:54", "pdf_url": "http://arxiv.org/pdf/2506.03032v1", "comment": "24 pages,10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03035v1", "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning", "authors": ["Pierre Lepagnol", "Sahar Ghannay", "Thomas Gerald", "Christophe Servan", "Sophie Rosset"], "abstract": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-03 16:18:45", "updated": "2025-06-03 16:18:45", "pdf_url": "http://arxiv.org/pdf/2506.03035v1", "comment": "Conference paper accepted to INTERSPEECH 2025", "doi": null, "journal_ref": "INTERSPEECH 2025"}
{"arxiv_id": "2506.03046v1", "title": "EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment", "authors": ["Mikolaj Walczak", "Romina Aalishah", "Wyatt Mackey", "Brittany Story", "David L. Boothe Jr.", "Nicholas Waytowich", "Xiaomin Lin", "Tinoosh Mohsenin"], "abstract": "Deep reinforcement learning agents are often fragile while humans remain\nadaptive and flexible to varying scenarios. To bridge this gap, we present\nEDEN, a biologically inspired navigation framework that integrates learned\nentorhinal-like grid cell representations and reinforcement learning to enable\nautonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,\nEDEN allows agents to perform path integration and vector-based navigation\nusing visual and motion sensor data. At the core of EDEN is a grid cell encoder\nthat transforms egocentric motion into periodic spatial codes, producing\nlow-dimensional, interpretable embeddings of position. To generate these\nactivations from raw sensory input, we combine fiducial marker detections in\nthe lightweight MiniWorld simulator and DINO-based visual features in the\nhigh-fidelity Gazebo simulator. These spatial representations serve as input to\na policy trained with Proximal Policy Optimization (PPO), enabling dynamic,\ngoal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid\nprototyping, and Gazebo, which offers realistic physics and perception noise.\nCompared to baseline agents using raw state inputs (e.g., position, velocity)\nor standard convolutional image encoders, EDEN achieves a 99% success rate,\nwithin the simple scenarios, and >94% within complex floorplans with occluded\npaths with more efficient and reliable step-wise navigation. In addition, as a\nreplacement of ground truth activations, we present a trainable Grid Cell\nencoder enabling the development of periodic grid-like patterns from vision and\nmotion sensor data, emulating the development of such patterns within\nbiological mammals. This work represents a step toward biologically grounded\nspatial intelligence in robotics, bridging neural navigation principles with\nreinforcement learning for scalable deployment.", "categories": ["cs.RO", "cs.AI"], "published": "2025-06-03 16:28:33", "updated": "2025-06-03 16:28:33", "pdf_url": "http://arxiv.org/pdf/2506.03046v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03051v1", "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs", "authors": ["Yuval Kansal", "Shmuel Berman", "Lydia Liu"], "abstract": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 16:31:52", "updated": "2025-06-03 16:31:52", "pdf_url": "http://arxiv.org/pdf/2506.03051v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03053v1", "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "abstract": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "published": "2025-06-03 16:33:47", "updated": "2025-06-03 16:33:47", "pdf_url": "http://arxiv.org/pdf/2506.03053v1", "comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03056v1", "title": "Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models", "authors": ["Ram Potham", "Max Harms"], "abstract": "Foundation models (FMs) face a critical safety challenge: as capabilities\nscale, instrumental convergence drives default trajectories toward loss of\nhuman control, potentially culminating in existential catastrophe. Current\nalignment approaches struggle with value specification complexity and fail to\naddress emergent power-seeking behaviors. We propose \"Corrigibility as a\nSingular Target\" (CAST)-designing FMs whose overriding objective is empowering\ndesignated human principals to guide, correct, and control them. This paradigm\nshift from static value-loading to dynamic human empowerment transforms\ninstrumental drives: self-preservation serves only to maintain the principal's\ncontrol; goal modification becomes facilitating principal guidance. We present\na comprehensive empirical research agenda spanning training methodologies\n(RLAIF, SFT, synthetic data generation), scalability testing across model\nsizes, and demonstrations of controlled instructability. Our vision: FMs that\nbecome increasingly responsive to human guidance as capabilities grow, offering\na path to beneficial AI that remains as tool-like as possible, rather than\nsupplanting human judgment. This addresses the core alignment problem at its\nsource, preventing the default trajectory toward misaligned instrumental\nconvergence.", "categories": ["cs.AI", "cs.CY", "cs.LG"], "published": "2025-06-03 16:36:03", "updated": "2025-06-03 16:36:03", "pdf_url": "http://arxiv.org/pdf/2506.03056v1", "comment": "Preprint. This work has been submitted to the Reliable and\n  Responsible Foundation Models Workshop at ICML 2025 for review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03065v1", "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers", "authors": ["Pengtao Chen", "Xianfang Zeng", "Maosen Zhao", "Peng Ye", "Mingzhu Shen", "Wei Cheng", "Gang Yu", "Tao Chen"], "abstract": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-06-03 16:42:37", "updated": "2025-06-03 16:42:37", "pdf_url": "http://arxiv.org/pdf/2506.03065v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03077v1", "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs", "authors": ["Qijun Luo", "Mengqi Li", "Lei Zhao", "Xiao Li"], "abstract": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 16:54:15", "updated": "2025-06-03 16:54:15", "pdf_url": "http://arxiv.org/pdf/2506.03077v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03083v1", "title": "Labelling Data with Unknown References", "authors": ["Adrian de Wynter"], "abstract": "An evaluator is trustworthy when there exists some agreed-upon way to measure\nits performance as a labeller. The two ways to establish trustworthiness are\neither by testing it, or by assuming the evaluator `knows' somehow the way to\nlabel the corpus. However, if labelled references (e.g., a development set) are\nunavailable, neither of these approaches work: the former requires the data,\nand the latter is an assumption, not evidence. To address this, we introduce an\nalgorithm (the `No-Data Algorithm') by which to establish trust in an evaluator\nwithout any existing references. Our algorithm works by successively posing\nchallenges to said evaluator. We show that this is sufficient to establish\ntrustworthiness w.h.p., in such a way that when the evaluator actually knows\nthe way to label the corpus, the No-Data Algorithm accepts its output; and,\nconversely, flags untrustworthy evaluators when these are unable to prove it.\nWe present formal proofs of correctness and limited experiments.", "categories": ["cs.DS", "cs.AI"], "published": "2025-06-03 17:04:22", "updated": "2025-06-03 17:04:22", "pdf_url": "http://arxiv.org/pdf/2506.03083v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03087v1", "title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment", "authors": ["Bin Ma", "Yuyuan Feng", "Minhua Lin", "Enyan Dai"], "abstract": "Graph Neural Networks (GNNs) have become essential tools for analyzing\ngraph-structured data in domains such as drug discovery and financial analysis,\nleading to growing demands for model transparency. Recent advances in\nexplainable GNNs have addressed this need by revealing important subgraphs that\ninfluence predictions, but these explanation mechanisms may inadvertently\nexpose models to security risks. This paper investigates how such explanations\npotentially leak critical decision logic that can be exploited for model\nstealing. We propose {\\method}, a novel stealing framework that integrates\nexplanation alignment for capturing decision logic with guided data\naugmentation for efficient training under limited queries, enabling effective\nreplication of both the predictive behavior and underlying reasoning patterns\nof target models. Experiments on molecular graph datasets demonstrate that our\napproach shows advantages over conventional methods in model stealing. This\nwork highlights important security considerations for the deployment of\nexplainable GNNs in sensitive domains and suggests the need for protective\nmeasures against explanation-based attacks. Our code is available at\nhttps://github.com/beanmah/EGSteal.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-03 17:11:05", "updated": "2025-06-03 17:11:05", "pdf_url": "http://arxiv.org/pdf/2506.03087v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03088v1", "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning", "authors": ["Lloyd Pellatt", "Fotios Drakopoulos", "Shievanie Sabesan", "Nicholas A. Lesica"], "abstract": "The mapping from sound to neural activity that underlies hearing is highly\nnon-linear. The first few stages of this mapping in the cochlea have been\nmodelled successfully, with biophysical models built by hand and, more\nrecently, with DNN models trained on datasets simulated by biophysical models.\nModelling the auditory brain has been a challenge because central auditory\nprocessing is too complex for models to be built by hand, and datasets for\ntraining DNN models directly have not been available. Recent work has taken\nadvantage of large-scale high resolution neural recordings from the auditory\nmidbrain to build a DNN model of normal hearing with great success. But this\nmodel assumes that auditory processing is the same in all brains, and therefore\nit cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space\nof hearing loss directly from recordings of neural activity in the auditory\nmidbrain of healthy and noise exposed animals. With hearing loss parametrised\nby only 6 free parameters per animal, our model accurately predicts 62\\% of the\nexplainable variance in neural responses from normal hearing animals and 68%\nfor hearing impaired animals, within a few percentage points of state of the\nart animal specific models. We demonstrate that the model can be used to\nsimulate realistic activity from out of sample animals by fitting only the\nlearned conditioning parameters with Bayesian optimisation, achieving\ncrossentropy loss within 2% of the optimum in 15-30 iterations. Including more\nanimals in the training data slightly improved the performance on unseen\nanimals. This model will enable future development of parametrised hearing loss\ncompensation models trained to directly restore normal neural coding in hearing\nimpaired brains, which can be quickly fitted for a new user by human in the\nloop optimisation.", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "published": "2025-06-03 17:12:21", "updated": "2025-06-03 17:12:21", "pdf_url": "http://arxiv.org/pdf/2506.03088v1", "comment": "12 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03095v1", "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents", "authors": ["Man Luo", "David Cobbley", "Xin Su", "Shachar Rosenman", "Vasudev Lal", "Shao-Yen Tseng", "Phillip Howard"], "abstract": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.", "categories": ["cs.AI", "cs.CV"], "published": "2025-06-03 17:27:04", "updated": "2025-06-03 17:27:04", "pdf_url": "http://arxiv.org/pdf/2506.03095v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03097v1", "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding", "authors": ["Ashwin Vinod", "Shrey Pandit", "Aditya Vavre", "Linshen Liu"], "abstract": "Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 17:28:00", "updated": "2025-06-03 17:28:00", "pdf_url": "http://arxiv.org/pdf/2506.03097v1", "comment": "Our Code can be found at https://github.com/adityavavre/VidEgoVLM", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03099v1", "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models", "authors": ["Chetwin Low", "Weimin Wang"], "abstract": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/", "categories": ["cs.SD", "cs.AI", "cs.GR"], "published": "2025-06-03 17:29:28", "updated": "2025-06-03 17:29:28", "pdf_url": "http://arxiv.org/pdf/2506.03099v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03100v1", "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds", "authors": ["Yang Guo", "Yutian Tao", "Yifei Ming", "Robert D. Nowak", "Yingyu Liang"], "abstract": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "math.ST", "stat.TH"], "published": "2025-06-03 17:31:53", "updated": "2025-06-03 17:31:53", "pdf_url": "http://arxiv.org/pdf/2506.03100v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03102v1", "title": "Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff", "authors": ["Sophie Greenwood", "Karen Levy", "Solon Barocas", "Hoda Heidari", "Jon Kleinberg"], "abstract": "As AI technologies improve, people are increasingly willing to delegate tasks\nto AI agents. In many cases, the human decision-maker chooses whether to\ndelegate to an AI agent based on properties of the specific instance of the\ndecision-making problem they are facing. Since humans typically lack full\nawareness of all the factors relevant to this choice for a given\ndecision-making instance, they perform a kind of categorization by treating\nindistinguishable instances -- those that have the same observable features --\nas the same. In this paper, we define the problem of designing the optimal\nalgorithmic delegate in the presence of categories. This is an important\ndimension in the design of algorithms to work with humans, since we show that\nthe optimal delegate can be an arbitrarily better teammate than the optimal\nstandalone algorithmic agent. The solution to this optimal delegation problem\nis not obvious: we discover that this problem is fundamentally combinatorial,\nand illustrate the complex relationship between the optimal design and the\nproperties of the decision-making task even in simple settings. Indeed, we show\nthat finding the optimal delegate is computationally hard in general. However,\nwe are able to find efficient algorithms for producing the optimal delegate in\nseveral broad cases of the problem, including when the optimal action may be\ndecomposed into functions of features observed by the human and the algorithm.\nFinally, we run computational experiments to simulate a designer updating an\nalgorithmic delegate over time to be optimized for when it is actually adopted\nby users, and show that while this process does not recover the optimal\ndelegate in general, the resulting delegate often performs quite well.", "categories": ["cs.GT", "cs.AI", "cs.CY"], "published": "2025-06-03 17:36:20", "updated": "2025-06-03 17:36:20", "pdf_url": "http://arxiv.org/pdf/2506.03102v1", "comment": "Accepted at the Twenty-Sixth ACM Conference on Economics and\n  Computation (EC'25)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03106v1", "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chao Yang", "Helen Meng"], "abstract": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 17:39:02", "updated": "2025-06-03 17:39:02", "pdf_url": "http://arxiv.org/pdf/2506.03106v1", "comment": "38 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03133v1", "title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation", "authors": ["Kai Lion", "Liang Zhang", "Bingcong Li", "Niao He"], "abstract": "We show that low-rank adaptation of large-scale models suffers from a low\nstable rank that is well below the linear algebraic rank of the subspace,\ndegrading fine-tuning performance. To mitigate the underutilization of the\nallocated subspace, we propose PoLAR, a parameterization inspired by the polar\ndecomposition that factorizes the low-rank update into two direction matrices\nconstrained to Stiefel manifolds and an unconstrained scale matrix. Our theory\nshows that PoLAR yields an exponentially faster convergence rate on a canonical\nlow-rank adaptation problem. Pairing the parameterization with Riemannian\noptimization leads to consistent gains on three different benchmarks testing\ngeneral language understanding, commonsense reasoning, and mathematical problem\nsolving with base model sizes ranging from 350M to 27B.", "categories": ["cs.LG", "cs.AI", "eess.SP", "math.OC"], "published": "2025-06-03 17:58:19", "updated": "2025-06-03 17:58:19", "pdf_url": "http://arxiv.org/pdf/2506.03133v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03135v1", "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "abstract": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-03 17:58:29", "updated": "2025-06-03 17:58:29", "pdf_url": "http://arxiv.org/pdf/2506.03135v1", "comment": "Project Page: https://qizekun.github.io/omnispatial/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03139v1", "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation", "authors": ["Siqi Chen", "Xinyu Dong", "Haolei Xu", "Xingyu Wu", "Fei Tang", "Hang Zhang", "Yuchen Yan", "Linjuan Wu", "Wenqi Zhang", "Guiyang Hou", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "abstract": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-03 17:58:57", "updated": "2025-06-03 17:58:57", "pdf_url": "http://arxiv.org/pdf/2506.03139v1", "comment": "19 pages,4 figures, Project page:\n  https://zju-real.github.io/SVGenius, Code:\n  https://github.com/ZJU-REAL/SVGenius-Bench", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03143v1", "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "abstract": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-06-03 17:59:08", "updated": "2025-06-03 17:59:08", "pdf_url": "http://arxiv.org/pdf/2506.03143v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03145v1", "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "abstract": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 17:59:18", "updated": "2025-06-03 17:59:18", "pdf_url": "http://arxiv.org/pdf/2506.03145v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03147v1", "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "abstract": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-03 17:59:33", "updated": "2025-06-03 17:59:33", "pdf_url": "http://arxiv.org/pdf/2506.03147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03149v1", "title": "Causal Estimation of Tokenisation Bias", "authors": ["Pietro Lesci", "Clara Meister", "Thomas Hofmann", "Andreas Vlachos", "Tiago Pimentel"], "abstract": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-03 17:59:47", "updated": "2025-06-03 17:59:47", "pdf_url": "http://arxiv.org/pdf/2506.03149v1", "comment": "Published as a conference paper at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03150v1", "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation", "authors": ["Yuanze Lin", "Yi-Wen Chen", "Yi-Hsuan Tsai", "Ronald Clark", "Ming-Hsuan Yang"], "abstract": "Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "published": "2025-06-03 17:59:52", "updated": "2025-06-03 17:59:52", "pdf_url": "http://arxiv.org/pdf/2506.03150v1", "comment": "Tech Report", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02338v1", "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL", "authors": ["Hyungjoo Chae", "Dongjin Kang", "Jihyuk Kim", "Beong-woo Kwak", "Sunghyun Park", "Haeju Park", "Jinyoung Yeo", "Moontae Lee", "Kyungjae Lee"], "abstract": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.", "categories": ["cs.CL"], "published": "2025-06-03 00:29:15", "updated": "2025-06-03 00:29:15", "pdf_url": "http://arxiv.org/pdf/2506.02338v1", "comment": "ACL 2025 Industry", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02347v1", "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation", "authors": ["Jiaming Li", "Yukun Chen", "Ziqiang Liu", "Minghuan Tan", "Lei Zhang", "Yunshui Li", "Run Luo", "Longze Chen", "Jing Luo", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Wei Zhou", "Min Yang"], "abstract": "Stories are central to human culture, serving to share ideas, preserve\ntraditions, and foster connections. Automatic story generation, a key\nadvancement in artificial intelligence (AI), offers new possibilities for\ncreating personalized content, exploring creative ideas, and enhancing\ninteractive experiences. However, existing methods struggle to maintain\nnarrative coherence and logical consistency. This disconnect compromises the\noverall storytelling experience, underscoring the need for substantial\nimprovements. Inspired by human cognitive processes, we introduce Storyteller,\na novel approach that systemically improves the coherence and consistency of\nautomatically generated stories. Storyteller introduces a plot node structure\nbased on linguistically grounded subject verb object (SVO) triplets, which\ncapture essential story events and ensure a consistent logical flow. Unlike\nprevious methods, Storyteller integrates two dynamic modules, the STORYLINE and\nnarrative entity knowledge graph (NEKG),that continuously interact with the\nstory generation process. This integration produces structurally sound,\ncohesive and immersive narratives. Extensive experiments demonstrate that\nStoryteller significantly outperforms existing approaches, achieving an 84.33%\naverage win rate through human preference evaluation. At the same time, it is\nalso far ahead in other aspects including creativity, coherence, engagement,\nand relevance.", "categories": ["cs.CL"], "published": "2025-06-03 00:54:00", "updated": "2025-06-03 00:54:00", "pdf_url": "http://arxiv.org/pdf/2506.02347v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02350v1", "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Zhi Zeng", "Zhixiong Su"], "abstract": "Misinformation detection models often rely on superficial cues (i.e.,\n\\emph{shortcuts}) that correlate with misinformation in training data but fail\nto generalize to the diverse and evolving nature of real-world misinformation.\nThis issue is exacerbated by large language models (LLMs), which can easily\ngenerate convincing misinformation through simple prompts. We introduce\nTruthOverTricks, a unified evaluation paradigm for measuring shortcut learning\nin misinformation detection. TruthOverTricks categorizes shortcut behaviors\ninto intrinsic shortcut induction and extrinsic shortcut injection, and\nevaluates seven representative detectors across 14 popular benchmarks, along\nwith two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.\nEmpirical results reveal that existing detectors suffer severe performance\ndegradation when exposed to both naturally occurring and adversarially crafted\nshortcuts. To address this, we propose SMF, an LLM-augmented data augmentation\nframework that mitigates shortcut reliance through paraphrasing, factual\nsummarization, and sentiment normalization. SMF consistently enhances\nrobustness across 16 benchmarks, encouraging models to rely on deeper semantic\nunderstanding rather than shortcut cues. To promote the development of\nmisinformation detectors, we have published the resources publicly at\nhttps://github.com/whr000001/TruthOverTricks.", "categories": ["cs.CL"], "published": "2025-06-03 01:09:55", "updated": "2025-06-03 01:09:55", "pdf_url": "http://arxiv.org/pdf/2506.02350v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02351v1", "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "abstract": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-06-03 01:10:20", "updated": "2025-06-03 01:10:20", "pdf_url": "http://arxiv.org/pdf/2506.02351v1", "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02372v1", "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output", "authors": ["Hisami Suzuki", "Satoru Katsumata", "Takashi Kodama", "Tetsuro Takahashi", "Kouta Nakayama", "Satoshi Sekine"], "abstract": "In this paper we present AnswerCarefully, a dataset for promoting the safety\nand appropriateness of Japanese LLM outputs. The dataset consists of 1,800\npairs of questions and reference answers, where the questions require special\nattention in answering. It covers a wide range of risk categories established\nin prior English-language datasets, but the data samples are original in that\nthey are manually created to reflect the socio-cultural context of LLM usage in\nJapan. We show that using this dataset for instruction to fine-tune a Japanese\nLLM led to improved output safety without compromising the utility of general\nresponses. We also report the results of a safety evaluation of 12 Japanese\nLLMs using this dataset as a benchmark. Finally, we describe the latest update\non the dataset which provides English translations and annotations of the\nquestions, aimed at facilitating the derivation of similar datasets in\ndifferent languages and regions.", "categories": ["cs.CL"], "published": "2025-06-03 02:18:59", "updated": "2025-06-03 02:18:59", "pdf_url": "http://arxiv.org/pdf/2506.02372v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02378v1", "title": "Exploring Explanations Improves the Robustness of In-Context Learning", "authors": ["Ukyo Honda", "Tatsushi Oka"], "abstract": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-03 02:29:14", "updated": "2025-06-03 02:29:14", "pdf_url": "http://arxiv.org/pdf/2506.02378v1", "comment": "Accepted to ACL 2025 (Main Conference)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02391v1", "title": "Consultant Decoding: Yet Another Synergistic Mechanism", "authors": ["Chuanghao Ding", "Jiaping Wang", "Ziqing Yang", "Xiaoliang Wang", "Dahua Lin", "Cam-Tu Nguyen", "Fei Tan"], "abstract": "The synergistic mechanism based on Speculative Decoding (SD) has garnered\nconsiderable attention as a simple yet effective approach for accelerating the\ninference of large language models (LLMs). Nonetheless, the high rejection\nrates require repeated LLMs calls to validate draft tokens, undermining the\noverall efficiency gain of SD. In this work, we revisit existing verification\nmechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).\nUnlike SD, which relies on a metric derived from importance sampling for\nverification, CD verifies candidate drafts using token-level likelihoods\ncomputed solely by the LLM. CD achieves up to a 2.5-fold increase in inference\nspeed compared to the target model, while maintaining comparable generation\nquality (around 100% of the target model's performance). Interestingly, this is\nachieved by combining models whose parameter sizes differ by two orders of\nmagnitude. In addition, CD reduces the call frequency of the large target model\nto below 10%, particularly in more demanding tasks. CD's performance was even\nfound to surpass that of the large target model, which theoretically represents\nthe upper bound for speculative decoding.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 03:13:27", "updated": "2025-06-03 03:13:27", "pdf_url": "http://arxiv.org/pdf/2506.02391v1", "comment": "ACL 2025 findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02404v1", "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "abstract": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 03:44:26", "updated": "2025-06-03 03:44:26", "pdf_url": "http://arxiv.org/pdf/2506.02404v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02412v1", "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning", "authors": ["Zhengyuan Liu", "Geyu Lin", "Hui Li Tan", "Huayun Zhang", "Yanfeng Lu", "Xiaoxue Gao", "Stella Xin Yin", "He Sun", "Hock Huan Goh", "Lung Hsiang Wong", "Nancy F. Chen"], "abstract": "The integration of generative artificial intelligence into educational\napplications has enhanced personalized and interactive learning experiences,\nand it shows strong potential to promote young learners language acquisition.\nHowever, it is still challenging to ensure consistent and robust performance\nacross different languages and cultural contexts, and kids-friendly design\nrequires simplified instructions, engaging interactions, and age-appropriate\nscaffolding to maintain motivation and optimize learning outcomes. In this\nwork, we introduce SingaKids, a dialogic tutor designed to facilitate language\nlearning through picture description tasks. Our system integrates dense image\ncaptioning, multilingual dialogic interaction, speech understanding, and\nengaging speech generation to create an immersive learning environment in four\nlanguages: English, Mandarin, Malay, and Tamil. We further improve the system\nthrough multilingual pre-training, task-specific tuning, and scaffolding\noptimization. Empirical studies with elementary school students demonstrate\nthat SingaKids provides effective dialogic teaching, benefiting learners at\ndifferent performance levels.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 03:56:45", "updated": "2025-06-03 03:56:45", "pdf_url": "http://arxiv.org/pdf/2506.02412v1", "comment": "ACL 2025 Industry Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02414v1", "title": "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion", "authors": ["Fengjin Li", "Jie Wang", "Yadong Niu", "Yongqing Wang", "Meng Meng", "Jian Luan", "Zhiyong Wu"], "abstract": "Voice Conversion (VC) modifies speech to match a target speaker while\npreserving linguistic content. Traditional methods usually extract speaker\ninformation directly from speech while neglecting the explicit utilization of\nlinguistic content. Since VC fundamentally involves disentangling speaker\nidentity from linguistic content, leveraging structured semantic features could\nenhance conversion performance. However, previous attempts to incorporate\nsemantic features into VC have shown limited effectiveness, motivating the\nintegration of explicit text modeling. We propose StarVC, a unified\nautoregressive VC framework that first predicts text tokens before synthesizing\nacoustic features. The experiments demonstrate that StarVC outperforms\nconventional VC methods in preserving both linguistic content (i.e., WER and\nCER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found\nat: https://thuhcsi.github.io/StarVC/.", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-03 04:00:53", "updated": "2025-06-03 04:00:53", "pdf_url": "http://arxiv.org/pdf/2506.02414v1", "comment": "5 pages, 2 figures, Accepted by Interspeech 2025, Demo:\n  https://thuhcsi.github.io/StarVC/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02425v1", "title": "Gender Inequality in English Textbooks Around the World: an NLP Approach", "authors": ["Tairan Liu"], "abstract": "Textbooks play a critical role in shaping children's understanding of the\nworld. While previous studies have identified gender inequality in individual\ncountries' textbooks, few have examined the issue cross-culturally. This study\napplies natural language processing methods to quantify gender inequality in\nEnglish textbooks from 22 countries across 7 cultural spheres. Metrics include\ncharacter count, firstness (which gender is mentioned first), and TF-IDF word\nassociations by gender. The analysis also identifies gender patterns in proper\nnames appearing in TF-IDF word lists, tests whether large language models can\ndistinguish between gendered word lists, and uses GloVe embeddings to examine\nhow closely keywords associate with each gender. Results show consistent\noverrepresentation of male characters in terms of count, firstness, and named\nentities. All regions exhibit gender inequality, with the Latin cultural sphere\nshowing the least disparity.", "categories": ["cs.CL", "stat.AP"], "published": "2025-06-03 04:16:09", "updated": "2025-06-03 04:16:09", "pdf_url": "http://arxiv.org/pdf/2506.02425v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02426v1", "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "abstract": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at\n\\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "published": "2025-06-03 04:19:47", "updated": "2025-06-03 04:19:47", "pdf_url": "http://arxiv.org/pdf/2506.02426v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02431v1", "title": "From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Gene Louis Kim", "Anshuman Chhabra"], "abstract": "Emotions are a fundamental facet of human experience, varying across\nindividuals, cultural contexts, and nationalities. Given the recent success of\nLarge Language Models (LLMs) as role-playing agents, we examine whether LLMs\nexhibit emotional stereotypes when assigned nationality-specific personas.\nSpecifically, we investigate how different countries are represented in\npre-trained LLMs through emotion attributions and whether these attributions\nalign with cultural norms. Our analysis reveals significant nationality-based\ndifferences, with emotions such as shame, fear, and joy being\ndisproportionately assigned across regions. Furthermore, we observe notable\nmisalignment between LLM-generated and human emotional responses, particularly\nfor negative emotions, highlighting the presence of reductive and potentially\nbiased stereotypes in LLM outputs.", "categories": ["cs.CL"], "published": "2025-06-03 04:35:51", "updated": "2025-06-03 04:35:51", "pdf_url": "http://arxiv.org/pdf/2506.02431v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02442v1", "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "abstract": "This paper presents a systematic evaluation of Large Language Models' (LLMs)\nbehavior on long-tail distributed (encrypted) texts and their safety\nimplications. We introduce a two-dimensional framework for assessing LLM\nsafety: (1) instruction refusal-the ability to reject harmful obfuscated\ninstructions, and (2) generation safety-the suppression of generating harmful\nresponses. Through comprehensive experiments, we demonstrate that models that\npossess capabilities to decrypt ciphers may be susceptible to\nmismatched-generalization attacks: their safety mechanisms fail on at least one\nsafety dimension, leading to unsafe responses or over-refusal. Based on these\nfindings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss\ntheir strengths and limitations. This work contributes to understanding the\nsafety of LLM in long-tail text scenarios and provides directions for\ndeveloping robust safety mechanisms.", "categories": ["cs.CL"], "published": "2025-06-03 05:00:12", "updated": "2025-06-03 05:00:12", "pdf_url": "http://arxiv.org/pdf/2506.02442v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02449v1", "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "abstract": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.", "categories": ["cs.CL", "cs.HC"], "published": "2025-06-03 05:14:11", "updated": "2025-06-03 05:14:11", "pdf_url": "http://arxiv.org/pdf/2506.02449v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02454v1", "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework", "authors": ["Zhaorui Yang", "Bo Pan", "Han Wang", "Yiyao Wang", "Xingyu Liu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "abstract": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 05:18:19", "updated": "2025-06-03 05:18:19", "pdf_url": "http://arxiv.org/pdf/2506.02454v1", "comment": "47 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02460v1", "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework", "authors": ["Yupeng Qi", "Ziyu Lyu", "Min Yang", "Yanlin Wang", "Lu Bai", "Lixin Cui"], "abstract": "As large language models (LLMs) are increasingly applied across various\ndomains, enhancing safety while maintaining the helpfulness of LLMs has become\na critical challenge. Recent studies solve this problem through\nsafety-constrained online preference optimization or safety-constrained offline\npreference optimization. However, the safety-constrained online methods often\nsuffer from excessive safety, which might reduce helpfulness, while the\nsafety-constrained offline methods perform poorly in adaptively balancing\nsafety and helpfulness. To address these limitations, we propose MidPO, a\n\\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness\n\\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference\nenhanced direct preference optimization approach to transform the base model\ninto two independent experts, termed safety and helpfulness experts, and\nfine-tunes the two independent experts for optimal safety or helpfulness\nperformance. Secondly, to achieve an effective balance between safety and\nhelpfulness, MidPO incorporates the two experts into the MoE framework and\ndesigns a dynamic routing mechanism to allocate contributions from each expert\nadaptively. We conduct quantitative and qualitative experiments on three\npopular datasets to demonstrate the proposed MidPO significantly outperforms\nstate-of-the-art approaches in both safety and helpfulness. The code and models\nwill be released.", "categories": ["cs.CL"], "published": "2025-06-03 05:23:09", "updated": "2025-06-03 05:23:09", "pdf_url": "http://arxiv.org/pdf/2506.02460v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02461v1", "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models", "authors": ["Chunkit Chan", "Yauwai Yim", "Hongchuan Zeng", "Zhiying Zou", "Xinyuan Cheng", "Zhifan Sun", "Zheye Deng", "Kawai Chung", "Yuzhuo Ao", "Yixiang Fan", "Cheng Jiayang", "Ercong Nie", "Ginny Y. Wong", "Helmut Schmid", "Hinrich Sch\u00fctze", "Simon See", "Yangqiu Song"], "abstract": "Theory of Mind (ToM), the ability to infer mental states in others, is\npivotal for human social cognition. Existing evaluations of ToM in LLMs are\nlargely limited to English, neglecting the linguistic diversity that shapes\nhuman cognition. This limitation raises a critical question: can LLMs exhibit\nMultilingual Theory of Mind, which is the capacity to reason about mental\nstates across diverse linguistic contexts? To address this gap, we present\nXToM, a rigorously validated multilingual benchmark that evaluates ToM across\nfive languages and incorporates diverse, contextually rich task scenarios.\nUsing XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a\npronounced dissonance: while models excel in multilingual language\nunderstanding, their ToM performance varies across languages. Our findings\nexpose limitations in LLMs' ability to replicate human-like mentalizing across\nlinguistic contexts.", "categories": ["cs.CL"], "published": "2025-06-03 05:23:25", "updated": "2025-06-03 05:23:25", "pdf_url": "http://arxiv.org/pdf/2506.02461v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02475v1", "title": "Comba: Improving Nonlinear RNNs with Closed-loop Control", "authors": ["Jiaxi Hu", "Yongqi Pan", "Jusen Du", "Disen Lan", "Xiaqiang Tang", "Qingsong Wen", "Yuxuan Liang", "Weigao Sun"], "abstract": "Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and\nRWKV-7 have achieved performance improvements by supervising the recurrent\nmemory management through Delta learning rule. Unlike previous state-space\nmodels (e.g., Mamba) and gated linear attentions (e.g., GLA), these models\nintroduce interactions between the recurrent state and the key vector,\nresulting in a nonlinear recursive structure. In this paper, we first introduce\nthe concept of Nonlinear RNNs with a comprehensive analysis on the advantages\nand limitations of these models. Then, based on closed-loop control theory, we\npropose a novel Nonlinear RNN variant named Comba, which adopts a\nscalar-plus-low-rank state transition, with both state feedback and output\nfeedback corrections. We also implement a hardware-efficient chunk-wise\nparallel kernel in Triton and train models with 340M/1.3B parameters on\nlarge-scale corpus. Comba demonstrates its superior performance and computation\nefficiency in both language and vision modeling.", "categories": ["cs.LG", "cs.CL"], "published": "2025-06-03 05:44:50", "updated": "2025-06-03 05:44:50", "pdf_url": "http://arxiv.org/pdf/2506.02475v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02478v1", "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging", "authors": ["Zijian Li", "Xiaocheng Feng", "Huixin Liu", "Yichong Huang", "Ting Liu", "Bing Qin"], "abstract": "With the development of large language models, fine-tuning has emerged as an\neffective method to enhance performance in specific scenarios by injecting\ndomain-specific knowledge. In this context, model merging techniques provide a\nsolution for fusing knowledge from multiple fine-tuning models by combining\ntheir parameters. However, traditional methods often encounter task\ninterference when merging full fine-tuning models, and this problem becomes\neven more evident in parameter-efficient fine-tuning scenarios. In this paper,\nwe introduce an improvement to the RegMean method, which indirectly leverages\nthe training data to approximate the outputs of the linear layers before and\nafter merging. We propose an adaptive merging method called FroM, which\ndirectly measures the model parameters using the Frobenius norm, without any\ntraining data. By introducing an additional hyperparameter for control, FroM\noutperforms baseline methods across various fine-tuning scenarios, alleviating\nthe task interference problem.", "categories": ["cs.CL"], "published": "2025-06-03 05:50:09", "updated": "2025-06-03 05:50:09", "pdf_url": "http://arxiv.org/pdf/2506.02478v1", "comment": "12 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02479v1", "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage", "authors": ["Kalyan Nakka", "Nitesh Saxena"], "abstract": "The inherent risk of generating harmful and unsafe content by Large Language\nModels (LLMs), has highlighted the need for their safety alignment. Various\ntechniques like supervised fine-tuning, reinforcement learning from human\nfeedback, and red-teaming were developed for ensuring the safety alignment of\nLLMs. However, the robustness of these aligned LLMs is always challenged by\nadversarial attacks that exploit unexplored and underlying vulnerabilities of\nthe safety alignment. In this paper, we develop a novel black-box jailbreak\nattack, called BitBypass, that leverages hyphen-separated bitstream camouflage\nfor jailbreaking aligned LLMs. This represents a new direction in jailbreaking\nby exploiting fundamental information representation of data as continuous\nbits, rather than leveraging prompt engineering or adversarial manipulations.\nOur evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude\n3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the\ncapabilities of BitBypass in bypassing their safety alignment and tricking them\ninto generating harmful and unsafe content. Further, we observed that BitBypass\noutperforms several state-of-the-art jailbreak attacks in terms of stealthiness\nand attack success. Overall, these results highlights the effectiveness and\nefficiency of BitBypass in jailbreaking these state-of-the-art LLMs.", "categories": ["cs.CR", "cs.CL"], "published": "2025-06-03 05:51:18", "updated": "2025-06-03 05:51:18", "pdf_url": "http://arxiv.org/pdf/2506.02479v1", "comment": "24 pages, 24 figures, and 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02480v1", "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities", "authors": ["Yifan Duan", "Yihong Tang", "Kehai Chen", "Liqiang Nie", "Min Zhang"], "abstract": "High-quality prompts are crucial for eliciting outstanding performance from\nlarge language models (LLMs) on complex tasks. Existing research has explored\nmodel-driven strategies for prompt optimization. However, these methods often\nsuffer from high computational overhead or require strong optimization\ncapabilities from the model itself, which limits their broad applicability.To\naddress these challenges, we propose ORPP (Optimized Role-Playing Prompt),a\nframework that enhances model performance by optimizing and generating\nrole-playing prompts. The core idea of ORPP is to confine the prompt search\nspace to role-playing scenarios, thereby fully activating the model's intrinsic\ncapabilities through carefully crafted, high-quality role-playing prompts.\nSpecifically, ORPP first performs iterative optimization on a small subset of\ntraining samples to generate high-quality role-playing prompts. Then,\nleveraging the model's few-shot learning capability, it transfers the\noptimization experience to efficiently generate suitable prompts for the\nremaining samples.Our experimental results show that ORPP not only matches but\nin most cases surpasses existing mainstream prompt optimization methods in\nterms of performance. Notably, ORPP demonstrates superior \"plug-and-play\"\ncapability. In most cases, it can be integrated with various other prompt\nmethods and further enhance their effectiveness.", "categories": ["cs.CL"], "published": "2025-06-03 05:51:35", "updated": "2025-06-03 05:51:35", "pdf_url": "http://arxiv.org/pdf/2506.02480v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02481v1", "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths", "authors": ["Inderjeet Nair", "Lu Wang"], "abstract": "Evaluations of LLMs' ethical risks and value inclinations often rely on\nshort-form surveys and psychometric tests, yet real-world use involves\nlong-form, open-ended responses -- leaving value-related risks and preferences\nin practical settings largely underexplored. In this work, we ask: Do value\npreferences inferred from short-form tests align with those expressed in\nlong-form outputs? To address this question, we compare value preferences\nelicited from short-form reactions and long-form responses, varying the number\nof arguments in the latter to capture users' differing verbosity preferences.\nAnalyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),\nwe find (1) a weak correlation between value preferences inferred from\nshort-form and long-form responses across varying argument counts, and (2)\nsimilarly weak correlation between preferences derived from any two distinct\nlong-form generation settings. (3) Alignment yields only modest gains in the\nconsistency of value expression. Further, we examine how long-form generation\nattributes relate to value preferences, finding that argument specificity\nnegatively correlates with preference strength, while representation across\nscenarios shows a positive correlation. Our findings underscore the need for\nmore robust methods to ensure consistent value expression across diverse\napplications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 05:52:03", "updated": "2025-06-03 05:52:03", "pdf_url": "http://arxiv.org/pdf/2506.02481v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02483v1", "title": "Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks", "authors": ["Sina Bagheri Nezhad", "Ameeta Agrawal"], "abstract": "Large language models (LLMs) often struggle to perform multi-target reasoning\nin long-context scenarios where relevant information is scattered across\nextensive documents. To address this challenge, we introduce NeuroSymbolic\nAugmented Reasoning (NSAR), which combines the benefits of neural and symbolic\nreasoning during inference. NSAR explicitly extracts symbolic facts from text\nand generates executable Python code to handle complex reasoning steps. Through\nextensive experiments across seven languages and diverse context lengths, we\ndemonstrate that NSAR significantly outperforms both a vanilla RAG baseline and\nadvanced prompting strategies in accurately identifying and synthesizing\nmultiple pieces of information. Our results highlight the effectiveness of\ncombining explicit symbolic operations with neural inference for robust,\ninterpretable, and scalable reasoning in multilingual settings.", "categories": ["cs.CL"], "published": "2025-06-03 05:54:20", "updated": "2025-06-03 05:54:20", "pdf_url": "http://arxiv.org/pdf/2506.02483v1", "comment": "Accepted at 19th Conference on Neurosymbolic Learning and Reasoning\n  (NeSy 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02494v1", "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "abstract": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-06-03 06:17:16", "updated": "2025-06-03 06:17:16", "pdf_url": "http://arxiv.org/pdf/2506.02494v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02503v1", "title": "KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG", "authors": ["Yongjian Li", "HaoCheng Chu", "Yukun Yan", "Zhenghao Liu", "Shi Yu", "Zheni Zeng", "Ruobing Wang", "Sen Song", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess broader knowledge sources, yet factual inconsistencies persist due to\nnoise in retrieved documents-even with advanced retrieval methods. We\ndemonstrate that enhancing generative models' capacity to process noisy content\nis equally critical for robust performance. In this paper, we present KARE-RAG\n(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge\nutilization through three key innovations: (1) structured knowledge\nrepresentations that facilitate error detection during training, (2) Dense\nDirect Preference Optimization (DDPO)-a refined training objective that\nprioritizes correction of critical errors, and (3) a contrastive data\ngeneration pipeline that maintains semantic consistency while rectifying\nfactual inaccuracies. Experiments show our method significantly enhances\nstandard RAG pipelines across model scales, improving both in-domain and\nout-of-domain task performance without compromising general capabilities.\nNotably, these gains are achieved with modest training data, suggesting\ndata-efficient optimization is possible through targeted learning strategies.\nOur findings establish a new direction for RAG improvement: by improving how\nmodels learn to process retrieved content, we can enhance performance across\ndiverse inference paradigms. All data and code will be publicly available on\nGithub.", "categories": ["cs.CL"], "published": "2025-06-03 06:31:17", "updated": "2025-06-03 06:31:17", "pdf_url": "http://arxiv.org/pdf/2506.02503v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02510v1", "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset", "authors": ["Jie Zhu", "Junhui Li", "Yalong Wen", "Xiandong Li", "Lifan Guo", "Feng Chen"], "abstract": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\n$\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\n$\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 06:41:09", "updated": "2025-06-03 06:41:09", "pdf_url": "http://arxiv.org/pdf/2506.02510v1", "comment": "Accepted by ACL-2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02515v1", "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning", "authors": ["Zhuohan Xie", "Dhruv Sahnan", "Debopriyo Banerjee", "Georgi Georgiev", "Rushil Thareja", "Hachem Madmoun", "Jinyan Su", "Aaryamonvikram Singh", "Yuxia Wang", "Rui Xing", "Fajri Koto", "Haonan Li", "Ivan Koychev", "Tanmoy Chakraborty", "Salem Lahlou", "Veselin Stoyanov", "Preslav Nakov"], "abstract": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-03 06:44:42", "updated": "2025-06-03 06:44:42", "pdf_url": "http://arxiv.org/pdf/2506.02515v1", "comment": "15 pages, 8 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02519v1", "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning", "authors": ["Sohan Patnaik", "Milan Aggarwal", "Sumit Bhatia", "Balaji Krishnamurthy"], "abstract": "LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions\nby generating step-by-step rationales. Prior works have utilized this\ncapability to improve smaller and cheaper LMs (say, with 7B parameters).\nHowever, various practical constraints, such as copyright and legal issues,\nowing to lack of transparency in the pre-training data of large (often closed)\nmodels, prevent their use in commercial settings. Little focus has been given\nto improving the innate reasoning ability of smaller models without distilling\ninformation from larger LLMs. To address this, we propose COLLATE, a trainable\nframework that tunes a (small) LLM to generate those outputs from a pool of\ndiverse rationales that selectively improves the downstream task. COLLATE\nenforces multiple instances of the same LLM to exhibit distinct behavior and\nemploys them to generate rationales to obtain diverse outputs. The LLM is then\ntuned via preference optimization to choose the candidate rationale which\nmaximizes the likelihood of ground-truth answer. COLLATE outperforms several\ntrainable and prompting baselines on 5 datasets across 3 domains: maths problem\nsolving, natural language inference, and commonsense reasoning. We show the eff\nicacy of COLLATE on LLMs from different model families across varying parameter\nscales (1B to 8B) and demonstrate the benefit of multiple rationale providers\nguided by the end task through ablations. Code is released here\n(https://github.com/Sohanpatnaik106/collate).", "categories": ["cs.CL"], "published": "2025-06-03 06:50:08", "updated": "2025-06-03 06:50:08", "pdf_url": "http://arxiv.org/pdf/2506.02519v1", "comment": "Accepted at ACL Main 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02527v1", "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base", "authors": ["Yingying Zhuang", "Aman Gupta", "Anurag Beniwal"], "abstract": "Multilingual information retrieval has emerged as powerful tools for\nexpanding knowledge sharing across languages. On the other hand, resources on\nhigh quality knowledge base are often scarce and in limited languages,\ntherefore an effective embedding model to transform sentences from different\nlanguages into a feature vector space same as the knowledge base language\nbecomes the key ingredient for cross language knowledge sharing, especially to\ntransfer knowledge available in high-resource languages to low-resource ones.\nIn this paper we propose a novel strategy to fine-tune multilingual embedding\nmodels with weighted sampling for contrastive learning, enabling multilingual\ninformation retrieval with a monolingual knowledge base. We demonstrate that\nthe weighted sampling strategy produces performance gains compared to standard\nones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our\nproposed methodology is language agnostic and applicable for both multilingual\nand code switching use cases.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-03 07:05:49", "updated": "2025-06-03 07:05:49", "pdf_url": "http://arxiv.org/pdf/2506.02527v1", "comment": "6 pages, accepted at GENNEXT@SIGIR25", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02529v1", "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs", "authors": ["Nguyen-Khang Le", "Quan Minh Bui", "Minh Ngoc Nguyen", "Hiep Nguyen", "Trung Vo", "Son T. Luu", "Shoshin Nomura", "Minh Le Nguyen"], "abstract": "Web applications are critical to modern software ecosystems, yet ensuring\ntheir reliability remains challenging due to the complexity and dynamic nature\nof web interfaces. Recent advances in large language models (LLMs) have shown\npromise in automating complex tasks, but limitations persist in handling\ndynamic navigation flows and complex form interactions. This paper presents an\nautomated system for generating test cases for two key aspects of web\napplication testing: site navigation and form filling. For site navigation, the\nsystem employs screen transition graphs and LLMs to model navigation flows and\ngenerate test scenarios. For form filling, it uses state graphs to handle\nconditional forms and automates Selenium script generation. Key contributions\ninclude: (1) a novel integration of graph structures and LLMs for site\nnavigation testing, (2) a state graph-based approach for automating\nform-filling test cases, and (3) a comprehensive dataset for evaluating\nform-interaction testing. Experimental results demonstrate the system's\neffectiveness in improving test coverage and robustness, advancing the state of\nweb application testing.", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "published": "2025-06-03 07:08:21", "updated": "2025-06-03 07:08:21", "pdf_url": "http://arxiv.org/pdf/2506.02529v1", "comment": "Published in the Proceedings of JSAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02532v1", "title": "ReasoningFlow: Semantic Structure of Complex Reasoning Traces", "authors": ["Jinu Lee", "Sagnik Mukherjee", "Dilek Hakkani-Tur", "Julia Hockenmaier"], "abstract": "Large reasoning models (LRMs) generate complex reasoning traces with\nplanning, reflection, verification, and backtracking. In this work, we\nintroduce ReasoningFlow, a unified schema for analyzing the semantic structures\nof these complex traces. ReasoningFlow parses traces into directed acyclic\ngraphs, enabling the characterization of distinct reasoning patterns as\nsubgraph structures. This human-interpretable representation offers promising\napplications in understanding, evaluating, and enhancing the reasoning\nprocesses of LRMs.", "categories": ["cs.CL"], "published": "2025-06-03 07:11:34", "updated": "2025-06-03 07:11:34", "pdf_url": "http://arxiv.org/pdf/2506.02532v1", "comment": "10 pages, 6 figures. ArgMining 2025 Workshop (Non-archival) @ ACL\n  2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02533v1", "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Carina Weinmann", "Marike Bormann", "Mira Warne", "Stefan Harmeling"], "abstract": "Political online participation in the form of discussing political issues and\nexchanging opinions among citizens is gaining importance with more and more\nformats being held digitally. To come to a decision, a careful discussion and\nconsideration of opinions and a civil exchange of arguments, which is defined\nas the act of deliberation, is desirable. The quality of discussions and\nparticipation processes in terms of their deliberativeness highly depends on\nthe design of platforms and processes. To facilitate online communication for\nboth participants and initiators, machine learning methods offer a lot of\npotential. In this work we want to showcase which issues occur in political\nonline discussions and how machine learning can be used to counteract these\nissues and enhance deliberation.", "categories": ["cs.CL", "cs.HC"], "published": "2025-06-03 07:11:49", "updated": "2025-06-03 07:11:49", "pdf_url": "http://arxiv.org/pdf/2506.02533v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02536v1", "title": "Answer Convergence as a Signal for Early Stopping in Reasoning", "authors": ["Xin Liu", "Lu Wang"], "abstract": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.", "categories": ["cs.CL"], "published": "2025-06-03 07:20:54", "updated": "2025-06-03 07:20:54", "pdf_url": "http://arxiv.org/pdf/2506.02536v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02544v1", "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "abstract": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge\n\\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG}\n(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles\ninconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage\npipeline: it first generates an internal response from parametric knowledge,\nthen selects the most relevant multimodal evidence via joint similarity\nassessment, generates an external response, and finally integrates both to\nproduce a reliable answer. Additionally, a specialized training paradigm\nenhances knowledge source discrimination, multimodal integration, and unified\nanswer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG\nachieves substantial improvements over baseline methods, achieving 5.6\\% and\n9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We\nrelease code and data at\n\\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 07:32:40", "updated": "2025-06-03 07:32:40", "pdf_url": "http://arxiv.org/pdf/2506.02544v1", "comment": "Accepted to ACL 2025 Main", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02553v1", "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective", "authors": ["Shenghua He", "Tian Xia", "Xuan Zhou", "Hui Wei"], "abstract": "We study a common challenge in reinforcement learning for large language\nmodels (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,\nintermediate token generations) receive zero task-specific immediate reward,\nwhile only the final token receives a reward for the entire response. This\nassumption arises frequently in practice, as precise token-level rewards are\noften difficult or infeasible to obtain in LLM applications. In this work, we\nprovide a unifying theoretical perspective. We introduce the Trajectory Policy\nGradient Theorem, which shows that the policy gradient based on true, unknown\ntoken-level rewards can be unbiasedly estimated using only a response-level\nreward model, regardless of whether the Zero-Reward Assumption holds or not,\nfor algorithms in the REINFORCE and Actor-Critic families. This result reveals\nthat widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess\nthe capacity to model token-level reward signals, offering a theoretical\njustification for response-level reward approaches. Our findings pave the way\nfor more practical, efficient LLM fine-tuning, allowing developers to treat\ntraining algorithms as black boxes and focus on improving the response-level\nreward model with auxiliary sub-models. We also offer a detailed analysis of\npopular RL and non-RL methods, comparing their theoretical foundations and\npractical advantages across common LLM tasks. Finally, we propose a new\nalgorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically\ngrounded method that is simpler than PPO, matches GRPO in memory efficiency,\nand holds promise for broad applicability.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-03 07:44:31", "updated": "2025-06-03 07:44:31", "pdf_url": "http://arxiv.org/pdf/2506.02553v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02561v1", "title": "Pruning General Large Language Models into Customized Expert Models", "authors": ["Yirao Zhao", "Guizhen Chen", "Kenji Kawaguchi", "Lidong Bing", "Wenxuan Zhang"], "abstract": "Large language models (LLMs) have revolutionized natural language processing,\nyet their substantial model sizes often require substantial computational\nresources. To preserve computing resources and accelerate inference speed, it\nis crucial to prune redundant parameters, especially for experienced users who\noften need compact expert models tailored to specific downstream scenarios.\nHowever, most existing pruning methods focus on preserving the model's general\ncapabilities, often requiring extensive post-training or suffering from\ndegraded performance due to coarse-grained pruning. In this work, we design a\n$\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to\nprune a large general model into a smaller lightweight expert model, which is\npositioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying\nand pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates\nexpert models without any post-training. Our experiments demonstrate that\n$\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal\nloss in both expert and general capabilities across various models from\ndifferent model families and sizes.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 07:47:30", "updated": "2025-06-03 07:47:30", "pdf_url": "http://arxiv.org/pdf/2506.02561v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02573v1", "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages", "authors": ["Muhammad Falensi Azmi", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "abstract": "Although region-specific large language models (LLMs) are increasingly\ndeveloped, their safety remains underexplored, particularly in culturally\ndiverse settings like Indonesia, where sensitivity to local norms is essential\nand highly valued by the community. In this work, we present IndoSafety, the\nfirst high-quality, human-verified safety evaluation dataset tailored for the\nIndonesian context, covering five language varieties: formal and colloquial\nIndonesian, along with three major local languages: Javanese, Sundanese, and\nMinangkabau. IndoSafety is constructed by extending prior safety frameworks to\ndevelop a taxonomy that captures Indonesia's sociocultural context. We find\nthat existing Indonesian-centric LLMs often generate unsafe outputs,\nparticularly in colloquial and local language settings, while fine-tuning on\nIndoSafety significantly improves safety while preserving task performance. Our\nwork highlights the critical need for culturally grounded safety evaluation and\nprovides a concrete step toward responsible LLM deployment in multilingual\nsettings. Warning: This paper contains example data that may be offensive,\nharmful, or biased.", "categories": ["cs.CL"], "published": "2025-06-03 07:53:55", "updated": "2025-06-03 07:53:55", "pdf_url": "http://arxiv.org/pdf/2506.02573v1", "comment": "25 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02584v1", "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning", "authors": ["Sarenne Wallbridge", "Christoph Minixhofer", "Catherine Lai", "Peter Bell"], "abstract": "People exploit the predictability of lexical structures during text\ncomprehension. Though predictable structure is also present in speech, the\ndegree to which prosody, e.g. intonation, tempo, and loudness, contributes to\nsuch structure independently of the lexical content is unclear. This study\nleverages self-supervised learning (SSL) to examine the temporal granularity of\nstructures in the acoustic correlates of prosody. Representations from our\nproposed Masked Prosody Model can predict perceptual labels dependent on local\ninformation, such as word boundaries, but provide the most value for labels\ninvolving longer-term structures, like emotion recognition. Probing experiments\nacross various perceptual labels show strong relative gains over untransformed\npitch, energy, and voice activity features. Our results reveal the importance\nof SSL training objective timescale and highlight the value of complex\nSSL-encoded structures compared to more constrained classical structures.", "categories": ["cs.CL", "cs.AI", "eess.AS"], "published": "2025-06-03 08:04:03", "updated": "2025-06-03 08:04:03", "pdf_url": "http://arxiv.org/pdf/2506.02584v1", "comment": "Accepted at INTERSPEECH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02589v1", "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM", "authors": ["Maria Levchenko"], "abstract": "This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements.", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; H.3.3"], "published": "2025-06-03 08:11:16", "updated": "2025-06-03 08:11:16", "pdf_url": "http://arxiv.org/pdf/2506.02589v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02590v1", "title": "Synthetic Speech Source Tracing using Metric Learning", "authors": ["Dimitrios Koutsianos", "Stavros Zacharopoulos", "Yannis Panagakis", "Themos Stafylakis"], "abstract": "This paper addresses source tracing in synthetic speech-identifying\ngenerative systems behind manipulated audio via speaker recognition-inspired\npipelines. While prior work focuses on spoofing detection, source tracing lacks\nrobust solutions. We evaluate two approaches: classification-based and\nmetric-learning. We tested our methods on the MLAADv5 benchmark using ResNet\nand self-supervised learning (SSL) backbones. The results show that ResNet\nachieves competitive performance with the metric learning approach, matching\nand even exceeding SSL-based systems. Our work demonstrates ResNet's viability\nfor source tracing while underscoring the need to optimize SSL representations\nfor this task. Our work bridges speaker recognition methodologies with audio\nforensic challenges, offering new directions for combating synthetic media\nmanipulation.", "categories": ["cs.SD", "cs.CL"], "published": "2025-06-03 08:12:15", "updated": "2025-06-03 08:12:15", "pdf_url": "http://arxiv.org/pdf/2506.02590v1", "comment": "Submitted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02591v1", "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures", "authors": ["Minh Duc Bui", "Kyung Eun Park", "Goran Glava\u0161", "Fabian David Schmidt", "Katharina von der Wense"], "abstract": "Measurement systems (e.g., currencies) differ across cultures, but the\nconversions between them are well defined so that humans can state facts using\nany measurement system of their choice. Being available to users from diverse\ncultural backgrounds, large language models (LLMs) should also be able to\nprovide accurate information irrespective of the measurement system at hand.\nUsing newly compiled datasets we test if this is the case for seven open-source\nLLMs, addressing three key research questions: (RQ1) What is the default system\nused by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their\naccuracy vary across different measurement systems? (RQ3) Can LLMs mitigate\npotential challenges w.r.t. underrepresented systems via reasoning? Our\nfindings show that LLMs default to the measurement system predominantly used in\nthe data. Additionally, we observe considerable instability and variance in\nperformance across different measurement systems. While this instability can in\npart be mitigated by employing reasoning methods such as chain-of-thought\n(CoT), this implies longer responses and thereby significantly increases\ntest-time compute (and inference costs), marginalizing users from cultural\nbackgrounds that use underrepresented measurement systems.", "categories": ["cs.CL"], "published": "2025-06-03 08:12:28", "updated": "2025-06-03 08:12:28", "pdf_url": "http://arxiv.org/pdf/2506.02591v1", "comment": "Accepted to ACL 2025 Main (Camera-Ready Version)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02592v1", "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments", "authors": ["Zhi-Yuan Chen", "Hao Wang", "Xinyu Zhang", "Enrui Hu", "Yankai Lin"], "abstract": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.", "categories": ["cs.CL"], "published": "2025-06-03 08:12:47", "updated": "2025-06-03 08:12:47", "pdf_url": "http://arxiv.org/pdf/2506.02592v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02596v1", "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing", "authors": ["Fan Gao", "Dongyuan Li", "Ding Xia", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Baojun Wang"], "abstract": "Chinese essay writing and its evaluation are critical in educational\ncontexts, yet the capabilities of Large Language Models (LLMs) in this domain\nremain largely underexplored. Existing benchmarks often rely on coarse-grained\ntext quality metrics, largely overlooking the structural and rhetorical\ncomplexities of Chinese essays, particularly across diverse genres. To address\nthis gap, we propose \\benchName, a multi-genre benchmark specifically designed\nfor Chinese essay writing across four major genres: Argumentative, Narrative,\nDescriptive, and Expository. We curate and refine a total of 728 real-world\nprompts to ensure authenticity and meticulously categorize them into the\n\\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing\nscenarios. To reliably evaluate generated essays, we develop a fine-grained,\ngenre-specific scoring framework that hierarchically aggregates scores. We\nfurther validate our evaluation protocol through a comprehensive human\nagreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their\nstrengths and limitations across genres and instruction types. With \\benchName,\nwe aim to advance LLM-based Chinese essay evaluation and inspire future\nresearch on improving essay generation in educational settings.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 08:14:46", "updated": "2025-06-03 08:14:46", "pdf_url": "http://arxiv.org/pdf/2506.02596v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02627v1", "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning", "authors": ["\u00d6mer Tarik \u00d6zyilmaz", "Matt Coler", "Matias Valdenegro-Toro"], "abstract": "Although commercial Arabic automatic speech recognition (ASR) systems support\nModern Standard Arabic (MSA), they struggle with dialectal speech. We\ninvestigate the effect of fine-tuning OpenAI's Whisper on five major Arabic\ndialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common\nVoice for MSA and the MASC dataset for dialectal speech. We evaluate MSA\ntraining size effects, benefits of pre-training on MSA data, and\ndialect-specific versus dialect-pooled models. We find that small amounts of\nMSA fine-tuning data yield substantial improvements for smaller models,\nmatching larger non-fine-tuned models. While MSA pre-training shows minimal\nbenefit, suggesting limited shared features between MSA and dialects, our\ndialect-pooled models perform comparably to dialect-specific ones. This\nindicates that pooling dialectal data, when properly balanced, can help address\ndata scarcity in low-resource ASR without significant performance loss.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-03 08:41:49", "updated": "2025-06-03 08:41:49", "pdf_url": "http://arxiv.org/pdf/2506.02627v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02659v1", "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs", "authors": ["Manon Reusens", "Bart Baesens", "David Jurgens"], "abstract": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub.", "categories": ["cs.CL"], "published": "2025-06-03 09:12:23", "updated": "2025-06-03 09:12:23", "pdf_url": "http://arxiv.org/pdf/2506.02659v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02672v1", "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "authors": ["Shihan Dou", "Ming Zhang", "Chenhao Huang", "Jiayi Chen", "Feng Chen", "Shichun Liu", "Yan Liu", "Chenxiao Liu", "Cheng Zhong", "Zongzhang Zhang", "Tao Gui", "Chao Xin", "Wei Chengzhi", "Lin Yan", "Qi Zhang", "Xuanjing Huang"], "abstract": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 09:18:33", "updated": "2025-06-03 09:18:33", "pdf_url": "http://arxiv.org/pdf/2506.02672v1", "comment": "47 pages, 24 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02678v1", "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon.", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "published": "2025-06-03 09:23:41", "updated": "2025-06-03 09:23:41", "pdf_url": "http://arxiv.org/pdf/2506.02678v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02683v1", "title": "Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints", "authors": ["Zhengdong Lu", "Weikai Lu", "Yiling Tao", "Yun Dai", "ZiXuan Chen", "Huiping Zhuang", "Cen Chen", "Hao Peng", "Ziqian Zeng"], "abstract": "Despite significant advances in Large Language Models (LLMs), planning tasks\nstill present challenges for LLM-based agents. Existing planning methods face\ntwo key limitations: heavy constraints and cascading errors. To address these\nlimitations, we propose a novel parallel planning paradigm, which Decomposes,\nPlans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).\nSpecifically, DPPM decomposes the complex task based on constraints into\nsubtasks, generates the subplan for each subtask in parallel, and merges them\ninto a global plan. In addition, our approach incorporates a verification and\nrefinement module, enabling error correction and conflict resolution.\nExperimental results demonstrate that DPPM significantly outperforms existing\nmethods in travel planning tasks.", "categories": ["cs.CL"], "published": "2025-06-03 09:33:13", "updated": "2025-06-03 09:33:13", "pdf_url": "http://arxiv.org/pdf/2506.02683v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02689v1", "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching", "authors": ["Liang Yue", "Yihong Tang", "Kehai Chen", "Jie Liu", "Min Zhang"], "abstract": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'\ninstruction-following capabilities and task-specific performance. However,\nobtaining high-quality fine-tuning data for large models is challenging due to\ndata collection difficulties and high production costs. To address this, we\npropose MASTER, a novel data augmentation method that enriches original data\nthrough interactions among multiple agents with varying cognitive levels. We\nsimulate three pedagogically grounded teaching scenarios, leveraging\nmulti-agent conversations to generate high-quality teacher-student interaction\ndata. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented\nfrom existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.\nExperiments show that models fine-tuned with BOOST-QA perform excellently\nacross multiple benchmarks, demonstrating strong multitask generalization.\nNotably, MASTER significantly improves models' reasoning abilities in complex\ntasks, providing valuable insights for future research.", "categories": ["cs.CL"], "published": "2025-06-03 09:41:35", "updated": "2025-06-03 09:41:35", "pdf_url": "http://arxiv.org/pdf/2506.02689v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02701v1", "title": "On Entity Identification in Language Models", "authors": ["Masaki Sakata", "Sho Yokoi", "Benjamin Heinzerling", "Takumi Ito", "Kentaro Inui"], "abstract": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information.", "categories": ["cs.CL"], "published": "2025-06-03 09:55:21", "updated": "2025-06-03 09:55:21", "pdf_url": "http://arxiv.org/pdf/2506.02701v1", "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02708v1", "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation", "authors": ["Naoto Tanji", "Toshihiko Yamasaki"], "abstract": "Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations.", "categories": ["cs.CV", "cs.CL"], "published": "2025-06-03 10:04:19", "updated": "2025-06-03 10:04:19", "pdf_url": "http://arxiv.org/pdf/2506.02708v1", "comment": "Accepted to ICIP2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02720v1", "title": "Benchmarking and Advancing Large Language Models for Local Life Services", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "abstract": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-03 10:18:19", "updated": "2025-06-03 10:18:19", "pdf_url": "http://arxiv.org/pdf/2506.02720v1", "comment": "KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02726v1", "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "authors": ["Qihang Yan", "Xinyu Zhang", "Luming Guo", "Qi Zhang", "Feifan Liu"], "abstract": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; H.3.3"], "published": "2025-06-03 10:36:38", "updated": "2025-06-03 10:36:38", "pdf_url": "http://arxiv.org/pdf/2506.02726v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02730v1", "title": "An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models", "authors": ["Po-Chieh Yu"], "abstract": "We present an exploratory framework to test whether noise-like input can\ninduce structured responses in language models. Instead of assuming that\nextraterrestrial signals must be decoded, we evaluate whether inputs can\ntrigger linguistic behavior in generative systems. This shifts the focus from\ndecoding to viewing structured output as a sign of underlying regularity in the\ninput. We tested GPT-2 small, a 117M-parameter model trained on English text,\nusing four types of acoustic input: human speech, humpback whale vocalizations,\nPhylloscopus trochilus birdsong, and algorithmically generated white noise. All\ninputs were treated as noise-like, without any assumed symbolic encoding. To\nassess reactivity, we defined a composite score called Semantic Induction\nPotential (SIP), combining entropy, syntax coherence, compression gain, and\nrepetition penalty. Results showed that whale and bird vocalizations had higher\nSIP scores than white noise, while human speech triggered only moderate\nresponses. This suggests that language models may detect latent structure even\nin data without conventional semantics. We propose that this approach could\ncomplement traditional SETI methods, especially in cases where communicative\nintent is unknown. Generative reactivity may offer a different way to identify\ndata worth closer attention.", "categories": ["astro-ph.IM", "cs.CL"], "published": "2025-06-03 10:46:57", "updated": "2025-06-03 10:46:57", "pdf_url": "http://arxiv.org/pdf/2506.02730v1", "comment": "submitted to the International Journal of Astrobiology", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02740v1", "title": "Stereotypical gender actions can be extracted from Web text", "authors": ["Ama\u00e7 Herda\u011fdelen", "Marco Baroni"], "abstract": "We extracted gender-specific actions from text corpora and Twitter, and\ncompared them to stereotypical expectations of people. We used Open Mind Common\nSense (OMCS), a commonsense knowledge repository, to focus on actions that are\npertinent to common sense and daily life of humans. We use the gender\ninformation of Twitter users and Web-corpus-based pronoun/name gender\nheuristics to compute the gender bias of the actions. With high recall, we\nobtained a Spearman correlation of 0.47 between corpus-based predictions and a\nhuman gold standard, and an area under the ROC curve of 0.76 when predicting\nthe polarity of the gold standard. We conclude that it is feasible to use\nnatural text (and a Twitter-derived corpus in particular) in order to augment\ncommonsense repositories with the stereotypical gender expectations of actions.\nWe also present a dataset of 441 commonsense actions with human judges' ratings\non whether the action is typically/slightly masculine/feminine (or neutral),\nand another larger dataset of 21,442 actions automatically rated by the methods\nwe investigate in this study.", "categories": ["cs.CL"], "published": "2025-06-03 10:55:00", "updated": "2025-06-03 10:55:00", "pdf_url": "http://arxiv.org/pdf/2506.02740v1", "comment": null, "doi": null, "journal_ref": "Herda\\u{g}delen, Ama\\c{c}, and Marco Baroni. \"Stereotypical gender\n  actions can be extracted from web text.\" Journal of the American Society for\n  Information Science and Technology 62.9 (2011): 1741-1749"}
{"arxiv_id": "2506.02753v1", "title": "Multi-task Learning with Active Learning for Arabic Offensive Speech Detection", "authors": ["Aisha Alansari", "Hamzah Luqman"], "abstract": "The rapid growth of social media has amplified the spread of offensive,\nviolent, and vulgar speech, which poses serious societal and cybersecurity\nconcerns. Detecting such content in Arabic text is particularly complex due to\nlimited labeled data, dialectal variations, and the language's inherent\ncomplexity. This paper proposes a novel framework that integrates multi-task\nlearning (MTL) with active learning to enhance offensive speech detection in\nArabic social media text. By jointly training on two auxiliary tasks, violent\nand vulgar speech, the model leverages shared representations to improve the\ndetection accuracy of the offensive speech. Our approach dynamically adjusts\ntask weights during training to balance the contribution of each task and\noptimize performance. To address the scarcity of labeled data, we employ an\nactive learning strategy through several uncertainty sampling techniques to\niteratively select the most informative samples for model training. We also\nintroduce weighted emoji handling to better capture semantic cues. Experimental\nresults on the OSACT2022 dataset show that the proposed framework achieves a\nstate-of-the-art macro F1-score of 85.42%, outperforming existing methods while\nusing significantly fewer fine-tuning samples. The findings of this study\nhighlight the potential of integrating MTL with active learning for efficient\nand accurate offensive language detection in resource-constrained settings.", "categories": ["cs.CL"], "published": "2025-06-03 11:17:03", "updated": "2025-06-03 11:17:03", "pdf_url": "http://arxiv.org/pdf/2506.02753v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02758v1", "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs", "authors": ["Stefano Bann\u00f2", "Kate Knill", "Mark Gales"], "abstract": "Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 11:23:57", "updated": "2025-06-03 11:23:57", "pdf_url": "http://arxiv.org/pdf/2506.02758v1", "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02761v1", "title": "Rethinking Machine Unlearning in Image Generation Models", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "abstract": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU.", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "published": "2025-06-03 11:25:14", "updated": "2025-06-03 11:25:14", "pdf_url": "http://arxiv.org/pdf/2506.02761v1", "comment": "Accepted by ACM CCS 2025", "doi": null, "journal_ref": "ACM Conference on Computer and Communications Security (CCS 2025)"}
{"arxiv_id": "2506.02803v1", "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "abstract": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "categories": ["cs.CL", "cs.CV"], "published": "2025-06-03 12:33:47", "updated": "2025-06-03 12:33:47", "pdf_url": "http://arxiv.org/pdf/2506.02803v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02818v1", "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations", "authors": ["Ekaterina Grishina", "Mikhail Gorbunov", "Maxim Rakhuba"], "abstract": "Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT", "categories": ["cs.CL", "cs.LG"], "published": "2025-06-03 12:47:23", "updated": "2025-06-03 12:47:23", "pdf_url": "http://arxiv.org/pdf/2506.02818v1", "comment": "Accepted by ACL Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02827v1", "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference", "authors": ["Yulin Dou", "Jiangming Liu"], "abstract": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks.", "categories": ["cs.CL"], "published": "2025-06-03 12:58:07", "updated": "2025-06-03 12:58:07", "pdf_url": "http://arxiv.org/pdf/2506.02827v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02867v1", "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning", "authors": ["Chen Qian", "Dongrui Liu", "Haochen Wen", "Zhen Bai", "Yong Liu", "Jing Shao"], "abstract": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-03 13:31:10", "updated": "2025-06-03 13:31:10", "pdf_url": "http://arxiv.org/pdf/2506.02867v1", "comment": "Preprint. Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02872v1", "title": "Token and Span Classification for Entity Recognition in French Historical Encyclopedias", "authors": ["Ludovic Moncla", "H\u00e9di Zeghidi"], "abstract": "Named Entity Recognition (NER) in historical texts presents unique challenges\ndue to non-standardized language, archaic orthography, and nested or\noverlapping entities. This study benchmarks a diverse set of NER approaches,\nranging from classical Conditional Random Fields (CRFs) and spaCy-based models\nto transformer-based architectures such as CamemBERT and sequence-labeling\nmodels like Flair. Experiments are conducted on the GeoEDdA dataset, a richly\nannotated corpus derived from 18th-century French encyclopedias. We propose\nframing NER as both token-level and span-level classification to accommodate\ncomplex nested entity structures typical of historical documents. Additionally,\nwe evaluate the emerging potential of few-shot prompting with generative\nlanguage models for low-resource scenarios. Our results demonstrate that while\ntransformer-based models achieve state-of-the-art performance, especially on\nnested entities, generative models offer promising alternatives when labeled\ndata are scarce. The study highlights ongoing challenges in historical NER and\nsuggests avenues for hybrid approaches combining symbolic and neural methods to\nbetter capture the intricacies of early modern French text.", "categories": ["cs.CL", "cs.IR"], "published": "2025-06-03 13:37:44", "updated": "2025-06-03 13:37:44", "pdf_url": "http://arxiv.org/pdf/2506.02872v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02878v1", "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective", "authors": ["Jintian Shao", "Yiming Cheng"], "abstract": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 13:45:01", "updated": "2025-06-03 13:45:01", "pdf_url": "http://arxiv.org/pdf/2506.02878v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02890v1", "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights", "authors": ["Jakub Krajewski", "Marcin Chochowski", "Daniel Korzekwa"], "abstract": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-03 13:55:48", "updated": "2025-06-03 13:55:48", "pdf_url": "http://arxiv.org/pdf/2506.02890v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02894v1", "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation", "authors": ["Verena Blaschke", "Miriam Winkler", "Constantin F\u00f6rster", "Gabriele Wenger-Glemser", "Barbara Plank"], "abstract": "Although Germany has a diverse landscape of dialects, they are\nunderrepresented in current automatic speech recognition (ASR) research. To\nenable studies of how robust models are towards dialectal variation, we present\nBetthupferl, an evaluation dataset containing four hours of read speech in\nthree dialect groups spoken in Southeast Germany (Franconian, Bavarian,\nAlemannic), and half an hour of Standard German speech. We provide both\ndialectal and Standard German transcriptions, and analyze the linguistic\ndifferences between them. We benchmark several multilingual state-of-the-art\nASR models on speech translation into Standard German, and find differences\nbetween how much the output resembles the dialectal vs. standardized\ntranscriptions. Qualitative error analyses of the best ASR model reveal that it\nsometimes normalizes grammatical differences, but often stays closer to the\ndialectal constructions.", "categories": ["cs.CL", "eess.AS"], "published": "2025-06-03 14:02:52", "updated": "2025-06-03 14:02:52", "pdf_url": "http://arxiv.org/pdf/2506.02894v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02899v1", "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator", "authors": ["Yusuke Sakai", "Takumi Goto", "Taro Watanabe"], "abstract": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 14:05:37", "updated": "2025-06-03 14:05:37", "pdf_url": "http://arxiv.org/pdf/2506.02899v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02911v1", "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "abstract": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "published": "2025-06-03 14:16:53", "updated": "2025-06-03 14:16:53", "pdf_url": "http://arxiv.org/pdf/2506.02911v1", "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02921v1", "title": "A Controllable Examination for Long-Context Language Models", "authors": ["Yijun Yang", "Zeyu Huang", "Wenhao Zhu", "Zihan Qiu", "Fei Yuan", "Jeff Z. Pan", "Ivan Titov"], "abstract": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: $\\textit{seamless context}$, $\\textit{controllable setting}$, and\n$\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\n$\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$.\nOur experimental evaluation, which includes $\\textbf{18}$ LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.", "categories": ["cs.CL"], "published": "2025-06-03 14:23:06", "updated": "2025-06-03 14:23:06", "pdf_url": "http://arxiv.org/pdf/2506.02921v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02924v1", "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification", "authors": ["Diogo A. P. Nunes", "Eug\u00e9nio Ribeiro"], "abstract": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; J.3; H.3.3"], "published": "2025-06-03 14:25:12", "updated": "2025-06-03 14:25:12", "pdf_url": "http://arxiv.org/pdf/2506.02924v1", "comment": "12 pages, 1 figure, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02945v1", "title": "Quantitative LLM Judges", "authors": ["Aishwarya Sahoo", "Jeevana Kruthi Karnuthala", "Tushar Parmanand Budhwani", "Pranchal Agarwal", "Sankaran Vaidyanathan", "Alexa Siu", "Franck Dernoncourt", "Jennifer Healey", "Nedim Lipka", "Ryan Rossi", "Uttaran Bhattacharya", "Branislav Kveton"], "abstract": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.", "categories": ["cs.CL", "cs.LG"], "published": "2025-06-03 14:44:23", "updated": "2025-06-03 14:44:23", "pdf_url": "http://arxiv.org/pdf/2506.02945v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02951v1", "title": "Adaptive Graph Pruning for Multi-Agent Communication", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "abstract": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.", "categories": ["cs.CL", "cs.MA"], "published": "2025-06-03 14:46:00", "updated": "2025-06-03 14:46:00", "pdf_url": "http://arxiv.org/pdf/2506.02951v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02959v1", "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "abstract": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 14:52:44", "updated": "2025-06-03 14:52:44", "pdf_url": "http://arxiv.org/pdf/2506.02959v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02961v1", "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models", "authors": ["Yan Gao", "Massimo Roberto Scamarcia", "Javier Fernandez-Marques", "Mohammad Naseri", "Chong Shen Ng", "Dimitris Stripelis", "Zexi Li", "Tao Shen", "Jiamu Bai", "Daoyuan Chen", "Zikai Zhang", "Rui Hu", "InSeo Song", "Lee KangYoon", "Hong Jia", "Ting Dang", "Junyan Wang", "Zheyuan Liu", "Daniel Janes Beutel", "Lingjuan Lyu", "Nicholas D. Lane"], "abstract": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications.", "categories": ["cs.CL"], "published": "2025-06-03 14:54:12", "updated": "2025-06-03 14:54:12", "pdf_url": "http://arxiv.org/pdf/2506.02961v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02973v1", "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation", "authors": ["Dingwei Chen", "Ziqiang Liu", "Feiteng Fang", "Chak Tou Leong", "Shiwen Ni", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.", "categories": ["cs.CL"], "published": "2025-06-03 15:07:13", "updated": "2025-06-03 15:07:13", "pdf_url": "http://arxiv.org/pdf/2506.02973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02979v1", "title": "Towards a Japanese Full-duplex Spoken Dialogue System", "authors": ["Atsumoto Ohashi", "Shinya Iizuka", "Jingjing Jiang", "Ryuichiro Higashinaka"], "abstract": "Full-duplex spoken dialogue systems, which can model simultaneous\nbidirectional features of human conversations such as speech overlaps and\nbackchannels, have attracted significant attention recently. However, the study\nof full-duplex spoken dialogue systems for the Japanese language has been\nlimited, and the research on their development in Japanese remains scarce. In\nthis paper, we present the first publicly available full-duplex spoken dialogue\nmodel in Japanese, which is built upon Moshi, a full-duplex dialogue model in\nEnglish. Our model is trained through a two-stage process: pre-training on a\nlarge-scale spoken dialogue data in Japanese, followed by fine-tuning on\nhigh-quality stereo spoken dialogue data. We further enhance the model's\nperformance by incorporating synthetic dialogue data generated by a\nmulti-stream text-to-speech system. Evaluation experiments demonstrate that the\ntrained model outperforms Japanese baseline models in both naturalness and\nmeaningfulness.", "categories": ["cs.CL", "eess.AS"], "published": "2025-06-03 15:16:50", "updated": "2025-06-03 15:16:50", "pdf_url": "http://arxiv.org/pdf/2506.02979v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02987v1", "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "authors": ["Richard Armitage"], "abstract": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-06-03 15:25:38", "updated": "2025-06-03 15:25:38", "pdf_url": "http://arxiv.org/pdf/2506.02987v1", "comment": "12 pages, 1 Table", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02992v1", "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation", "authors": ["Li Zhang", "Kevin D. Ashley"], "abstract": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/", "categories": ["cs.AI", "cs.CL", "cs.LG", "68T50", "I.2"], "published": "2025-06-03 15:28:30", "updated": "2025-06-03 15:28:30", "pdf_url": "http://arxiv.org/pdf/2506.02992v1", "comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02995v1", "title": "It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems", "authors": ["Iuliia Zaitova", "Badr M. Abdullah", "Wei Xue", "Dietrich Klakow", "Bernd M\u00f6bius", "Tania Avgustinova"], "abstract": "Idioms are defined as a group of words with a figurative meaning not\ndeducible from their individual components. Although modern machine translation\nsystems have made remarkable progress, translating idioms remains a major\nchallenge, especially for speech-to-text systems, where research on this topic\nis notably sparse. In this paper, we systematically evaluate idiom translation\nas compared to conventional news translation in both text-to-text machine\ntranslation (MT) and speech-to-text translation (SLT) systems across two\nlanguage pairs (German to English, Russian to English). We compare\nstate-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large\nv3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large\nLanguage Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal\nthat SLT systems experience a pronounced performance drop on idiomatic data,\noften reverting to literal translations even in higher layers, whereas MT\nsystems and Large Language Models demonstrate better handling of idioms. These\nfindings underscore the need for idiom-specific strategies and improved\ninternal representations in SLT architectures.", "categories": ["cs.CL"], "published": "2025-06-03 15:29:52", "updated": "2025-06-03 15:29:52", "pdf_url": "http://arxiv.org/pdf/2506.02995v1", "comment": "13 pages, 3 figures, ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.02998v1", "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems", "authors": ["\u0110or\u0111e Klisura", "Astrid R Bernaga Torres", "Anna Karen G\u00e1rate-Escamilla", "Rajesh Roshan Biswal", "Ke Yang", "Hilal Pataci", "Anthony Rios"], "abstract": "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.", "categories": ["cs.CL"], "published": "2025-06-03 15:32:20", "updated": "2025-06-03 15:32:20", "pdf_url": "http://arxiv.org/pdf/2506.02998v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03009v1", "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech", "authors": ["Florian Ludwig", "Torsten Zesch", "Frederike Zufall"], "abstract": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 15:50:27", "updated": "2025-06-03 15:50:27", "pdf_url": "http://arxiv.org/pdf/2506.03009v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03011v1", "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers", "authors": ["Aditya Bharat Soni", "Boxuan Li", "Xingyao Wang", "Valerie Chen", "Graham Neubig"], "abstract": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research.", "categories": ["cs.CL"], "published": "2025-06-03 15:50:55", "updated": "2025-06-03 15:50:55", "pdf_url": "http://arxiv.org/pdf/2506.03011v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03035v1", "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning", "authors": ["Pierre Lepagnol", "Sahar Ghannay", "Thomas Gerald", "Christophe Servan", "Sophie Rosset"], "abstract": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-03 16:18:45", "updated": "2025-06-03 16:18:45", "pdf_url": "http://arxiv.org/pdf/2506.03035v1", "comment": "Conference paper accepted to INTERSPEECH 2025", "doi": null, "journal_ref": "INTERSPEECH 2025"}
{"arxiv_id": "2506.03038v1", "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "authors": ["Jintian Shao", "Yiming Cheng"], "abstract": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.", "categories": ["cs.CL"], "published": "2025-06-03 16:20:47", "updated": "2025-06-03 16:20:47", "pdf_url": "http://arxiv.org/pdf/2506.03038v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03051v1", "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs", "authors": ["Yuval Kansal", "Shmuel Berman", "Lydia Liu"], "abstract": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 16:31:52", "updated": "2025-06-03 16:31:52", "pdf_url": "http://arxiv.org/pdf/2506.03051v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03053v1", "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "abstract": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "published": "2025-06-03 16:33:47", "updated": "2025-06-03 16:33:47", "pdf_url": "http://arxiv.org/pdf/2506.03053v1", "comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03090v1", "title": "Literary Evidence Retrieval via Long-Context Language Models", "authors": ["Katherine Thai", "Mohit Iyyer"], "abstract": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.", "categories": ["cs.CL"], "published": "2025-06-03 17:19:45", "updated": "2025-06-03 17:19:45", "pdf_url": "http://arxiv.org/pdf/2506.03090v1", "comment": "ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03100v1", "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds", "authors": ["Yang Guo", "Yutian Tao", "Yifei Ming", "Robert D. Nowak", "Yingyu Liang"], "abstract": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "math.ST", "stat.TH"], "published": "2025-06-03 17:31:53", "updated": "2025-06-03 17:31:53", "pdf_url": "http://arxiv.org/pdf/2506.03100v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03101v1", "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales", "authors": ["Jonas F. Lotz", "Ant\u00f3nio V. Lopes", "Stephan Peitz", "Hendra Setiawan", "Leonardo Emili"], "abstract": "The choice of tokenizer can profoundly impact language model performance, yet\naccessible and reliable evaluations of tokenizer quality remain an open\nchallenge. Inspired by scaling consistency, we show that smaller models can\naccurately predict significant differences in tokenizer impact on larger models\nat a fraction of the compute cost. By systematically evaluating both\nEnglish-centric and multilingual tokenizers, we find that tokenizer choice has\nnegligible effects on tasks in English but results in consistent performance\ndifferences in multilingual settings. We propose new intrinsic tokenizer\nmetrics inspired by Zipf's law that correlate more strongly with downstream\nperformance than text compression when modeling unseen languages. By combining\nseveral metrics to capture multiple aspects of tokenizer behavior, we develop a\nreliable framework for intrinsic tokenizer evaluations. Our work offers a more\nefficient path to informed tokenizer selection in future language model\ndevelopment.", "categories": ["cs.CL"], "published": "2025-06-03 17:35:56", "updated": "2025-06-03 17:35:56", "pdf_url": "http://arxiv.org/pdf/2506.03101v1", "comment": "ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03106v1", "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chao Yang", "Helen Meng"], "abstract": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 17:39:02", "updated": "2025-06-03 17:39:02", "pdf_url": "http://arxiv.org/pdf/2506.03106v1", "comment": "38 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03122v1", "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Vandana Mukherjee", "Xin Zhang"], "abstract": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.", "categories": ["cs.CL"], "published": "2025-06-03 17:54:30", "updated": "2025-06-03 17:54:30", "pdf_url": "http://arxiv.org/pdf/2506.03122v1", "comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03135v1", "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "abstract": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-03 17:58:29", "updated": "2025-06-03 17:58:29", "pdf_url": "http://arxiv.org/pdf/2506.03135v1", "comment": "Project Page: https://qizekun.github.io/omnispatial/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03136v1", "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "authors": ["Yinjie Wang", "Ling Yang", "Ye Tian", "Ke Shen", "Mengdi Wang"], "abstract": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE", "categories": ["cs.CL"], "published": "2025-06-03 17:58:42", "updated": "2025-06-03 17:58:42", "pdf_url": "http://arxiv.org/pdf/2506.03136v1", "comment": "Project: https://github.com/Gen-Verse/CURE", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03143v1", "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "abstract": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-06-03 17:59:08", "updated": "2025-06-03 17:59:08", "pdf_url": "http://arxiv.org/pdf/2506.03143v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03144v1", "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "abstract": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.", "categories": ["cs.CV", "cs.CL", "cs.MM"], "published": "2025-06-03 17:59:14", "updated": "2025-06-03 17:59:14", "pdf_url": "http://arxiv.org/pdf/2506.03144v1", "comment": "Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03145v1", "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "abstract": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-03 17:59:18", "updated": "2025-06-03 17:59:18", "pdf_url": "http://arxiv.org/pdf/2506.03145v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03147v1", "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "abstract": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-03 17:59:33", "updated": "2025-06-03 17:59:33", "pdf_url": "http://arxiv.org/pdf/2506.03147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03149v1", "title": "Causal Estimation of Tokenisation Bias", "authors": ["Pietro Lesci", "Clara Meister", "Thomas Hofmann", "Andreas Vlachos", "Tiago Pimentel"], "abstract": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-03 17:59:47", "updated": "2025-06-03 17:59:47", "pdf_url": "http://arxiv.org/pdf/2506.03149v1", "comment": "Published as a conference paper at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03469v1", "title": "Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration", "authors": ["Tuan Le", "Risal Shefin", "Debashis Gupta", "Thai Le", "Sarra Alqahtani"], "abstract": "Ensuring the safety of reinforcement learning (RL) policies in high-stakes\nenvironments requires not only formal verification but also interpretability\nand targeted falsification. While model checking provides formal guarantees,\nits effectiveness is limited by abstraction quality and the completeness of the\nunderlying trajectory dataset. We propose a hybrid framework that integrates\n(1) explainability, (2) model checking, and (3) risk-guided falsification to\nachieve both rigor and coverage. Our approach begins by constructing a\nhuman-interpretable abstraction of the RL policy using Comprehensible Abstract\nPolicy Summarization (CAPS). This abstract graph, derived from offline\ntrajectories, is both verifier-friendly, semantically meaningful, and can be\nused as input to Storm probabilistic model checker to verify satisfaction of\ntemporal safety specifications. If the model checker identifies a violation, it\nwill return an interpretable counterexample trace by which the policy fails the\nsafety requirement. However, if no violation is detected, we cannot conclude\nsatisfaction due to potential limitation in the abstraction and coverage of the\noffline dataset. In such cases, we estimate associated risk during model\nchecking to guide a falsification strategy that prioritizes searching in\nhigh-risk states and regions underrepresented in the trajectory dataset. We\nfurther provide PAC-style guarantees on the likelihood of uncovering undetected\nviolations. Finally, we incorporate a lightweight safety shield that switches\nto a fallback policy at runtime when such a risk exceeds a threshold,\nfacilitating failure mitigation without retraining.", "categories": ["cs.AI", "cs.LG"], "published": "2025-06-04 00:54:01", "updated": "2025-06-04 00:54:01", "pdf_url": "http://arxiv.org/pdf/2506.03469v1", "comment": "8 pages, 7 figures, European Conference on Artificial Intelligence\n  (ECAI)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03474v1", "title": "CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design", "authors": ["Yifeng Xiao", "Yurong Xu", "Ning Yan", "Masood Mortazavi", "Pierluigi Nuzzo"], "abstract": "Simulation-based design space exploration (DSE) aims to efficiently optimize\nhigh-dimensional structured designs under complex constraints and expensive\nevaluation costs. Existing approaches, including heuristic and multi-step\nreinforcement learning (RL) methods, struggle to balance sampling efficiency\nand constraint satisfaction due to sparse, delayed feedback, and large hybrid\naction spaces. In this paper, we introduce CORE, a constraint-aware, one-step\nRL method for simulationguided DSE. In CORE, the policy agent learns to sample\ndesign configurations by defining a structured distribution over them,\nincorporating dependencies via a scaling-graph-based decoder, and by reward\nshaping to penalize invalid designs based on the feedback obtained from\nsimulation. CORE updates the policy using a surrogate objective that compares\nthe rewards of designs within a sampled batch, without learning a value\nfunction. This critic-free formulation enables efficient learning by\nencouraging the selection of higher-reward designs. We instantiate CORE for\nhardware-mapping co-design of neural network accelerators, demonstrating that\nit significantly improves sample efficiency and achieves better accelerator\nconfigurations compared to state-of-the-art baselines. Our approach is general\nand applicable to a broad class of discrete-continuous constrained design\nproblems.", "categories": ["cs.LG", "cs.AI", "cs.AR", "I.2.6; C.3"], "published": "2025-06-04 01:08:34", "updated": "2025-06-04 01:08:34", "pdf_url": "http://arxiv.org/pdf/2506.03474v1", "comment": "Preprint. 10 pages + appendix. Submitted to NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03484v1", "title": "Explainable AI: XAI-Guided Context-Aware Data Augmentation", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Atnafu Lambebo Tonja", "Hassan Shakil", "Samer Iskander", "Olga Kolesnikova", "Jugal Kalita"], "abstract": "Explainable AI (XAI) has emerged as a powerful tool for improving the\nperformance of AI models, going beyond providing model transparency and\ninterpretability. The scarcity of labeled data remains a fundamental challenge\nin developing robust and generalizable AI models, particularly for low-resource\nlanguages. Conventional data augmentation techniques introduce noise, cause\nsemantic drift, disrupt contextual coherence, lack control, and lead to\noverfitting. To address these challenges, we propose XAI-Guided Context-Aware\nData Augmentation. This novel framework leverages XAI techniques to modify less\ncritical features while selectively preserving most task-relevant features. Our\napproach integrates an iterative feedback loop, which refines augmented data\nover multiple augmentation cycles based on explainability-driven insights and\nthe model performance gain. Our experimental results demonstrate that XAI-SR-BT\nand XAI-PR-BT improve the accuracy of models on hate speech and sentiment\nanalysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using\nthe Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform\nexisting augmentation techniques by 4.8% and 5%, respectively, on the same\ndataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform\nboth baseline and conventional augmentation techniques across all tasks and\nmodels. This study provides a more controlled, interpretable, and context-aware\nsolution to data augmentation, addressing critical limitations of existing\naugmentation techniques and offering a new paradigm shift for leveraging XAI\ntechniques to enhance AI model training.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 01:47:24", "updated": "2025-06-04 01:47:24", "pdf_url": "http://arxiv.org/pdf/2506.03484v1", "comment": null, "doi": "10.1016/j.eswa.2025.128364", "journal_ref": null}
{"arxiv_id": "2506.03489v1", "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding", "authors": ["Mingxu Tao", "Jie Hu", "Mingchuan Yang", "Yunhuai Liu", "Dongyan Zhao", "Yansong Feng"], "abstract": "The remarkable performance of Large language models (LLMs) relies heavily on\nthe availability of abundant high-quality training data. However, the high cost\nof acquiring annotated data often prevents models from obtaining capabilities\nto tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe\nthat boosts model performance in data-scarcity scenarios without extra\ntraining. We first employ model extrapolation to enhance a finetuned model with\nits inferior version, and then adopt contrastive decoding to further reduce\npredicted errors, by comparing the logit scores given by the extrapolated and\nthe vanilla finetuned model. Experiments across three tasks over four different\nLLMs show that EpiCoDe consistently outperforms existing methods with\nsignificant and robust improvement. We also propose a new theoretical framework\nto reveal the mechanism behind contrastive decoding in data-scarcity scenarios,\nwhich further helps us better understand the effectiveness of EpiCoDe.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 02:11:54", "updated": "2025-06-04 02:11:54", "pdf_url": "http://arxiv.org/pdf/2506.03489v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03501v1", "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing", "authors": ["Yuchen Guo", "Zhicheng Dou", "Huy H. Nguyen", "Ching-Chun Chang", "Saku Sugawara", "Isao Echizen"], "abstract": "Content creation has dramatically progressed with the rapid advancement of\nlarge language models like ChatGPT and Claude. While this progress has greatly\nenhanced various aspects of life and work, it has also negatively affected\ncertain areas of society. A recent survey revealed that nearly 30% of college\nstudents use generative AI to help write academic papers and reports. Most\ncountermeasures treat the detection of AI-generated text as a binary\nclassification task and thus lack robustness. This approach overlooks human\ninvolvement in the generation of content even though human-machine\ncollaboration is becoming mainstream. Besides generating entire texts, people\nmay use machines to complete or revise texts. Such human involvement varies\ncase by case, which makes binary classification a less than satisfactory\napproach. We refer to this situation as participation detection obfuscation. We\npropose using BERTScore as a metric to measure human involvement in the\ngeneration process and a multi-task RoBERTa-based regressor trained on a token\nclassification task to address this problem. To evaluate the effectiveness of\nthis approach, we simulated academic-based scenarios and created a continuous\ndataset reflecting various levels of human involvement. All of the existing\ndetectors we examined failed to detect the level of human involvement on this\ndataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor\nmean squared error of 0.004). Moreover, it demonstrated some generalizability\nacross generative models. Our code is available at\nhttps://github.com/gyc-nii/CAS-CS-and-dual-head-detector", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 02:31:36", "updated": "2025-06-04 02:31:36", "pdf_url": "http://arxiv.org/pdf/2506.03501v1", "comment": "IJCNN2025 accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03503v1", "title": "Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis", "authors": ["Shan Shan"], "abstract": "The quantification of social science remains a longstanding challenge,\nlargely due to the philosophical nature of its foundational theories. Although\nquantum computing has advanced rapidly in recent years, its relevance to social\ntheory remains underexplored. Most existing research focuses on micro-cognitive\nmodels or philosophical analogies, leaving a gap in system-level applications\nof quantum principles to the analysis of social systems. This study addresses\nthat gap by proposing a theoretical and computational framework that combines\nquantum mechanics with Generative AI to simulate the emergence and evolution of\nsocial norms. Drawing on core quantum concepts--such as superposition,\nentanglement, and probabilistic measurement--this research models society as a\ndynamic, uncertain system and sets up five ideal-type experiments. These\nscenarios are simulated using 25 generative agents, each assigned evolving\nroles as compliers, resistors, or enforcers. Within a simulated environment\nmonitored by a central observer (the Watcher), agents interact, respond to\nsurveillance, and adapt to periodic normative disruptions. These interactions\nallow the system to self-organize under external stress and reveal emergent\npatterns. Key findings show that quantum principles, when integrated with\ngenerative AI, enable the modeling of uncertainty, emergence, and\ninterdependence in complex social systems. Simulations reveal patterns\nincluding convergence toward normative order, the spread of resistance, and the\nspontaneous emergence of new equilibria in social rules. In conclusion, this\nstudy introduces a novel computational lens that lays the groundwork for a\nquantum-informed social theory. It offers interdisciplinary insights into how\nsociety can be understood not just as a structure to observe but as a dynamic\nsystem to simulate and redesign through quantum technologies.", "categories": ["cs.AI"], "published": "2025-06-04 02:40:53", "updated": "2025-06-04 02:40:53", "pdf_url": "http://arxiv.org/pdf/2506.03503v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03511v1", "title": "POLARIS: A High-contrast Polarimetric Imaging Benchmark Dataset for Exoplanetary Disk Representation Learning", "authors": ["Fangyi Cao", "Bin Ren", "Zihao Wang", "Shiwei Fu", "Youbin Mo", "Xiaoyang Liu", "Yuzhou Chen", "Weixin Yao"], "abstract": "With over 1,000,000 images from more than 10,000 exposures using\nstate-of-the-art high-contrast imagers (e.g., Gemini Planet Imager, VLT/SPHERE)\nin the search for exoplanets, can artificial intelligence (AI) serve as a\ntransformative tool in imaging Earth-like exoplanets in the coming decade? In\nthis paper, we introduce a benchmark and explore this question from a\npolarimetric image representation learning perspective. Despite extensive\ninvestments over the past decade, only a few new exoplanets have been directly\nimaged. Existing imaging approaches rely heavily on labor-intensive labeling of\nreference stars, which serve as background to extract circumstellar objects\n(disks or exoplanets) around target stars. With our POLARIS (POlarized Light\ndAta for total intensity Representation learning of direct Imaging of\nexoplanetary Systems) dataset, we classify reference star and circumstellar\ndisk images using the full public SPHERE/IRDIS polarized-light archive since\n2014, requiring less than 10 percent manual labeling. We evaluate a range of\nmodels including statistical, generative, and large vision-language models and\nprovide baseline performance. We also propose an unsupervised generative\nrepresentation learning framework that integrates these models, achieving\nsuperior performance and enhanced representational power. To our knowledge,\nthis is the first uniformly reduced, high-quality exoplanet imaging dataset,\nrare in astrophysics and machine learning. By releasing this dataset and\nbaselines, we aim to equip astrophysicists with new tools and engage data\nscientists in advancing direct exoplanet imaging, catalyzing major\ninterdisciplinary breakthroughs.", "categories": ["astro-ph.EP", "astro-ph.IM", "cs.AI", "eess.IV"], "published": "2025-06-04 02:55:02", "updated": "2025-06-04 02:55:02", "pdf_url": "http://arxiv.org/pdf/2506.03511v1", "comment": "9 pages main text with 5 figures, 9 pages appendix with 9 figures.\n  Submitted to NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03516v1", "title": "SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models", "authors": ["Arnab Debnath", "Gregory J. Stein", "Jana Kosecka"], "abstract": "Object goal navigation is a fundamental task in embodied AI, where an agent\nis instructed to locate a target object in an unexplored environment.\nTraditional learning-based methods rely heavily on large-scale annotated data\nor require extensive interaction with the environment in a reinforcement\nlearning setting, often failing to generalize to novel environments and\nlimiting scalability. To overcome these challenges, we explore a zero-shot\nsetting where the agent operates without task-specific training, enabling more\nscalable and adaptable solution. Recent advances in Vision Foundation Models\n(VFMs) offer powerful capabilities for visual understanding and reasoning,\nmaking them ideal for agents to comprehend scenes, identify relevant regions,\nand infer the likely locations of objects. In this work, we present a zero-shot\nobject goal navigation framework that integrates the perceptual strength of\nVFMs with a model-based planner that is capable of long-horizon decision making\nthrough frontier exploration. We evaluate our approach on the HM3D dataset\nusing the Habitat simulator and demonstrate that our method achieves\nstate-of-the-art performance in terms of success weighted by path length for\nzero-shot object goal navigation.", "categories": ["cs.RO", "cs.AI"], "published": "2025-06-04 03:04:54", "updated": "2025-06-04 03:04:54", "pdf_url": "http://arxiv.org/pdf/2506.03516v1", "comment": "Accepted at CVPR 2025 workshop - Foundation Models Meet Embodied\n  Agents", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03525v1", "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning", "authors": ["Daeun Lee", "Jaehong Yoon", "Jaemin Cho", "Mohit Bansal"], "abstract": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-04 03:18:01", "updated": "2025-06-04 03:18:01", "pdf_url": "http://arxiv.org/pdf/2506.03525v1", "comment": "Project website: https://video-skill-cot.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03541v1", "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "abstract": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 03:52:20", "updated": "2025-06-04 03:52:20", "pdf_url": "http://arxiv.org/pdf/2506.03541v1", "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03543v1", "title": "CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications", "authors": ["Wanghao Ye", "Sihan Chen", "Yiting Wang", "Shwai He", "Bowei Tian", "Guoheng Sun", "Ziyi Wang", "Ziyao Wang", "Yexiao He", "Zheyu Shen", "Meng Liu", "Yuning Zhang", "Meng Feng", "Yang Wang", "Siyuan Peng", "Yilong Dai", "Zhenle Duan", "Hanzhang Qin", "Ang Li"], "abstract": "Current large language model (LLM) agents lack authentic human psychological\nprocesses necessary for genuine digital twins and social AI applications. To\naddress this limitation, we present a computational implementation of Global\nWorkspace Theory (GNWT) that integrates human cognitive architecture principles\ninto LLM agents, creating specialized sub-agents for emotion, memory, social\nnorms, planning, and goal-tracking coordinated through a global workspace\nmechanism. However, authentic digital twins require accurate personality\ninitialization. We therefore develop a novel adventure-based personality test\nthat evaluates true personality through behavioral choices within interactive\nscenarios, bypassing self-presentation bias found in traditional assessments.\nBuilding on these innovations, our CogniPair platform enables digital twins to\nengage in realistic simulated dating interactions and job interviews before\nreal encounters, providing bidirectional cultural fit assessment for both\nromantic compatibility and workplace matching. Validation using 551 GNWT-Agents\nand Columbia University Speed Dating dataset demonstrates 72% correlation with\nhuman attraction patterns, 77.8% match prediction accuracy, and 74% agreement\nin human validation studies. This work advances psychological authenticity in\nLLM agents and establishes a foundation for intelligent dating platforms and HR\ntechnology solutions.", "categories": ["cs.AI", "cs.CY", "cs.MA"], "published": "2025-06-04 03:54:30", "updated": "2025-06-04 03:54:30", "pdf_url": "http://arxiv.org/pdf/2506.03543v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03546v1", "title": "From Virtual Agents to Robot Teams: A Multi-Robot Framework Evaluation in High-Stakes Healthcare Context", "authors": ["Yuanchen Bai", "Zijian Ding", "Angelique Taylor"], "abstract": "Advancements in generative models have enabled multi-agent systems (MAS) to\nperform complex virtual tasks such as writing and code generation, which do not\ngeneralize well to physical multi-agent robotic teams. Current frameworks often\ntreat agents as conceptual task executors rather than physically embodied\nentities, and overlook critical real-world constraints such as spatial context,\nrobotic capabilities (e.g., sensing and navigation). To probe this gap, we\nreconfigure and stress-test a hierarchical multi-agent robotic team built on\nthe CrewAI framework in a simulated emergency department onboarding scenario.\nWe identify five persistent failure modes: role misalignment; tool access\nviolations; lack of in-time handling of failure reports; noncompliance with\nprescribed workflows; bypassing or false reporting of task completion. Based on\nthis analysis, we propose three design guidelines emphasizing process\ntransparency, proactive failure recovery, and contextual grounding. Our work\ninforms the development of more resilient and robust multi-agent robotic\nsystems (MARS), including opportunities to extend virtual multi-agent\nframeworks to the real world.", "categories": ["cs.RO", "cs.AI", "cs.MA"], "published": "2025-06-04 04:05:38", "updated": "2025-06-04 04:05:38", "pdf_url": "http://arxiv.org/pdf/2506.03546v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03548v1", "title": "SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization", "authors": ["Chenglong Ye", "Gang Xiong", "Junyou Shang", "Xingyuan Dai", "Xiaoyan Gong", "Yisheng Lv"], "abstract": "Traffic simulation tools, such as SUMO, are essential for urban mobility\nresearch. However, such tools remain challenging for users due to complex\nmanual workflows involving network download, demand generation, simulation\nsetup, and result analysis. In this paper, we introduce SUMO-MCP, a novel\nplatform that not only wraps SUMO' s core utilities into a unified tool suite\nbut also provides additional auxiliary utilities for common preprocessing and\npostprocessing tasks. Using SUMO-MCP, users can issue simple natural-language\nprompts to generate traffic scenarios from OpenStreetMap data, create demand\nfrom origin-destination matrices or random patterns, run batch simulations with\nmultiple signal-control strategies, perform comparative analyses with automated\nreporting, and detect congestion for signal-timing optimization. Furthermore,\nthe platform allows flexible custom workflows by dynamically combining exposed\nSUMO tools without additional coding. Experiments demonstrate that SUMO-MCP\nsignificantly makes traffic simulation more accessible and reliable for\nresearchers. We will release code for SUMO-MCP at\nhttps://github.com/ycycycl/SUMO-MCP in the future.", "categories": ["cs.AI"], "published": "2025-06-04 04:08:11", "updated": "2025-06-04 04:08:11", "pdf_url": "http://arxiv.org/pdf/2506.03548v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03566v1", "title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding", "authors": ["Langlin Huang", "Chengsong Huang", "Jixuan Leng", "Di Huang", "Jiaxin Huang"], "abstract": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 04:30:30", "updated": "2025-06-04 04:30:30", "pdf_url": "http://arxiv.org/pdf/2506.03566v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03568v1", "title": "Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving", "authors": ["Li Zeqiao", "Wang Yijing", "Wang Haoyu", "Li Zheng", "Li Peng", "Zuo zhiqiang", "Hu Chuan"], "abstract": "Autonomous driving promises significant advancements in mobility, road safety\nand traffic efficiency, yet reinforcement learning and imitation learning face\nsafe-exploration and distribution-shift challenges. Although human-AI\ncollaboration alleviates these issues, it often relies heavily on extensive\nhuman intervention, which increases costs and reduces efficiency. This paper\ndevelops a confidence-guided human-AI collaboration (C-HAC) strategy to\novercome these limitations. First, C-HAC employs a distributional proxy value\npropagation method within the distributional soft actor-critic (DSAC)\nframework. By leveraging return distributions to represent human intentions\nC-HAC achieves rapid and stable learning of human-guided policies with minimal\nhuman interaction. Subsequently, a shared control mechanism is activated to\nintegrate the learned human-guided policy with a self-learning policy that\nmaximizes cumulative rewards. This enables the agent to explore independently\nand continuously enhance its performance beyond human guidance. Finally, a\npolicy confidence evaluation algorithm capitalizes on DSAC's return\ndistribution networks to facilitate dynamic switching between human-guided and\nself-learning policies via a confidence-based intervention function. This\nensures the agent can pursue optimal policies while maintaining safety and\nperformance guarantees. Extensive experiments across diverse driving scenarios\nreveal that C-HAC significantly outperforms conventional methods in terms of\nsafety, efficiency, and overall performance, achieving state-of-the-art\nresults. The effectiveness of the proposed method is further validated through\nreal-world road tests in complex traffic conditions. The videos and code are\navailable at: https://github.com/lzqw/C-HAC.", "categories": ["cs.RO", "cs.AI"], "published": "2025-06-04 04:31:10", "updated": "2025-06-04 04:31:10", "pdf_url": "http://arxiv.org/pdf/2506.03568v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03571v1", "title": "DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network", "authors": ["Chong Hyun Lee", "Kibae Lee"], "abstract": "We propose DaigNet, a new approach to object detection with which we can\ndetect an object bounding box using diagonal constraints on adjacency matrix of\na graph convolutional network (GCN). We propose two diagonalization algorithms\nbased on hard and soft constraints on adjacency matrix and two loss functions\nusing diagonal constraint and complementary constraint. The DaigNet eliminates\nthe need for designing a set of anchor boxes commonly used. To prove\nfeasibility of our novel detector, we adopt detection head in YOLO models.\nExperiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than\nYOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%\nhigher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 04:34:48", "updated": "2025-06-04 04:34:48", "pdf_url": "http://arxiv.org/pdf/2506.03571v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03576v1", "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "abstract": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 04:47:24", "updated": "2025-06-04 04:47:24", "pdf_url": "http://arxiv.org/pdf/2506.03576v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03582v1", "title": "ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels", "authors": ["Rui Yann", "Xianglei Xing"], "abstract": "We present ViTSGMM, an image recognition network that leverages\nsemi-supervised learning in a highly efficient manner. Existing works often\nrely on complex training techniques and architectures, while their\ngeneralization ability when dealing with extremely limited labeled data remains\nto be improved. To address these limitations, we construct a hierarchical\nmixture density classification decision mechanism by optimizing mutual\ninformation between feature representations and target classes, compressing\nredundant information while retaining crucial discriminative components.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance on STL-10 and CIFAR-10/100 datasets when using negligible labeled\nsamples. Notably, this paper also reveals a long-overlooked data leakage issue\nin the STL-10 dataset for semi-supervised learning tasks and removes duplicates\nto ensure the reliability of experimental results. Code available at\nhttps://github.com/Shu1L0n9/ViTSGMM.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-06-04 05:24:28", "updated": "2025-06-04 05:24:28", "pdf_url": "http://arxiv.org/pdf/2506.03582v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03586v1", "title": "Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach", "authors": ["Yu Ma", "Chongtao Guo", "Le Liang", "Xiao Li", "Shi Jin"], "abstract": "This paper investigates a joint phase design and resource allocation problem\nin downlink reconfigurable intelligent surface (RIS)-assisted orthogonal\nfrequency division multiplexing (OFDM) systems to optimize average delay, where\ndata packets for each user arrive at the base station stochastically. The\nsequential optimization problem is inherently a Markov decision process (MDP),\nmaking it fall within the scope of reinforcement learning. To effectively\nhandle the mixed action space and reduce the state space dimensionality, a\nhybrid deep reinforcement learning (DRL) approach is proposed. Specifically,\nproximal policy optimization (PPO)-$\\Theta$ is employed to optimize RIS phase\nshift design, while PPO-N is responsible for subcarrier allocation decisions.\nTo further mitigate the curse of dimensionality associated with subcarrier\nallocation, a multi-agent strategy is introduced to optimize subcarrier\nallocation indicater more efficiently. Moreover, to achieve more adaptive\nresource allocation and accurately capture network dynamics, key factors\nclosely related to average delay, including the number of backlogged packets in\nbuffers and the current packet arrivals, are incorporated into the state space.\nFurthermore, a transfer learning framework is introduced to enhance training\nefficiency and accelerate convergence. Simulation results demonstrate that the\nproposed algorithm significantly reduces average delay, enhances resource\nallocation efficiency, and achieves superior system robustness and fairness\ncompared to baseline methods.", "categories": ["cs.AI", "cs.IT", "math.IT"], "published": "2025-06-04 05:33:33", "updated": "2025-06-04 05:33:33", "pdf_url": "http://arxiv.org/pdf/2506.03586v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03588v1", "title": "A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems", "authors": ["Hiroki Shiraishi", "Hisao Ishibuchi", "Masaya Nakata"], "abstract": "The decision-making process significantly influences the predictions of\nmachine learning models. This is especially important in rule-based systems\nsuch as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and\napplication of rules directly determine prediction accuracy and reliability.\nLFCSs combine evolutionary algorithms with supervised learning to optimize\nfuzzy classification rules, offering enhanced interpretability and robustness.\nDespite these advantages, research on improving decision-making mechanisms\n(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use\nvoting-based or single-winner-based inference schemes. These schemes rely on\nclassification performance on training data and may not perform well on unseen\ndata, risking overfitting. To address these limitations, this article\nintroduces a novel class inference scheme for LFCSs based on the\nDempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles\nuncertainty well. By using the DS theory, the scheme calculates belief masses\n(i.e., measures of belief) for each specific class and the ``I don't know''\nstate from each fuzzy rule and infers a class from these belief masses. Unlike\nthe conventional schemes, the proposed scheme also considers the ``I don't\nknow'' state that reflects uncertainty, thereby improving the transparency and\nreliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the\nproposed scheme demonstrates statistically significant improvements in terms of\ntest macro F1 scores across 30 real-world datasets compared to conventional\nvoting-based and single-winner-based fuzzy inference schemes. It forms smoother\ndecision boundaries, provides reliable confidence measures, and enhances the\nrobustness and generalizability of LFCSs in real-world applications. Our\nimplementation is available at https://github.com/YNU-NakataLab/jUCS.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "published": "2025-06-04 05:38:49", "updated": "2025-06-04 05:38:49", "pdf_url": "http://arxiv.org/pdf/2506.03588v1", "comment": null, "doi": "10.1145/3717613", "journal_ref": "ACM Transactions on Evolutionary Learning and Optimization (2025)"}
{"arxiv_id": "2506.03589v1", "title": "BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Anh Nguyen", "Ngan Le"], "abstract": "Text-video retrieval (TVR) systems often suffer from visual-linguistic biases\npresent in datasets, which cause pre-trained vision-language models to overlook\nkey details. To address this, we propose BiMa, a novel framework designed to\nmitigate biases in both visual and textual representations. Our approach begins\nby generating scene elements that characterize each video by identifying\nrelevant entities/objects and activities. For visual debiasing, we integrate\nthese scene elements into the video embeddings, enhancing them to emphasize\nfine-grained and salient details. For textual debiasing, we introduce a\nmechanism to disentangle text features into content and bias components,\nenabling the model to focus on meaningful content while separately handling\nbiased information. Extensive experiments and ablation studies across five\nmajor TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)\ndemonstrate the competitive performance of BiMa. Additionally, the model's bias\nmitigation capability is consistently validated by its strong results on\nout-of-distribution retrieval tasks.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-04 05:40:54", "updated": "2025-06-04 05:40:54", "pdf_url": "http://arxiv.org/pdf/2506.03589v1", "comment": "22 pages, 14 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03595v1", "title": "Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner", "authors": ["Runa Eschenhagen", "Aaron Defazio", "Tsung-Hsien Lee", "Richard E. Turner", "Hao-Jun Michael Shi"], "abstract": "The recent success of Shampoo in the AlgoPerf contest has sparked renewed\ninterest in Kronecker-factorization-based optimization algorithms for training\nneural networks. Despite its success, Shampoo relies heavily on several\nheuristics such as learning rate grafting and stale preconditioning to achieve\nperformance at-scale. These heuristics increase algorithmic complexity,\nnecessitate further hyperparameter tuning, and lack theoretical justification.\nThis paper investigates these heuristics from the angle of Frobenius norm\napproximation to full-matrix Adam and decouples the preconditioner's\neigenvalues and eigenbasis updates. We show that grafting from Adam mitigates\nthe staleness and mis-scaling of the preconditioner's eigenvalues and how\ncorrecting the eigenvalues directly can eliminate the need for learning rate\ngrafting. To manage the error induced by infrequent eigenbasis computations, we\npropose an adaptive criterion for determining the eigenbasis computation\nfrequency motivated by terminating a warm-started QR algorithm. This criterion\ndecouples the update frequency of different preconditioner matrices and enables\nus to investigate the impact of approximation error on convergence. These\npractical techniques offer a principled angle towards removing Shampoo's\nheuristics and developing improved Kronecker-factorization-based training\nalgorithms.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-06-04 05:55:41", "updated": "2025-06-04 05:55:41", "pdf_url": "http://arxiv.org/pdf/2506.03595v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03598v1", "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "abstract": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.", "categories": ["cs.CL", "cs.AI", "68T50"], "published": "2025-06-04 06:04:46", "updated": "2025-06-04 06:04:46", "pdf_url": "http://arxiv.org/pdf/2506.03598v1", "comment": "4 pages,2 figures,EITCE 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03602v1", "title": "Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems", "authors": ["Hiroki Shiraishi", "Yohei Hayamizu", "Tomonori Hashiyama", "Keiki Takadama", "Hisao Ishibuchi", "Masaya Nakata"], "abstract": "Rule representations significantly influence the search capabilities and\ndecision boundaries within the search space of Learning Classifier Systems\n(LCSs), a family of rule-based machine learning systems that evolve\ninterpretable models through evolutionary processes. However, it is very\ndifficult to choose an appropriate rule representation for each problem.\nAdditionally, some problems benefit from using different representations for\ndifferent subspaces within the input space. Thus, an adaptive mechanism is\nneeded to choose an appropriate rule representation for each rule in LCSs. This\narticle introduces a flexible rule representation using a four-parameter beta\ndistribution and integrates it into a fuzzy-style LCS. The four-parameter beta\ndistribution can form various function shapes, and this flexibility enables our\nLCS to automatically select appropriate representations for different\nsubspaces. Our rule representation can represent crisp/fuzzy decision\nboundaries in various boundary shapes, such as rectangles and bells, by\ncontrolling four parameters, compared to the standard representations such as\ntrapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the\nappropriate rule representation for each subspace. Moreover, our LCS\nincorporates a generalization bias favoring crisp rules where feasible,\nenhancing model interpretability without compromising accuracy. Experimental\nresults on real-world classification tasks show that our LCS achieves\nsignificantly superior test accuracy and produces more compact rule sets. Our\nimplementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An\nextended abstract related to this work is available at\nhttps://doi.org/10.36227/techrxiv.174900805.59801248/v1.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "published": "2025-06-04 06:19:49", "updated": "2025-06-04 06:19:49", "pdf_url": "http://arxiv.org/pdf/2506.03602v1", "comment": null, "doi": "10.1109/TEVC.2025.3550915", "journal_ref": "IEEE Transactions on Evolutionary Computation (2025)"}
{"arxiv_id": "2506.03606v1", "title": "Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models", "authors": ["Parismita Gogoi", "Sishir Kalita", "Wendy Lalhminghlui", "Viyazonuo Terhiija", "Moakala Tzudir", "Priyankoo Sarmah", "S. R. M. Prasanna"], "abstract": "This study explores the use of self-supervised learning (SSL) models for tone\nrecognition in three low-resource languages from North Eastern India: Angami,\nAo, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on\nboth tonal and non-tonal languages. We analyze tone-wise performance across the\nlayers for all three languages and compare the different models. Our results\nshow that tone recognition works best for Mizo and worst for Angami. The middle\nlayers of the SSL models are the most important for tone recognition,\nregardless of the pre-training language, i.e. tonal or non-tonal. We have also\nfound that the tone inventory, tone types, and dialectal variations affect tone\nrecognition. These findings provide useful insights into the strengths and\nweaknesses of SSL-based embeddings for tonal languages and highlight the\npotential for improving tone recognition in low-resource settings. The source\ncode is available at GitHub 1 .", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "published": "2025-06-04 06:32:12", "updated": "2025-06-04 06:32:12", "pdf_url": "http://arxiv.org/pdf/2506.03606v1", "comment": "Accepted in Interspeech2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03610v1", "title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", "authors": ["Dongmin Park", "Minkyu Kim", "Beongjun Choi", "Junhyuck Kim", "Keon Lee", "Jonghyun Lee", "Inkyu Park", "Byeong-Uk Lee", "Jaeyoung Hwang", "Jaewoo Ahn", "Ameya S. Mahabaleshwarkar", "Bilal Kartal", "Pritam Biswas", "Yoshi Suhara", "Kangwook Lee", "Jaewoong Cho"], "abstract": "Large Language Model (LLM) agents are reshaping the game industry,\nparticularly with more intelligent and human-preferable game characters.\nHowever, existing game benchmarks fall short of practical needs: they lack\nevaluations of diverse LLM capabilities across various game genres, studies of\nagentic modules crucial for complex gameplay, and fine-tuning datasets for\naligning pre-trained LLMs into gaming agents. To fill these gaps, we present\n\\textbf{\\benchname{}}, a foundational benchmark designed to train and evaluate\nLLM agents across diverse real-world video games. Unlike existing benchmarks,\nOrak includes 12 popular video games spanning all major genres, enabling\ncomprehensive studies of LLM capabilities and agentic modules essential for\nintricate game scenarios. To support consistent evaluation of LLMs, we\nintroduce a plug-and-play interface based on Model Context Protocol (MCP) that\nenables LLMs to seamlessly connect with games and manipulate agentic modules.\nAdditionally, we propose a fine-tuning dataset, consisting of LLM gameplay\ntrajectories across diverse game genres. Orak offers a comprehensive evaluation\nframework, encompassing general game score leaderboards, LLM battle arenas, and\nin-depth analyses of visual input state, agentic strategies, and fine-tuning\neffects, establishing a foundation towards building generic gaming agents. Code\nis available at https://github.com/krafton-ai/Orak.", "categories": ["cs.AI"], "published": "2025-06-04 06:40:33", "updated": "2025-06-04 06:40:33", "pdf_url": "http://arxiv.org/pdf/2506.03610v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03613v1", "title": "Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations", "authors": ["Shaoshan Liu", "Fan Wang", "Hongjun Zhou", "Yuanfeng Wang"], "abstract": "While theory and practice are often seen as separate domains, this article\nshows that theoretical insight is essential for overcoming real-world\nengineering barriers. We begin with a practical challenge: training a\ncross-morphology embodied AI policy that generalizes across diverse robot\nmorphologies. We formalize this as the Heterogeneous Embodied Agent Training\n(HEAT) problem and prove it reduces to a structured Partially Observable Markov\nDecision Process (POMDP) that is PSPACE-complete. This result explains why\ncurrent reinforcement learning pipelines break down under morphological\ndiversity, due to sequential training constraints, memory-policy coupling, and\ndata incompatibility. We further explore Collective Adaptation, a distributed\nlearning alternative inspired by biological systems. Though NEXP-complete in\ntheory, it offers meaningful scalability and deployment benefits in practice.\nThis work illustrates how computational theory can illuminate system design\ntrade-offs and guide the development of more robust, scalable embodied AI. For\npractitioners and researchers to explore this problem, the implementation code\nof this work has been made publicly available at\nhttps://github.com/airs-admin/HEAT", "categories": ["cs.AI", "cs.CC"], "published": "2025-06-04 06:44:49", "updated": "2025-06-04 06:44:49", "pdf_url": "http://arxiv.org/pdf/2506.03613v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03614v1", "title": "VLMs Can Aggregate Scattered Training Patches", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Chao Yang", "Chaochao Lu"], "abstract": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as $\\textit{visual\nstitching}$ -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch},\n\\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-04 06:46:06", "updated": "2025-06-04 06:46:06", "pdf_url": "http://arxiv.org/pdf/2506.03614v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03618v1", "title": "GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS", "authors": ["Jiayi Wan", "Xiang Zhu", "Fanzhen Liu", "Wei Fan", "Xiaolong Xu"], "abstract": "Federated learning, as a distributed architecture, shows great promise for\napplications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the\nprivacy risks inherent in CPSS, the integration of differential privacy with\nfederated learning has attracted considerable attention. Existing research\nmainly focuses on dynamically adjusting the noise added or discarding certain\ngradients to mitigate the noise introduced by differential privacy. However,\nthese approaches fail to remove the noise that hinders convergence and correct\nthe gradients affected by the noise, which significantly reduces the accuracy\nof model classification. To overcome these challenges, this paper proposes a\nnovel framework for differentially private federated learning that balances\nrigorous privacy guarantees with accuracy by introducing a server-side gradient\ncorrection mechanism. Specifically, after clients perform gradient clipping and\nnoise perturbation, our framework detects deviations in the noisy local\ngradients and employs a projection mechanism to correct them, mitigating the\nnegative impact of noise. Simultaneously, gradient projection promotes the\nalignment of gradients from different clients and guides the model towards\nconvergence to a global optimum. We evaluate our framework on several benchmark\ndatasets, and the experimental results demonstrate that it achieves\nstate-of-the-art performance under the same privacy budget.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-04 06:52:37", "updated": "2025-06-04 06:52:37", "pdf_url": "http://arxiv.org/pdf/2506.03618v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03621v1", "title": "Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation", "authors": ["Chaehun Shin", "Jooyoung Choi", "Johan Barthelemy", "Jungbeom Lee", "Sungroh Yoon"], "abstract": "We present Subject Fidelity Optimization (SFO), a novel comparative learning\nframework for zero-shot subject-driven generation that enhances subject\nfidelity. Beyond supervised fine-tuning methods that rely only on positive\ntargets and use the diffusion loss as in the pre-training stage, SFO introduces\nsynthetic negative targets and explicitly guides the model to favor positives\nover negatives through pairwise comparison. For negative targets, we propose\nCondition-Degradation Negative Sampling (CDNS), which automatically generates\ndistinctive and informative negatives by intentionally degrading visual and\ntextual cues without expensive human annotations. Moreover, we reweight the\ndiffusion timesteps to focus finetuning on intermediate steps where subject\ndetails emerge. Extensive experiments demonstrate that SFO with CDNS\nsignificantly outperforms baselines in terms of both subject fidelity and text\nalignment on a subject-driven generation benchmark. Project page:\nhttps://subjectfidelityoptimization.github.io/", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 06:59:25", "updated": "2025-06-04 06:59:25", "pdf_url": "http://arxiv.org/pdf/2506.03621v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03627v1", "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks", "authors": ["Lin Mu", "Guowei Chu", "Li Ni", "Lei Sang", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 07:13:27", "updated": "2025-06-04 07:13:27", "pdf_url": "http://arxiv.org/pdf/2506.03627v1", "comment": "13pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03637v1", "title": "RewardAnything: Generalizable Principle-Following Reward Models", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "abstract": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 07:30:16", "updated": "2025-06-04 07:30:16", "pdf_url": "http://arxiv.org/pdf/2506.03637v1", "comment": "23 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03642v1", "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data", "authors": ["Haoyu Zhang", "Meng Liu", "Zaijing Li", "Haokun Wen", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "abstract": "Visual-spatial understanding, the ability to infer object relationships and\nlayouts from visual input, is fundamental to downstream tasks such as robotic\nnavigation and embodied interaction. However, existing methods face spatial\nuncertainty and data scarcity, limiting the 3D spatial reasoning capability of\npre-trained vision-language models (VLMs). To address these challenges, we\npresent a unified framework for enhancing 3D spatial reasoning in pre-trained\nVLMs without modifying their architecture. This framework combines SpatialMind,\na structured prompting strategy that decomposes complex scenes and questions\ninto interpretable reasoning steps, with ScanForgeQA, a scalable\nquestion-answering dataset built from diverse 3D simulation scenes through an\nautomated construction process designed for fine-tuning. Extensive experiments\nacross multiple benchmarks demonstrate the individual and combined\neffectiveness of our prompting and fine-tuning strategies, and yield insights\nthat may inspire future research on visual-spatial understanding.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 07:36:33", "updated": "2025-06-04 07:36:33", "pdf_url": "http://arxiv.org/pdf/2506.03642v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03654v1", "title": "MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection", "authors": ["Xiaochun Lei", "Siqi Wu", "Weilin Wu", "Zetao Jiang"], "abstract": "Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6\\% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 07:46:24", "updated": "2025-06-04 07:46:24", "pdf_url": "http://arxiv.org/pdf/2506.03654v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03667v1", "title": "Accelerating SfM-based Pose Estimation with Dominating Set", "authors": ["Joji Joseph", "Bharadwaj Amrutur", "Shalabh Bhatnagar"], "abstract": "This paper introduces a preprocessing technique to speed up\nStructure-from-Motion (SfM) based pose estimation, which is critical for\nreal-time applications like augmented reality (AR), virtual reality (VR), and\nrobotics. Our method leverages the concept of a dominating set from graph\ntheory to preprocess SfM models, significantly enhancing the speed of the pose\nestimation process without losing significant accuracy. Using the OnePose\ndataset, we evaluated our method across various SfM-based pose estimation\ntechniques. The results demonstrate substantial improvements in processing\nspeed, ranging from 1.5 to 14.48 times, and a reduction in reference images and\npoint cloud size by factors of 17-23 and 2.27-4, respectively. This work offers\na promising solution for efficient and accurate 3D pose estimation, balancing\nspeed and accuracy in real-time applications.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 07:56:38", "updated": "2025-06-04 07:56:38", "pdf_url": "http://arxiv.org/pdf/2506.03667v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03673v1", "title": "Reason from Future: Reverse Thought Chain Enhances LLM Reasoning", "authors": ["Yinlong Xu", "Yanzhao Zheng", "Shuoshuo Sun", "Shuaihan Huang", "Baohua Dong", "Hangcheng Zhu", "Ruohui Huang", "Gang Yu", "Hongxia Xu", "Jian Wu"], "abstract": "It has been demonstrated that carefully designed reasoning paradigms, like\nChain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning\ncapabilities of small language models by detailed thinking and extensive\nthought searching, unbounded branching factors in the searching space create\nprohibitive reasoning consumption. However these methods fall into the trap of\nlocal optimum reasoning, which means the model lacks a global perspective while\nsolving problems. We propose a novel reasoning paradigm called Reason from\nFuture (RFF), which generates reasoning paths by bidirectional reasoning that\ncombines top-down planning with bottom-up reasoning accumulation. The essence\nof RFF lies in its reverse reasoning mechanism, which prioritizes core logical\nrelationships and imposes goal-oriented constraints on intermediate steps,\nthereby reducing the searching space and mitigating error accumulation inherent\nin sequential forward reasoning. Empirical evaluations across diverse\nexperiments demonstrate that RFF outperforms conventional paradigms with higher\naccuracy and less searching space to solve complex tasks.", "categories": ["cs.AI"], "published": "2025-06-04 08:03:17", "updated": "2025-06-04 08:03:17", "pdf_url": "http://arxiv.org/pdf/2506.03673v1", "comment": "Accepted by ACL 2025 findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03682v1", "title": "How PARTs assemble into wholes: Learning the relative composition of images", "authors": ["Melika Ayoughi", "Samira Abnar", "Chen Huang", "Chris Sandino", "Sayeri Lala", "Eeshan Gunesh Dhekane", "Dan Busbridge", "Shuangfei Zhai", "Vimal Thilak", "Josh Susskind", "Pascal Mettes", "Paul Groth", "Hanlin Goh"], "abstract": "The composition of objects and their parts, along with object-object\npositional relationships, provides a rich source of information for\nrepresentation learning. Hence, spatial-aware pretext tasks have been actively\nexplored in self-supervised learning. Existing works commonly start from a grid\nstructure, where the goal of the pretext task involves predicting the absolute\nposition index of patches within a fixed grid. However, grid-based approaches\nfall short of capturing the fluid and continuous nature of real-world object\ncompositions. We introduce PART, a self-supervised learning approach that\nleverages continuous relative transformations between off-grid patches to\novercome these limitations. By modeling how parts relate to each other in a\ncontinuous space, PART learns the relative composition of images-an off-grid\nstructural relative positioning process that generalizes beyond occlusions and\ndeformations. In tasks requiring precise spatial understanding such as object\ndetection and time series prediction, PART outperforms strong grid-based\nmethods like MAE and DropPos, while also maintaining competitive performance on\nglobal classification tasks with minimal hyperparameter tuning. By breaking\nfree from grid constraints, PART opens up an exciting new trajectory for\nuniversal self-supervised pretraining across diverse datatypes-from natural\nimages to EEG signals-with promising potential in video, medical imaging, and\naudio.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-06-04 08:12:18", "updated": "2025-06-04 08:12:18", "pdf_url": "http://arxiv.org/pdf/2506.03682v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03710v1", "title": "OSGNet @ Ego4D Episodic Memory Challenge 2025", "authors": ["Yisen Feng", "Haoyu Zhang", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "abstract": "In this report, we present our champion solutions for the three egocentric\nvideo localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.\nAll tracks require precise localization of the interval within an untrimmed\negocentric video. Previous unified video localization approaches often rely on\nlate fusion strategies, which tend to yield suboptimal results. To address\nthis, we adopt an early fusion-based video localization model to tackle all\nthree tasks, aiming to enhance localization accuracy. Ultimately, our method\nachieved first place in the Natural Language Queries, Goal Step, and Moment\nQueries tracks, demonstrating its effectiveness. Our code can be found at\nhttps://github.com/Yisen-Feng/OSGNet.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 08:41:42", "updated": "2025-06-04 08:41:42", "pdf_url": "http://arxiv.org/pdf/2506.03710v1", "comment": "The champion solutions for the three egocentric video localization\n  tracks(Natural Language Queries, Goal Step, and Moment Queries tracks) of the\n  Ego4D Episodic Memory Challenge at CVPR EgoVis Workshop 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03723v1", "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "authors": ["Chaeyun Jang", "Moonseok Choi", "Yegon Kim", "Hyungi Lee", "Juho Lee"], "abstract": "Uncertainty calibration is essential for the safe deployment of large\nlanguage models (LLMs), particularly when users rely on verbalized confidence\nestimates. While prior work has focused on classifiers or short-form\ngeneration, confidence calibration for chain-of-thought (CoT) reasoning remains\nlargely unexplored. Surprisingly, we find that supervised fine-tuning with\nscalar confidence labels alone suffices to elicit self-verification behavior of\nlanguage models, without any explicit reasoning supervision or reinforcement\nlearning-based rewards. Despite being trained only to produce a verbalized\nconfidence score without any self-verifying examples, the model learns to\ngenerate longer and self-checking responses for low-confidence queries while\nproviding more concise answers for high-confidence ones. We further propose a\nsimple rethinking method that boosts performance via test-time scaling based on\ncalibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such\nas MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning\nimproves both calibration and accuracy, while also enhancing interpretability\nby aligning the model's reasoning path with its confidence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 08:56:24", "updated": "2025-06-04 08:56:24", "pdf_url": "http://arxiv.org/pdf/2506.03723v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03735v1", "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "abstract": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-06-04 09:08:11", "updated": "2025-06-04 09:08:11", "pdf_url": "http://arxiv.org/pdf/2506.03735v1", "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03737v1", "title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices", "authors": ["Hao Yu", "Tangyu Jiang", "Shuning Jia", "Shannan Yan", "Shunning Liu", "Haolong Qian", "Guanghao Li", "Shuting Dong", "Huaisong Zhang", "Chun Yuan"], "abstract": "The Transformer architecture has revolutionized various regions since it was\nproposed, and its effectiveness largely depends on the ability to encode\npositional information. Traditional position encoding methods exhibit\nsignificant limitations due to lack of robustness and flexibility of position.\nTherefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these\nissues, which integrates positional information by rotating the embeddings in\nthe attention mechanism. However, RoPE requires manually defined rotation\nmatrices with limited transformation space, constraining the model's capacity.\nIn this work, we propose ComRoPE, which generalizes RoPE by defining it in\nterms of trainable commuting angle matrices. Specifically, we demonstrate that\npairwise commutativity of these matrices is essential for RoPE to achieve\nscalability and positional robustness. We formally define the RoPE Equation,\nwhich is an essential condition that ensures consistent performance with\nposition offsets. Based on the theoretical analysis, we present two types of\ntrainable commuting angle matrices as sufficient solutions to the RoPE\nequation, which significantly improve performance, surpassing the current\nstate-of-the-art method by 1.6% at training resolution and 2.9% at higher\nresolution on the ImageNet-1K dataset. Furthermore, our framework shows\nversatility in generalizing to existing RoPE formulations and offering new\ninsights for future positional encoding research. To ensure reproducibility,\nthe source code and instructions are available at\nhttps://github.com/Longin-Yu/ComRoPE", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 09:10:02", "updated": "2025-06-04 09:10:02", "pdf_url": "http://arxiv.org/pdf/2506.03737v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03740v1", "title": "SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution", "authors": ["Jianfeng Wu", "Nannan Xu"], "abstract": "Single image super-resolution is a well-known downstream task which aims to\nrestore low-resolution images into high-resolution images. At present, models\nbased on Transformers have shone brightly in the field of super-resolution due\nto their ability to capture long-term dependencies in information. However,\ncurrent methods typically compute self-attention in nonoverlapping windows to\nsave computational costs, and the standard self-attention computation only\nfocuses on its results, thereby neglecting the useful information across\nchannels and the rich spatial structural information generated in the\nintermediate process. Channel attention and spatial attention have,\nrespectively, brought significant improvements to various downstream visual\ntasks in terms of extracting feature dependency and spatial structure\nrelationships, but the synergistic relationship between channel and spatial\nattention has not been fully explored yet.To address these issues, we propose a\nnovel model. Synergistic Alternating Aggregation Transformer (SAAT), which can\nbetter utilize the potential information of features. In SAAT, we introduce the\nEfficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial\n& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines\nefficient channel attention with shifted window attention, enhancing non-local\nfeature fusion, and producing more visually appealing results. On the other\nhand, SWSAG leverages spatial attention to capture rich structured feature\ninformation, thereby enabling SAAT to more effectively extract structural\nfeatures.Extensive experimental results and ablation studies demonstrate the\neffectiveness of SAAT in the field of super-resolution. SAAT achieves\nperformance comparable to that of the state-of-the-art (SOTA) under the same\nquantity of parameters.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 09:12:24", "updated": "2025-06-04 09:12:24", "pdf_url": "http://arxiv.org/pdf/2506.03740v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03755v1", "title": "Misalignment or misuse? The AGI alignment tradeoff", "authors": ["Max Hellrigel-Holderbaum", "Leonard Dung"], "abstract": "Creating systems that are aligned with our goals is seen as a leading\napproach to create safe and beneficial AI in both leading AI companies and the\nacademic field of AI safety. We defend the view that misaligned AGI - future,\ngenerally intelligent (robotic) AI agents - poses catastrophic risks. At the\nsame time, we support the view that aligned AGI creates a substantial risk of\ncatastrophic misuse by humans. While both risks are severe and stand in tension\nwith one another, we show that - in principle - there is room for alignment\napproaches which do not increase misuse risk. We then investigate how the\ntradeoff between misalignment and misuse looks empirically for different\ntechnical approaches to AI alignment. Here, we argue that many current\nalignment techniques and foreseeable improvements thereof plausibly increase\nrisks of catastrophic misuse. Since the impacts of AI depend on the social\ncontext, we close by discussing important social factors and suggest that to\nreduce the risk of a misuse catastrophe due to aligned AGI, techniques such as\nrobustness, AI control methods and especially good governance seem essential.", "categories": ["cs.CY", "cs.AI"], "published": "2025-06-04 09:22:37", "updated": "2025-06-04 09:22:37", "pdf_url": "http://arxiv.org/pdf/2506.03755v1", "comment": "Forthcoming in Philosophical Studies", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03758v1", "title": "Scaling CrossQ with Weight Normalization", "authors": ["Daniel Palenicek", "Florian Vogt", "Jan Peters"], "abstract": "Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics which are\nemphasized by higher UTDs, particularly Q-bias explosion and the growing\nmagnitude of critic network weights. To address this, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nprevents potential loss of plasticity and keeps the effective learning rate\nconstant. Our proposed approach reliably scales with increasing UTD ratios,\nachieving competitive or superior performance across a range of challenging\ntasks on the DeepMind control benchmark, notably the complex dog and humanoid\nenvironments. This work eliminates the need for drastic interventions, such as\nnetwork resets, and offers a robust pathway for improving sample efficiency and\nscalability in model-free reinforcement learning.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-04 09:24:17", "updated": "2025-06-04 09:24:17", "pdf_url": "http://arxiv.org/pdf/2506.03758v1", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.07523", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03762v1", "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models", "authors": ["Yifeng Gu", "Zicong Jiang", "Jianxiu Jin", "Kailing Guo", "Ziyang Zhang", "Xiangmin Xu"], "abstract": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 09:25:53", "updated": "2025-06-04 09:25:53", "pdf_url": "http://arxiv.org/pdf/2506.03762v1", "comment": "14 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03784v1", "title": "When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective", "authors": ["Beatrix M. G. Nielsen", "Emanuele Marconato", "Andrea Dittadi", "Luigi Gresele"], "abstract": "When and why representations learned by different deep neural networks are\nsimilar is an active research topic. We choose to address these questions from\nthe perspective of identifiability theory, which suggests that a measure of\nrepresentational similarity should be invariant to transformations that leave\nthe model distribution unchanged. Focusing on a model family which includes\nseveral popular pre-training approaches, e.g., autoregressive language models,\nwe explore when models which generate distributions that are close have similar\nrepresentations. We prove that a small Kullback-Leibler divergence between the\nmodel distributions does not guarantee that the corresponding representations\nare similar. This has the important corollary that models arbitrarily close to\nmaximizing the likelihood can still learn dissimilar representations, a\nphenomenon mirrored in our empirical observations on models trained on\nCIFAR-10. We then define a distributional distance for which closeness implies\nrepresentational similarity, and in synthetic experiments, we find that wider\nnetworks learn distributions which are closer with respect to our distance and\nhave more similar representations. Our results establish a link between\ncloseness in distribution and representational similarity.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-06-04 09:44:22", "updated": "2025-06-04 09:44:22", "pdf_url": "http://arxiv.org/pdf/2506.03784v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03785v1", "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "abstract": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-06-04 09:46:43", "updated": "2025-06-04 09:46:43", "pdf_url": "http://arxiv.org/pdf/2506.03785v1", "comment": "4 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03827v1", "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "abstract": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-04 10:57:18", "updated": "2025-06-04 10:57:18", "pdf_url": "http://arxiv.org/pdf/2506.03827v1", "comment": "Accepted by SIGIR2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03828v1", "title": "AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance", "authors": ["Dhaval Patel", "Shuxin Lin", "James Rayfield", "Nianjun Zhou", "Roman Vaculin", "Natalia Martinez", "Fearghal O'donncha", "Jayant Kalagnanam"], "abstract": "AI for Industrial Asset Lifecycle Management aims to automate complex\noperational workflows -- such as condition monitoring, maintenance planning,\nand intervention scheduling -- to reduce human workload and minimize system\ndowntime. Traditional AI/ML approaches have primarily tackled these problems in\nisolation, solving narrow tasks within the broader operational pipeline. In\ncontrast, the emergence of AI agents and large language models (LLMs)\nintroduces a next-generation opportunity: enabling end-to-end automation across\nthe entire asset lifecycle. This paper envisions a future where AI agents\nautonomously manage tasks that previously required distinct expertise and\nmanual coordination. To this end, we introduce AssetOpsBench -- a unified\nframework and environment designed to guide the development, orchestration, and\nevaluation of domain-specific agents tailored for Industry 4.0 applications. We\noutline the key requirements for such holistic systems and provide actionable\ninsights into building agents that integrate perception, reasoning, and control\nfor real-world industrial operations. The software is available at\nhttps://github.com/IBM/AssetOpsBench.", "categories": ["cs.AI", "cs.MA"], "published": "2025-06-04 10:57:35", "updated": "2025-06-04 10:57:35", "pdf_url": "http://arxiv.org/pdf/2506.03828v1", "comment": "39 pages, 18 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03837v1", "title": "HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature Superconductors for AI-Driven Critical Temperature Prediction", "authors": ["Xiao-Qi Han", "Ze-Feng Gao", "Xin-De Wang", "Zhenfeng Ouyang", "Peng-Jie Guo", "Zhong-Yi Lu"], "abstract": "The discovery of high-temperature superconducting materials holds great\nsignificance for human industry and daily life. In recent years, research on\npredicting superconducting transition temperatures using artificial\nintelligence~(AI) has gained popularity, with most of these tools claiming to\nachieve remarkable accuracy. However, the lack of widely accepted benchmark\ndatasets in this field has severely hindered fair comparisons between different\nAI algorithms and impeded further advancement of these methods. In this work,\nwe present the HTSC-2025, an ambient-pressure high-temperature superconducting\nbenchmark dataset. This comprehensive compilation encompasses theoretically\npredicted superconducting materials discovered by theoretical physicists from\n2023 to 2025 based on BCS superconductivity theory, including the renowned\nX$_2$YH$_6$ system, perovskite MXH$_3$ system, M$_3$XH$_8$ system, cage-like\nBCN-doped metal atomic systems derived from LaH$_{10}$ structural evolution,\nand two-dimensional honeycomb-structured systems evolving from MgB$_2$. The\nHTSC-2025 benchmark has been open-sourced at\nhttps://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This\nbenchmark holds significant importance for accelerating the discovery of\nsuperconducting materials using AI-based methods.", "categories": ["cond-mat.supr-con", "cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "published": "2025-06-04 11:14:00", "updated": "2025-06-04 11:14:00", "pdf_url": "http://arxiv.org/pdf/2506.03837v1", "comment": "7 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03872v1", "title": "JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting", "authors": ["Yang Xiao", "Guoan Xu", "Qiang Wu", "Wenjing Jia"], "abstract": "Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge\nwith wide applications. Recent advances in feed-forward 3D Gaussian sparse-view\nreconstruction methods provide an efficient solution for real-time novel view\nsynthesis by leveraging geometric priors learned from large-scale multi-view\ndatasets and computing 3D Gaussian centers via back-projection. Despite\noffering strong geometric cues, both feed-forward multi-view depth estimation\nand flow-depth joint estimation face key limitations: the former suffers from\nmislocation and artifact issues in low-texture or repetitive regions, while the\nlatter is prone to local noise and global inconsistency due to unreliable\nmatches when ground-truth flow supervision is unavailable. To overcome this, we\npropose JointSplat, a unified framework that leverages the complementarity\nbetween optical flow and depth via a novel probabilistic optimization\nmechanism. Specifically, this pixel-level mechanism scales the information\nfusion between depth and flow based on the matching probability of optical flow\nduring training. Building upon the above mechanism, we further propose a novel\nmulti-view depth-consistency loss to leverage the reliability of supervision\nwhile suppressing misleading gradients in uncertain areas. Evaluated on\nRealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art\n(SOTA) methods, demonstrating the effectiveness and robustness of our proposed\nprobabilistic joint flow-depth optimization approach for high-fidelity\nsparse-view 3D reconstruction.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 12:04:40", "updated": "2025-06-04 12:04:40", "pdf_url": "http://arxiv.org/pdf/2506.03872v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03880v1", "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Jianhua Tao"], "abstract": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 12:16:41", "updated": "2025-06-04 12:16:41", "pdf_url": "http://arxiv.org/pdf/2506.03880v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03915v1", "title": "Causal Explanations Over Time: Articulated Reasoning for Interactive Environments", "authors": ["Sebastian R\u00f6dling", "Matej Ze\u010devi\u0107", "Devendra Singh Dhami", "Kristian Kersting"], "abstract": "Structural Causal Explanations (SCEs) can be used to automatically generate\nexplanations in natural language to questions about given data that are\ngrounded in a (possibly learned) causal model. Unfortunately they work for\nsmall data only. In turn they are not attractive to offer reasons for events,\ne.g., tracking causal changes over multiple time steps, or a behavioral\ncomponent that involves feedback loops through actions of an agent. To this\nend, we generalize SCEs to a (recursive) formulation of explanation trees to\ncapture the temporal interactions between reasons. We show the benefits of this\nmore general SCE algorithm on synthetic time-series data and a 2D grid game,\nand further compare it to the base SCE and other existing methods for causal\nexplanations.", "categories": ["cs.AI"], "published": "2025-06-04 13:07:16", "updated": "2025-06-04 13:07:16", "pdf_url": "http://arxiv.org/pdf/2506.03915v1", "comment": "Main paper: 9 pages, References: 2 pages, Supplementary: 9 pages.\n  Number of figures: 10, number of tables: 3", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03930v1", "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation", "authors": ["Yuansheng Ni", "Ping Nie", "Kai Zou", "Xiang Yue", "Wenhu Chen"], "abstract": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.", "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2025-06-04 13:24:44", "updated": "2025-06-04 13:24:44", "pdf_url": "http://arxiv.org/pdf/2506.03930v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03933v1", "title": "DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models", "authors": ["Jia Fu", "Yongtao Wu", "Yihang Chen", "Kunyu Peng", "Xiao Zhang", "Volkan Cevher", "Sepideh Pashami", "Anders Holst"], "abstract": "Vision Language Models (VLMs) have shown remarkable capabilities in\nmultimodal understanding, yet their susceptibility to perturbations poses a\nsignificant threat to their reliability in real-world applications. Despite\noften being imperceptible to humans, these perturbations can drastically alter\nmodel outputs, leading to erroneous interpretations and decisions. This paper\nintroduces DiffCAP, a novel diffusion-based purification strategy that can\neffectively neutralize adversarial corruptions in VLMs. We observe that adding\nminimal noise to an adversarially corrupted image significantly alters its\nlatent embedding with respect to VLMs. Building on this insight, DiffCAP\ncumulatively injects random Gaussian noise into adversarially perturbed input\ndata. This process continues until the embeddings of two consecutive noisy\nimages reach a predefined similarity threshold, indicating a potential approach\nto neutralize the adversarial effect. Subsequently, a pretrained diffusion\nmodel is employed to denoise the stabilized image, recovering a clean\nrepresentation suitable for the VLMs to produce an output. Through extensive\nexperiments across six datasets with three VLMs under varying attack strengths\nin three task scenarios, we show that DiffCAP consistently outperforms existing\ndefense techniques by a substantial margin. Notably, DiffCAP significantly\nreduces both hyperparameter tuning complexity and the required diffusion time,\nthereby accelerating the denoising process. Equipped with strong theoretical\nand empirical support, DiffCAP provides a robust and practical solution for\nsecurely deploying VLMs in adversarial environments.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 13:26:33", "updated": "2025-06-04 13:26:33", "pdf_url": "http://arxiv.org/pdf/2506.03933v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03939v1", "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning", "authors": ["Junqi Gao", "Xiang Zou", "YIng Ai", "Dong Li", "Yichen Niu", "Biqing Qi", "Jianxing Liu"], "abstract": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-04 13:31:21", "updated": "2025-06-04 13:31:21", "pdf_url": "http://arxiv.org/pdf/2506.03939v1", "comment": "Accepted by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03941v1", "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations", "authors": ["Vivian Nguyen", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil"], "abstract": "During a conversation, there can come certain moments where its outcome hangs\nin the balance. In these pivotal moments, how one responds can put the\nconversation on substantially different trajectories leading to significantly\ndifferent outcomes. Systems that can detect when such moments arise could\nassist conversationalists in domains with highly consequential outcomes, such\nas mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting\nsuch pivotal moments as they happen, in an online fashion. Our approach relies\non the intuition that a moment is pivotal if our expectation of the outcome\nvaries widely depending on what might be said next. By applying our method to\ncrisis counseling conversations, we first validate it by showing that it aligns\nwith human perception -- counselors take significantly longer to respond during\nmoments detected by our method -- and with the eventual conversational\ntrajectory -- which is more likely to change course at these times. We then use\nour framework to explore the relation of the counselor's response during\npivotal moments with the eventual outcome of the session.", "categories": ["cs.CL", "cs.AI", "cs.CY", "physics.soc-ph"], "published": "2025-06-04 13:31:58", "updated": "2025-06-04 13:31:58", "pdf_url": "http://arxiv.org/pdf/2506.03941v1", "comment": "To appear in the Proceedings of ACL 2025. Code and demo available in\n  ConvoKit (convokit.cornell.edu)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03954v1", "title": "HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark", "authors": ["Jianqing Zhang", "Xinghao Wu", "Yanbing Zhou", "Xiaoting Sun", "Qiqi Cai", "Yang Liu", "Yang Hua", "Zhenzhe Zheng", "Jian Cao", "Qiang Yang"], "abstract": "As AI evolves, collaboration among heterogeneous models helps overcome data\nscarcity by enabling knowledge transfer across institutions and devices.\nTraditional Federated Learning (FL) only supports homogeneous models, limiting\ncollaboration among clients with heterogeneous model architectures. To address\nthis, Heterogeneous Federated Learning (HtFL) methods are developed to enable\ncollaboration across diverse heterogeneous models while tackling the data\nheterogeneity issue at the same time. However, a comprehensive benchmark for\nstandardized evaluation and analysis of the rapidly growing HtFL methods is\nlacking. Firstly, the highly varied datasets, model heterogeneity scenarios,\nand different method implementations become hurdles to making easy and fair\ncomparisons among HtFL methods. Secondly, the effectiveness and robustness of\nHtFL methods are under-explored in various scenarios, such as the medical\ndomain and sensor signal modality. To fill this gap, we introduce the first\nHeterogeneous Federated Learning Library (HtFLlib), an easy-to-use and\nextensible framework that integrates multiple datasets and model heterogeneity\nscenarios, offering a robust benchmark for research and practical applications.\nSpecifically, HtFLlib integrates (1) 12 datasets spanning various domains,\nmodalities, and data heterogeneity scenarios; (2) 40 model architectures,\nranging from small to large, across three modalities; (3) a modularized and\neasy-to-extend HtFL codebase with implementations of 10 representative HtFL\nmethods; and (4) systematic evaluations in terms of accuracy, convergence,\ncomputation costs, and communication costs. We emphasize the advantages and\npotential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze\nadvancing HtFL research and enable its broader applications. The code is\nreleased at https://github.com/TsingZ0/HtFLlib.", "categories": ["cs.LG", "cs.AI", "cs.DC"], "published": "2025-06-04 13:44:00", "updated": "2025-06-04 13:44:00", "pdf_url": "http://arxiv.org/pdf/2506.03954v1", "comment": "Accepted by KDD2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03964v1", "title": "Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection", "authors": ["HyunGi Kim", "Jisoo Mok", "Dongjun Lee", "Jaihyun Lew", "Sungjae Kim", "Sungroh Yoon"], "abstract": "Utilizing the complex inter-variable causal relationships within multivariate\ntime-series provides a promising avenue toward more robust and reliable\nmultivariate time-series anomaly detection (MTSAD) but remains an underexplored\narea of research. This paper proposes Causality-Aware contrastive learning for\nRObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that\nincorporates the notion of causality into contrastive learning. CAROTS employs\ntwo data augmentors to obtain causality-preserving and -disturbing samples that\nserve as a wide range of normal variations and synthetic anomalies,\nrespectively. With causality-preserving and -disturbing samples as positives\nand negatives, CAROTS performs contrastive learning to train an encoder whose\nlatent space separates normal and abnormal samples based on causality.\nMoreover, CAROTS introduces a similarity-filtered one-class contrastive loss\nthat encourages the contrastive learning process to gradually incorporate more\nsemantically diverse samples with common causal relationships. Extensive\nexperiments on five real-world and two synthetic datasets validate that the\nintegration of causal relationships endows CAROTS with improved MTSAD\ncapabilities. The code is available at https://github.com/kimanki/CAROTS.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-04 13:57:11", "updated": "2025-06-04 13:57:11", "pdf_url": "http://arxiv.org/pdf/2506.03964v1", "comment": "Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03997v1", "title": "A framework for Conditional Reasoning in Answer Set Programming", "authors": ["Mario Alviano", "Laura Giordano", "Daniele Theseider Dupr\u00e9"], "abstract": "In this paper we introduce a Conditional Answer Set Programming framework\n(Conditional ASP) for the definition of conditional extensions of Answer Set\nProgramming (ASP). The approach builds on a conditional logic with typicality,\nand on the combination of a conditional knowledge base with an ASP program, and\nallows for conditional reasoning over the answer sets of the program. The\nformalism relies on a multi-preferential semantics (and on the KLM preferential\nsemantics, as a special case) to provide an interpretation of conditionals.", "categories": ["cs.AI", "cs.LO", "I.2.4"], "published": "2025-06-04 14:25:34", "updated": "2025-06-04 14:25:34", "pdf_url": "http://arxiv.org/pdf/2506.03997v1", "comment": "19 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04001v1", "title": "CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor", "authors": ["Han Ji", "Yuqi Feng", "Jiahao Fan", "Yanan Sun"], "abstract": "Performance predictors have emerged as a promising method to accelerate the\nevaluation stage of neural architecture search (NAS). These predictors estimate\nthe performance of unseen architectures by learning from the correlation\nbetween a small set of trained architectures and their performance. However,\nmost existing predictors ignore the inherent distribution shift between limited\ntraining samples and diverse test samples. Hence, they tend to learn spurious\ncorrelations as shortcuts to predictions, leading to poor generalization. To\naddress this, we propose a Causality-guided Architecture Representation\nLearning (CARL) method aiming to separate critical (causal) and redundant\n(non-causal) features of architectures for generalizable architecture\nperformance prediction. Specifically, we employ a substructure extractor to\nsplit the input architecture into critical and redundant substructures in the\nlatent space. Then, we generate multiple interventional samples by pairing\ncritical representations with diverse redundant representations to prioritize\ncritical features. Extensive experiments on five NAS search spaces demonstrate\nthe state-of-the-art accuracy and superior interpretability of CARL. For\ninstance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-04 14:30:55", "updated": "2025-06-04 14:30:55", "pdf_url": "http://arxiv.org/pdf/2506.04001v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04006v1", "title": "TransClean: Finding False Positives in Multi-Source Entity Matching under Real-World Conditions via Transitive Consistency", "authors": ["Fernando de Meer Pardo", "Branka Hadji Misheva", "Martin Braschler", "Kurt Stockinger"], "abstract": "We present TransClean, a method for detecting false positive predictions of\nentity matching algorithms under real-world conditions characterized by\nlarge-scale, noisy, and unlabeled multi-source datasets that undergo\ndistributional shifts. TransClean is explicitly designed to operate with\nmultiple data sources in an efficient, robust and fast manner while accounting\nfor edge cases and requiring limited manual labeling. TransClean leverages the\nTransitive Consistency of a matching, a measure of the consistency of a\npairwise matching model f_theta on the matching it produces G_f_theta, based\nboth on its predictions on directly evaluated record pairs and its predictions\non implied record pairs. TransClean iteratively modifies a matching through\ngradually removing false positive matches while removing as few true positive\nmatches as possible. In each of these steps, the estimation of the Transitive\nConsistency is exclusively done through model evaluations and produces\nquantities that can be used as proxies of the amounts of true and false\npositives in the matching while not requiring any manual labeling, producing an\nestimate of the quality of the matching and indicating which record groups are\nlikely to contain false positives. In our experiments, we compare combining\nTransClean with a naively trained pairwise matching model (DistilBERT) and with\na state-of-the-art end-to-end matching method (CLER) and illustrate the\nflexibility of TransClean in being able to detect most of the false positives\nof either setup across a variety of datasets. Our experiments show that\nTransClean induces an average +24.42 F1 score improvement for entity matching\nin a multi-source setting when compared to traditional pair-wise matching\nalgorithms.", "categories": ["cs.DB", "cs.AI", "cs.LG"], "published": "2025-06-04 14:33:41", "updated": "2025-06-04 14:33:41", "pdf_url": "http://arxiv.org/pdf/2506.04006v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04013v1", "title": "Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion", "authors": ["Seymanur Akti", "Tuan Nam Nguyen", "Alexander Waibel"], "abstract": "Expressive voice conversion aims to transfer both speaker identity and\nexpressive attributes from a target speech to a given source speech. In this\nwork, we improve over a self-supervised, non-autoregressive framework with a\nconditional variational autoencoder, focusing on reducing source timbre leakage\nand improving linguistic-acoustic disentanglement for better style transfer. To\nminimize style leakage, we use multilingual discrete speech units for content\nrepresentation and reinforce embeddings with augmentation-based similarity loss\nand mix-style layer normalization. To enhance expressivity transfer, we\nincorporate local F0 information via cross-attention and extract style\nembeddings enriched with global pitch and energy features. Experiments show our\nmodel outperforms baselines in emotion and speaker similarity, demonstrating\nsuperior style adaptation and reduced source style leakage.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-06-04 14:42:12", "updated": "2025-06-04 14:42:12", "pdf_url": "http://arxiv.org/pdf/2506.04013v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04018v1", "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Goun\u00e9", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "abstract": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.11; K.4.1; I.2.6"], "published": "2025-06-04 14:46:47", "updated": "2025-06-04 14:46:47", "pdf_url": "http://arxiv.org/pdf/2506.04018v1", "comment": "Prepint, under review for NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04022v1", "title": "Interpretability by Design for Efficient Multi-Objective Reinforcement Learning", "authors": ["Qiyue Xia", "J. Michael Herrmann"], "abstract": "Multi-objective reinforcement learning (MORL) aims at optimising several,\noften conflicting goals in order to improve flexibility and reliability of RL\nin practical tasks. This can be achieved by finding diverse policies that are\noptimal for some objective preferences and non-dominated by optimal policies\nfor other preferences so that they form a Pareto front in the multi-objective\nperformance space. The relation between the multi-objective performance space\nand the parameter space that represents the policies is generally non-unique.\nUsing a training scheme that is based on a locally linear map between the\nparameter space and the performance space, we show that an approximate Pareto\nfront can provide an interpretation of the current parameter vectors in terms\nof the objectives which enables an effective search within contiguous solution\ndomains. Experiments are conducted with and without retraining across different\ndomains, and the comparison with previous methods demonstrates the efficiency\nof our approach.", "categories": ["cs.AI", "cs.LG"], "published": "2025-06-04 14:52:18", "updated": "2025-06-04 14:52:18", "pdf_url": "http://arxiv.org/pdf/2506.04022v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04036v1", "title": "Privacy and Security Threat for OpenAI GPTs", "authors": ["Wei Wenying", "Zhao Kaifa", "Xue Lei", "Fan Ming"], "abstract": "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications.", "categories": ["cs.CR", "cs.AI"], "published": "2025-06-04 14:58:29", "updated": "2025-06-04 14:58:29", "pdf_url": "http://arxiv.org/pdf/2506.04036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04038v1", "title": "Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems", "authors": ["Sven Kirchner", "Alois C. Knoll"], "abstract": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements.", "categories": ["cs.SE", "cs.AI"], "published": "2025-06-04 15:01:59", "updated": "2025-06-04 15:01:59", "pdf_url": "http://arxiv.org/pdf/2506.04038v1", "comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04039v1", "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "authors": ["Jiulong Wu", "Zhengliang Shi", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Lingyong Yan", "Min Cao", "Min Zhang"], "abstract": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-04 15:03:50", "updated": "2025-06-04 15:03:50", "pdf_url": "http://arxiv.org/pdf/2506.04039v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04043v1", "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "abstract": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "published": "2025-06-04 15:09:20", "updated": "2025-06-04 15:09:20", "pdf_url": "http://arxiv.org/pdf/2506.04043v1", "comment": "Accepted at ACL WOAH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04044v1", "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs", "authors": ["Aleksey Kudelya", "Alexander Shirnin"], "abstract": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 15:10:09", "updated": "2025-06-04 15:10:09", "pdf_url": "http://arxiv.org/pdf/2506.04044v1", "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04050v1", "title": "Explainability-Based Token Replacement on LLM-Generated Text", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "abstract": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 15:15:42", "updated": "2025-06-04 15:15:42", "pdf_url": "http://arxiv.org/pdf/2506.04050v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04051v1", "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "authors": ["Tim Franzmeyer", "Archie Sravankumar", "Lijuan Liu", "Yuning Mao", "Rui Hou", "Sinong Wang", "Jakob N. Foerster", "Luke Zettlemoyer", "Madian Khabsa"], "abstract": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 15:16:21", "updated": "2025-06-04 15:16:21", "pdf_url": "http://arxiv.org/pdf/2506.04051v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04058v1", "title": "Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays", "authors": ["Bulat Maksudov", "Kathleen Curran", "Alessandra Mileo"], "abstract": "An essential step in deploying medical imaging models is ensuring alignment\nwith clinical knowledge and interpretability. We focus on mapping clinical\nconcepts into the latent space of generative models to identify Concept\nActivation Vectors (CAVs). Using a simple reconstruction autoencoder, we link\nuser-defined concepts to image-level features without explicit label training.\nThe extracted concepts are stable across datasets, enabling visual explanations\nthat highlight clinically relevant features. By traversing latent space along\nconcept directions, we produce counterfactuals that exaggerate or reduce\nspecific clinical features. Preliminary results on chest X-rays show promise\nfor large pathologies like cardiomegaly, while smaller pathologies remain\nchallenging due to reconstruction limits. Although not outperforming baselines,\nthis approach offers a path toward interpretable, concept-based explanations\naligned with clinical knowledge.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-06-04 15:23:12", "updated": "2025-06-04 15:23:12", "pdf_url": "http://arxiv.org/pdf/2506.04058v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04078v1", "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "authors": ["Ming Zhang", "Yujiong Shen", "Zelin Li", "Huayu Sha", "Binze Hu", "Yuhui Wang", "Chenhao Huang", "Shichun Liu", "Jingqi Tong", "Changhao Jiang", "Mingxu Chai", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "abstract": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 15:43:14", "updated": "2025-06-04 15:43:14", "pdf_url": "http://arxiv.org/pdf/2506.04078v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04079v1", "title": "EuroLLM-9B: Technical Report", "authors": ["Pedro Henrique Martins", "Jo\u00e3o Alves", "Patrick Fernandes", "Nuno M. Guerreiro", "Ricardo Rei", "Amin Farajian", "Mateusz Klimaszewski", "Duarte M. Alves", "Jos\u00e9 Pombal", "Manuel Faysse", "Pierre Colombo", "Fran\u00e7ois Yvon", "Barry Haddow", "Jos\u00e9 G. C. de Souza", "Alexandra Birch", "Andr\u00e9 F. T. Martins"], "abstract": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 15:43:31", "updated": "2025-06-04 15:43:31", "pdf_url": "http://arxiv.org/pdf/2506.04079v1", "comment": "56 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04088v1", "title": "Multimodal Tabular Reasoning with Privileged Structured Information", "authors": ["Jun-Peng Jiang", "Yu Xia", "Hai-Long Sun", "Shiyin Lu", "Qing-Guo Chen", "Weihua Luo", "Kaifu Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "abstract": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-06-04 15:46:30", "updated": "2025-06-04 15:46:30", "pdf_url": "http://arxiv.org/pdf/2506.04088v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04089v1", "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment", "authors": ["Anastasiia Ivanova", "Eva Bakaeva", "Zoya Volovikova", "Alexey K. Kovalev", "Aleksandr I. Panov"], "abstract": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "published": "2025-06-04 15:47:07", "updated": "2025-06-04 15:47:07", "pdf_url": "http://arxiv.org/pdf/2506.04089v1", "comment": "ACL 2025 (Main Conference)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04098v1", "title": "TextAtari: 100K Frames Game Playing with Language Agents", "authors": ["Wenhao Li", "Wenwu Li", "Chuyun Shen", "Junjie Sheng", "Zixiao Huang", "Di Wu", "Yun Hua", "Wei Yin", "Xiangfeng Wang", "Hongyuan Zha", "Bo Jin"], "abstract": "We present TextAtari, a benchmark for evaluating language agents on very\nlong-horizon decision-making tasks spanning up to 100,000 steps. By translating\nthe visual state representations of classic Atari games into rich textual\ndescriptions, TextAtari creates a challenging test bed that bridges sequential\ndecision-making with natural language processing. The benchmark includes nearly\n100 distinct tasks with varying complexity, action spaces, and planning\nhorizons, all rendered as text through an unsupervised representation learning\nframework (AtariARI). We evaluate three open-source large language models\n(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks\n(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how\ndifferent forms of prior knowledge affect performance on these long-horizon\nchallenges. Four scenarios-Basic, Obscured, Manual Augmentation, and\nReference-based-investigate the impact of semantic understanding, instruction\ncomprehension, and expert demonstrations on agent decision-making. Our results\nreveal significant performance gaps between language agents and human players\nin extensive planning tasks, highlighting challenges in sequential reasoning,\nstate tracking, and strategic planning across tens of thousands of steps.\nTextAtari provides standardized evaluation protocols, baseline implementations,\nand a framework for advancing research at the intersection of language models\nand planning.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 15:55:27", "updated": "2025-06-04 15:55:27", "pdf_url": "http://arxiv.org/pdf/2506.04098v1", "comment": "51 pages, 39 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04116v1", "title": "A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging", "authors": ["Xuanru Zhou", "Jiarun Liu", "Shoujun Yu", "Hao Yang", "Cheng Li", "Tao Tan", "Shanshan Wang"], "abstract": "In medical imaging, 4D MRI enables dynamic 3D visualization, yet the\ntrade-off between spatial and temporal resolution requires prolonged scan time\nthat can compromise temporal fidelity--especially during rapid, large-amplitude\nmotion. Traditional approaches typically rely on registration-based\ninterpolation to generate intermediate frames. However, these methods struggle\nwith large deformations, resulting in misregistration, artifacts, and\ndiminished spatial consistency. To address these challenges, we propose\nTSSC-Net, a novel framework that generates intermediate frames while preserving\nspatial consistency. To improve temporal fidelity under fast motion, our\ndiffusion-based temporal super-resolution network generates intermediate frames\nusing the start and end frames as key references, achieving 6x temporal\nsuper-resolution in a single inference step. Additionally, we introduce a novel\ntri-directional Mamba-based module that leverages long-range contextual\ninformation to effectively resolve spatial inconsistencies arising from\ncross-slice misalignment, thereby enhancing volumetric coherence and correcting\ncross-slice errors. Extensive experiments were performed on the public ACDC\ncardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results\ndemonstrate that TSSC-Net can generate high-resolution dynamic MRI from\nfast-motion data while preserving structural fidelity and spatial consistency.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-06-04 16:09:19", "updated": "2025-06-04 16:09:19", "pdf_url": "http://arxiv.org/pdf/2506.04116v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04121v1", "title": "A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "abstract": "Over the past decade, Medical Image Segmentation (MIS) using Deep Neural\nNetworks (DNNs) has achieved significant performance improvements and holds\ngreat promise for future developments. This paper presents a comprehensive\nstudy on MIS based on DNNs. Intelligent Vision Systems are often evaluated\nbased on their output levels, such as Data, Information, Knowledge,\nIntelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at\nthese levels are the focus of research. Additionally, Explainable Artificial\nIntelligence (XAI) has become an important research direction, as it aims to\nuncover the \"black box\" nature of previous DNN architectures to meet the\nrequirements of transparency and ethics. The study emphasizes the importance of\nMIS in disease diagnosis and early detection, particularly for increasing the\nsurvival rate of cancer patients through timely diagnosis. XAI and early\nprediction are considered two important steps in the journey from\n\"intelligence\" to \"wisdom.\" Additionally, the paper addresses existing\nchallenges and proposes potential solutions to enhance the efficiency of\nimplementing DNN-based MIS.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-06-04 16:15:03", "updated": "2025-06-04 16:15:03", "pdf_url": "http://arxiv.org/pdf/2506.04121v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04129v1", "title": "Recent Advances in Medical Image Classification", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "abstract": "Medical image classification is crucial for diagnosis and treatment,\nbenefiting significantly from advancements in artificial intelligence. The\npaper reviews recent progress in the field, focusing on three levels of\nsolutions: basic, specific, and applied. It highlights advances in traditional\nmethods using deep learning models like Convolutional Neural Networks and\nVision Transformers, as well as state-of-the-art approaches with Vision\nLanguage Models. These models tackle the issue of limited labeled data, and\nenhance and explain predictive results through Explainable Artificial\nIntelligence.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-06-04 16:20:26", "updated": "2025-06-04 16:20:26", "pdf_url": "http://arxiv.org/pdf/2506.04129v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04131v1", "title": "CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues", "authors": ["Disha Sheshanarayana", "Tanishka Magar", "Ayushi Mittal", "Neelam Chaplot"], "abstract": "Courtrooms are places where lives are determined and fates are sealed, yet\nthey are not impervious to manipulation. Strategic use of manipulation in legal\njargon can sway the opinions of judges and affect the decisions. Despite the\ngrowing advancements in NLP, its application in detecting and analyzing\nmanipulation within the legal domain remains largely unexplored. Our work\naddresses this gap by introducing LegalCon, a dataset of 1,063 annotated\ncourtroom conversations labeled for manipulation detection, identification of\nprimary manipulators, and classification of manipulative techniques, with a\nfocus on long conversations. Furthermore, we propose CLAIM, a two-stage,\nIntent-driven Multi-agent framework designed to enhance manipulation analysis\nby enabling context-aware and informed decision-making. Our results highlight\nthe potential of incorporating agentic frameworks to improve fairness and\ntransparency in judicial processes. We hope that this contributes to the\nbroader application of NLP in legal discourse analysis and the development of\nrobust tools to support fairness in legal decision-making. Our code and data\nare available at https://github.com/Disha1001/CLAIM.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 16:22:59", "updated": "2025-06-04 16:22:59", "pdf_url": "http://arxiv.org/pdf/2506.04131v1", "comment": "Accepted to SICon 2025 ACL", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04132v1", "title": "Plant Bioelectric Early Warning Systems: A Five-Year Investigation into Human-Plant Electromagnetic Communication", "authors": ["Peter A. Gloor"], "abstract": "We present a comprehensive investigation into plant bioelectric responses to\nhuman presence and emotional states, building on five years of systematic\nresearch. Using custom-built plant sensors and machine learning classification,\nwe demonstrate that plants generate distinct bioelectric signals correlating\nwith human proximity, emotional states, and physiological conditions. A deep\nlearning model based on ResNet50 architecture achieved 97% accuracy in\nclassifying human emotional states through plant voltage spectrograms, while\ncontrol models with shuffled labels achieved only 30% accuracy. This study\nsynthesizes findings from multiple experiments spanning 2020-2025, including\nindividual recognition (66% accuracy), eurythmic gesture detection, stress\nprediction, and responses to human voice and movement. We propose that these\nphenomena represent evolved anti-herbivory early warning systems, where plants\ndetect approaching animals through bioelectric field changes before physical\ncontact. Our results challenge conventional understanding of plant sensory\ncapabilities and suggest practical applications in agriculture, healthcare, and\nhuman-plant interaction research.", "categories": ["q-bio.OT", "cs.AI"], "published": "2025-06-04 16:23:06", "updated": "2025-06-04 16:23:06", "pdf_url": "http://arxiv.org/pdf/2506.04132v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04133v1", "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems", "authors": ["Shaina Raza", "Ranjan Sapkota", "Manoj Karkee", "Christos Emmanouilidis"], "abstract": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.", "categories": ["cs.AI"], "published": "2025-06-04 16:26:11", "updated": "2025-06-04 16:26:11", "pdf_url": "http://arxiv.org/pdf/2506.04133v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04135v1", "title": "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents", "authors": ["Pei Yang", "Hai Ci", "Mike Zheng Shou"], "abstract": "Graphical User Interface (GUI) agents show promising capabilities for\nautomating computer-use tasks and facilitating accessibility, but existing\ninteractive benchmarks are mostly English-only, covering web-use or Windows,\nLinux, and Android environments, but not macOS. macOS is a major OS with\ndistinctive GUI patterns and exclusive applications. To bridge the gaps, we\npresent macOSWorld, the first comprehensive benchmark for evaluating GUI agents\non macOS. macOSWorld features 202 multilingual interactive tasks across 30\napplications (28 macOS-exclusive), with task instructions and OS interfaces\noffered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As\nGUI agents are shown to be vulnerable to deception attacks, macOSWorld also\nincludes a dedicated safety benchmarking subset. Our evaluation on six GUI\nagents reveals a dramatic gap: proprietary computer-use agents lead at above\n30% success rate, while open-source lightweight research models lag at below\n2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks\nalso expose common weaknesses, especially in Arabic, with a 27.5% average\ndegradation compared to English. Results from safety benchmarking also\nhighlight that deception attacks are more general and demand immediate\nattention. macOSWorld is available at https://github.com/showlab/macosworld.", "categories": ["cs.AI"], "published": "2025-06-04 16:26:56", "updated": "2025-06-04 16:26:56", "pdf_url": "http://arxiv.org/pdf/2506.04135v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04143v1", "title": "Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology", "authors": ["Ngoc Q. Ly", "Hieu N. M. Cao", "Thi T. Nguyen"], "abstract": "Person Re-Identification (Re-ID) is a very important task in video\nsurveillance systems such as tracking people, finding people in public places,\nor analysing customer behavior in supermarkets. Although there have been many\nworks to solve this problem, there are still remaining challenges such as\nlarge-scale datasets, imbalanced data, viewpoint, fine grained data\n(attributes), the Local Features are not employed at semantic level in online\nstage of Re-ID task, furthermore, the imbalanced data problem of attributes are\nnot taken into consideration. This paper has proposed a Unified Re-ID system\nconsisted of three main modules such as Pedestrian Attribute Ontology (PAO),\nLocal Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main\npoint of our Re-ID system is the power of mutual support of PAO, Local MDCNN\nand IDS to exploit the inner-group correlations of attributes and pre-filter\nthe mismatch candidates from Gallery set based on semantic information as\nFashion Attributes and Facial Attributes, to solve the imbalanced data of\nattributes without adjusting network architecture and data augmentation. We\nexperimented on the well-known Market1501 dataset. The experimental results\nhave shown the effectiveness of our Re-ID system and it could achieve the\nhigher performance on Market1501 dataset in comparison to some state-of-the-art\nRe-ID methods.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-04 16:34:31", "updated": "2025-06-04 16:34:31", "pdf_url": "http://arxiv.org/pdf/2506.04143v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04147v1", "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL", "authors": ["Jiaheng Hu", "Peter Stone", "Roberto Mart\u00edn-Mart\u00edn"], "abstract": "Building capable household and industrial robots requires mastering the\ncontrol of versatile, high-degree-of-freedom (DoF) systems such as mobile\nmanipulators. While reinforcement learning (RL) holds promise for autonomously\nacquiring robot control policies, scaling it to high-DoF embodiments remains\nchallenging. Direct RL in the real world demands both safe exploration and high\nsample efficiency, which are difficult to achieve in practice. Sim-to-real RL,\non the other hand, is often brittle due to the reality gap. This paper\nintroduces SLAC, a method that renders real-world RL feasible for complex\nembodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic\nlatent action space. SLAC trains this latent action space via a customized\nunsupervised skill discovery method designed to promote temporal abstraction,\ndisentanglement, and safety, thereby facilitating efficient downstream\nlearning. Once a latent action space is learned, SLAC uses it as the action\ninterface for a novel off-policy RL algorithm to autonomously learn downstream\ntasks through real-world interactions. We evaluate SLAC against existing\nmethods on a suite of bimanual mobile manipulation tasks, where it achieves\nstate-of-the-art performance. Notably, SLAC learns contact-rich whole-body\ntasks in under an hour of real-world interactions, without relying on any\ndemonstrations or hand-crafted behavior priors. More information, code, and\nvideos at robo-rl.github.io", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-06-04 16:41:55", "updated": "2025-06-04 16:41:55", "pdf_url": "http://arxiv.org/pdf/2506.04147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04168v1", "title": "Horizon Reduction Makes RL Scalable", "authors": ["Seohong Park", "Kevin Frans", "Deepinder Mann", "Benjamin Eysenbach", "Aviral Kumar", "Sergey Levine"], "abstract": "In this work, we study the scalability of offline reinforcement learning (RL)\nalgorithms. In principle, a truly scalable offline RL algorithm should be able\nto solve any given problem, regardless of its complexity, given sufficient\ndata, compute, and model capacity. We investigate if and how current offline RL\nalgorithms match up to this promise on diverse, challenging, previously\nunsolved tasks, using datasets up to 1000x larger than typical offline RL\ndatasets. We observe that despite scaling up data, many existing offline RL\nalgorithms exhibit poor scaling behavior, saturating well below the maximum\nperformance. We hypothesize that the horizon is the main cause behind the poor\nscaling of offline RL. We empirically verify this hypothesis through several\nanalysis experiments, showing that long horizons indeed present a fundamental\nbarrier to scaling up offline RL. We then show that various horizon reduction\ntechniques substantially enhance scalability on challenging tasks. Based on our\ninsights, we also introduce a minimal yet scalable method named SHARSA that\neffectively reduces the horizon. SHARSA achieves the best asymptotic\nperformance and scaling behavior among our evaluation methods, showing that\nexplicitly reducing the horizon unlocks the scalability of offline RL. Code:\nhttps://github.com/seohongpark/horizon-reduction", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-04 17:06:54", "updated": "2025-06-04 17:06:54", "pdf_url": "http://arxiv.org/pdf/2506.04168v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04171v1", "title": "Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints", "authors": ["Utkarsh Utkarsh", "Pengfei Cai", "Alan Edelman", "Rafael Gomez-Bombarelli", "Christopher Vincent Rackauckas"], "abstract": "Deep generative models have recently been applied to physical systems\ngoverned by partial differential equations (PDEs), offering scalable simulation\nand uncertainty-aware inference. However, enforcing physical constraints, such\nas conservation laws (linear and nonlinear) and physical consistencies, remains\nchallenging. Existing methods often rely on soft penalties or architectural\nbiases that fail to guarantee hard constraints. In this work, we propose\nPhysics-Constrained Flow Matching (PCFM), a zero-shot inference framework that\nenforces arbitrary nonlinear constraints in pretrained flow-based generative\nmodels. PCFM continuously guides the sampling process through physics-based\ncorrections applied to intermediate solution states, while remaining aligned\nwith the learned flow and satisfying physical constraints. Empirically, PCFM\noutperforms both unconstrained and constrained baselines on a range of PDEs,\nincluding those with shocks, discontinuities, and sharp features, while\nensuring exact constraint satisfaction at the final solution. Our method\nprovides a general framework for enforcing hard constraints in both scientific\nand general-purpose generative models, especially in applications where\nconstraint satisfaction is essential.", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "published": "2025-06-04 17:12:37", "updated": "2025-06-04 17:12:37", "pdf_url": "http://arxiv.org/pdf/2506.04171v1", "comment": "27 pages, 9 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04195v1", "title": "MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures", "authors": ["Elena Zamaraeva", "Christopher M. Collins", "George R. Darling", "Matthew S. Dyer", "Bei Peng", "Rahul Savani", "Dmytro Antypov", "Vladimir V. Gusev", "Judith Clymo", "Paul G. Spirakis", "Matthew J. Rosseinsky"], "abstract": "Geometry optimization of atomic structures is a common and crucial task in\ncomputational chemistry and materials design. Following the learning to\noptimize paradigm, we propose a new multi-agent reinforcement learning method\ncalled Multi-Agent Crystal Structure optimization (MACS) to address periodic\ncrystal structure optimization. MACS treats geometry optimization as a\npartially observable Markov game in which atoms are agents that adjust their\npositions to collectively discover a stable configuration. We train MACS across\nvarious compositions of reported crystalline materials to obtain a policy that\nsuccessfully optimizes structures from the training compositions as well as\nstructures of larger sizes and unseen compositions, confirming its excellent\nscalability and zero-shot transferability. We benchmark our approach against a\nbroad range of state-of-the-art optimization methods and demonstrate that MACS\noptimizes periodic crystal structures significantly faster, with fewer energy\ncalculations, and the lowest failure rate.", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.11"], "published": "2025-06-04 17:40:57", "updated": "2025-06-04 17:40:57", "pdf_url": "http://arxiv.org/pdf/2506.04195v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04202v1", "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs", "authors": ["Yanting Wang", "Wei Zou", "Runpeng Geng", "Jinyuan Jia"], "abstract": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "published": "2025-06-04 17:48:16", "updated": "2025-06-04 17:48:16", "pdf_url": "http://arxiv.org/pdf/2506.04202v1", "comment": "To appear in USENIX Security Symposium 2025. The code and data are\n  at: https://github.com/Wang-Yanting/TracLLM", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04207v1", "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "abstract": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-06-04 17:51:08", "updated": "2025-06-04 17:51:08", "pdf_url": "http://arxiv.org/pdf/2506.04207v1", "comment": "19 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04210v1", "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-04 17:55:09", "updated": "2025-06-04 17:55:09", "pdf_url": "http://arxiv.org/pdf/2506.04210v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04215v1", "title": "Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally Interdependent Multi-Agent MDPs", "authors": ["Alex DeWeese", "Guannan Qu"], "abstract": "Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are\nknown to be NEXP-Complete and intractable to solve. However, for problems such\nas cooperative navigation, obstacle avoidance, and formation control, basic\nassumptions can be made about local visibility and local dependencies. The work\nDeWeese and Qu 2024 formalized these assumptions in the construction of the\nLocally Interdependent Multi-Agent MDP. In this setting, it establishes three\nclosed-form policies that are tractable to compute in various situations and\nare exponentially close to optimal with respect to visibility. However, it is\nalso shown that these solutions can have poor performance when the visibility\nis small and fixed, often getting stuck during simulations due to the so called\n\"Penalty Jittering\" phenomenon. In this work, we establish the Extended Cutoff\nPolicy Class which is, to the best of our knowledge, the first non-trivial\nclass of near optimal closed-form partially observable policies that are\nexponentially close to optimal with respect to the visibility for any Locally\nInterdependent Multi-Agent MDP. These policies are able to remember agents\nbeyond their visibilities which allows them to perform significantly better in\nmany small and fixed visibility settings, resolve Penalty Jittering\noccurrences, and under certain circumstances guarantee fully observable joint\noptimal behavior despite the partial observability. We also propose a\ngeneralized form of the Locally Interdependent Multi-Agent MDP that allows for\ntransition dependence and extended reward dependence, then replicate our\ntheoretical results in this setting.", "categories": ["cs.MA", "cs.AI", "cs.LG", "math.OC"], "published": "2025-06-04 17:57:30", "updated": "2025-06-04 17:57:30", "pdf_url": "http://arxiv.org/pdf/2506.04215v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04217v1", "title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis", "authors": ["Junting Chen", "Haotian Liang", "Lingxiao Du", "Weiyun Wang", "Mengkang Hu", "Yao Mu", "Wenhai Wang", "Jifeng Dai", "Ping Luo", "Wenqi Shao", "Lin Shao"], "abstract": "The rapid progress of navigation, manipulation, and vision models has made\nmobile manipulators capable in many specialized tasks. However, the open-world\nmobile manipulation (OWMM) task remains a challenge due to the need for\ngeneralization to open-ended instructions and environments, as well as the\nsystematic complexity to integrate high-level decision making with low-level\nrobot control based on both global scene understanding and current agent state.\nTo address this complexity, we propose a novel multi-modal agent architecture\nthat maintains multi-view scene frames and agent states for decision-making and\ncontrols the robot by function calling. A second challenge is the hallucination\nfrom domain shift. To enhance the agent performance, we further introduce an\nagentic data synthesis pipeline for the OWMM task to adapt the VLM model to our\ntask domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM\nas the first dedicated foundation model for mobile manipulators with global\nscene understanding, robot state tracking, and multi-modal action generation in\na unified model. Through experiments, we demonstrate that our model achieves\nSOTA performance compared to other foundation models including GPT-4o and\nstrong zero-shot generalization in real world. The project page is at\nhttps://github.com/HHYHRHY/OWMM-Agent", "categories": ["cs.RO", "cs.AI", "I.2.4; I.2.9; I.2.10"], "published": "2025-06-04 17:57:44", "updated": "2025-06-04 17:57:44", "pdf_url": "http://arxiv.org/pdf/2506.04217v1", "comment": "9 pages of main content, 19 pages in total", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04218v1", "title": "Pseudo-Simulation for Autonomous Driving", "authors": ["Wei Cao", "Marcel Hallgarten", "Tianyu Li", "Daniel Dauner", "Xunjiang Gu", "Caojun Wang", "Yakov Miron", "Marco Aiello", "Hongyang Li", "Igor Gilitschenski", "Boris Ivanovic", "Marco Pavone", "Andreas Geiger", "Kashyap Chitta"], "abstract": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical\nlimitations. Real-world evaluation is often challenging due to safety concerns\nand a lack of reproducibility, whereas closed-loop simulation can face\ninsufficient realism or high computational costs. Open-loop evaluation, while\nbeing efficient and data-driven, relies on metrics that generally overlook\ncompounding errors. In this paper, we propose pseudo-simulation, a novel\nparadigm that addresses these limitations. Pseudo-simulation operates on real\ndatasets, similar to open-loop evaluation, but augments them with synthetic\nobservations generated prior to evaluation using 3D Gaussian Splatting. Our key\nidea is to approximate potential future states the AV might encounter by\ngenerating a diverse set of observations that vary in position, heading, and\nspeed. Our method then assigns a higher importance to synthetic observations\nthat best match the AV's likely behavior using a novel proximity-based\nweighting scheme. This enables evaluating error recovery and the mitigation of\ncausal confusion, as in closed-loop benchmarks, without requiring sequential\ninteractive simulation. We show that pseudo-simulation is better correlated\nwith closed-loop simulations (R^2=0.8) than the best existing open-loop\napproach (R^2=0.7). We also establish a public leaderboard for the community to\nbenchmark new methodologies with pseudo-simulation. Our code is available at\nhttps://github.com/autonomousvision/navsim.", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "published": "2025-06-04 17:57:53", "updated": "2025-06-04 17:57:53", "pdf_url": "http://arxiv.org/pdf/2506.04218v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04226v1", "title": "Efficient Knowledge Editing via Minimal Precomputation", "authors": ["Akshat Gupta", "Maochuan Lu", "Thomas Hartvigsen", "Gopala Anumanchipalli"], "abstract": "Knowledge editing methods like MEMIT are able to make data and compute\nefficient updates of factual knowledge by using a single sentence to update\nfacts and their consequences. However, what is often overlooked is a\n\"precomputation step\", which requires a one-time but significant computational\ncost. The authors of MEMIT originally precompute approximately 44 million\nhidden vectors per edited layer, which requires a forward pass over 44 million\ntokens. For GPT-J (6B), this precomputation step takes 36 hours on a single\nGPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this\nprecomputation time grows with model size. In this paper, we show that this\nexcessive computational cost is unnecessary. Knowledge editing using MEMIT and\nrelated methods, such as ROME and EMMET, can be performed by pre-computing a\nvery small portion of the 44 million hidden vectors. We first present the\ntheoretical minimum number of hidden vector precomputation required for\nsolutions of these editing methods to exist. We then empirically show that\nknowledge editing using these methods can be done by pre-computing\nsignificantly fewer hidden vectors. Specifically, we show that the\nprecomputation step can be done with less than 0.3% of the originally\nstipulated number of hidden vectors. This saves a significant amount of\nprecomputation time and allows users to begin editing new models within a few\nminutes.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 17:59:05", "updated": "2025-06-04 17:59:05", "pdf_url": "http://arxiv.org/pdf/2506.04226v1", "comment": "ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04227v1", "title": "Object-centric 3D Motion Field for Robot Learning from Human Videos", "authors": ["Zhao-Heng Yin", "Sherry Yang", "Pieter Abbeel"], "abstract": "Learning robot control policies from human videos is a promising direction\nfor scaling up robot learning. However, how to extract action knowledge (or\naction representations) from videos for policy learning remains a key\nchallenge. Existing action representations such as video frames, pixelflow, and\npointcloud flow have inherent limitations such as modeling complexity or loss\nof information. In this paper, we propose to use object-centric 3D motion field\nto represent actions for robot learning from human videos, and present a novel\nframework for extracting this representation from videos for zero-shot control.\nWe introduce two novel components in its implementation. First, a novel\ntraining pipeline for training a ''denoising'' 3D motion field estimator to\nextract fine object 3D motions from human videos with noisy depth robustly.\nSecond, a dense object-centric 3D motion field prediction architecture that\nfavors both cross-embodiment transfer and policy generalization to background.\nWe evaluate the system in real world setups. Experiments show that our method\nreduces 3D motion estimation error by over 50% compared to the latest method,\nachieve 55% average success rate in diverse tasks where prior approaches\nfail~($\\lesssim 10$\\%), and can even acquire fine-grained manipulation skills\nlike insertion.", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "published": "2025-06-04 17:59:06", "updated": "2025-06-04 17:59:06", "pdf_url": "http://arxiv.org/pdf/2506.04227v1", "comment": "Project: https://zhaohengyin.github.io/3DMF", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03476v1", "title": "Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection", "authors": ["Chuyuan Li", "Raymond Li", "Thalia S. Field", "Giuseppe Carenini"], "abstract": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that\nleads to dementia, and early intervention can greatly benefit from analyzing\nlinguistic abnormalities. In this work, we explore the potential of Large\nLanguage Models (LLMs) as health assistants for AD diagnosis from\npatient-generated text using in-context learning (ICL), where tasks are defined\nthrough a few input-output examples. Empirical results reveal that conventional\nICL methods, such as similarity-based selection, perform poorly for AD\ndiagnosis, likely due to the inherent complexity of this task. To address this,\nwe introduce Delta-KNN, a novel demonstration selection strategy that enhances\nICL performance. Our method leverages a delta score to assess the relative\ngains of each training example, coupled with a KNN-based retriever that\ndynamically selects optimal \"representatives\" for a given input. Experiments on\ntwo AD detection datasets across three open-source LLMs demonstrate that\nDelta-KNN consistently outperforms existing ICL baselines. Notably, when using\nthe Llama-3.1 model, our approach achieves new state-of-the-art results,\nsurpassing even supervised classifiers.", "categories": ["cs.CL"], "published": "2025-06-04 01:14:07", "updated": "2025-06-04 01:14:07", "pdf_url": "http://arxiv.org/pdf/2506.03476v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03483v1", "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training", "authors": ["Jun Rao", "Zepeng Lin", "Xuebo Liu", "Xiaopeng Ke", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "abstract": "Large Language Models (LLMs) often require domain-specific fine-tuning to\naddress targeted tasks, which risks degrading their general capabilities.\nMaintaining a balance between domain-specific enhancements and general model\nutility is a key challenge. This paper proposes a novel approach named APT\n(Weakness Case Acquisition and Iterative Preference Training) to enhance\ndomain-specific performance with self-generated dis-preferred weakness data\n(bad cases and similar cases). APT uniquely focuses on training the model using\nonly those samples where errors occur, alongside a small, similar set of\nsamples retrieved for this purpose. This targeted training minimizes\ninterference with the model's existing knowledge base, effectively retaining\ngeneric capabilities. Experimental results on the LLama-2 and Mistral-V0.3\nmodels across various benchmarks demonstrate that APT ensures no reduction in\ngeneric capacity and achieves superior performance on downstream tasks compared\nto various existing methods. This validates our method as an effective strategy\nfor enhancing domain-specific capabilities without sacrificing the model's\nbroader applicability.", "categories": ["cs.CL"], "published": "2025-06-04 01:46:38", "updated": "2025-06-04 01:46:38", "pdf_url": "http://arxiv.org/pdf/2506.03483v1", "comment": "ACL2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03484v1", "title": "Explainable AI: XAI-Guided Context-Aware Data Augmentation", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Atnafu Lambebo Tonja", "Hassan Shakil", "Samer Iskander", "Olga Kolesnikova", "Jugal Kalita"], "abstract": "Explainable AI (XAI) has emerged as a powerful tool for improving the\nperformance of AI models, going beyond providing model transparency and\ninterpretability. The scarcity of labeled data remains a fundamental challenge\nin developing robust and generalizable AI models, particularly for low-resource\nlanguages. Conventional data augmentation techniques introduce noise, cause\nsemantic drift, disrupt contextual coherence, lack control, and lead to\noverfitting. To address these challenges, we propose XAI-Guided Context-Aware\nData Augmentation. This novel framework leverages XAI techniques to modify less\ncritical features while selectively preserving most task-relevant features. Our\napproach integrates an iterative feedback loop, which refines augmented data\nover multiple augmentation cycles based on explainability-driven insights and\nthe model performance gain. Our experimental results demonstrate that XAI-SR-BT\nand XAI-PR-BT improve the accuracy of models on hate speech and sentiment\nanalysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using\nthe Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform\nexisting augmentation techniques by 4.8% and 5%, respectively, on the same\ndataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform\nboth baseline and conventional augmentation techniques across all tasks and\nmodels. This study provides a more controlled, interpretable, and context-aware\nsolution to data augmentation, addressing critical limitations of existing\naugmentation techniques and offering a new paradigm shift for leveraging XAI\ntechniques to enhance AI model training.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 01:47:24", "updated": "2025-06-04 01:47:24", "pdf_url": "http://arxiv.org/pdf/2506.03484v1", "comment": null, "doi": "10.1016/j.eswa.2025.128364", "journal_ref": null}
{"arxiv_id": "2506.03487v1", "title": "ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking", "authors": ["Xianming Li", "Aamir Shakir", "Rui Huang", "Julius Lipp", "Jing Li"], "abstract": "Reranking is fundamental to information retrieval and retrieval-augmented\ngeneration, with recent Large Language Models (LLMs) significantly advancing\nreranking quality. While recent advances with LLMs have significantly improved\ndocument reranking quality, current approaches primarily rely on large-scale\nLLMs (>7B parameters) through zero-shot prompting, presenting high\ncomputational costs. Small Language Models (SLMs) offer a promising alternative\nbecause of their efficiency, but our preliminary quantitative analysis reveals\nthey struggle with understanding task prompts without fine-tuning. This limits\ntheir effectiveness for document reranking tasks. To address this issue, we\nintroduce a novel two-stage training approach, ProRank, for SLM-based document\nreranking. First, we propose a prompt warmup stage using reinforcement learning\nGRPO to steer SLMs to understand task prompts and generate more accurate\ncoarse-grained binary relevance scores for document reranking. Then, we\ncontinuously fine-tune the SLMs with a fine-grained score learning stage\nwithout introducing additional layers to further improve the reranking quality.\nComprehensive experimental results demonstrate that the proposed ProRank\nconsistently outperforms both the most advanced open-source and proprietary\nreranking models. Notably, our lightweight ProRank-0.5B model even surpasses\nthe powerful 32B LLM reranking model on the BEIR benchmark, establishing that\nproperly trained SLMs can achieve superior document reranking performance while\nmaintaining computational efficiency.", "categories": ["cs.IR", "cs.CL"], "published": "2025-06-04 02:00:44", "updated": "2025-06-04 02:00:44", "pdf_url": "http://arxiv.org/pdf/2506.03487v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03489v1", "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding", "authors": ["Mingxu Tao", "Jie Hu", "Mingchuan Yang", "Yunhuai Liu", "Dongyan Zhao", "Yansong Feng"], "abstract": "The remarkable performance of Large language models (LLMs) relies heavily on\nthe availability of abundant high-quality training data. However, the high cost\nof acquiring annotated data often prevents models from obtaining capabilities\nto tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe\nthat boosts model performance in data-scarcity scenarios without extra\ntraining. We first employ model extrapolation to enhance a finetuned model with\nits inferior version, and then adopt contrastive decoding to further reduce\npredicted errors, by comparing the logit scores given by the extrapolated and\nthe vanilla finetuned model. Experiments across three tasks over four different\nLLMs show that EpiCoDe consistently outperforms existing methods with\nsignificant and robust improvement. We also propose a new theoretical framework\nto reveal the mechanism behind contrastive decoding in data-scarcity scenarios,\nwhich further helps us better understand the effectiveness of EpiCoDe.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 02:11:54", "updated": "2025-06-04 02:11:54", "pdf_url": "http://arxiv.org/pdf/2506.03489v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03490v1", "title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "authors": ["Shigeng Chen", "Linhao Luo", "Zhangchi Qiu", "Yanan Cao", "Carl Yang", "Shirui Pan"], "abstract": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications.", "categories": ["cs.CL"], "published": "2025-06-04 02:14:43", "updated": "2025-06-04 02:14:43", "pdf_url": "http://arxiv.org/pdf/2506.03490v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03501v1", "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing", "authors": ["Yuchen Guo", "Zhicheng Dou", "Huy H. Nguyen", "Ching-Chun Chang", "Saku Sugawara", "Isao Echizen"], "abstract": "Content creation has dramatically progressed with the rapid advancement of\nlarge language models like ChatGPT and Claude. While this progress has greatly\nenhanced various aspects of life and work, it has also negatively affected\ncertain areas of society. A recent survey revealed that nearly 30% of college\nstudents use generative AI to help write academic papers and reports. Most\ncountermeasures treat the detection of AI-generated text as a binary\nclassification task and thus lack robustness. This approach overlooks human\ninvolvement in the generation of content even though human-machine\ncollaboration is becoming mainstream. Besides generating entire texts, people\nmay use machines to complete or revise texts. Such human involvement varies\ncase by case, which makes binary classification a less than satisfactory\napproach. We refer to this situation as participation detection obfuscation. We\npropose using BERTScore as a metric to measure human involvement in the\ngeneration process and a multi-task RoBERTa-based regressor trained on a token\nclassification task to address this problem. To evaluate the effectiveness of\nthis approach, we simulated academic-based scenarios and created a continuous\ndataset reflecting various levels of human involvement. All of the existing\ndetectors we examined failed to detect the level of human involvement on this\ndataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor\nmean squared error of 0.004). Moreover, it demonstrated some generalizability\nacross generative models. Our code is available at\nhttps://github.com/gyc-nii/CAS-CS-and-dual-head-detector", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 02:31:36", "updated": "2025-06-04 02:31:36", "pdf_url": "http://arxiv.org/pdf/2506.03501v1", "comment": "IJCNN2025 accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03510v1", "title": "Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information", "authors": ["Seungcheol Park", "Sojin Lee", "Jongjin Kim", "Jinsik Lee", "Hyunjik Jo", "U Kang"], "abstract": "How can we accelerate large language models(LLMs) without sacrificing\naccuracy? The slow inference speed of LLMs hinders us to benefit from their\nremarkable performance in diverse applications. This is mainly because numerous\nsublayers are stacked together in LLMs. Sublayer pruning compresses and\nexpedites LLMs via removing unnecessary sublayers. However, existing sublayer\npruning algorithms are limited in accuracy since they naively select sublayers\nto prune, overlooking the different characteristics of each sublayer. In this\npaper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability\nInformation), an accurate sublayer pruning method for LLMs. SPRINT accurately\nselects a target sublayer to prune by considering 1) the amount of latency\nreduction after pruning and 2) the tunability of sublayers. SPRINT iteratively\nprunes redundant sublayers and swiftly tunes the parameters of remaining\nsublayers. Experiments show that SPRINT achieves the best accuracy-speedup\ntrade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense\nreasoning benchmarks compared to existing pruning algorithms.", "categories": ["cs.CL", "68T50", "I.2.7"], "published": "2025-06-04 02:53:34", "updated": "2025-06-04 02:53:34", "pdf_url": "http://arxiv.org/pdf/2506.03510v1", "comment": "IJCAI 2025 Main Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03519v1", "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals", "authors": ["Yangyang Zhao", "Ben Niu", "Libo Qin", "Shihan Wang"], "abstract": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks.", "categories": ["cs.CL"], "published": "2025-06-04 03:07:55", "updated": "2025-06-04 03:07:55", "pdf_url": "http://arxiv.org/pdf/2506.03519v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03523v1", "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment", "authors": ["Chong Li", "Jiajun Zhang", "Chengqing Zong"], "abstract": "Tokenization serves as a foundational step for Large Language Models (LLMs)\nto process text. In new domains or languages, the inefficiency of the tokenizer\nwill slow down the training and generation of LLM. The mismatch in vocabulary\nalso hinders deep knowledge transfer between LLMs like token-level\ndistillation. To mitigate this gap, we propose an efficient method named\nTokAlign to replace the vocabulary of LLM from the token co-occurrences view,\nand further transfer the token-level knowledge between models. It first aligns\nthe source vocabulary to the target one by learning a one-to-one mapping matrix\nfor token IDs. Model parameters, including embeddings, are rearranged and\nprogressively fine-tuned for the new vocabulary. Our method significantly\nimproves multilingual text compression rates and vocabulary initialization for\nLLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods\nto 1.2$\\text{e}^2$ after initialization. Experimental results on models across\nmultiple parameter scales demonstrate the effectiveness and generalization of\nTokAlign, which costs as few as 5k steps to restore the performance of the\nvanilla model. After unifying vocabularies between LLMs, token-level\ndistillation can remarkably boost (+4.4% than sentence-level distillation) the\nbase model, costing only 235M tokens.", "categories": ["cs.CL"], "published": "2025-06-04 03:15:57", "updated": "2025-06-04 03:15:57", "pdf_url": "http://arxiv.org/pdf/2506.03523v1", "comment": "ACL 2025, our codes and models are available at\n  https://github.com/ZNLP/TokAlign", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03524v1", "title": "Seed-Coder: Let the Code Model Curate Data for Itself", "authors": ["Yuyu Zhang", "Jing Su", "Yifan Sun", "Chenguang Xi", "Xia Xiao", "Shen Zheng", "Anxiang Zhang", "Kaibo Liu", "Daoguang Zan", "Tao Sun", "Jinhua Zhu", "Shulin Xin", "Dong Huang", "Yetao Bai", "Lixin Dong", "Chao Li", "Jianchong Chen", "Hanzhi Zhou", "Yifan Huang", "Guanghan Ning", "Xierui Song", "Jiaze Chen", "Siyao Liu", "Kai Shen", "Liang Xiang", "Yonghui Wu"], "abstract": "Code data in large language model (LLM) pretraining is recognized crucial not\nonly for code-related tasks but also for enhancing general intelligence of\nLLMs. Current open-source LLMs often heavily rely on human effort to produce\ntheir code pretraining data, such as employing hand-crafted filtering rules\ntailored to individual programming languages, or using human-annotated data to\ntrain quality filters. However, these approaches are inherently limited in\nscalability, prone to subjective biases, and costly to extend and maintain\nacross diverse programming languages. To address these challenges, we introduce\nSeed-Coder, a series of open-source LLMs comprising base, instruct and\nreasoning models of 8B size, minimizing human involvement in data construction.\nOur code pretraining data is produced by a model-centric data pipeline, which\npredominantly leverages LLMs for scoring and filtering code data. The instruct\nmodel is further trained via supervised fine-tuning and preference\noptimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)\nreinforcement learning to improve multi-step code reasoning. Seed-Coder\nachieves state-of-the-art results among open-source models of similar size and\neven surpasses some much larger models, demonstrating superior performance in\ncode generation, code completion, code editing, code reasoning, and software\nengineering tasks.", "categories": ["cs.CL", "cs.SE"], "published": "2025-06-04 03:17:19", "updated": "2025-06-04 03:17:19", "pdf_url": "http://arxiv.org/pdf/2506.03524v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03525v1", "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning", "authors": ["Daeun Lee", "Jaehong Yoon", "Jaemin Cho", "Mohit Bansal"], "abstract": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-04 03:18:01", "updated": "2025-06-04 03:18:01", "pdf_url": "http://arxiv.org/pdf/2506.03525v1", "comment": "Project website: https://video-skill-cot.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03533v1", "title": "Go-Browse: Training Web Agents with Structured Exploration", "authors": ["Apurva Gandhi", "Graham Neubig"], "abstract": "One of the fundamental problems in digital agents is their lack of\nunderstanding of their environment. For instance, a web browsing agent may get\nlost in unfamiliar websites, uncertain what pages must be visited to achieve\nits goals. To address this, we propose Go-Browse, a method for automatically\ncollecting diverse and realistic web agent data at scale through structured\nexploration of web environments. Go-Browse achieves efficient exploration by\nframing data collection as a graph search, enabling reuse of information across\nexploration episodes. We instantiate our method on the WebArena benchmark,\ncollecting a dataset of 10K successful task-solving trajectories and 40K\ninteraction steps across 100 URLs. Fine-tuning a 7B parameter language model on\nthis dataset achieves a success rate of 21.7% on the WebArena benchmark,\nbeating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for\nsub-10B parameter models by 2.9%.", "categories": ["cs.CL"], "published": "2025-06-04 03:27:56", "updated": "2025-06-04 03:27:56", "pdf_url": "http://arxiv.org/pdf/2506.03533v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03541v1", "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "abstract": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 03:52:20", "updated": "2025-06-04 03:52:20", "pdf_url": "http://arxiv.org/pdf/2506.03541v1", "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03557v1", "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization", "authors": ["Lin Sun", "Chuang Liu", "Peng Liu", "Bingyang Li", "Weijia Lu", "Ning Wu"], "abstract": "Direct Preference Optimization (DPO) have emerged as a popular method for\naligning Large Language Models (LLMs) with human preferences. While DPO\neffectively preserves the relative ordering between chosen and rejected\nresponses through pairwise ranking losses, it often neglects absolute reward\nmagnitudes. This oversight can decrease the likelihood of chosen responses and\nincrease the risk of generating out-of-distribution responses, leading to poor\nperformance. We term this issue Degraded Chosen Responses (DCR).To address this\nissue, we propose Balanced Preference Optimization (BPO), a novel framework\nthat dynamically balances the optimization of chosen and rejected responses\nthrough two key components: balanced reward margin and gap adaptor. Unlike\nprevious methods, BPO can fundamentally resolve DPO's DCR issue, without\nintroducing additional constraints to the loss function. Experimental results\non multiple mathematical reasoning tasks show that BPO significantly\noutperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%\nto 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses\nDPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over\nCal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a\nsingle line of code modification, making it simple to implement and fully\ncompatible with existing DPO-based frameworks.", "categories": ["cs.CL"], "published": "2025-06-04 04:21:01", "updated": "2025-06-04 04:21:01", "pdf_url": "http://arxiv.org/pdf/2506.03557v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03558v1", "title": "ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch", "authors": ["Jiawei Chen", "Xinyan Guan", "Qianhao Yuan", "Guozhao Mo", "Weixiang Zhou", "Yaojie Lu", "Hongyu Lin", "Ben He", "Le Sun", "Xianpei Han"], "abstract": "Current instruction data synthesis methods primarily focus on single-turn\ninstructions and often neglect cross-turn coherence, resulting in context drift\nand reduced task completion rates in extended conversations. To address this\nlimitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a\nframework that constrains multi-turn instruction synthesis by explicitly\nmodeling human conversational intent. It operates in two stages: (1) Intent\nModeling, which captures the global structure of human dialogues by assigning\neach conversation to one of nine well-defined intent trajectories, ensuring a\ncoherent and goal-oriented information flow; and (2) Skeleton Generation, which\nconstructs a structurally grounded sequence of user queries aligned with the\nmodeled intent, thereby serving as a scaffold that constrains and guides the\ndownstream instruction synthesis process. Based on this process, we construct\nConsistentChat, a multi-turn instruction dataset with approximately 15,000\nmulti-turn conversations and 224,392 utterances. Experiments on the Light,\nTopdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat\nachieve a 20-30% improvement in chat consistency and up to a 15% increase in\ntask success rate, significantly outperforming models trained on existing\nsingle-turn and multi-turn instruction datasets.", "categories": ["cs.CL"], "published": "2025-06-04 04:21:48", "updated": "2025-06-04 04:21:48", "pdf_url": "http://arxiv.org/pdf/2506.03558v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03566v1", "title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding", "authors": ["Langlin Huang", "Chengsong Huang", "Jixuan Leng", "Di Huang", "Jiaxin Huang"], "abstract": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 04:30:30", "updated": "2025-06-04 04:30:30", "pdf_url": "http://arxiv.org/pdf/2506.03566v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03569v1", "title": "MiMo-VL Technical Report", "authors": ["Xiaomi LLM-Core Team", ":", "Zihao Yue", "Zhenru Lin", "Yifan Song", "Weikun Wang", "Shuhuai Ren", "Shuhao Gu", "Shicheng Li", "Peidian Li", "Liang Zhao", "Lei Li", "Kainan Bao", "Hao Tian", "Hailin Zhang", "Gang Wang", "Dawei Zhu", "Cici", "Chenhong He", "Bowen Ye", "Bowen Shen", "Zihan Zhang", "Zihan Jiang", "Zhixian Zheng", "Zhichao Song", "Zhenbo Luo", "Yue Yu", "Yudong Wang", "Yuanyuan Tian", "Yu Tu", "Yihan Yan", "Yi Huang", "Xu Wang", "Xinzhe Xu", "Xingchen Song", "Xing Zhang", "Xing Yong", "Xin Zhang", "Xiangwei Deng", "Wenyu Yang", "Wenhan Ma", "Weiwei Lv", "Weiji Zhuang", "Wei Liu", "Sirui Deng", "Shuo Liu", "Shimao Chen", "Shihua Yu", "Shaohui Liu", "Shande Wang", "Rui Ma", "Qiantong Wang", "Peng Wang", "Nuo Chen", "Menghang Zhu", "Kangyang Zhou", "Kang Zhou", "Kai Fang", "Jun Shi", "Jinhao Dong", "Jiebao Xiao", "Jiaming Xu", "Huaqiu Liu", "Hongshen Xu", "Heng Qu", "Haochen Zhao", "Hanglong Lv", "Guoan Wang", "Duo Zhang", "Dong Zhang", "Di Zhang", "Chong Ma", "Chang Liu", "Can Cai", "Bingquan Xia"], "abstract": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.", "categories": ["cs.CL"], "published": "2025-06-04 04:32:54", "updated": "2025-06-04 04:32:54", "pdf_url": "http://arxiv.org/pdf/2506.03569v1", "comment": "32 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03570v1", "title": "FreePRM: Training Process Reward Models Without Ground Truth Process Labels", "authors": ["Lin Sun", "Chuang Liu", "Xiaofeng Ma", "Tao Yang", "Weijia Lu", "Ning Wu"], "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated that\nProcess Reward Models (PRMs) play a crucial role in enhancing model\nperformance. However, training PRMs typically requires step-level labels,\neither manually annotated or automatically generated, which can be costly and\ndifficult to obtain at scale. To address this challenge, we introduce FreePRM,\na weakly supervised framework for training PRMs without access to ground-truth\nstep-level labels. FreePRM first generates pseudo step-level labels based on\nthe correctness of final outcome, and then employs Buffer Probability to\neliminate impact of noise inherent in pseudo labeling. Experimental results\nshow that FreePRM achieves an average F1 score of 53.0% on ProcessBench,\noutperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared\nto other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B\n(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by\n+10.9%. This work introduces a new paradigm in PRM training, significantly\nreducing reliance on costly step-level annotations while maintaining strong\nperformance.", "categories": ["cs.CL"], "published": "2025-06-04 04:33:53", "updated": "2025-06-04 04:33:53", "pdf_url": "http://arxiv.org/pdf/2506.03570v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03573v1", "title": "Exchange of Perspective Prompting Enhances Reasoning in Large Language Models", "authors": ["Lin Sun", "Can Zhang"], "abstract": "Large language models (LLMs) have made significant advancements in addressing\ndiverse natural language processing (NLP) tasks. However, their performance is\noften limited by inherent comprehension of problems. To address this\nlimitation, we propose Exchange-of-Perspective (EoP), a novel framework\ndesigned to exchange perspectives across different definitions of problem, so\nthat it can break the fixed mindset from any particular formulation of the\nquestion. We conducted extensive and comprehensive experiments on 8 benchmarks.\nThe results show that EoP can significantly improve performance. For instance,\ncompared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we\nobserve a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP\ndemonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a\n3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using\nQwen-2.5-72b.", "categories": ["cs.CL"], "published": "2025-06-04 04:43:15", "updated": "2025-06-04 04:43:15", "pdf_url": "http://arxiv.org/pdf/2506.03573v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03576v1", "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "abstract": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 04:47:24", "updated": "2025-06-04 04:47:24", "pdf_url": "http://arxiv.org/pdf/2506.03576v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03580v1", "title": "Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models", "authors": ["Enrico Benedetti", "Akiko Aizawa", "Florian Boudin"], "abstract": "Providing example sentences that are diverse and aligned with learners'\nproficiency levels is essential for fostering effective language acquisition.\nThis study examines the use of Pre-trained Language Models (PLMs) to produce\nexample sentences targeting L2 Japanese learners. We utilize PLMs in two ways:\nas quality scoring components in a retrieval system that draws from a newly\ncurated corpus of Japanese sentences, and as direct sentence generators using\nzero-shot learning. We evaluate the quality of sentences by considering\nmultiple aspects such as difficulty, diversity, and naturalness, with a panel\nof raters consisting of learners of Japanese, native speakers -- and GPT-4. Our\nfindings suggest that there is inherent disagreement among participants on the\nratings of sentence qualities, except for difficulty. Despite that, the\nretrieval approach was preferred by all evaluators, especially for beginner and\nadvanced target proficiency, while the generative approaches received lower\nscores on average. Even so, our experiments highlight the potential for using\nPLMs to enhance the adaptability of sentence suggestion systems and therefore\nimprove the language learning journey.", "categories": ["cs.CL"], "published": "2025-06-04 05:13:05", "updated": "2025-06-04 05:13:05", "pdf_url": "http://arxiv.org/pdf/2506.03580v1", "comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Volume 4: Student Research Workshop)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03587v1", "title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing", "authors": ["Florian Boudin", "Akiko Aizawa"], "abstract": "The rapid growth of scholarly literature makes it increasingly difficult for\nresearchers to keep up with new knowledge. Automated tools are now more\nessential than ever to help navigate and interpret this vast body of\ninformation. Scientific papers pose unique difficulties, with their complex\nlanguage, specialized terminology, and diverse formats, requiring advanced\nmethods to extract reliable and actionable insights. Large language models\n(LLMs) offer new opportunities, enabling tasks such as literature reviews,\nwriting assistance, and interactive exploration of research. This special issue\nof the TAL journal highlights research addressing these challenges and, more\nbroadly, research on natural language processing and information retrieval for\nscholarly and scientific documents.", "categories": ["cs.DL", "cs.CL"], "published": "2025-06-04 05:35:39", "updated": "2025-06-04 05:35:39", "pdf_url": "http://arxiv.org/pdf/2506.03587v1", "comment": null, "doi": null, "journal_ref": "Traitement Automatique des Langues (TAL), volume 25, n{\\deg}2/2024"}
{"arxiv_id": "2506.03589v1", "title": "BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Anh Nguyen", "Ngan Le"], "abstract": "Text-video retrieval (TVR) systems often suffer from visual-linguistic biases\npresent in datasets, which cause pre-trained vision-language models to overlook\nkey details. To address this, we propose BiMa, a novel framework designed to\nmitigate biases in both visual and textual representations. Our approach begins\nby generating scene elements that characterize each video by identifying\nrelevant entities/objects and activities. For visual debiasing, we integrate\nthese scene elements into the video embeddings, enhancing them to emphasize\nfine-grained and salient details. For textual debiasing, we introduce a\nmechanism to disentangle text features into content and bias components,\nenabling the model to focus on meaningful content while separately handling\nbiased information. Extensive experiments and ablation studies across five\nmajor TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)\ndemonstrate the competitive performance of BiMa. Additionally, the model's bias\nmitigation capability is consistently validated by its strong results on\nout-of-distribution retrieval tasks.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-04 05:40:54", "updated": "2025-06-04 05:40:54", "pdf_url": "http://arxiv.org/pdf/2506.03589v1", "comment": "22 pages, 14 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03592v1", "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "authors": ["Viktor Hangya", "Fabian K\u00fcch", "Darina Gold"], "abstract": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. We plan to publish our benchmark adaptions.", "categories": ["cs.CL"], "published": "2025-06-04 05:46:40", "updated": "2025-06-04 05:46:40", "pdf_url": "http://arxiv.org/pdf/2506.03592v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03593v1", "title": "Is linguistically-motivated data augmentation worth it?", "authors": ["Ray Groshan", "Michael Ginn", "Alexis Palmer"], "abstract": "Data augmentation, a widely-employed technique for addressing data scarcity,\ninvolves generating synthetic data examples which are then used to augment\navailable training data. Researchers have seen surprising success from simple\nmethods, such as random perturbations from natural examples, where models seem\nto benefit even from data with nonsense words, or data that doesn't conform to\nthe rules of the language. A second line of research produces synthetic data\nthat does in fact follow all linguistic constraints; these methods require some\nlinguistic expertise and are generally more challenging to implement. No\nprevious work has done a systematic, empirical comparison of both\nlinguistically-naive and linguistically-motivated data augmentation strategies,\nleaving uncertainty about whether the additional time and effort of\nlinguistically-motivated data augmentation work in fact yields better\ndownstream performance.\n  In this work, we conduct a careful and comprehensive comparison of\naugmentation strategies (both linguistically-naive and\nlinguistically-motivated) for two low-resource languages with different\nmorphological properties, Uspanteko and Arapaho. We evaluate the effectiveness\nof many different strategies and their combinations across two important\nsequence-to-sequence tasks for low-resource languages: machine translation and\ninterlinear glossing. We find that linguistically-motivated strategies can have\nbenefits over naive approaches, but only when the new examples they produce are\nnot significantly unlike the training data distribution.", "categories": ["cs.CL"], "published": "2025-06-04 05:48:20", "updated": "2025-06-04 05:48:20", "pdf_url": "http://arxiv.org/pdf/2506.03593v1", "comment": "Accepted to ACL 2025 Main. First two authors contributed equally", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03598v1", "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "abstract": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.", "categories": ["cs.CL", "cs.AI", "68T50"], "published": "2025-06-04 06:04:46", "updated": "2025-06-04 06:04:46", "pdf_url": "http://arxiv.org/pdf/2506.03598v1", "comment": "4 pages,2 figures,EITCE 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03606v1", "title": "Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models", "authors": ["Parismita Gogoi", "Sishir Kalita", "Wendy Lalhminghlui", "Viyazonuo Terhiija", "Moakala Tzudir", "Priyankoo Sarmah", "S. R. M. Prasanna"], "abstract": "This study explores the use of self-supervised learning (SSL) models for tone\nrecognition in three low-resource languages from North Eastern India: Angami,\nAo, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on\nboth tonal and non-tonal languages. We analyze tone-wise performance across the\nlayers for all three languages and compare the different models. Our results\nshow that tone recognition works best for Mizo and worst for Angami. The middle\nlayers of the SSL models are the most important for tone recognition,\nregardless of the pre-training language, i.e. tonal or non-tonal. We have also\nfound that the tone inventory, tone types, and dialectal variations affect tone\nrecognition. These findings provide useful insights into the strengths and\nweaknesses of SSL-based embeddings for tonal languages and highlight the\npotential for improving tone recognition in low-resource settings. The source\ncode is available at GitHub 1 .", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "published": "2025-06-04 06:32:12", "updated": "2025-06-04 06:32:12", "pdf_url": "http://arxiv.org/pdf/2506.03606v1", "comment": "Accepted in Interspeech2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03614v1", "title": "VLMs Can Aggregate Scattered Training Patches", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Chao Yang", "Chaochao Lu"], "abstract": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as $\\textit{visual\nstitching}$ -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch},\n\\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-04 06:46:06", "updated": "2025-06-04 06:46:06", "pdf_url": "http://arxiv.org/pdf/2506.03614v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03616v1", "title": "Learning to Insert [PAUSE] Tokens for Better Reasoning", "authors": ["Eunki Kim", "Sangryul Kim", "James Thorne"], "abstract": "To enhance reasoning capabilities, previous works have explored incorporating\nspecial-purpose tokens into the training process. These strategies strengthen\nthe learning mechanism of transformer-based large language models (LLMs).\nBuilding on prior research, in which inserting dummy tokens consecutively just\nbefore reasoning steps can enhance effectiveness, we introduce a novel approach\ntermed Dynamic Inserting Tokens Training (DIT). Our method identifies positions\nwithin sequences where model confidence is lowest according to token\nlog-likelihood. Strategically inserting [PAUSE] tokens on these positions\nbolsters the model's predictive capabilities for subsequent tokens.\nExperimental results across diverse datasets and models, from the 2.7B model to\nthe 8B model, demonstrate that DIT consistently outperforms traditional\nfine-tuning and previous token insertion methods. With this simple yet\neffective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on\nAQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work\nshows a model-based, dynamic approach rather than a heuristic one, thereby\nbroadening the scope of research in reasoning.", "categories": ["cs.CL"], "published": "2025-06-04 06:48:41", "updated": "2025-06-04 06:48:41", "pdf_url": "http://arxiv.org/pdf/2506.03616v1", "comment": "18 pages, 5 figures, ACL findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03619v1", "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales", "authors": ["Ayuto Tsutsumi", "Yuu Jinnai"], "abstract": "Although Large Language Models (LLMs) have demonstrated strong language\nunderstanding and generation abilities across various languages, their cultural\nknowledge is often limited to English-speaking communities, which can\nmarginalize the cultures of non-English communities. To address the problem,\nevaluation of the cultural awareness of the LLMs and the methods to develop\nculturally aware LLMs have been investigated. In this study, we focus on\nevaluating knowledge of folktales, a key medium for conveying and circulating\nculture. In particular, we focus on Japanese folktales, specifically on\nknowledge of Yokai. Yokai are supernatural creatures originating from Japanese\nfolktales that continue to be popular motifs in art and entertainment today.\nYokai have long served as a medium for cultural expression, making them an\nideal subject for assessing the cultural awareness of LLMs. We introduce\nYokaiEval, a benchmark dataset consisting of 809 multiple-choice questions\n(each with four options) designed to probe knowledge about yokai. We evaluate\nthe performance of 31 Japanese and multilingual LLMs on this dataset. The\nresults show that models trained with Japanese language resources achieve\nhigher accuracy than English-centric models, with those that underwent\ncontinued pretraining in Japanese, particularly those based on Llama-3,\nperforming especially well. The code and dataset are available at\nhttps://github.com/CyberAgentA ILab/YokaiEval.", "categories": ["cs.CL"], "published": "2025-06-04 06:58:19", "updated": "2025-06-04 06:58:19", "pdf_url": "http://arxiv.org/pdf/2506.03619v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03627v1", "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks", "authors": ["Lin Mu", "Guowei Chu", "Li Ni", "Lei Sang", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 07:13:27", "updated": "2025-06-04 07:13:27", "pdf_url": "http://arxiv.org/pdf/2506.03627v1", "comment": "13pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03637v1", "title": "RewardAnything: Generalizable Principle-Following Reward Models", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "abstract": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 07:30:16", "updated": "2025-06-04 07:30:16", "pdf_url": "http://arxiv.org/pdf/2506.03637v1", "comment": "23 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03659v1", "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "authors": ["Yinuo Wang", "Robert E. Mercer", "Frank Rudzicz", "Sudipta Singha Roy", "Pengjie Ren", "Zhumin Chen", "Xindi Wang"], "abstract": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.", "categories": ["cs.CL"], "published": "2025-06-04 07:48:10", "updated": "2025-06-04 07:48:10", "pdf_url": "http://arxiv.org/pdf/2506.03659v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03665v1", "title": "ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling", "authors": ["Hern\u00e1n Maina", "Guido Ivetta", "Mateo Lione Stuto", "Julian Martin Eisenschlos", "Jorge S\u00e1nchez", "Luciana Benotti"], "abstract": "Visually impaired people could benefit from Visual Question Answering (VQA)\nsystems to interpret text in their surroundings. However, current models often\nstruggle with recognizing text in the photos taken by this population. Through\nin-depth interviews with visually impaired individuals, we identified common\nframing conventions that frequently result in misaligned text. Existing VQA\nbenchmarks primarily feature well-oriented text captured by sighted users,\nunder-representing these challenges. To address this gap, we introduce ROtated\nSAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich\nimages with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7\nabsolute points in the best-performing model.", "categories": ["cs.CL", "cs.CV"], "published": "2025-06-04 07:56:13", "updated": "2025-06-04 07:56:13", "pdf_url": "http://arxiv.org/pdf/2506.03665v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03681v1", "title": "Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering", "authors": ["Pradeep Rangappa", "Andres Carofilis", "Jeena Prakash", "Shashi Kumar", "Sergio Burdisso", "Srikanth Madikeri", "Esau Villatoro-Tello", "Bidisha Sharma", "Petr Motlicek", "Kadri Hacioglu", "Shankar Venkatesan", "Saurabh Vyas", "Andreas Stolcke"], "abstract": "Fine-tuning pretrained ASR models for specific domains is challenging for\nsmall organizations with limited labeled data and computational resources.\nHere, we explore different data selection pipelines and propose a robust\napproach that improves ASR adaptation by filtering pseudo-labels generated\nusing Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach\nintegrates multiple selection strategies -- including word error rate (WER)\nprediction, named entity recognition (NER), and character error rate (CER)\nanalysis -- to extract high-quality training segments. We evaluate our method\non Whisper and Zipformer using a 7500-hour baseline, comparing it to a\nCER-based approach relying on hypotheses from three ASR systems. Fine-tuning on\n7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our\nfiltering reduces the dataset to 100 hours (1.4%) with similar performance; a\nsimilar trend is observed on Fisher English.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-04 08:11:24", "updated": "2025-06-04 08:11:24", "pdf_url": "http://arxiv.org/pdf/2506.03681v1", "comment": "Accepted at Interspeech 2025, Netherlands", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03690v1", "title": "Robust Preference Optimization via Dynamic Target Margins", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "abstract": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.", "categories": ["cs.CL"], "published": "2025-06-04 08:19:37", "updated": "2025-06-04 08:19:37", "pdf_url": "http://arxiv.org/pdf/2506.03690v1", "comment": "18 pages, 6 figures, accepted to The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03700v1", "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism", "authors": ["Zhepei Wei", "Wei-Lin Chen", "Xinyu Zhu", "Yu Meng"], "abstract": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.", "categories": ["cs.CL"], "published": "2025-06-04 08:32:30", "updated": "2025-06-04 08:32:30", "pdf_url": "http://arxiv.org/pdf/2506.03700v1", "comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03704v1", "title": "ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation", "authors": ["Pei-Yun Lin", "Yen-lung Tsai"], "abstract": "This research introduces ScoreRAG, an approach to enhance the quality of\nautomated news generation. Despite advancements in Natural Language Processing\nand large language models, current news generation methods often struggle with\nhallucinations, factual inconsistencies, and lack of domain-specific expertise\nwhen producing news articles. ScoreRAG addresses these challenges through a\nmulti-stage framework combining retrieval-augmented generation, consistency\nrelevance evaluation, and structured summarization. The system first retrieves\nrelevant news documents from a vector database, maps them to complete news\nitems, and assigns consistency relevance scores based on large language model\nevaluations. These documents are then reranked according to relevance, with\nlow-quality items filtered out. The framework proceeds to generate graded\nsummaries based on relevance scores, which guide the large language model in\nproducing complete news articles following professional journalistic standards.\nThrough this methodical approach, ScoreRAG aims to significantly improve the\naccuracy, coherence, informativeness, and professionalism of generated news\narticles while maintaining stability and consistency throughout the generation\nprocess. The code and demo are available at:\nhttps://github.com/peiyun2260/ScoreRAG.", "categories": ["cs.CL", "68T50", "I.2.7"], "published": "2025-06-04 08:35:06", "updated": "2025-06-04 08:35:06", "pdf_url": "http://arxiv.org/pdf/2506.03704v1", "comment": "11 pages, 8 figures. Code and demo available at\n  https://github.com/peiyun2260/ScoreRAG. Submitted to arXiv for public access;\n  journal submission planned", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03722v1", "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition", "authors": ["Yinfeng Xia", "Huiyan Li", "Chenyang Le", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "abstract": "Applying large pre-trained speech models like Whisper has shown promise in\nreducing training costs for various speech tasks. However, integrating these\nmodels into streaming systems remains a challenge. This paper presents a novel\nprefix-to-prefix training framework for streaming recognition by fine-tuning\nthe Whisper. We introduce the Continuous Integrate-and-Fire mechanism to\nestablish a quasi-monotonic alignment between continuous speech sequences and\ndiscrete text tokens. Additionally, we design Monotonic Finite Look-ahead\nAttention, allowing each token to attend to infinite left-context and finite\nright-context from the speech sequences. We also employ the wait-k decoding\nstrategy to simplify the decoding process while ensuring consistency between\ntraining and testing. Our theoretical analysis and experiments demonstrate that\nthis approach achieves a controllable trade-off between latency and quality,\nmaking it suitable for various streaming applications.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-04 08:53:40", "updated": "2025-06-04 08:53:40", "pdf_url": "http://arxiv.org/pdf/2506.03722v1", "comment": "Accepted by Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03723v1", "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "authors": ["Chaeyun Jang", "Moonseok Choi", "Yegon Kim", "Hyungi Lee", "Juho Lee"], "abstract": "Uncertainty calibration is essential for the safe deployment of large\nlanguage models (LLMs), particularly when users rely on verbalized confidence\nestimates. While prior work has focused on classifiers or short-form\ngeneration, confidence calibration for chain-of-thought (CoT) reasoning remains\nlargely unexplored. Surprisingly, we find that supervised fine-tuning with\nscalar confidence labels alone suffices to elicit self-verification behavior of\nlanguage models, without any explicit reasoning supervision or reinforcement\nlearning-based rewards. Despite being trained only to produce a verbalized\nconfidence score without any self-verifying examples, the model learns to\ngenerate longer and self-checking responses for low-confidence queries while\nproviding more concise answers for high-confidence ones. We further propose a\nsimple rethinking method that boosts performance via test-time scaling based on\ncalibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such\nas MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning\nimproves both calibration and accuracy, while also enhancing interpretability\nby aligning the model's reasoning path with its confidence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 08:56:24", "updated": "2025-06-04 08:56:24", "pdf_url": "http://arxiv.org/pdf/2506.03723v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03735v1", "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "abstract": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-06-04 09:08:11", "updated": "2025-06-04 09:08:11", "pdf_url": "http://arxiv.org/pdf/2506.03735v1", "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03741v1", "title": "PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing", "authors": ["Rifat Mehreen Amin", "Oliver Hans K\u00fchle", "Daniel Buschek", "Andreas Butz"], "abstract": "We introduce PromptCanvas, a concept that transforms prompting into a\ncomposable, widget-based experience on an infinite canvas. Users can generate,\ncustomize, and arrange interactive widgets representing various facets of their\ntext, offering greater control over AI-generated content. PromptCanvas allows\nwidget creation through system suggestions, user prompts, or manual input,\nproviding a flexible environment tailored to individual needs. This enables\ndeeper engagement with the creative process. In a lab study with 18\nparticipants, PromptCanvas outperformed a traditional conversational UI on the\nCreativity Support Index. Participants found that it reduced cognitive load,\nwith lower mental demand and frustration. Qualitative feedback revealed that\nthe visual organization of thoughts and easy iteration encouraged new\nperspectives and ideas. A follow-up field study (N=10) confirmed these results,\nshowcasing the potential of dynamic, customizable interfaces in improving\ncollaborative writing with AI.", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "published": "2025-06-04 09:13:51", "updated": "2025-06-04 09:13:51", "pdf_url": "http://arxiv.org/pdf/2506.03741v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03761v1", "title": "Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Zheyu Ye", "Zhoujun Li", "Zuozhu Liu"], "abstract": "As interest in using Large Language Models (LLMs) for interactive and\nemotionally rich experiences grows, virtual pet companionship emerges as a\nnovel yet underexplored application. Existing approaches focus on basic pet\nrole-playing interactions without systematically benchmarking LLMs for\ncomprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated\nbenchmark that evaluates LLMs across both self-interaction and\nhuman-interaction dimensions. Unlike prior work, Pet-Bench emphasizes\nself-evolution and developmental behaviors alongside interactive engagement,\noffering a more realistic reflection of pet companionship. It features diverse\ntasks such as intelligent scheduling, memory-based dialogues, and psychological\nconversations, with over 7,500 interaction instances designed to simulate\ncomplex pet behaviors. Evaluation of 28 LLMs reveals significant performance\nvariations linked to model size and inherent capabilities, underscoring the\nneed for specialized optimization in this domain. Pet-Bench serves as a\nfoundational resource for benchmarking pet-related LLM abilities and advancing\nemotionally immersive human-pet interactions.", "categories": ["cs.CL"], "published": "2025-06-04 09:25:52", "updated": "2025-06-04 09:25:52", "pdf_url": "http://arxiv.org/pdf/2506.03761v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03762v1", "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models", "authors": ["Yifeng Gu", "Zicong Jiang", "Jianxiu Jin", "Kailing Guo", "Ziyang Zhang", "Xiangmin Xu"], "abstract": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 09:25:53", "updated": "2025-06-04 09:25:53", "pdf_url": "http://arxiv.org/pdf/2506.03762v1", "comment": "14 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03763v1", "title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations", "authors": ["Quang Hieu Pham", "Thuy Duong Nguyen", "Tung Pham", "Anh Tuan Luu", "Dat Quoc Nguyen"], "abstract": "The capabilities of large language models (LLMs) have been enhanced by\ntraining on data that reflects human thought processes, such as the\nChain-of-Thought format. However, evidence suggests that the conventional\nscheme of next-word prediction may not fully capture how humans learn to think.\nInspired by how humans generalize mathematical reasoning, we propose a new\napproach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our\nClozeMath involves a text-infilling task that predicts masked equations from a\ngiven solution, analogous to cloze exercises used in human learning.\nExperiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the\nstrong baseline Masked Thought in performance and robustness, with two\ntest-time scaling decoding algorithms, Beam Search and Chain-of-Thought\ndecoding. Additionally, we conduct an ablation study to analyze the effects of\nvarious architectural and implementation choices on our approach.", "categories": ["cs.CL"], "published": "2025-06-04 09:27:21", "updated": "2025-06-04 09:27:21", "pdf_url": "http://arxiv.org/pdf/2506.03763v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03781v1", "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models", "authors": ["Seungcheol Park", "Jeongin Bae", "Beomseok Kwon", "Minjun Kim", "Byeongwook Kim", "Se Jung Kwon", "U Kang", "Dongsoo Lee"], "abstract": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.", "categories": ["cs.CL", "68T50", "I.2.7"], "published": "2025-06-04 09:42:17", "updated": "2025-06-04 09:42:17", "pdf_url": "http://arxiv.org/pdf/2506.03781v1", "comment": "ACL 2025 Main Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03785v1", "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "abstract": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-06-04 09:46:43", "updated": "2025-06-04 09:46:43", "pdf_url": "http://arxiv.org/pdf/2506.03785v1", "comment": "4 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03793v1", "title": "Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts", "authors": ["Sidharth Pulipaka", "Sparsh Jain", "Ashwin Sankar", "Raj Dabre"], "abstract": "Punctuation plays a vital role in structuring meaning, yet current models\noften struggle to restore it accurately in transcripts of spontaneous speech,\nespecially in the presence of disfluencies such as false starts and\nbacktracking. These limitations hinder the performance of downstream tasks like\ntranslation, text to speech, summarization, etc. where sentence boundaries are\ncritical for preserving quality. In this work, we introduce Cadence, a\ngeneralist punctuation restoration model adapted from a pretrained large\nlanguage model. Cadence is designed to handle both clean written text and\nhighly spontaneous spoken transcripts. It surpasses the previous state of the\nart in performance while expanding support from 14 to all 22 Indian languages\nand English. We conduct a comprehensive analysis of model behavior across\npunctuation types and language families, identifying persistent challenges\nunder domain shift and with rare punctuation marks. Our findings demonstrate\nthe efficacy of utilizing pretrained language models for multilingual\npunctuation restoration and highlight Cadence practical value for low resource\nNLP pipelines at scale.", "categories": ["cs.CL"], "published": "2025-06-04 09:54:38", "updated": "2025-06-04 09:54:38", "pdf_url": "http://arxiv.org/pdf/2506.03793v1", "comment": "Work in Progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03820v1", "title": "Automatic Correction of Writing Anomalies in Hausa Texts", "authors": ["Ahmad Mustapha Wali", "Sergiu Nisioi"], "abstract": "Hausa texts are often characterized by writing anomalies such as incorrect\ncharacter substitutions and spacing errors, which sometimes hinder natural\nlanguage processing (NLP) applications. This paper presents an approach to\nautomatically correct the anomalies by finetuning transformer-based models.\nUsing a corpus gathered from several public sources, we created a large-scale\nparallel dataset of over 450,000 noisy-clean Hausa sentence pairs by\nintroducing synthetically generated noise, fine-tuned to mimic realistic\nwriting errors. Moreover, we adapted several multilingual and African\nlanguage-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT\nvariants for this correction task using SentencePiece tokenization. Our\nexperimental results demonstrate significant increases in F1, BLEU and METEOR\nscores, as well as reductions in Character Error Rate (CER) and Word Error Rate\n(WER). This research provides a robust methodology, a publicly available\ndataset, and effective models to improve Hausa text quality, thereby advancing\nNLP capabilities for the language and offering transferable insights for other\nlow-resource languages.", "categories": ["cs.CL"], "published": "2025-06-04 10:46:19", "updated": "2025-06-04 10:46:19", "pdf_url": "http://arxiv.org/pdf/2506.03820v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03822v1", "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents", "authors": ["Fabian Karl", "Ansgar Scherp"], "abstract": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.", "categories": ["cs.CL", "cs.IR"], "published": "2025-06-04 10:52:55", "updated": "2025-06-04 10:52:55", "pdf_url": "http://arxiv.org/pdf/2506.03822v1", "comment": "Accepted at SCOLIA 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03827v1", "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "abstract": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-04 10:57:18", "updated": "2025-06-04 10:57:18", "pdf_url": "http://arxiv.org/pdf/2506.03827v1", "comment": "Accepted by SIGIR2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03832v1", "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "authors": ["Omer Moussa", "Mariya Toneva"], "abstract": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "published": "2025-06-04 10:59:11", "updated": "2025-06-04 10:59:11", "pdf_url": "http://arxiv.org/pdf/2506.03832v1", "comment": "Proceedings of Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03857v1", "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation", "authors": ["Mingxuan Xia", "Haobo Wang", "Yixuan Li", "Zewei Yu", "Jindong Wang", "Junbo Zhao", "Runze Wu"], "abstract": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist.", "categories": ["cs.LG", "cs.CL"], "published": "2025-06-04 11:42:37", "updated": "2025-06-04 11:42:37", "pdf_url": "http://arxiv.org/pdf/2506.03857v1", "comment": "Accepted to ACL 2025 (Main conference)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03861v1", "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading", "authors": ["Qiuhan Han", "Qian Wang", "Atsushi Yoshikawa", "Masayuki Yamamura"], "abstract": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia.", "categories": ["cs.CL"], "published": "2025-06-04 11:48:51", "updated": "2025-06-04 11:48:51", "pdf_url": "http://arxiv.org/pdf/2506.03861v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03867v1", "title": "EuroGEST: Investigating gender stereotypes in multilingual language models", "authors": ["Jacqueline Rowe", "Mateusz Klimaszewski", "Liane Guillou", "Shannon Vallor", "Alexandra Birch"], "abstract": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages.", "categories": ["cs.CL"], "published": "2025-06-04 11:58:18", "updated": "2025-06-04 11:58:18", "pdf_url": "http://arxiv.org/pdf/2506.03867v1", "comment": "8 pages, 6 figures, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03880v1", "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Jianhua Tao"], "abstract": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 12:16:41", "updated": "2025-06-04 12:16:41", "pdf_url": "http://arxiv.org/pdf/2506.03880v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03884v1", "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages", "authors": ["Utkarsh Pathak", "Chandra Sai Krishna Gunda", "Anusha Prakash", "Keshav Agarwal", "Hema A. Murthy"], "abstract": "Text-to-speech (TTS) systems typically require high-quality studio data and\naccurate transcriptions for training. India has 1369 languages, with 22\nofficial using 13 scripts. Training a TTS system for all these languages, most\nof which have no digital resources, seems a Herculean task. Our work focuses on\nzero-shot synthesis, particularly for languages whose scripts and phonotactics\ncome from different families. The novelty of our work is in the augmentation of\na shared phone representation and modifying the text parsing rules to match the\nphonotactics of the target language, thus reducing the synthesiser overhead and\nenabling rapid adaptation. Intelligible and natural speech was generated for\nSanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging\nlinguistic connections across languages with suitable synthesisers. Evaluations\nconfirm the effectiveness of this approach, highlighting its potential to\nexpand speech technology access for under-represented languages.", "categories": ["cs.CL", "cs.CV", "I.5.4"], "published": "2025-06-04 12:22:24", "updated": "2025-06-04 12:22:24", "pdf_url": "http://arxiv.org/pdf/2506.03884v1", "comment": "Accepted at INTERSPEECH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03887v1", "title": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation", "authors": ["Junyi Chen", "Shihao Bai", "Zaijun Wang", "Siyu Wu", "Chuheng Du", "Hailong Yang", "Ruihao Gong", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "abstract": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm.", "categories": ["cs.CL"], "published": "2025-06-04 12:30:30", "updated": "2025-06-04 12:30:30", "pdf_url": "http://arxiv.org/pdf/2506.03887v1", "comment": "Published as a conference paper at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03901v1", "title": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems", "authors": ["Yuxin Zhang", "Yan Wang", "Yongrui Chen", "Shenyu Zhang", "Xinbang Dai", "Sheng Bi", "Guilin Qi"], "abstract": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge.\n  However, RAG systems are highly sensitive to retrieval noise prevalent in\nreal-world scenarios.\n  Existing benchmarks fail to emulate the complex and heterogeneous noise\ndistributions encountered in real-world retrieval environments, undermining\nreliable robustness assessment.\n  In this paper, we define four categories of retrieval noise based on\nlinguistic properties and noise characteristics, aiming to reflect the\nheterogeneity of noise in real-world scenarios.\n  Building on this, we introduce Magic Mushroom, a benchmark for replicating\n\"magic mushroom\" noise: contexts that appear relevant on the surface but\ncovertly mislead RAG systems.\n  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer\npairs.\n  More importantly, Magic Mushroom enables researchers to flexibly configure\ncombinations of retrieval noise according to specific research objectives or\napplication scenarios, allowing for highly controlled evaluation setups.\n  We evaluate LLM generators of varying parameter scales and classic RAG\ndenoising strategies under diverse noise distributions to investigate their\nperformance dynamics during progressive noise encroachment.\n  Our analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions.\n  Magic Mushroom emerges as a promising tool for evaluating and advancing\nnoise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications.\n  The Magic Mushroom benchmark is available at the\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.", "categories": ["cs.CL"], "published": "2025-06-04 12:55:59", "updated": "2025-06-04 12:55:59", "pdf_url": "http://arxiv.org/pdf/2506.03901v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03902v1", "title": "The Harmonic Structure of Information Contours", "authors": ["Eleftheria Tsipidi", "Samuel Kiegeland", "Franz Nowak", "Tianyang Xu", "Ethan Wilcox", "Alex Warstadt", "Ryan Cotterell", "Mario Giulianelli"], "abstract": "The uniform information density (UID) hypothesis proposes that speakers aim\nto distribute information evenly throughout a text, balancing production effort\nand listener comprehension difficulty. However, language typically does not\nmaintain a strictly uniform information rate; instead, it fluctuates around a\nglobal average. These fluctuations are often explained by factors such as\nsyntactic constraints, stylistic choices, or audience design. In this work, we\nexplore an alternative perspective: that these fluctuations may be influenced\nby an implicit linguistic pressure towards periodicity, where the information\nrate oscillates at regular intervals, potentially across multiple frequencies\nsimultaneously. We apply harmonic regression and introduce a novel extension\ncalled time scaling to detect and test for such periodicity in information\ncontours. Analyzing texts in English, Spanish, German, Dutch, Basque, and\nBrazilian Portuguese, we find consistent evidence of periodic patterns in\ninformation rate. Many dominant frequencies align with discourse structure,\nsuggesting these oscillations reflect meaningful linguistic organization.\nBeyond highlighting the connection between information rate and discourse\nstructure, our approach offers a general framework for uncovering structural\npressures at various levels of linguistic granularity.", "categories": ["cs.CL"], "published": "2025-06-04 12:56:30", "updated": "2025-06-04 12:56:30", "pdf_url": "http://arxiv.org/pdf/2506.03902v1", "comment": "ACL 2025 (main conference)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03913v1", "title": "When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning", "authors": ["Claire Barale", "Michael Rovatsos", "Nehal Bhuta"], "abstract": "Legal decisions are increasingly evaluated for fairness, consistency, and\nbias using machine learning (ML) techniques. In high-stakes domains like\nrefugee adjudication, such methods are often applied to detect disparities in\noutcomes. Yet it remains unclear whether statistical methods can meaningfully\nassess fairness in legal contexts shaped by discretion, normative complexity,\nand limited ground truth.\n  In this paper, we empirically evaluate three common ML approaches\n(feature-based analysis, semantic clustering, and predictive modeling) on a\nlarge, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our\nexperiments show that these methods produce divergent and sometimes\ncontradictory signals, that predictive modeling often depends on contextual and\nprocedural features rather than legal features, and that semantic clustering\nfails to capture substantive legal reasoning.\n  We show limitations of statistical fairness evaluation, challenge the\nassumption that statistical regularity equates to fairness, and argue that\ncurrent computational approaches fall short of evaluating fairness in legally\ndiscretionary domains. We argue that evaluating fairness in law requires\nmethods grounded not only in data, but in legal reasoning and institutional\ncontext.", "categories": ["cs.CL", "cs.LG"], "published": "2025-06-04 13:05:37", "updated": "2025-06-04 13:05:37", "pdf_url": "http://arxiv.org/pdf/2506.03913v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03916v1", "title": "Compositional Generalisation for Explainable Hate Speech Detection", "authors": ["Agostina Calabrese", "Tom Sherborne", "Bj\u00f6rn Ross", "Mirella Lapata"], "abstract": "Hate speech detection is key to online content moderation, but current models\nstruggle to generalise beyond their training data. This has been linked to\ndataset biases and the use of sentence-level labels, which fail to teach models\nthe underlying structure of hate speech. In this work, we show that even when\nmodels are trained with more fine-grained, span-level annotations (e.g.,\n\"artists\" is labeled as target and \"are parasites\" as dehumanising comparison),\nthey struggle to disentangle the meaning of these labels from the surrounding\ncontext. As a result, combinations of expressions that deviate from those seen\nduring training remain particularly difficult for models to detect. We\ninvestigate whether training on a dataset where expressions occur with equal\nfrequency across all contexts can improve generalisation. To this end, we\ncreate U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel\ncompositional generalisation benchmark of ~8,000 manually validated posts.\nTraining on a combination of U-PLEAD and real data improves compositional\ngeneralisation while achieving state-of-the-art performance on the\nhuman-sourced PLEAD.", "categories": ["cs.CL"], "published": "2025-06-04 13:07:36", "updated": "2025-06-04 13:07:36", "pdf_url": "http://arxiv.org/pdf/2506.03916v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03922v1", "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "authors": ["Zhaolu Kang", "Junhao Gong", "Jiaxu Yan", "Wanke Xia", "Yian Wang", "Ziwen Wang", "Huaxuan Ding", "Zhuo Cheng", "Wenhao Cao", "Zhiyuan Feng", "Siqi He", "Shannan Yan", "Junzhe Chen", "Xiaomin He", "Chaoya Jiang", "Wei Ye", "Kaidong Yu", "Xuelong Li"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant\npotential to advance a broad range of domains. However, current benchmarks for\nevaluating MLLMs primarily emphasize general knowledge and vertical\nstep-by-step reasoning typical of STEM disciplines, while overlooking the\ndistinct needs and potential of the Humanities and Social Sciences (HSS). Tasks\nin the HSS domain require more horizontal, interdisciplinary thinking and a\ndeep integration of knowledge across related fields, which presents unique\nchallenges for MLLMs, particularly in linking abstract concepts with\ncorresponding visual representations. Addressing this gap, we present HSSBench,\na dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks\nin multiple languages, including the six official languages of the United\nNations. We also introduce a novel data generation pipeline tailored for HSS\nscenarios, in which multiple domain experts and automated agents collaborate to\ngenerate and iteratively refine each sample. HSSBench contains over 13,000\nmeticulously designed samples, covering six key categories. We benchmark more\nthan 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant\nchallenges even for state-of-the-art models. We hope that this benchmark will\ninspire further research into enhancing the cross-disciplinary reasoning\nabilities of MLLMs, especially their capacity to internalize and connect\nknowledge across fields.", "categories": ["cs.CL"], "published": "2025-06-04 13:14:13", "updated": "2025-06-04 13:14:13", "pdf_url": "http://arxiv.org/pdf/2506.03922v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03923v1", "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "abstract": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.", "categories": ["cs.CL"], "published": "2025-06-04 13:15:01", "updated": "2025-06-04 13:15:01", "pdf_url": "http://arxiv.org/pdf/2506.03923v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03930v1", "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation", "authors": ["Yuansheng Ni", "Ping Nie", "Kai Zou", "Xiang Yue", "Wenhu Chen"], "abstract": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.", "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2025-06-04 13:24:44", "updated": "2025-06-04 13:24:44", "pdf_url": "http://arxiv.org/pdf/2506.03930v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03939v1", "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning", "authors": ["Junqi Gao", "Xiang Zou", "YIng Ai", "Dong Li", "Yichen Niu", "Biqing Qi", "Jianxing Liu"], "abstract": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-04 13:31:21", "updated": "2025-06-04 13:31:21", "pdf_url": "http://arxiv.org/pdf/2506.03939v1", "comment": "Accepted by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03941v1", "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations", "authors": ["Vivian Nguyen", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil"], "abstract": "During a conversation, there can come certain moments where its outcome hangs\nin the balance. In these pivotal moments, how one responds can put the\nconversation on substantially different trajectories leading to significantly\ndifferent outcomes. Systems that can detect when such moments arise could\nassist conversationalists in domains with highly consequential outcomes, such\nas mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting\nsuch pivotal moments as they happen, in an online fashion. Our approach relies\non the intuition that a moment is pivotal if our expectation of the outcome\nvaries widely depending on what might be said next. By applying our method to\ncrisis counseling conversations, we first validate it by showing that it aligns\nwith human perception -- counselors take significantly longer to respond during\nmoments detected by our method -- and with the eventual conversational\ntrajectory -- which is more likely to change course at these times. We then use\nour framework to explore the relation of the counselor's response during\npivotal moments with the eventual outcome of the session.", "categories": ["cs.CL", "cs.AI", "cs.CY", "physics.soc-ph"], "published": "2025-06-04 13:31:58", "updated": "2025-06-04 13:31:58", "pdf_url": "http://arxiv.org/pdf/2506.03941v1", "comment": "To appear in the Proceedings of ACL 2025. Code and demo available in\n  ConvoKit (convokit.cornell.edu)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03949v1", "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "abstract": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.", "categories": ["cs.CL"], "published": "2025-06-04 13:39:01", "updated": "2025-06-04 13:39:01", "pdf_url": "http://arxiv.org/pdf/2506.03949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03968v1", "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding", "authors": ["Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "abstract": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.", "categories": ["cs.CL"], "published": "2025-06-04 14:00:47", "updated": "2025-06-04 14:00:47", "pdf_url": "http://arxiv.org/pdf/2506.03968v1", "comment": "To be published at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03978v1", "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Viet Anh Nguyen"], "abstract": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets.", "categories": ["cs.CL", "cs.LG"], "published": "2025-06-04 14:08:44", "updated": "2025-06-04 14:08:44", "pdf_url": "http://arxiv.org/pdf/2506.03978v1", "comment": "Accepted to ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03980v1", "title": "Voice Activity Projection Model with Multimodal Encoders", "authors": ["Takeshi Saga", "Catherine Pelachaud"], "abstract": "Turn-taking management is crucial for any social interaction. Still, it is\nchallenging to model human-machine interaction due to the complexity of the\nsocial context and its multimodal nature. Unlike conventional systems based on\nsilence duration, previous existing voice activity projection (VAP) models\nsuccessfully utilized a unified representation of turn-taking behaviors as\nprediction targets, which improved turn-taking prediction performance.\nRecently, a multimodal VAP model outperformed the previous state-of-the-art\nmodel by a significant margin. In this paper, we propose a multimodal model\nenhanced with pre-trained audio and face encoders to improve performance by\ncapturing subtle expressions. Our model performed competitively, and in some\ncases, even better than state-of-the-art models on turn-taking metrics. All the\nsource codes and pretrained models are available at\nhttps://github.com/sagatake/VAPwithAudioFaceEncoders.", "categories": ["cs.CL"], "published": "2025-06-04 14:10:03", "updated": "2025-06-04 14:10:03", "pdf_url": "http://arxiv.org/pdf/2506.03980v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03984v1", "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place", "authors": ["Carolin Holtermann", "Paul R\u00f6ttger", "Anne Lauscher"], "abstract": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks.", "categories": ["cs.CL"], "published": "2025-06-04 14:14:28", "updated": "2025-06-04 14:14:28", "pdf_url": "http://arxiv.org/pdf/2506.03984v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03989v1", "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models", "authors": ["Alex Laitenberger", "Christopher D. Manning", "Nelson F. Liu"], "abstract": "With the rise of long-context language models (LMs) capable of processing\ntens of thousands of tokens in a single pass, do multi-stage\nretrieval-augmented generation (RAG) pipelines still offer measurable benefits\nover simpler, single-stage approaches? To assess this question, we conduct a\ncontrolled evaluation for QA tasks under systematically scaled token budgets,\ncomparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three\nbaselines, including DOS RAG (Document's Original Structure RAG), a simple\nretrieve-then-read method that preserves original passage order. Despite its\nstraightforward design, DOS RAG consistently matches or outperforms more\nintricate methods on multiple long-context QA benchmarks. We recommend\nestablishing DOS RAG as a simple yet strong baseline for future RAG\nevaluations, pairing it with emerging embedding and language models to assess\ntrade-offs between complexity and effectiveness as model capabilities evolve.", "categories": ["cs.CL"], "published": "2025-06-04 14:16:28", "updated": "2025-06-04 14:16:28", "pdf_url": "http://arxiv.org/pdf/2506.03989v1", "comment": "10 pages, 5 figures, for associated source code, see\n  https://github.com/alex-laitenberger/stronger-baselines-rag", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03990v1", "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding", "authors": ["Hongzhi Zhang", "Jingyuan Zhang", "Xingguang Ji", "Qi Wang", "Fuzheng Zhang"], "abstract": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.", "categories": ["cs.CL", "cs.CV"], "published": "2025-06-04 14:17:42", "updated": "2025-06-04 14:17:42", "pdf_url": "http://arxiv.org/pdf/2506.03990v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.03993v1", "title": "Words of Warmth: Trust and Sociability Norms for over 26k English Words", "authors": ["Saif M. Mohammad"], "abstract": "Social psychologists have shown that Warmth (W) and Competence (C) are the\nprimary dimensions along which we assess other people and groups. These\ndimensions impact various aspects of our lives from social competence and\nemotion regulation to success in the work place and how we view the world. More\nrecent work has started to explore how these dimensions develop, why they have\ndeveloped, and what they constitute. Of particular note, is the finding that\nwarmth has two distinct components: Trust (T) and Sociability (S). In this\nwork, we introduce Words of Warmth, the first large-scale repository of\nmanually derived word--warmth (as well as word--trust and word--sociability)\nassociations for over 26k English words. We show that the associations are\nhighly reliable. We use the lexicons to study the rate at which children\nacquire WCTS words with age. Finally, we show that the lexicon enables a wide\nvariety of bias and stereotype research through case studies on various target\nentities. Words of Warmth is freely available at:\nhttp://saifmohammad.com/warmth.html", "categories": ["cs.CL", "cs.CY"], "published": "2025-06-04 14:18:32", "updated": "2025-06-04 14:18:32", "pdf_url": "http://arxiv.org/pdf/2506.03993v1", "comment": "In Proceedings of ACL 2025 Main", "doi": null, "journal_ref": "In Proceedings of ACL 2025 Main"}
{"arxiv_id": "2506.03994v1", "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "abstract": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities.", "categories": ["cs.CL", "cs.CV"], "published": "2025-06-04 14:18:35", "updated": "2025-06-04 14:18:35", "pdf_url": "http://arxiv.org/pdf/2506.03994v1", "comment": "ACL Findings 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04018v1", "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Goun\u00e9", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "abstract": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.11; K.4.1; I.2.6"], "published": "2025-06-04 14:46:47", "updated": "2025-06-04 14:46:47", "pdf_url": "http://arxiv.org/pdf/2506.04018v1", "comment": "Prepint, under review for NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04019v1", "title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "authors": ["Neeva Oza", "Ishaan Govil", "Parul Gupta", "Dinesh Khandelwal", "Dinesh Garg", "Parag Singla"], "abstract": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)", "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5"], "published": "2025-06-04 14:47:14", "updated": "2025-06-04 14:47:14", "pdf_url": "http://arxiv.org/pdf/2506.04019v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04020v1", "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering", "authors": ["An Quang Tang", "Xiuzhen Zhang", "Minh Ngoc Dinh", "Zhuang Li"], "abstract": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM", "categories": ["cs.CL"], "published": "2025-06-04 14:50:32", "updated": "2025-06-04 14:50:32", "pdf_url": "http://arxiv.org/pdf/2506.04020v1", "comment": "Paper accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04032v1", "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "abstract": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.", "categories": ["cs.CL"], "published": "2025-06-04 14:56:08", "updated": "2025-06-04 14:56:08", "pdf_url": "http://arxiv.org/pdf/2506.04032v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04037v1", "title": "The mutual exclusivity bias of bilingual visually grounded speech models", "authors": ["Dan Oneata", "Leanne Nortje", "Yevgen Matusevych", "Herman Kamper"], "abstract": "Mutual exclusivity (ME) is a strategy where a novel word is associated with a\nnovel object rather than a familiar one, facilitating language learning in\nchildren. Recent work has found an ME bias in a visually grounded speech (VGS)\nmodel trained on English speech with paired images. But ME has also been\nstudied in bilingual children, who may employ it less due to cross-lingual\nambiguity. We explore this pattern computationally using bilingual VGS models\ntrained on combinations of English, French, and Dutch. We find that bilingual\nmodels generally exhibit a weaker ME bias than monolingual models, though\nexceptions exist. Analyses show that the combined visual embeddings of\nbilingual models have a smaller variance for familiar data, partly explaining\nthe increase in confusion between novel and familiar concepts. We also provide\nnew insights into why the ME bias exists in VGS models in the first place. Code\nand data: https://github.com/danoneata/me-vgs", "categories": ["cs.CL", "eess.AS"], "published": "2025-06-04 14:59:22", "updated": "2025-06-04 14:59:22", "pdf_url": "http://arxiv.org/pdf/2506.04037v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04039v1", "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "authors": ["Jiulong Wu", "Zhengliang Shi", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Lingyong Yan", "Min Cao", "Min Zhang"], "abstract": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-06-04 15:03:50", "updated": "2025-06-04 15:03:50", "pdf_url": "http://arxiv.org/pdf/2506.04039v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04041v1", "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events", "authors": ["Claire Barale", "Leslie Barrett", "Vikram Sunil Bajaj", "Michael Rovatsos"], "abstract": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.", "categories": ["cs.CL"], "published": "2025-06-04 15:06:27", "updated": "2025-06-04 15:06:27", "pdf_url": "http://arxiv.org/pdf/2506.04041v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04042v1", "title": "Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness", "authors": ["Xiyu Liu", "Zhengxiao Liu", "Naibin Gu", "Zheng Lin", "Ji Xiang", "Weiping Wang"], "abstract": "Knowledge editing aims to alternate the target knowledge predicted by large\nlanguage models while ensuring the least side effects on unrelated knowledge.\nAn effective way to achieve knowledge editing is to identify pivotal parameters\nfor predicting factual associations and modify them with an optimization\nprocess to update the predictions. However, these locate-then-edit methods are\nuncontrollable since they tend to modify most unrelated relations connected to\nthe subject of target editing. We unveil that this failure of controllable\nediting is due to a shortcut learning issue during the optimization process.\nSpecifically, we discover two crucial features that are the subject feature and\nthe relation feature for models to learn during optimization, but the current\noptimization process tends to over-learning the subject feature while\nneglecting the relation feature. To eliminate this shortcut learning of the\nsubject feature, we propose a novel two-stage optimization process that\nbalances the learning of the subject feature and the relation feature.\nExperimental results demonstrate that our approach successfully prevents\nknowledge editing from shortcut learning and achieves the optimal overall\nperformance, contributing to controllable knowledge editing.", "categories": ["cs.CL"], "published": "2025-06-04 15:06:46", "updated": "2025-06-04 15:06:46", "pdf_url": "http://arxiv.org/pdf/2506.04042v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04043v1", "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "abstract": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "published": "2025-06-04 15:09:20", "updated": "2025-06-04 15:09:20", "pdf_url": "http://arxiv.org/pdf/2506.04043v1", "comment": "Accepted at ACL WOAH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04044v1", "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs", "authors": ["Aleksey Kudelya", "Alexander Shirnin"], "abstract": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 15:10:09", "updated": "2025-06-04 15:10:09", "pdf_url": "http://arxiv.org/pdf/2506.04044v1", "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04047v1", "title": "On Support Samples of Next Word Prediction", "authors": ["Yuqian Li", "Yupei Du", "Yufang Liu", "Feifei Feng", "Mou Xiao Feng", "Yuanbin Wu"], "abstract": "Language models excel in various tasks by making complex decisions, yet\nunderstanding the rationale behind these decisions remains a challenge. This\npaper investigates \\emph{data-centric interpretability} in language models,\nfocusing on the next-word prediction task. Using representer theorem, we\nidentify two types of \\emph{support samples}-those that either promote or deter\nspecific predictions. Our findings reveal that being a support sample is an\nintrinsic property, predictable even before training begins. Additionally,\nwhile non-support samples are less influential in direct predictions, they play\na critical role in preventing overfitting and shaping generalization and\nrepresentation learning. Notably, the importance of non-support samples\nincreases in deeper layers, suggesting their significant role in intermediate\nrepresentation formation.These insights shed light on the interplay between\ndata and model decisions, offering a new dimension to understanding language\nmodel behavior and interpretability.", "categories": ["cs.CL"], "published": "2025-06-04 15:13:22", "updated": "2025-06-04 15:13:22", "pdf_url": "http://arxiv.org/pdf/2506.04047v1", "comment": "Accepted to ACL2025(Main Conference)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04050v1", "title": "Explainability-Based Token Replacement on LLM-Generated Text", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "abstract": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 15:15:42", "updated": "2025-06-04 15:15:42", "pdf_url": "http://arxiv.org/pdf/2506.04050v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04051v1", "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "authors": ["Tim Franzmeyer", "Archie Sravankumar", "Lijuan Liu", "Yuning Mao", "Rui Hou", "Sinong Wang", "Jakob N. Foerster", "Luke Zettlemoyer", "Madian Khabsa"], "abstract": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 15:16:21", "updated": "2025-06-04 15:16:21", "pdf_url": "http://arxiv.org/pdf/2506.04051v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04065v1", "title": "Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning", "authors": ["Muling Wu", "Qi Qian", "Wenhao Liu", "Xiaohua Wang", "Zisu Huang", "Di Liang", "LI Miao", "Shihan Dou", "Changze Lv", "Zhenghua Wang", "Zhibo Xu", "Lina Chen", "Tianlong Li", "Xiaoqing Zheng", "Xuanjing Huang"], "abstract": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.", "categories": ["cs.CL"], "published": "2025-06-04 15:31:46", "updated": "2025-06-04 15:31:46", "pdf_url": "http://arxiv.org/pdf/2506.04065v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04070v1", "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward", "authors": ["Yi Zhao", "Siqi Wang", "Jing Li"], "abstract": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.", "categories": ["cs.CL", "cs.MM"], "published": "2025-06-04 15:34:33", "updated": "2025-06-04 15:34:33", "pdf_url": "http://arxiv.org/pdf/2506.04070v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04072v1", "title": "Controlling Difficulty of Generated Text for AI-Assisted Language Learning", "authors": ["Meiqing Jin", "Liam Dugan", "Chris Callison-Burch"], "abstract": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.", "categories": ["cs.CL", "cs.HC", "I.2.7"], "published": "2025-06-04 15:38:21", "updated": "2025-06-04 15:38:21", "pdf_url": "http://arxiv.org/pdf/2506.04072v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04076v1", "title": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems", "authors": ["Jhen-Ke Lin", "Hao-Chien Lu", "Chung-Chun Wang", "Hong-Yun Lin", "Berlin Chen"], "abstract": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-04 15:41:53", "updated": "2025-06-04 15:41:53", "pdf_url": "http://arxiv.org/pdf/2506.04076v1", "comment": "submitted to the ISCA SLaTE-2025 Workshop", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04077v1", "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions", "authors": ["Chung-Chun Wang", "Jhen-Ke Lin", "Hao-Chien Lu", "Hong-Yun Lin", "Berlin Chen"], "abstract": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-04 15:42:53", "updated": "2025-06-04 15:42:53", "pdf_url": "http://arxiv.org/pdf/2506.04077v1", "comment": "submitted to the ISCA SLaTE-2025 Workshop", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04078v1", "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "authors": ["Ming Zhang", "Yujiong Shen", "Zelin Li", "Huayu Sha", "Binze Hu", "Yuhui Wang", "Chenhao Huang", "Shichun Liu", "Jingqi Tong", "Changhao Jiang", "Mingxu Chai", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "abstract": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 15:43:14", "updated": "2025-06-04 15:43:14", "pdf_url": "http://arxiv.org/pdf/2506.04078v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04079v1", "title": "EuroLLM-9B: Technical Report", "authors": ["Pedro Henrique Martins", "Jo\u00e3o Alves", "Patrick Fernandes", "Nuno M. Guerreiro", "Ricardo Rei", "Amin Farajian", "Mateusz Klimaszewski", "Duarte M. Alves", "Jos\u00e9 Pombal", "Manuel Faysse", "Pierre Colombo", "Fran\u00e7ois Yvon", "Barry Haddow", "Jos\u00e9 G. C. de Souza", "Alexandra Birch", "Andr\u00e9 F. T. Martins"], "abstract": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 15:43:31", "updated": "2025-06-04 15:43:31", "pdf_url": "http://arxiv.org/pdf/2506.04079v1", "comment": "56 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04088v1", "title": "Multimodal Tabular Reasoning with Privileged Structured Information", "authors": ["Jun-Peng Jiang", "Yu Xia", "Hai-Long Sun", "Shiyin Lu", "Qing-Guo Chen", "Weihua Luo", "Kaifu Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "abstract": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-06-04 15:46:30", "updated": "2025-06-04 15:46:30", "pdf_url": "http://arxiv.org/pdf/2506.04088v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04089v1", "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment", "authors": ["Anastasiia Ivanova", "Eva Bakaeva", "Zoya Volovikova", "Alexey K. Kovalev", "Aleksandr I. Panov"], "abstract": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "published": "2025-06-04 15:47:07", "updated": "2025-06-04 15:47:07", "pdf_url": "http://arxiv.org/pdf/2506.04089v1", "comment": "ACL 2025 (Main Conference)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04098v1", "title": "TextAtari: 100K Frames Game Playing with Language Agents", "authors": ["Wenhao Li", "Wenwu Li", "Chuyun Shen", "Junjie Sheng", "Zixiao Huang", "Di Wu", "Yun Hua", "Wei Yin", "Xiangfeng Wang", "Hongyuan Zha", "Bo Jin"], "abstract": "We present TextAtari, a benchmark for evaluating language agents on very\nlong-horizon decision-making tasks spanning up to 100,000 steps. By translating\nthe visual state representations of classic Atari games into rich textual\ndescriptions, TextAtari creates a challenging test bed that bridges sequential\ndecision-making with natural language processing. The benchmark includes nearly\n100 distinct tasks with varying complexity, action spaces, and planning\nhorizons, all rendered as text through an unsupervised representation learning\nframework (AtariARI). We evaluate three open-source large language models\n(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks\n(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how\ndifferent forms of prior knowledge affect performance on these long-horizon\nchallenges. Four scenarios-Basic, Obscured, Manual Augmentation, and\nReference-based-investigate the impact of semantic understanding, instruction\ncomprehension, and expert demonstrations on agent decision-making. Our results\nreveal significant performance gaps between language agents and human players\nin extensive planning tasks, highlighting challenges in sequential reasoning,\nstate tracking, and strategic planning across tens of thousands of steps.\nTextAtari provides standardized evaluation protocols, baseline implementations,\nand a framework for advancing research at the intersection of language models\nand planning.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 15:55:27", "updated": "2025-06-04 15:55:27", "pdf_url": "http://arxiv.org/pdf/2506.04098v1", "comment": "51 pages, 39 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04108v1", "title": "Rectified Sparse Attention", "authors": ["Yutao Sun", "Tianzhu Ye", "Li Dong", "Yuqing Xia", "Jian Chen", "Yizhao Gao", "Shijie Cao", "Jianyong Wang", "Furu Wei"], "abstract": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.", "categories": ["cs.CL"], "published": "2025-06-04 16:01:48", "updated": "2025-06-04 16:01:48", "pdf_url": "http://arxiv.org/pdf/2506.04108v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04131v1", "title": "CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues", "authors": ["Disha Sheshanarayana", "Tanishka Magar", "Ayushi Mittal", "Neelam Chaplot"], "abstract": "Courtrooms are places where lives are determined and fates are sealed, yet\nthey are not impervious to manipulation. Strategic use of manipulation in legal\njargon can sway the opinions of judges and affect the decisions. Despite the\ngrowing advancements in NLP, its application in detecting and analyzing\nmanipulation within the legal domain remains largely unexplored. Our work\naddresses this gap by introducing LegalCon, a dataset of 1,063 annotated\ncourtroom conversations labeled for manipulation detection, identification of\nprimary manipulators, and classification of manipulative techniques, with a\nfocus on long conversations. Furthermore, we propose CLAIM, a two-stage,\nIntent-driven Multi-agent framework designed to enhance manipulation analysis\nby enabling context-aware and informed decision-making. Our results highlight\nthe potential of incorporating agentic frameworks to improve fairness and\ntransparency in judicial processes. We hope that this contributes to the\nbroader application of NLP in legal discourse analysis and the development of\nrobust tools to support fairness in legal decision-making. Our code and data\nare available at https://github.com/Disha1001/CLAIM.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-04 16:22:59", "updated": "2025-06-04 16:22:59", "pdf_url": "http://arxiv.org/pdf/2506.04131v1", "comment": "Accepted to SICon 2025 ACL", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04139v1", "title": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?", "authors": ["Ratna Kandala", "Katie Hoemann"], "abstract": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research.", "categories": ["cs.CL"], "published": "2025-06-04 16:31:37", "updated": "2025-06-04 16:31:37", "pdf_url": "http://arxiv.org/pdf/2506.04139v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04141v1", "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos", "authors": ["Kejian Zhu", "Zhuoran Jin", "Hongbang Yuan", "Jiachun Li", "Shangqing Tu", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "abstract": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.", "categories": ["cs.CV", "cs.CL"], "published": "2025-06-04 16:33:41", "updated": "2025-06-04 16:33:41", "pdf_url": "http://arxiv.org/pdf/2506.04141v1", "comment": "Project Page: https://mmr-v.github.io", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04142v1", "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis", "authors": ["Kejian Zhu", "Shangqing Tu", "Zhuoran Jin", "Lei Hou", "Juanzi Li", "Jun Zhao"], "abstract": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation", "categories": ["cs.CL"], "published": "2025-06-04 16:33:44", "updated": "2025-06-04 16:33:44", "pdf_url": "http://arxiv.org/pdf/2506.04142v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04156v1", "title": "A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization", "authors": ["Sarvesh Soni", "Dina Demner-Fushman"], "abstract": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.", "categories": ["cs.CL"], "published": "2025-06-04 16:55:08", "updated": "2025-06-04 16:55:08", "pdf_url": "http://arxiv.org/pdf/2506.04156v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04179v1", "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling", "authors": ["Anhao Zhao", "Fanghua Ye", "Yingqi Fan", "Junlong Tong", "Zhiwei Fei", "Hui Su", "Xiaoyu Shen"], "abstract": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT.", "categories": ["cs.CL"], "published": "2025-06-04 17:26:31", "updated": "2025-06-04 17:26:31", "pdf_url": "http://arxiv.org/pdf/2506.04179v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04180v1", "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Juanzi Li", "Roy Ka-Wei Lee"], "abstract": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.", "categories": ["cs.CL"], "published": "2025-06-04 17:27:42", "updated": "2025-06-04 17:27:42", "pdf_url": "http://arxiv.org/pdf/2506.04180v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04182v1", "title": "Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models", "authors": ["Ruiqi Zhang", "Changyi Xiao", "Yixin Cao"], "abstract": "With the rapid advancement of large reasoning models, long Chain-of-Thought\n(CoT) prompting has demonstrated strong performance on complex tasks. However,\nthis often comes with a significant increase in token usage. In this paper, we\nconduct a comprehensive empirical analysis comparing long and short CoT\nstrategies. Our findings reveal that while long CoT can lead to performance\nimprovements, its benefits are often marginal relative to its significantly\nhigher token consumption. Specifically, long CoT tends to outperform when ample\ngeneration budgets are available, whereas short CoT is more effective under\ntighter budget constraints. These insights underscore the need for a dynamic\napproach that selects the proper CoT strategy based on task context and\nresource availability. To address this, we propose SwitchCoT, an automatic\nframework that adaptively chooses between long and short CoT strategies to\nbalance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is\ndesigned to be budget-aware, making it broadly applicable across scenarios with\nvarying resource constraints. Experimental results demonstrate that SwitchCoT\ncan reduce inference costs by up to 50% while maintaining high accuracy.\nNotably, under limited token budgets, it achieves performance comparable to, or\neven exceeding, that of using either long or short CoT alone.", "categories": ["cs.CL"], "published": "2025-06-04 17:28:38", "updated": "2025-06-04 17:28:38", "pdf_url": "http://arxiv.org/pdf/2506.04182v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04185v1", "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning", "authors": ["Qingfei Zhao", "Ruobing Wang", "Dingling Xu", "Daren Zha", "Limin Liu"], "abstract": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.", "categories": ["cs.CL"], "published": "2025-06-04 17:29:22", "updated": "2025-06-04 17:29:22", "pdf_url": "http://arxiv.org/pdf/2506.04185v1", "comment": "16 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04207v1", "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "abstract": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-06-04 17:51:08", "updated": "2025-06-04 17:51:08", "pdf_url": "http://arxiv.org/pdf/2506.04207v1", "comment": "19 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04210v1", "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-04 17:55:09", "updated": "2025-06-04 17:55:09", "pdf_url": "http://arxiv.org/pdf/2506.04210v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04226v1", "title": "Efficient Knowledge Editing via Minimal Precomputation", "authors": ["Akshat Gupta", "Maochuan Lu", "Thomas Hartvigsen", "Gopala Anumanchipalli"], "abstract": "Knowledge editing methods like MEMIT are able to make data and compute\nefficient updates of factual knowledge by using a single sentence to update\nfacts and their consequences. However, what is often overlooked is a\n\"precomputation step\", which requires a one-time but significant computational\ncost. The authors of MEMIT originally precompute approximately 44 million\nhidden vectors per edited layer, which requires a forward pass over 44 million\ntokens. For GPT-J (6B), this precomputation step takes 36 hours on a single\nGPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this\nprecomputation time grows with model size. In this paper, we show that this\nexcessive computational cost is unnecessary. Knowledge editing using MEMIT and\nrelated methods, such as ROME and EMMET, can be performed by pre-computing a\nvery small portion of the 44 million hidden vectors. We first present the\ntheoretical minimum number of hidden vector precomputation required for\nsolutions of these editing methods to exist. We then empirically show that\nknowledge editing using these methods can be done by pre-computing\nsignificantly fewer hidden vectors. Specifically, we show that the\nprecomputation step can be done with less than 0.3% of the originally\nstipulated number of hidden vectors. This saves a significant amount of\nprecomputation time and allows users to begin editing new models within a few\nminutes.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-04 17:59:05", "updated": "2025-06-04 17:59:05", "pdf_url": "http://arxiv.org/pdf/2506.04226v1", "comment": "ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04534v1", "title": "Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs", "authors": ["William Sheffield", "Kanishka Misra", "Valentina Pyatkin", "Ashwini Deo", "Kyle Mahowald", "Junyi Jessy Li"], "abstract": "Discourse particles are crucial elements that subtly shape the meaning of\ntext. These words, often polyfunctional, give rise to nuanced and often quite\ndisparate semantic/discourse effects, as exemplified by the diverse uses of the\nparticle \"just\" (e.g., exclusive, temporal, emphatic). This work investigates\nthe capacity of LLMs to distinguish the fine-grained senses of English \"just\",\na well-studied example in formal semantics, using data meticulously created and\nlabeled by expert linguists. Our findings reveal that while LLMs exhibit some\nability to differentiate between broader categories, they struggle to fully\ncapture more subtle nuances, highlighting a gap in their understanding of\ndiscourse particles.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 00:59:05", "updated": "2025-06-05 00:59:05", "pdf_url": "http://arxiv.org/pdf/2506.04534v1", "comment": "To be published in Findings of The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025). The main paper is 5\n  pages and contains 3 figures and 1 table. In total, the paper is 12 pages and\n  contains 8 figures and 5 tables (References + Appendix)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04536v1", "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models", "authors": ["Luca Ghafourpour", "Valentin Duruisseaux", "Bahareh Tolooshams", "Philip H. Wong", "Costas A. Anastassiou", "Anima Anandkumar"], "abstract": "Characterizing the diverse computational properties of human neurons via\nmultimodal electrophysiological, transcriptomic, and morphological data\nprovides the foundation for constructing and validating bio-realistic neuron\nmodels that can advance our understanding of fundamental mechanisms underlying\nbrain function. However, current modeling approaches remain constrained by the\nlimited availability and intrinsic variability of experimental neuronal data.\nTo capture variability, ensembles of deterministic models are often used, but\nare difficult to scale as model generation requires repeating computationally\nexpensive optimization for each neuron. While deep learning is becoming\nincreasingly relevant in this space, it fails to capture the full biophysical\ncomplexity of neurons, their nonlinear voltage dynamics, and variability. To\naddress these shortcomings, we introduce NOBLE, a neural operator framework\nthat learns a mapping from a continuous frequency-modulated embedding of\ninterpretable neuron features to the somatic voltage response induced by\ncurrent injection. Trained on data generated from biophysically realistic\nneuron models, NOBLE predicts distributions of neural dynamics accounting for\nthe intrinsic experimental variability. Unlike conventional bio-realistic\nneuron models, interpolating within the embedding space offers models whose\ndynamics are consistent with experimentally observed responses. NOBLE is the\nfirst scaled-up deep learning framework validated on real experimental data,\nenabling efficient generation of synthetic neurons that exhibit trial-to-trial\nvariability and achieve a $4200\\times$ speedup over numerical solvers. To this\nend, NOBLE captures fundamental neural properties, opening the door to a better\nunderstanding of cellular composition and computations, neuromorphic\narchitectures, large-scale brain circuits, and general neuroAI applications.", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "published": "2025-06-05 01:01:18", "updated": "2025-06-05 01:01:18", "pdf_url": "http://arxiv.org/pdf/2506.04536v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04544v1", "title": "hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation", "authors": ["Charles Hong", "Brendan Roberts", "Huijae An", "Alex Um", "Advay Ratan", "Yakun Sophia Shao"], "abstract": "Large language models (LLMs) are playing an increasingly large role in\ndomains such as code generation, including hardware code generation, where\nVerilog is the key language. However, the amount of publicly available Verilog\ncode pales in comparison to the amount of code available for software languages\nlike Python. In this work, we present hdl2v (\"HDL-to-Verilog\"), a dataset which\nseeks to increase the amount of available human-written Verilog data by\ntranslating or compiling three other hardware description languages - VHDL,\nChisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v\nin enhancing LLM Verilog generation by improving performance of a 32\nbillion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,\nwithout utilizing any data augmentation or knowledge distillation from larger\nmodels. We also show hdl2v's ability to boost the performance of a data\naugmentation-based fine-tuning approach by 63%. Finally, we characterize and\nanalyze our dataset to better understand which characteristics of\nHDL-to-Verilog datasets can be expanded upon in future work for even better\nperformance.", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.PL"], "published": "2025-06-05 01:29:18", "updated": "2025-06-05 01:29:18", "pdf_url": "http://arxiv.org/pdf/2506.04544v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04556v1", "title": "BESA: Boosting Encoder Stealing Attack with Perturbation Recovery", "authors": ["Xuhao Ren", "Haotian Liang", "Yajie Wang", "Chuan Zhang", "Zehui Xiong", "Liehuang Zhu"], "abstract": "To boost the encoder stealing attack under the perturbation-based defense\nthat hinders the attack performance, we propose a boosting encoder stealing\nattack with perturbation recovery named BESA. It aims to overcome\nperturbation-based defenses. The core of BESA consists of two modules:\nperturbation detection and perturbation recovery, which can be combined with\ncanonical encoder stealing attacks. The perturbation detection module utilizes\nthe feature vectors obtained from the target encoder to infer the defense\nmechanism employed by the service provider. Once the defense mechanism is\ndetected, the perturbation recovery module leverages the well-designed\ngenerative model to restore a clean feature vector from the perturbed one.\nThrough extensive evaluations based on various datasets, we demonstrate that\nBESA significantly enhances the surrogate encoder accuracy of existing encoder\nstealing attacks by up to 24.63\\% when facing state-of-the-art defenses and\ncombinations of multiple defenses.", "categories": ["cs.CR", "cs.AI"], "published": "2025-06-05 02:14:30", "updated": "2025-06-05 02:14:30", "pdf_url": "http://arxiv.org/pdf/2506.04556v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04557v1", "title": "SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?", "authors": ["Senyu Li", "Jiayi Wang", "Felermino D. M. A. Ali", "Colin Cherry", "Daniel Deutsch", "Eleftheria Briakou", "Rui Sousa-Silva", "Henrique Lopes Cardoso", "Pontus Stenetorp", "David Ifeoluwa Adelani"], "abstract": "Evaluating machine translation (MT) quality for under-resourced African\nlanguages remains a significant challenge, as existing metrics often suffer\nfrom limited language coverage and poor performance in low-resource settings.\nWhile recent efforts, such as AfriCOMET, have addressed some of the issues,\nthey are still constrained by small evaluation sets, a lack of publicly\navailable training data tailored to African languages, and inconsistent\nperformance in extremely low-resource scenarios. In this work, we introduce\nSSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 13\nAfrican language pairs from the News domain, with over 63,000 sentence-level\nannotations from a diverse set of MT systems. Based on this data, we develop\nSSA-COMET and SSA-COMET-QE, improved reference-based and reference-free\nevaluation metrics. We also benchmark prompting-based approaches using\nstate-of-the-art LLMs like GPT-4o and Claude. Our experimental results show\nthat SSA-COMET models significantly outperform AfriCOMET and are competitive\nwith the strongest LLM (Gemini 2.5 Pro) evaluated in our study, particularly on\nlow-resource languages such as Twi, Luo, and Yoruba. All resources are released\nunder open licenses to support future research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 02:16:56", "updated": "2025-06-05 02:16:56", "pdf_url": "http://arxiv.org/pdf/2506.04557v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04566v1", "title": "Clustering and Median Aggregation Improve Differentially Private Inference", "authors": ["Kareem Amin", "Salman Avestimehr", "Sara Babakniya", "Alex Bie", "Weiwei Kong", "Natalia Ponomareva", "Umar Syed"], "abstract": "Differentially private (DP) language model inference is an approach for\ngenerating private synthetic text. A sensitive input example is used to prompt\nan off-the-shelf large language model (LLM) to produce a similar example.\nMultiple examples can be aggregated together to formally satisfy the DP\nguarantee.\n  Prior work creates inference batches by sampling sensitive inputs uniformly\nat random. We show that uniform sampling degrades the quality of privately\ngenerated text, especially when the sensitive examples concern heterogeneous\ntopics.\n  We remedy this problem by clustering the input data before selecting\ninference batches. Next, we observe that clustering also leads to more similar\nnext-token predictions across inferences. We use this insight to introduce a\nnew algorithm that aggregates next token statistics by privately computing\nmedians instead of averages. This approach leverages the fact that the median\nhas decreased local sensitivity when next token predictions are similar,\nallowing us to state a data-dependent and ex-post DP guarantee about the\nprivacy properties of this algorithm. Finally, we demonstrate improvements in\nterms of representativeness metrics (e.g., MAUVE) as well as downstream task\nperformance. We show that our method produces high-quality synthetic data at\nsignificantly lower privacy cost than a previous state-of-the-art method.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "published": "2025-06-05 02:34:50", "updated": "2025-06-05 02:34:50", "pdf_url": "http://arxiv.org/pdf/2506.04566v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04571v1", "title": "OpenAg: Democratizing Agricultural Intelligence", "authors": ["Srikanth Thudumu", "Jason Fisher"], "abstract": "Agriculture is undergoing a major transformation driven by artificial\nintelligence (AI), machine learning, and knowledge representation technologies.\nHowever, current agricultural intelligence systems often lack contextual\nunderstanding, explainability, and adaptability, especially for smallholder\nfarmers with limited resources. General-purpose large language models (LLMs),\nwhile powerful, typically lack the domain-specific knowledge and contextual\nreasoning needed for practical decision support in farming. They tend to\nproduce recommendations that are too generic or unrealistic for real-world\napplications. To address these challenges, we present OpenAg, a comprehensive\nframework designed to advance agricultural artificial general intelligence\n(AGI). OpenAg combines domain-specific foundation models, neural knowledge\ngraphs, multi-agent reasoning, causal explainability, and adaptive transfer\nlearning to deliver context-aware, explainable, and actionable insights. The\nsystem includes: (i) a unified agricultural knowledge base that integrates\nscientific literature, sensor data, and farmer-generated knowledge; (ii) a\nneural agricultural knowledge graph for structured reasoning and inference;\n(iii) an adaptive multi-agent reasoning system where AI agents specialize and\ncollaborate across agricultural domains; and (iv) a causal transparency\nmechanism that ensures AI recommendations are interpretable, scientifically\ngrounded, and aligned with real-world constraints. OpenAg aims to bridge the\ngap between scientific knowledge and the tacit expertise of experienced farmers\nto support scalable and locally relevant agricultural decision-making.", "categories": ["cs.AI"], "published": "2025-06-05 02:44:38", "updated": "2025-06-05 02:44:38", "pdf_url": "http://arxiv.org/pdf/2506.04571v1", "comment": "10 pages, 1 figure", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04574v1", "title": "Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis", "authors": ["Dimitris Vamvourellis", "Dhagash Mehta"], "abstract": "We investigate the effectiveness of large language models (LLMs), including\nreasoning-based and non-reasoning models, in performing zero-shot financial\nsentiment analysis. Using the Financial PhraseBank dataset annotated by domain\nexperts, we evaluate how various LLMs and prompting strategies align with\nhuman-labeled sentiment in a financial context. We compare three proprietary\nLLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that\nsimulate System 1 (fast and intuitive) or System 2 (slow and deliberate)\nthinking and benchmark them against two smaller models (FinBERT-Prosus,\nFinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest\nthat reasoning, either through prompting or inherent model design, does not\nimprove performance on this task. Surprisingly, the most accurate and\nhuman-aligned combination of model and method was GPT-4o without any\nChain-of-Thought (CoT) prompting. We further explore how performance is\nimpacted by linguistic complexity and annotation agreement levels, uncovering\nthat reasoning may introduce overthinking, leading to suboptimal predictions.\nThis suggests that for financial sentiment classification, fast, intuitive\n\"System 1\"-like thinking aligns more closely with human judgment compared to\n\"System 2\"-style slower, deliberative reasoning simulated by reasoning models\nor CoT prompting. Our results challenge the default assumption that more\nreasoning always leads to better LLM decisions, particularly in high-stakes\nfinancial applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 02:47:23", "updated": "2025-06-05 02:47:23", "pdf_url": "http://arxiv.org/pdf/2506.04574v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04583v1", "title": "SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing", "authors": ["Hongjun Liu", "Yilun Zhao", "Arman Cohan", "Chen Zhao"], "abstract": "Automatic fact-checking has recently received more attention as a means of\ncombating misinformation. Despite significant advancements, fact-checking\nsystems based on retrieval-augmented language models still struggle to tackle\nadversarial claims, which are intentionally designed by humans to challenge\nfact-checking systems. To address these challenges, we propose a training-free\nmethod designed to rephrase the original claim, making it easier to locate\nsupporting evidence. Our modular framework, SUCEA, decomposes the task into\nthree steps: 1) Claim Segmentation and Decontextualization that segments\nadversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval\nand Claim Editing that iteratively retrieves evidence and edits the subclaim\nbased on the retrieved evidence; 3) Evidence Aggregation and Label Prediction\nthat aggregates all retrieved evidence and predicts the entailment label.\nExperiments on two challenging fact-checking datasets demonstrate that our\nframework significantly improves on both retrieval and entailment label\naccuracy, outperforming four strong claim-decomposition-based baselines.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 02:58:15", "updated": "2025-06-05 02:58:15", "pdf_url": "http://arxiv.org/pdf/2506.04583v1", "comment": "16 pages, 10 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04592v1", "title": "Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification", "authors": ["Chengwu Liu", "Ye Yuan", "Yichun Yin", "Yan Xu", "Xin Xu", "Zaoyu Chen", "Yasheng Wang", "Lifeng Shang", "Qun Liu", "Ming Zhang"], "abstract": "Chain-of-Thought (CoT) prompting has become the de facto method to elicit\nreasoning capabilities from large language models (LLMs). However, to mitigate\nhallucinations in CoT that are notoriously difficult to detect, current methods\nsuch as process reward models (PRMs) or self-consistency operate as opaque\nboxes and do not provide checkable evidence for their judgments, possibly\nlimiting their effectiveness. To address this issue, we draw inspiration from\nthe idea that \"the gold standard for supporting a mathematical claim is to\nprovide a proof\". We propose a retrospective, step-aware formal verification\nframework $Safe$. Rather than assigning arbitrary scores, we strive to\narticulate mathematical claims in formal mathematical language Lean 4 at each\nreasoning step and provide formal proofs to identify hallucinations. We\nevaluate our framework $Safe$ across multiple language models and various\nmathematical datasets, demonstrating a significant performance improvement\nwhile offering interpretable and verifiable evidence. We also propose\n$FormalStep$ as a benchmark for step correctness theorem proving with $30,809$\nformal statements. To the best of our knowledge, our work represents the first\nendeavor to utilize formal mathematical language Lean 4 for verifying natural\nlanguage content generated by LLMs, aligning with the reason why formal\nmathematical languages were created in the first place: to provide a robust\nfoundation for hallucination-prone human-written proofs.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 03:16:08", "updated": "2025-06-05 03:16:08", "pdf_url": "http://arxiv.org/pdf/2506.04592v1", "comment": "Accepted in ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04594v1", "title": "Intelligent Channel Allocation for IEEE 802.11be Multi-Link Operation: When MAB Meets LLM", "authors": ["Shumin Lian", "Jingwen Tong", "Jun Zhang", "Liqun Fu"], "abstract": "WiFi networks have achieved remarkable success in enabling seamless\ncommunication and data exchange worldwide. The IEEE 802.11be standard, known as\nWiFi 7, introduces Multi-Link Operation (MLO), a groundbreaking feature that\nenables devices to establish multiple simultaneous connections across different\nbands and channels. While MLO promises substantial improvements in network\nthroughput and latency reduction, it presents significant challenges in channel\nallocation, particularly in dense network environments. Current research has\npredominantly focused on performance analysis and throughput optimization\nwithin static WiFi 7 network configurations. In contrast, this paper addresses\nthe dynamic channel allocation problem in dense WiFi 7 networks with MLO\ncapabilities. We formulate this challenge as a combinatorial optimization\nproblem, leveraging a novel network performance analysis mechanism. Given the\ninherent lack of prior network information, we model the problem within a\nMulti-Armed Bandit (MAB) framework to enable online learning of optimal channel\nallocations. Our proposed Best-Arm Identification-enabled Monte Carlo Tree\nSearch (BAI-MCTS) algorithm includes rigorous theoretical analysis, providing\nupper bounds for both sample complexity and error probability. To further\nreduce sample complexity and enhance generalizability across diverse network\nscenarios, we put forth LLM-BAI-MCTS, an intelligent algorithm for the dynamic\nchannel allocation problem by integrating the Large Language Model (LLM) into\nthe BAI-MCTS algorithm. Numerical results demonstrate that the BAI-MCTS\nalgorithm achieves a convergence rate approximately $50.44\\%$ faster than the\nstate-of-the-art algorithms when reaching $98\\%$ of the optimal value. Notably,\nthe convergence rate of the LLM-BAI-MCTS algorithm increases by over $63.32\\%$\nin dense networks.", "categories": ["cs.NI", "cs.AI", "eess.SP", "I.2.7"], "published": "2025-06-05 03:19:57", "updated": "2025-06-05 03:19:57", "pdf_url": "http://arxiv.org/pdf/2506.04594v1", "comment": "This work has been accepted by JSAC 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04598v1", "title": "Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets", "authors": ["Marianna Nezhurina", "Tomer Porian", "Giovanni Pucceti", "Tommie Kerssies", "Romain Beaumont", "Mehdi Cherti", "Jenia Jitsev"], "abstract": "In studies of transferable learning, scaling laws are obtained for various\nimportant foundation models to predict their properties and performance at\nlarger scales. We show here how scaling law derivation can also be used for\nmodel and dataset comparison, allowing to decide which procedure is to be\npreferred for pre-training. For the first time, full scaling laws based on\ndense measurements across a wide span of model and samples seen scales are\nderived for two important language-vision learning procedures, CLIP and MaMMUT,\nthat use either contrastive only or contrastive and captioning text generative\nloss. Ensuring sufficient prediction accuracy for held out points, we use\nderived scaling laws to compare both models, obtaining evidence for MaMMUT's\nstronger improvement with scale and better sample efficiency than standard\nCLIP. To strengthen validity of the comparison, we show scaling laws for\nvarious downstream tasks, classification, retrieval, and segmentation, and for\ndifferent open datasets, DataComp, DFN and Re-LAION, observing consistently the\nsame trends. We show that comparison can also be performed when deriving\nscaling laws with a constant learning rate schedule, reducing compute cost.\nAccurate derivation of scaling laws provides thus means to perform model and\ndataset comparison across scale spans, avoiding misleading conclusions based on\nmeasurements from single reference scales only, paving the road for systematic\ncomparison and improvement of open foundation models and datasets for their\ncreation. We release all the pre-trained models with their intermediate\ncheckpoints, including openMaMMUT-L/14, which achieves $80.3\\%$ zero-shot\nImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for\nreproducing experiments in the paper and raw experiments data can be found at\nhttps://github.com/LAION-AI/scaling-laws-for-comparison.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-06-05 03:35:59", "updated": "2025-06-05 03:35:59", "pdf_url": "http://arxiv.org/pdf/2506.04598v1", "comment": "Preprint. In Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04610v1", "title": "Judicial Permission", "authors": ["Guido Governatori", "Antonino Rotolo"], "abstract": "This paper examines the significance of weak permissions in criminal trials\n(\\emph{judicial permission}). It introduces a dialogue game model to\nsystematically address judicial permissions, considering different standards of\nproof and argumentation semantics.", "categories": ["cs.AI", "cs.CY", "cs.LO"], "published": "2025-06-05 04:00:24", "updated": "2025-06-05 04:00:24", "pdf_url": "http://arxiv.org/pdf/2506.04610v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04613v1", "title": "DeePoly: A High-Order Accuracy and Efficiency Deep-Polynomial Framework for Scientific Machine Learning", "authors": ["Li Liu", "Heng Yong"], "abstract": "Recently, machine learning methods have gained significant traction in\nscientific computing, particularly for solving Partial Differential Equations\n(PDEs). However, methods based on deep neural networks (DNNs) often lack\nconvergence guarantees and computational efficiency compared to traditional\nnumerical schemes. This work introduces DeePoly, a novel framework that\ntransforms the solution paradigm from pure non-convex parameter optimization to\na two-stage approach: first employing a DNN to capture complex global features,\nfollowed by linear space optimization with combined DNN-extracted features\n(Scoper) and polynomial basis functions (Sniper). This strategic combination\nleverages the complementary strengths of both methods -- DNNs excel at\napproximating complex global features (i.e., high-gradient features) and\nstabilize the polynomial approximation while polynomial bases provide\nhigh-precision local corrections with convergence guarantees. Theoretical\nanalysis and numerical experiments demonstrate that this approach significantly\nenhances both high-order accuracy and efficiency across diverse problem types\nwhile maintaining mesh-free and scheme-free properties. This paper also serves\nas a theoretical exposition for the open-source project DeePoly.", "categories": ["cs.AI", "cs.LG"], "published": "2025-06-05 04:10:52", "updated": "2025-06-05 04:10:52", "pdf_url": "http://arxiv.org/pdf/2506.04613v1", "comment": "for associated mpeg file, see http://github.com/bfly123/DeePoly", "doi": "10.13140/RG.2.2.24649.25446", "journal_ref": null}
{"arxiv_id": "2506.04614v1", "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation", "authors": ["Yuyang Wanyan", "Xi Zhang", "Haiyang Xu", "Haowei Liu", "Junyang Wang", "Jiabo Ye", "Yutong Kou", "Ming Yan", "Fei Huang", "Xiaoshan Yang", "Weiming Dong", "Changsheng Xu"], "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.", "categories": ["cs.AI"], "published": "2025-06-05 04:12:36", "updated": "2025-06-05 04:12:36", "pdf_url": "http://arxiv.org/pdf/2506.04614v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04624v1", "title": "Static Word Embeddings for Sentence Semantic Representation", "authors": ["Takashi Wada", "Yuki Hirakawa", "Ryotaro Shimizu", "Takahiro Kawashima", "Yuki Saito"], "abstract": "We propose new static word embeddings optimised for sentence semantic\nrepresentation. We first extract word embeddings from a pre-trained Sentence\nTransformer, and improve them with sentence-level principal component analysis,\nfollowed by either knowledge distillation or contrastive learning. During\ninference, we represent sentences by simply averaging word embeddings, which\nrequires little computational cost. We evaluate models on both monolingual and\ncross-lingual tasks and show that our model substantially outperforms existing\nstatic models on sentence semantic tasks, and even rivals a basic Sentence\nTransformer model (SimCSE) on some data sets. Lastly, we perform a variety of\nanalyses and show that our method successfully removes word embedding\ncomponents that are irrelevant to sentence semantics, and adjusts the vector\nnorms based on the influence of words on sentence semantics.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 04:33:10", "updated": "2025-06-05 04:33:10", "pdf_url": "http://arxiv.org/pdf/2506.04624v1", "comment": "15 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04636v1", "title": "CHANCERY: Evaluating corporate governance reasoning capabilities in language models", "authors": ["Lucas Irwin", "Arda Kaz", "Peiyao Sheng", "Pramod Viswanath"], "abstract": "Law has long been a domain that has been popular in natural language\nprocessing (NLP) applications. Reasoning (ratiocination and the ability to make\nconnections to precedent) is a core part of the practice of the law in the real\nworld. Nevertheless, while multiple legal datasets exist, none have thus far\nfocused specifically on reasoning tasks. We focus on a specific aspect of the\nlegal landscape by introducing a corporate governance reasoning benchmark\n(CHANCERY) to test a model's ability to reason about whether\nexecutive/board/shareholder's proposed actions are consistent with corporate\ngovernance charters. This benchmark introduces a first-of-its-kind corporate\ngovernance reasoning test for language models - modeled after real world\ncorporate governance law. The benchmark consists of a corporate charter (a set\nof governing covenants) and a proposal for executive action. The model's task\nis one of binary classification: reason about whether the action is consistent\nwith the rules contained within the charter. We create the benchmark following\nestablished principles of corporate governance - 24 concrete corporate\ngovernance principles established in and 79 real life corporate charters\nselected to represent diverse industries from a total dataset of 10k real life\ncorporate charters. Evaluations on state-of-the-art (SOTA) reasoning models\nconfirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet\nand GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents\nexhibit superior performance, with agents based on the ReAct and CodeAct\nframeworks scoring 76.1% and 78.1% respectively, further confirming the\nadvanced legal reasoning capabilities required to score highly on the\nbenchmark. We also conduct an analysis of the types of questions which current\nreasoning models struggle on, revealing insights into the legal reasoning\ncapabilities of SOTA models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-05 05:13:32", "updated": "2025-06-05 05:13:32", "pdf_url": "http://arxiv.org/pdf/2506.04636v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04651v1", "title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning", "authors": ["Nikolas Belle", "Dakota Barnes", "Alfonso Amayuelas", "Ivan Bercovich", "Xin Eric Wang", "William Wang"], "abstract": "Recent advances in LLMs have enabled their use as autonomous agents across a\nrange of tasks, yet they continue to struggle with formulating and adhering to\ncoherent long-term strategies. In this paper, we investigate whether LLM agents\ncan self-improve when placed in environments that explicitly challenge their\nstrategic planning abilities. Using the board game Settlers of Catan, accessed\nthrough the open-source Catanatron framework, we benchmark a progression of\nLLM-based agents, from a simple game-playing agent to systems capable of\nautonomously rewriting their own prompts and their player agent's code. We\nintroduce a multi-agent architecture in which specialized roles (Analyzer,\nResearcher, Coder, and Player) collaborate to iteratively analyze gameplay,\nresearch new strategies, and modify the agent's logic or prompt. By comparing\nmanually crafted agents to those evolved entirely by LLMs, we evaluate how\neffectively these systems can diagnose failure and adapt over time. Our results\nshow that self-evolving agents, particularly when powered by models like Claude\n3.7 and GPT-4o, outperform static baselines by autonomously adopting their\nstrategies, passing along sample behavior to game-playing agents, and\ndemonstrating adaptive reasoning over multiple iterations.", "categories": ["cs.AI"], "published": "2025-06-05 05:45:24", "updated": "2025-06-05 05:45:24", "pdf_url": "http://arxiv.org/pdf/2506.04651v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04654v1", "title": "E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction", "authors": ["Zhichao Yang", "Jiashu He", "Mohammad B. Al-Khasawneh", "Darshan Pandit", "Cirillo Cinzia"], "abstract": "Electric bicycles (e-bikes) are rapidly increasing in use, raising safety\nconcerns due to a rise in accident reports. However, e-bike incident reports\noften use unstructured narrative formats, which hinders quantitative safety\nanalysis. This study introduces E-bike agents, a framework that uses large\nlanguage models (LLM) powered agents to classify and extract safety variables\nfrom unstructured incident reports. Our framework consists of four LLM agents,\nhandling data classification, information extraction, injury cause\ndetermination, and component linkage, to extract the key factors that could\nlead to E-bike accidents and cause varying severity levels. Furthermore, we\nused an ordered logit model to examine the relationship between the severity of\nthe incident and the factors retrieved, such as gender, the type of cause, and\nenvironmental conditions. Our research shows that equipment issues are slightly\nmore common than human-related ones, but human-related incidents are more often\nfatal. Specifically, pedals, tires, and brakes are frequent contributors to\naccidents. The model achieves a high weighted F1 score of 0.87 in\nclassification accuracy, highlighting the potential of using LLMs to extract\nunstructured data in niche domains, such as transportation. Our method offers a\nscalable solution to improve e-bike safety analytics and provides actionable\ninformation for policy makers, designers, and regulators.", "categories": ["cs.AI"], "published": "2025-06-05 05:49:41", "updated": "2025-06-05 05:49:41", "pdf_url": "http://arxiv.org/pdf/2506.04654v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04668v1", "title": "Feature-Based Lie Group Transformer for Real-World Applications", "authors": ["Takayuki Komatsu", "Yoshiyuki Ohmura", "Kayato Nishitsunoi", "Yasuo Kuniyoshi"], "abstract": "The main goal of representation learning is to acquire meaningful\nrepresentations from real-world sensory inputs without supervision.\nRepresentation learning explains some aspects of human development. Various\nneural network (NN) models have been proposed that acquire empirically good\nrepresentations. However, the formulation of a good representation has not been\nestablished. We recently proposed a method for categorizing changes between a\npair of sensory inputs. A unique feature of this approach is that\ntransformations between two sensory inputs are learned to satisfy algebraic\nstructural constraints. Conventional representation learning often assumes that\ndisentangled independent feature axes is a good representation; however, we\nfound that such a representation cannot account for conditional independence.\nTo overcome this problem, we proposed a new method using group decomposition in\nGalois algebra theory. Although this method is promising for defining a more\ngeneral representation, it assumes pixel-to-pixel translation without feature\nextraction, and can only process low-resolution images with no background,\nwhich prevents real-world application. In this study, we provide a simple\nmethod to apply our group decomposition theory to a more realistic scenario by\ncombining feature extraction and object segmentation. We replace pixel\ntranslation with feature translation and formulate object segmentation as\ngrouping features under the same transformation. We validated the proposed\nmethod on a practical dataset containing both real-world object and background.\nWe believe that our model will lead to a better understanding of human\ndevelopment of object recognition in the real world.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-05 06:30:11", "updated": "2025-06-05 06:30:11", "pdf_url": "http://arxiv.org/pdf/2506.04668v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04676v1", "title": "Gen-n-Val: Agentic Image Data Generation and Validation", "authors": ["Jing-En Huang", "I-Sheng Fang", "Tzuhsuan Huang", "Chih-Yu Wang", "Jun-Cheng Chen"], "abstract": "Recently, Large Language Models (LLMs) and Vision Large Language Models\n(VLLMs) have demonstrated impressive performance as agents across various tasks\nwhile data scarcity and label noise remain significant challenges in computer\nvision tasks, such as object detection and instance segmentation. A common\nsolution for resolving these issues is to generate synthetic data. However,\ncurrent synthetic data generation methods struggle with issues, such as\nmultiple objects per mask, inaccurate segmentation, and incorrect category\nlabels, limiting their effectiveness. To address these issues, we introduce\nGen-n-Val, a novel agentic data generation framework that leverages Layer\nDiffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks\nand diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt\nagent, an LLM, optimizes prompts for LD to generate high-quality foreground\ninstance images and segmentation masks. These optimized prompts ensure the\ngeneration of single-object synthetic data with precise instance masks and\nclean backgrounds. (2) The data validation agent, a VLLM, which filters out\nlow-quality synthetic instance images. The system prompts for both agents are\nrefined through TextGrad. Additionally, we use image harmonization to combine\nmultiple instances within scenes. Compared to state-of-the-art synthetic data\napproaches like MosaicFusion, our approach reduces invalid synthetic data from\n50% to 7% and improves performance by 1% mAP on rare classes in COCO instance\nsegmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant\nimprovements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object\ndetection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance\nof YOLOv9 and YOLO11 families in instance segmentation and object detection.", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA"], "published": "2025-06-05 06:52:26", "updated": "2025-06-05 06:52:26", "pdf_url": "http://arxiv.org/pdf/2506.04676v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04681v1", "title": "Urania: Differentially Private Insights into AI Use", "authors": ["Daogao Liu", "Edith Cohen", "Badih Ghazi", "Peter Kairouz", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi", "Adam Sealfon", "Da Yu", "Chiyuan Zhang"], "abstract": "We introduce $Urania$, a novel framework for generating insights about LLM\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\nframework employs a private clustering mechanism and innovative keyword\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\napproaches. By leveraging DP tools such as clustering, partition selection, and\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair\nsimilarity, and LLM-based metrics, benchmarking against a non-private\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\nempirical privacy evaluation that demonstrates the enhanced robustness of our\nDP pipeline. The results show the framework's ability to extract meaningful\nconversational insights while maintaining stringent user privacy, effectively\nbalancing data utility with privacy preservation.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CY"], "published": "2025-06-05 07:00:31", "updated": "2025-06-05 07:00:31", "pdf_url": "http://arxiv.org/pdf/2506.04681v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04688v1", "title": "MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models", "authors": ["Gio Paik", "Geewook Kim", "Jinbae Im"], "abstract": "This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-06-05 07:11:36", "updated": "2025-06-05 07:11:36", "pdf_url": "http://arxiv.org/pdf/2506.04688v1", "comment": "ACL Findings 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04690v1", "title": "Towards Better Generalization via Distributional Input Projection Network", "authors": ["Yifan Hao", "Yanxin Lu", "Xinwei Shen", "Tong Zhang"], "abstract": "As overparameterized models become increasingly prevalent, training loss\nalone offers limited insight into generalization performance. While smoothness\nhas been linked to improved generalization across various settings, directly\nenforcing smoothness in neural networks remains challenging. To address this,\nwe introduce Distributional Input Projection Networks (DIPNet), a novel\nframework that projects inputs into learnable distributions at each layer. This\ndistributional representation induces a smoother loss landscape with respect to\nthe input, promoting better generalization. We provide theoretical analysis\nshowing that DIPNet reduces both local smoothness measures and the Lipschitz\nconstant of the network, contributing to improved generalization performance.\nEmpirically, we validate DIPNet across a wide range of architectures and tasks,\nincluding Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and\nMLPs. Our method consistently enhances test performance under standard\nsettings, adversarial attacks, out-of-distribution inputs, and reasoning\nbenchmarks. We demonstrate that the proposed input projection strategy can be\nseamlessly integrated into existing models, providing a general and effective\napproach for boosting generalization performance in modern deep learning.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-05 07:13:59", "updated": "2025-06-05 07:13:59", "pdf_url": "http://arxiv.org/pdf/2506.04690v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04694v1", "title": "Influence Functions for Edge Edits in Non-Convex Graph Neural Networks", "authors": ["Jaeseung Heo", "Kyeongheung Yun", "Seokwon Yoon", "MoonJeong Park", "Jungseul Ok", "Dongwoo Kim"], "abstract": "Understanding how individual edges influence the behavior of graph neural\nnetworks (GNNs) is essential for improving their interpretability and\nrobustness. Graph influence functions have emerged as promising tools to\nefficiently estimate the effects of edge deletions without retraining. However,\nexisting influence prediction methods rely on strict convexity assumptions,\nexclusively consider the influence of edge deletions while disregarding edge\ninsertions, and fail to capture changes in message propagation caused by these\nmodifications. In this work, we propose a proximal Bregman response function\nspecifically tailored for GNNs, relaxing the convexity requirement and enabling\naccurate influence prediction for standard neural network architectures.\nFurthermore, our method explicitly accounts for message propagation effects and\nextends influence prediction to both edge deletions and insertions in a\nprincipled way. Experiments with real-world datasets demonstrate accurate\ninfluence predictions for different characteristics of GNNs. We further\ndemonstrate that the influence function is versatile in applications such as\ngraph rewiring and adversarial attacks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-05 07:15:46", "updated": "2025-06-05 07:15:46", "pdf_url": "http://arxiv.org/pdf/2506.04694v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04695v1", "title": "On the Mechanism of Reasoning Pattern Selection in Reinforcement Learning for Language Models", "authors": ["Xingwu Chen", "Tianle Li", "Difan Zou"], "abstract": "Reinforcement learning (RL) has demonstrated remarkable success in enhancing\nmodel capabilities, including instruction-following, preference learning, and\nreasoning. Yet despite its empirical successes, the mechanisms by which RL\nimproves reasoning abilities remain poorly understood. We present a systematic\nstudy of Reinforcement Learning with Verifiable Rewards (RLVR), showing that\nits primary benefit comes from optimizing the selection of existing reasoning\npatterns. Through extensive experiments, we demonstrate that RLVR-trained\nmodels preferentially adopt high-success-rate reasoning patterns while mostly\nmaintaining stable performance on individual patterns. We further develop\ntheoretical analyses on the convergence and training dynamics of RLVR based on\na simplified question-reason-answer model. We study the gradient flow and show\nthat RLVR can indeed find the solution that selects the reason pattern with the\nhighest success rate. Besides, our theoretical results\n  reveal two distinct regimes regarding the convergence of RLVR training: (1)\nrapid convergence for models with relatively strong initial reasoning\ncapabilities versus (2) slower optimization dynamics for weaker models.\nFurthermore, we show that the slower optimization for weaker models can be\nmitigated by applying the supervised fine-tuning (SFT) before RLVR, when using\na feasibly high-quality SFT dataset. We validate the theoretical findings\nthrough extensive experiments. This work advances our theoretical understanding\nof RL's role in LLM fine-tuning and offers insights for further enhancing\nreasoning capabilities.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-06-05 07:17:04", "updated": "2025-06-05 07:17:04", "pdf_url": "http://arxiv.org/pdf/2506.04695v1", "comment": "30 pages, 6 figures, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04699v1", "title": "Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling", "authors": ["Bihan Xu", "Shiwei Zhao", "Runze Wu", "Zhenya Huang", "Jiawei Wang", "Zhipeng Hu", "Kai Wang", "Haoyu Liu", "Tangjie Lv", "Le Li", "Changjie Fan", "Xin Tong", "Jiangze Han"], "abstract": "Within the domain of Massively Multiplayer Online (MMO) economy research,\nAgent-Based Modeling (ABM) has emerged as a robust tool for analyzing game\neconomics, evolving from rule-based agents to decision-making agents enhanced\nby reinforcement learning. Nevertheless, existing works encounter significant\nchallenges when attempting to emulate human-like economic activities among\nagents, particularly regarding agent reliability, sociability, and\ninterpretability. In this study, we take a preliminary step in introducing a\nnovel approach using Large Language Models (LLMs) in MMO economy simulation.\nLeveraging LLMs' role-playing proficiency, generative capacity, and reasoning\naptitude, we design LLM-driven agents with human-like decision-making and\nadaptability. These agents are equipped with the abilities of role-playing,\nperception, memory, and reasoning, addressing the aforementioned challenges\neffectively. Simulation experiments focusing on in-game economic activities\ndemonstrate that LLM-empowered agents can promote emergent phenomena like role\nspecialization and price fluctuations in line with market rules.", "categories": ["cs.AI"], "published": "2025-06-05 07:21:13", "updated": "2025-06-05 07:21:13", "pdf_url": "http://arxiv.org/pdf/2506.04699v1", "comment": "KDD2025 Accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04700v1", "title": "Explicit Density Approximation for Neural Implicit Samplers Using a Bernstein-Based Convex Divergence", "authors": ["Jos\u00e9 Manuel de Frutos", "Manuel A. V\u00e1zquez", "Pablo M. Olmos", "Joaqu\u00edn M\u00edguez"], "abstract": "Rank-based statistical metrics, such as the invariant statistical loss (ISL),\nhave recently emerged as robust and practically effective tools for training\nimplicit generative models. In this work, we introduce dual-ISL, a novel\nlikelihood-free objective for training implicit generative models that\ninterchanges the roles of the target and model distributions in the ISL\nframework, yielding a convex optimization problem in the space of model\ndensities. We prove that the resulting rank-based discrepancy $d_K$ is i)\ncontinuous under weak convergence and with respect to the $L^1$ norm, and ii)\nconvex in its first argument-properties not shared by classical divergences\nsuch as KL or Wasserstein distances. Building on this, we develop a theoretical\nframework that interprets $d_K$ as an $L^2$-projection of the density ratio $q\n= p/\\tilde p$ onto a Bernstein polynomial basis, from which we derive exact\nbounds on the truncation error, precise convergence rates, and a closed-form\nexpression for the truncated density approximation. We further extend our\nanalysis to the multivariate setting via random one-dimensional projections,\ndefining a sliced dual-ISL divergence that retains both convexity and\ncontinuity. We empirically show that these theoretical advantages translate\ninto practical ones. Specifically, across several benchmarks dual-ISL converges\nmore rapidly, delivers markedly smoother and more stable training, and more\neffectively prevents mode collapse than classical ISL and other leading\nimplicit generative methods-while also providing an explicit density\napproximation.", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.ML"], "published": "2025-06-05 07:21:54", "updated": "2025-06-05 07:21:54", "pdf_url": "http://arxiv.org/pdf/2506.04700v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04704v1", "title": "HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model", "authors": ["Youngwan Lee", "Kangsan Kim", "Kwanyong Park", "Ilcahe Jung", "Soojin Jang", "Seanie Lee", "Yong-Ju Lee", "Sung Ju Hwang"], "abstract": "Despite emerging efforts to enhance the safety of Vision-Language Models\n(VLMs), current approaches face two main shortcomings. 1) Existing\nsafety-tuning datasets and benchmarks only partially consider how image-text\ninteractions can yield harmful content, often overlooking contextually unsafe\noutcomes from seemingly benign pairs. This narrow coverage leaves VLMs\nvulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely\nprimarily on data-centric tuning, with limited architectural innovations to\nintrinsically strengthen safety. We address these gaps by introducing a\nholistic safety dataset and benchmark, HoliSafe, that spans all five\nsafe/unsafe image-text combinations, providing a more robust basis for both\ntraining and evaluation. We further propose SafeLLaVA, a novel VLM augmented\nwith a learnable safety meta token and a dedicated safety head. The meta token\nencodes harmful visual cues during training, intrinsically guiding the language\nmodel toward safer responses, while the safety head offers interpretable\nharmfulness classification aligned with refusal rationales. Experiments show\nthat SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety\nperformance across multiple VLM benchmarks. Additionally, the HoliSafe\nbenchmark itself reveals critical vulnerabilities in existing models. We hope\nthat HoliSafe and SafeLLaVA will spur further research into robust and\ninterpretable VLM safety, expanding future avenues for multimodal alignment.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-05 07:26:34", "updated": "2025-06-05 07:26:34", "pdf_url": "http://arxiv.org/pdf/2506.04704v1", "comment": "Project page: https://youngwanlee.github.io/holisafe", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04706v1", "title": "Line of Sight: On Linear Representations in VLLMs", "authors": ["Achyuta Rajaram", "Sarah Schwettmann", "Jacob Andreas", "Arthur Conmy"], "abstract": "Language models can be equipped with multimodal capabilities by fine-tuning\non embeddings of visual inputs. But how do such multimodal models represent\nimages in their hidden activations? We explore representations of image\nconcepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set\nof ImageNet classes represented via linearly decodable features in the residual\nstream. We show that the features are causal by performing targeted edits on\nthe model output. In order to increase the diversity of the studied linear\nfeatures, we train multimodal Sparse Autoencoders (SAEs), creating a highly\ninterpretable dictionary of text and image features. We find that although\nmodel representations across modalities are quite disjoint, they become\nincreasingly shared in deeper layers.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-05 07:30:58", "updated": "2025-06-05 07:30:58", "pdf_url": "http://arxiv.org/pdf/2506.04706v1", "comment": "8 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04712v1", "title": "UNO: Unlearning via Orthogonalization in Generative models", "authors": ["Pinak Mandal", "Georg A. Gottwald"], "abstract": "As generative models become increasingly powerful and pervasive, the ability\nto unlearn specific data, whether due to privacy concerns, legal requirements,\nor the correction of harmful content, has become increasingly important. Unlike\nin conventional training, where data are accumulated and knowledge is\nreinforced, unlearning aims to selectively remove the influence of particular\ndata points without costly retraining from scratch. To be effective and\nreliable, such algorithms need to achieve (i) forgetting of the undesired data,\n(ii) preservation of the quality of the generation, (iii) preservation of the\ninfluence of the desired training data on the model parameters, and (iv) small\nnumber of training steps. We propose fast unlearning algorithms based on loss\ngradient orthogonalization. We show that our algorithms are able to forget data\nwhile maintaining the fidelity of the original model. Using MNIST and CelebA\ndata, we demonstrate that our algorithms achieve orders of magnitude faster\nunlearning times than their predecessors, such as gradient surgery.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-05 07:37:02", "updated": "2025-06-05 07:37:02", "pdf_url": "http://arxiv.org/pdf/2506.04712v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04717v1", "title": "Using In-Context Learning for Automatic Defect Labelling of Display Manufacturing Data", "authors": ["Babar Hussain", "Qiang Liu", "Gang Chen", "Bihai She", "Dahai Yu"], "abstract": "This paper presents an AI-assisted auto-labeling system for display panel\ndefect detection that leverages in-context learning capabilities. We adopt and\nenhance the SegGPT architecture with several domain-specific training\ntechniques and introduce a scribble-based annotation mechanism to streamline\nthe labeling process. Our two-stage training approach, validated on industrial\ndisplay panel datasets, demonstrates significant improvements over the baseline\nmodel, achieving an average IoU increase of 0.22 and a 14% improvement in\nrecall across multiple product types, while maintaining approximately 60%\nauto-labeling coverage. Experimental results show that models trained on our\nauto-labeled data match the performance of those trained on human-labeled data,\noffering a practical solution for reducing manual annotation efforts in\nindustrial inspection systems.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-06-05 07:42:31", "updated": "2025-06-05 07:42:31", "pdf_url": "http://arxiv.org/pdf/2506.04717v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04723v1", "title": "Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning", "authors": ["Jiayu Wang", "Yifei Ming", "Zixuan Ke", "Caiming Xiong", "Shafiq Joty", "Aws Albarghouthi", "Frederic Sala"], "abstract": "Reinforcement learning (RL) has become the dominant paradigm for endowing\nlanguage models with advanced reasoning capabilities. Despite the substantial\nempirical gains demonstrated by RL-based training methods like GRPO, a granular\nunderstanding of their advantages is still lacking. To address this gap, we\nintroduce a fine-grained analytic framework to dissect the impact of RL on\nreasoning. Our framework specifically investigates key elements that have been\nhypothesized to benefit from RL training: (1) plan-following and execution, (2)\nproblem decomposition, and (3) improved reasoning and knowledge utilization.\nUsing this framework, we gain insights beyond mere accuracy. For instance,\nproviding models with explicit step-by-step plans surprisingly degrades\nperformance on the most challenging benchmarks, yet RL-tuned models exhibit\ngreater robustness, experiencing markedly smaller performance drops than their\nbase counterparts. This suggests that RL may not primarily enhance the\nexecution of external plans but rather empower models to formulate and follow\ninternal strategies better suited to their reasoning processes. Conversely, we\nobserve that RL enhances the model's capacity to integrate provided knowledge\ninto its reasoning process, leading to performance improvements across diverse\ntasks. We also study difficulty, showing improved training by developing new\nways to exploit hard problems. Our findings lay a foundation for more\nprincipled training and evaluation of reasoning models.", "categories": ["cs.AI"], "published": "2025-06-05 07:53:59", "updated": "2025-06-05 07:53:59", "pdf_url": "http://arxiv.org/pdf/2506.04723v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04734v1", "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design", "authors": ["Lin Sun", "Weihong Lin", "Jinzhu Wu", "Yongfu Zhu", "Xiaoqi Jian", "Guangxiang Zhao", "Change Jia", "Linglin Zhang", "Sai-er Hu", "Yuhan Wu", "Xiangzheng Zhang"], "abstract": "Reasoning models represented by the Deepseek-R1-Distill series have been\nwidely adopted by the open-source community due to their strong performance in\nmathematics, science, programming, and other domains. However, our study\nreveals that their benchmark evaluation results are subject to significant\nfluctuations caused by various factors. Subtle differences in evaluation\nconditions can lead to substantial variations in results. Similar phenomena are\nobserved in other open-source inference models fine-tuned based on the\nDeepseek-R1-Distill series, as well as in the QwQ-32B model, making their\nclaimed performance improvements difficult to reproduce reliably. Therefore, we\nadvocate for the establishment of a more rigorous paradigm for model\nperformance evaluation and present our empirical assessments of the\nDeepseek-R1-Distill series models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-05 08:09:11", "updated": "2025-06-05 08:09:11", "pdf_url": "http://arxiv.org/pdf/2506.04734v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04739v1", "title": "Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection", "authors": ["Ziyi Zhou", "Xiaoming Zhang", "Litian Zhang", "Yibo Zhang", "Zhenyu Guan", "Chaozhuo Li", "Philip S. Yu"], "abstract": "The widespread dissemination of fake news on social media has significantly\nimpacted society, resulting in serious consequences. Conventional deep learning\nmethodologies employing small language models (SLMs) suffer from extensive\nsupervised training requirements and difficulties adapting to evolving news\nenvironments due to data scarcity and distribution shifts. Large language\nmodels (LLMs), despite robust zero-shot capabilities, fall short in accurately\ndetecting fake news owing to outdated knowledge and the absence of suitable\ndemonstrations. In this paper, we propose a novel Continuous Collaborative\nEmergent Fake News Detection (C$^2$EFND) framework to address these challenges.\nThe C$^2$EFND framework strategically leverages both LLMs' generalization power\nand SLMs' classification expertise via a multi-round collaborative learning\nframework. We further introduce a lifelong knowledge editing module based on a\nMixture-of-Experts architecture to incrementally update LLMs and a replay-based\ncontinue learning method to ensure SLMs retain prior knowledge without\nretraining entirely. Extensive experiments on Pheme and Twitter16 datasets\ndemonstrate that C$^2$EFND significantly outperforms existed methods,\neffectively improving detection accuracy and adaptability in continuous\nemergent fake news scenarios.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 08:17:55", "updated": "2025-06-05 08:17:55", "pdf_url": "http://arxiv.org/pdf/2506.04739v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04742v1", "title": "Was Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?", "authors": ["Oliver G. S. Lundqvist", "Fabricio Oliveira"], "abstract": "Neural networks have been used to solve optimal control problems, typically\nby training neural networks using a combined loss function that considers data,\ndifferential equation residuals, and objective costs. We show that including\ncost functions in the training process is unnecessary, advocating for a simpler\narchitecture and streamlined approach by decoupling the optimal control problem\nfrom the training process. Thus, our work shows that a simple neural operator\narchitecture, such as DeepONet, coupled with an unconstrained optimization\nroutine, can solve multiple optimal control problems with a single\nphysics-informed training phase and a subsequent optimization phase. We achieve\nthis by adding a penalty term based on the differential equation residual to\nthe cost function and computing gradients with respect to the control using\nautomatic differentiation through the trained neural operator within an\niterative optimization routine. We showcase our method on nine distinct optimal\ncontrol problems by training three separate DeepONet models, each corresponding\nto a different differential equation. For each model, we solve three problems\nwith varying cost functions, demonstrating accurate and consistent performance\nacross all cases.", "categories": ["math.OC", "cs.AI"], "published": "2025-06-05 08:22:16", "updated": "2025-06-05 08:22:16", "pdf_url": "http://arxiv.org/pdf/2506.04742v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04753v1", "title": "Physics Informed Capsule Enhanced Variational AutoEncoder for Underwater Image Enhancement", "authors": ["Niki Martinel", "Rita Pucci"], "abstract": "We present a novel dual-stream architecture that achieves state-of-the-art\nunderwater image enhancement by explicitly integrating the Jaffe-McGlamery\nphysical model with capsule clustering-based feature representation learning.\nOur method simultaneously estimates transmission maps and spatially-varying\nbackground light through a dedicated physics estimator while extracting\nentity-level features via capsule clustering in a parallel stream. This\nphysics-guided approach enables parameter-free enhancement that respects\nunderwater formation constraints while preserving semantic structures and\nfine-grained details. Our approach also features a novel optimization objective\nensuring both physical adherence and perceptual quality across multiple spatial\nfrequencies. To validate our approach, we conducted extensive experiments\nacross six challenging benchmarks. Results demonstrate consistent improvements\nof $+0.5$dB PSNR over the best existing methods while requiring only one-third\nof their computational complexity (FLOPs), or alternatively, more than $+1$dB\nPSNR improvement when compared to methods with similar computational budgets.\nCode and data \\textit{will} be available at https://github.com/iN1k1/.", "categories": ["cs.CV", "cs.AI", "eess.IV"], "published": "2025-06-05 08:39:17", "updated": "2025-06-05 08:39:17", "pdf_url": "http://arxiv.org/pdf/2506.04753v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04755v1", "title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning", "authors": ["Shenshen Li", "Kaiyuan Deng", "Lei Wang", "Hao Yang", "Chong Peng", "Peng Yan", "Fumin Shen", "Heng Tao Shen", "Xing Xu"], "abstract": "While multi-modal large language models (MLLMs) have made significant\nprogress in complex reasoning tasks via reinforcement learning, it is commonly\nbelieved that extensive training data is necessary for improving multi-modal\nreasoning ability, inevitably leading to data redundancy and substantial\ncomputational costs. However, can smaller high-value datasets match or\noutperform full corpora for multi-modal reasoning in MLLMs? In this work, we\nchallenge this assumption through a key observation: meaningful multi-modal\nreasoning is triggered by only a sparse subset of training samples, termed\ncognitive samples, whereas the majority contribute marginally. Building on this\ninsight, we propose a novel data selection paradigm termed Reasoning Activation\nPotential (RAP), which identifies cognitive samples by estimating each sample's\npotential to stimulate genuine multi-modal reasoning by two complementary\nestimators: 1) Causal Discrepancy Estimator (CDE) based on the potential\noutcome model principle, eliminates samples that overly rely on language priors\nby comparing outputs between multi-modal and text-only inputs; 2) Attention\nConfidence Estimator (ACE), which exploits token-level self-attention to\ndiscard samples dominated by irrelevant but over-emphasized tokens in\nintermediate reasoning stages. Moreover, we introduce a Difficulty-aware\nReplacement Module (DRM) to substitute trivial instances with cognitively\nchallenging ones, thereby ensuring complexity for robust multi-modal reasoning.\nExperiments on six datasets show that our RAP method consistently achieves\nsuperior performance using only 9.3% of the training data, while reducing\ncomputational costs by over 43%. Our code is available at\nhttps://github.com/Leo-ssl/RAP.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "published": "2025-06-05 08:40:24", "updated": "2025-06-05 08:40:24", "pdf_url": "http://arxiv.org/pdf/2506.04755v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04756v1", "title": "Ontology-based knowledge representation for bone disease diagnosis: a foundation for safe and sustainable medical artificial intelligence systems", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "abstract": "Medical artificial intelligence (AI) systems frequently lack systematic\ndomain expertise integration, potentially compromising diagnostic reliability.\nThis study presents an ontology-based framework for bone disease diagnosis,\ndeveloped in collaboration with Ho Chi Minh City Hospital for Traumatology and\nOrthopedics. The framework introduces three theoretical contributions: (1) a\nhierarchical neural network architecture guided by bone disease ontology for\nsegmentation-classification tasks, incorporating Visual Language Models (VLMs)\nthrough prompts, (2) an ontology-enhanced Visual Question Answering (VQA)\nsystem for clinical reasoning, and (3) a multimodal deep learning model that\nintegrates imaging, clinical, and laboratory data through ontological\nrelationships. The methodology maintains clinical interpretability through\nsystematic knowledge digitization, standardized medical terminology mapping,\nand modular architecture design. The framework demonstrates potential for\nextension beyond bone diseases through its standardized structure and reusable\ncomponents. While theoretical foundations are established, experimental\nvalidation remains pending due to current dataset and computational resource\nlimitations. Future work will focus on expanding the clinical dataset and\nconducting comprehensive system validation.", "categories": ["cs.AI", "cs.CV", "eess.IV"], "published": "2025-06-05 08:41:23", "updated": "2025-06-05 08:41:23", "pdf_url": "http://arxiv.org/pdf/2506.04756v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04774v1", "title": "Fine-Grained Interpretation of Political Opinions in Large Language Models", "authors": ["Jingyu Hu", "Mengyue Yang", "Mengnan Du", "Weiru Liu"], "abstract": "Studies of LLMs' political opinions mainly rely on evaluations of their\nopen-ended responses. Recent work indicates that there is a misalignment\nbetween LLMs' responses and their internal intentions. This motivates us to\nprobe LLMs' internal mechanisms and help uncover their internal political\nstates. Additionally, we found that the analysis of LLMs' political opinions\noften relies on single-axis concepts, which can lead to concept confounds. In\nthis work, we extend the single-axis to multi-dimensions and apply\ninterpretable representation engineering techniques for more transparent LLM\npolitical concept learning. Specifically, we designed a four-dimensional\npolitical learning framework and constructed a corresponding dataset for\nfine-grained political concept vector learning. These vectors can be used to\ndetect and intervene in LLM internals. Experiments are conducted on eight\nopen-source LLMs with three representation engineering techniques. Results show\nthese vectors can disentangle political concept confounds. Detection tasks\nvalidate the semantic meaning of the vectors and show good generalization and\nrobustness in OOD settings. Intervention Experiments show these vectors can\nintervene in LLMs to generate responses with different political leanings.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 09:06:59", "updated": "2025-06-05 09:06:59", "pdf_url": "http://arxiv.org/pdf/2506.04774v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04788v1", "title": "Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques", "authors": ["Jisu An", "Junseok Lee", "Jeoungeun Lee", "Yongseok Son"], "abstract": "The rapid progress of Multimodal Large Language Models(MLLMs) has transformed\nthe AI landscape. These models combine pre-trained LLMs with various modality\nencoders. This integration requires a systematic understanding of how different\nmodalities connect to the language backbone. Our survey presents an LLM-centric\nanalysis of current approaches. We examine methods for transforming and\naligning diverse modal inputs into the language embedding space. This addresses\na significant gap in existing literature. We propose a classification framework\nfor MLLMs based on three key dimensions. First, we examine architectural\nstrategies for modality integration. This includes both the specific\nintegration mechanisms and the fusion level. Second, we categorize\nrepresentation learning techniques as either joint or coordinate\nrepresentations. Third, we analyze training paradigms, including training\nstrategies and objective functions. By examining 125 MLLMs developed between\n2021 and 2025, we identify emerging patterns in the field. Our taxonomy\nprovides researchers with a structured overview of current integration\ntechniques. These insights aim to guide the development of more robust\nmultimodal integration strategies for future models built on pre-trained\nfoundations.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 09:14:41", "updated": "2025-06-05 09:14:41", "pdf_url": "http://arxiv.org/pdf/2506.04788v1", "comment": "18 pages, 3 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04810v1", "title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study", "authors": ["Yujun Zhou", "Jiayi Ye", "Zipeng Ling", "Yufei Han", "Yue Huang", "Haomin Zhuang", "Zhenwen Liang", "Kehan Guo", "Taicheng Guo", "Xiangqi Wang", "Xiangliang Zhang"], "abstract": "Logical reasoning is a core capability for many applications of large\nlanguage models (LLMs), yet existing benchmarks often rely solely on\nfinal-answer accuracy, failing to capture the quality and structure of the\nreasoning process. We propose FineLogic, a fine-grained evaluation framework\nthat assesses logical reasoning across three dimensions: overall benchmark\naccuracy, stepwise soundness, and representation-level alignment. In addition,\nto better understand how reasoning capabilities emerge, we conduct a\ncomprehensive study on the effects of supervision format during fine-tuning. We\nconstruct four supervision styles (one natural language and three symbolic\nvariants) and train LLMs under each. Our findings reveal that natural language\nsupervision yields strong generalization even on out-of-distribution and\nlong-context tasks, while symbolic reasoning styles promote more structurally\nsound and atomic inference chains. Further, our representation-level probing\nshows that fine-tuning primarily improves reasoning behaviors through\nstep-by-step generation, rather than enhancing shortcut prediction or\ninternalized correctness. Together, our framework and analysis provide a more\nrigorous and interpretable lens for evaluating and improving logical reasoning\nin LLMs.", "categories": ["cs.CL", "cs.AI", "cs.LO"], "published": "2025-06-05 09:34:12", "updated": "2025-06-05 09:34:12", "pdf_url": "http://arxiv.org/pdf/2506.04810v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04824v1", "title": "A Reasoning-Based Approach to Cryptic Crossword Clue Solving", "authors": ["Martin Andrews", "Sam Witteveen"], "abstract": "Cryptic crossword clues are challenging language tasks for which new test\nsets are released daily by major newspapers on a global basis. Each cryptic\nclue contains both the definition of the answer to be placed in the crossword\ngrid (in common with regular crosswords), and 'wordplay' that proves that the\nanswer is correct (i.e. a human solver can be confident that an answer is\ncorrect without needing crossing words as confirmation). This work describes an\nLLM-based reasoning system built from open-licensed components that solves\ncryptic clues by (i) hypothesising answers; (ii) proposing wordplay\nexplanations; and (iii) using a verifier system that operates on codified\nreasoning steps. Overall, this system establishes a new state-of-the-art\nperformance on the challenging Cryptonite dataset of clues from The Times and\nThe Telegraph newspapers in the UK. Because each proved solution is expressed\nin Python, interpretable wordplay reasoning for proven answers is available for\ninspection.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 09:43:28", "updated": "2025-06-05 09:43:28", "pdf_url": "http://arxiv.org/pdf/2506.04824v1", "comment": "9 page paper plus Appendices. Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04828v1", "title": "Safe Planning and Policy Optimization via World Model Learning", "authors": ["Artem Latyshev", "Gregory Gorbov", "Aleksandr I. Panov"], "abstract": "Reinforcement Learning (RL) applications in real-world scenarios must\nprioritize safety and reliability, which impose strict constraints on agent\nbehavior. Model-based RL leverages predictive world models for action planning\nand policy optimization, but inherent model inaccuracies can lead to\ncatastrophic failures in safety-critical settings. We propose a novel\nmodel-based RL framework that jointly optimizes task performance and safety. To\naddress world model errors, our method incorporates an adaptive mechanism that\ndynamically switches between model-based planning and direct policy execution.\nWe resolve the objective mismatch problem of traditional model-based approaches\nusing an implicit world model. Furthermore, our framework employs dynamic\nsafety thresholds that adapt to the agent's evolving capabilities, consistently\nselecting actions that surpass safe policy suggestions in both performance and\nsafety. Experiments demonstrate significant improvements over non-adaptive\nmethods, showing that our approach optimizes safety and performance\nsimultaneously rather than merely meeting minimum safety requirements. The\nproposed framework achieves robust performance on diverse safety-critical\ncontinuous control tasks, outperforming existing methods.", "categories": ["cs.AI"], "published": "2025-06-05 09:50:02", "updated": "2025-06-05 09:50:02", "pdf_url": "http://arxiv.org/pdf/2506.04828v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04836v1", "title": "Oversight Structures for Agentic AI in Public-Sector Organizations", "authors": ["Chris Schmitz", "Jonathan Rystr\u00f8m", "Jan Batzner"], "abstract": "This paper finds that the introduction of agentic AI systems intensifies\nexisting challenges to traditional public sector oversight mechanisms -- which\nrely on siloed compliance units and episodic approvals rather than continuous,\nintegrated supervision. We identify five governance dimensions essential for\nresponsible agent deployment: cross-departmental implementation, comprehensive\nevaluation, enhanced security protocols, operational visibility, and systematic\nauditing. We evaluate the capacity of existing oversight structures to meet\nthese challenges, via a mixed-methods approach consisting of a literature\nreview and interviews with civil servants in AI-related roles. We find that\nagent oversight poses intensified versions of three existing governance\nchallenges: continuous oversight, deeper integration of governance and\noperational capabilities, and interdepartmental coordination. We propose\napproaches that both adapt institutional structures and design agent oversight\ncompatible with public sector constraints.", "categories": ["cs.CY", "cs.AI"], "published": "2025-06-05 09:57:15", "updated": "2025-06-05 09:57:15", "pdf_url": "http://arxiv.org/pdf/2506.04836v1", "comment": "To appear at REALM@ACL2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04838v1", "title": "On Automating Security Policies with Contemporary LLMs", "authors": ["Pablo Fern\u00e1ndez Saura", "K. R. Jayaram", "Vatche Isahagian", "Jorge Bernal Bernab\u00e9", "Antonio Skarmeta"], "abstract": "The complexity of modern computing environments and the growing\nsophistication of cyber threats necessitate a more robust, adaptive, and\nautomated approach to security enforcement. In this paper, we present a\nframework leveraging large language models (LLMs) for automating attack\nmitigation policy compliance through an innovative combination of in-context\nlearning and retrieval-augmented generation (RAG). We begin by describing how\nour system collects and manages both tool and API specifications, storing them\nin a vector database to enable efficient retrieval of relevant information. We\nthen detail the architectural pipeline that first decomposes high-level\nmitigation policies into discrete tasks and subsequently translates each task\ninto a set of actionable API calls. Our empirical evaluation, conducted using\npublicly available CTI policies in STIXv2 format and Windows API documentation,\ndemonstrates significant improvements in precision, recall, and F1-score when\nemploying RAG compared to a non-RAG baseline.", "categories": ["cs.CR", "cs.AI"], "published": "2025-06-05 09:58:00", "updated": "2025-06-05 09:58:00", "pdf_url": "http://arxiv.org/pdf/2506.04838v1", "comment": "Short Paper. Accepted To Appear in IEEE SSE 2025 (part of SERVICES\n  2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04849v1", "title": "Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles", "authors": ["Julien Soul\u00e9", "Jean-Paul Jamont", "Michel Occello", "Paul Th\u00e9ron", "Louis-Marie Traonouez"], "abstract": "As cyber-attacks show to be more and more complex and coordinated,\ncyber-defenders strategy through multi-agent approaches could be key to tackle\nagainst cyber-attacks as close as entry points in a networked system. This\npaper presents a Markovian modeling and implementation through a simulator of\nfighting cyber-attacker agents and cyber-defender agents deployed on host\nnetwork nodes. It aims to provide an experimental framework to implement\nrealistically based coordinated cyber-attack scenarios while assessing\ncyber-defenders dynamic organizations. We abstracted network nodes by sets of\nproperties including agents' ones. Actions applied by agents model how the\nnetwork reacts depending in a given state and what properties are to change.\nCollective choice of the actions brings the whole environment closer or farther\nfrom respective cyber-attackers and cyber-defenders goals. Using the simulator,\nwe implemented a realistically inspired scenario with several behavior\nimplementation approaches for cyber-defenders and cyber-attackers.", "categories": ["cs.AI"], "published": "2025-06-05 10:17:17", "updated": "2025-06-05 10:17:17", "pdf_url": "http://arxiv.org/pdf/2506.04849v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04851v1", "title": "Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights", "authors": ["Giorgio Biancini", "Alessio Ferrato", "Carla Limongelli"], "abstract": "Integrating Artificial Intelligence (AI) in educational settings has brought\nnew learning approaches, transforming the practices of both students and\neducators. Among the various technologies driving this transformation, Large\nLanguage Models (LLMs) have emerged as powerful tools for creating educational\nmaterials and question answering, but there are still space for new\napplications. Educators commonly use Multiple-Choice Questions (MCQs) to assess\nstudent knowledge, but manually generating these questions is\nresource-intensive and requires significant time and cognitive effort. In our\nopinion, LLMs offer a promising solution to these challenges. This paper\npresents a novel comparative analysis of three widely known LLMs - Llama 2,\nMistral, and GPT-3.5 - to explore their potential for creating informative and\nchallenging MCQs. In our approach, we do not rely on the knowledge of the LLM,\nbut we inject the knowledge into the prompt to contrast the hallucinations,\ngiving the educators control over the test's source text, too. Our experiment\ninvolving 21 educators shows that GPT-3.5 generates the most effective MCQs\nacross several known metrics. Additionally, it shows that there is still some\nreluctance to adopt AI in the educational field. This study sheds light on the\npotential of LLMs to generate MCQs and improve the educational experience,\nproviding valuable insights for the future.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 10:21:49", "updated": "2025-06-05 10:21:49", "pdf_url": "http://arxiv.org/pdf/2506.04851v1", "comment": "Copyright ACM 2024. This is the author's version of the work. It is\n  posted here for your personal use. Not for redistribution. The definitive\n  Version of Record was published in Adjunct Proceedings of the 32nd ACM\n  Conference on User Modeling, Adaptation and Personalization (UMAP Adjunct\n  '24), http://dx.doi.org/10.1145/3631700.3665233", "doi": "10.1145/3631700.3665233", "journal_ref": null}
{"arxiv_id": "2506.04859v1", "title": "Sparse Autoencoders, Again?", "authors": ["Yin Lu", "Tong He", "Xuening Zhu", "David Wipf"], "abstract": "Is there really much more to say about sparse autoencoders (SAEs)?\nAutoencoders in general, and SAEs in particular, represent deep architectures\nthat are capable of modeling low-dimensional latent structure in data. Such\nstructure could reflect, among other things, correlation patterns in large\nlanguage model activations, or complex natural image manifolds. And yet despite\nthe wide-ranging applicability, there have been relatively few changes to SAEs\nbeyond the original recipe from decades ago, namely, standard deep\nencoder/decoder layers trained with a classical/deterministic sparse\nregularizer applied within the latent space. One possible exception is the\nvariational autoencoder (VAE), which adopts a stochastic encoder module capable\nof producing sparse representations when applied to manifold data. In this work\nwe formalize underappreciated weaknesses with both canonical SAEs, as well as\nanalogous VAEs applied to similar tasks, and propose a hybrid alternative model\nthat circumvents these prior limitations. In terms of theoretical support, we\nprove that global minima of our proposed model recover certain forms of\nstructured data spread across a union of manifolds. Meanwhile, empirical\nevaluations on synthetic and real-world datasets substantiate the efficacy of\nour approach in accurately estimating underlying manifold dimensions and\nproducing sparser latent representations without compromising reconstruction\nerror. In general, we are able to exceed the performance of equivalent-capacity\nSAEs and VAEs, as well as recent diffusion models where applicable, within\ndomains such as images and language model activation patterns.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-05 10:26:06", "updated": "2025-06-05 10:26:06", "pdf_url": "http://arxiv.org/pdf/2506.04859v1", "comment": "Accepted to the International Conference on Machine Learning (ICML)\n  2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04860v1", "title": "Towards Network Data Analytics in 5G Systems and Beyond", "authors": ["Marcos Lima Romero", "Ricardo Suyama"], "abstract": "Data has become a critical asset in the digital economy, yet it remains\nunderutilized by Mobile Network Operators (MNOs), unlike Over-the-Top (OTT)\nplayers that lead global market valuations. To move beyond the commoditization\nof connectivity and deliver greater value to customers, data analytics emerges\nas a strategic enabler. Using data efficiently is essential for unlocking new\nservice opportunities, optimizing operational efficiency, and mitigating\noperational and business risks. Since Release 15, the 3rd Generation\nPartnership Project (3GPP) has introduced the Network Data Analytics Function\n(NWDAF) to provide powerful insights and predictions using data collected\nacross mobile networks, supporting both user-centric and network-oriented use\ncases. However, academic research has largely focused on a limited set of\nmethods and use cases, driven by the availability of datasets, restricting\nbroader exploration. This study analyzes trends and gaps in more than 70\narticles and proposes two novel use cases to promote the adoption of NWDAF and\nexplore its potential for monetization.", "categories": ["cs.NI", "cs.AI"], "published": "2025-06-05 10:26:53", "updated": "2025-06-05 10:26:53", "pdf_url": "http://arxiv.org/pdf/2506.04860v1", "comment": "Submitted to XLIII BRAZILIAN SYMPOSIUM ON TELECOMMUNICATIONS AND\n  SIGNAL PROCESSING - SBrT 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04867v1", "title": "LLMs for sensory-motor control: Combining in-context and iterative learning", "authors": ["J\u00f4nata Tyska Carvalho", "Stefano Nolfi"], "abstract": "We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. Initially, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. In most cases, it\nsuccessfully identifies optimal or high-performing solutions by integrating\nsymbolic knowledge derived through reasoning with sub-symbolic sensory-motor\ndata gathered as the agent interacts with its environment.", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.RO"], "published": "2025-06-05 10:38:28", "updated": "2025-06-05 10:38:28", "pdf_url": "http://arxiv.org/pdf/2506.04867v1", "comment": "24 pages (13 pages are from appendix), 6 figures, code for\n  experiments replication and supplementary material provided at\n  https://github.com/jtyska/llm-robotics-article/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04907v1", "title": "Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots", "authors": ["Alex Pan", "Mary-Anne Williams"], "abstract": "Large Language Models (LLMs), whilst great at extracting facts from text,\nstruggle with nested narrative reasoning. Existing long context and multi-hop\nQA benchmarks inadequately test this, lacking realistic distractors or failing\nto decouple context length from reasoning complexity, masking a fundamental LLM\nlimitation. We introduce Verbose ListOps, a novel benchmark that\nprogrammatically transposes ListOps computations into lengthy, coherent\nstories. This uniquely forces internal computation and state management of\nnested reasoning problems by withholding intermediate results, and offers\nfine-grained controls for both narrative size \\emph{and} reasoning difficulty.\nWhilst benchmarks like LongReason (2025) advance approaches for synthetically\nexpanding the context size of multi-hop QA problems, Verbose ListOps pinpoints\na specific LLM vulnerability: difficulty in state management for nested\nsub-reasoning amongst semantically-relevant, distracting narrative. Our\nexperiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse\nin performance on Verbose ListOps at modest (~10k token) narrative lengths,\ndespite effortlessly solving raw ListOps equations. Addressing this failure is\nparamount for real-world text interpretation which requires identifying key\nreasoning points, tracking conceptual intermediate results, and filtering\nirrelevant information. Verbose ListOps, and its extensible generation\nframework thus enables targeted reasoning enhancements beyond mere\ncontext-window expansion; a critical step to automating the world's knowledge\nwork.", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "published": "2025-06-05 11:41:05", "updated": "2025-06-05 11:41:05", "pdf_url": "http://arxiv.org/pdf/2506.04907v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04909v1", "title": "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models", "authors": ["Kai Wang", "Yihao Zhang", "Meng Sun"], "abstract": "The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "published": "2025-06-05 11:44:19", "updated": "2025-06-05 11:44:19", "pdf_url": "http://arxiv.org/pdf/2506.04909v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04912v1", "title": "Differentiable Logic Cellular Automata: From Game of Life to Pattern Generation", "authors": ["Pietro Miotti", "Eyvind Niklasson", "Ettore Randazzo", "Alexander Mordvintsev"], "abstract": "This paper introduces Differentiable Logic Cellular Automata (DiffLogic CA),\na novel combination of Neural Cellular Automata (NCA) and Differentiable Logic\nGates Networks (DLGNs). The fundamental computation units of the model are\ndifferentiable logic gates, combined into a circuit. During training, the model\nis fully end-to-end differentiable allowing gradient-based training, and at\ninference time it operates in a fully discrete state space. This enables\nlearning local update rules for cellular automata while preserving their\ninherent discrete nature. We demonstrate the versatility of our approach\nthrough a series of milestones: (1) fully learning the rules of Conway's Game\nof Life, (2) generating checkerboard patterns that exhibit resilience to noise\nand damage, (3) growing a lizard shape, and (4) multi-color pattern generation.\nOur model successfully learns recurrent circuits capable of generating desired\ntarget patterns. For simpler patterns, we observe success with both synchronous\nand asynchronous updates, demonstrating significant generalization capabilities\nand robustness to perturbations. We make the case that this combination of\nDLGNs and NCA represents a step toward programmable matter and robust computing\nsystems that combine binary logic, neural network adaptability, and localized\nprocessing. This work, to the best of our knowledge, is the first successful\napplication of differentiable logic gate networks in recurrent architectures.", "categories": ["cs.AI"], "published": "2025-06-05 11:45:43", "updated": "2025-06-05 11:45:43", "pdf_url": "http://arxiv.org/pdf/2506.04912v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04916v1", "title": "Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life", "authors": ["Atahan Karagoz"], "abstract": "This paper introduces Energentic Intelligence, a class of autonomous systems\ndefined not by task performance, but by their capacity to sustain themselves\nthrough internal energy regulation. Departing from conventional reward-driven\nparadigms, these agents treat survival-maintaining functional operation under\nfluctuating energetic and thermal conditions-as the central objective. We\nformalize this principle through an energy-based utility function and a\nviability-constrained survival horizon, and propose a modular architecture that\nintegrates energy harvesting, thermal regulation, and adaptive computation into\na closed-loop control system. A simulated environment demonstrates the\nemergence of stable, resource-aware behavior without external supervision.\nTogether, these contributions provide a theoretical and architectural\nfoundation for deploying autonomous agents in resource-volatile settings where\npersistence must be self-regulated and infrastructure cannot be assumed.", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "published": "2025-06-05 11:52:21", "updated": "2025-06-05 11:52:21", "pdf_url": "http://arxiv.org/pdf/2506.04916v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04920v1", "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback", "authors": ["Junior Cedric Tonga", "KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Fajri Koto", "Ekaterina Kochmar"], "abstract": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 11:53:04", "updated": "2025-06-05 11:53:04", "pdf_url": "http://arxiv.org/pdf/2506.04920v1", "comment": "Preprint, in submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04931v1", "title": "CzechLynx: A Dataset for Individual Identification and Pose Estimation of the Eurasian Lynx", "authors": ["Lukas Picek", "Elisa Belotti", "Michal Bojda", "Ludek Bufka", "Vojtech Cermak", "Martin Dula", "Rostislav Dvorak", "Luboslav Hrdy", "Miroslav Jirik", "Vaclav Kocourek", "Josefa Krausova", "Jir\u0131 Labuda", "Jakub Straka", "Ludek Toman", "Vlado Trul\u0131k", "Martin Vana", "Miroslav Kutal"], "abstract": "We introduce CzechLynx, the first large-scale, open-access dataset for\nindividual identification, 2D pose estimation, and instance segmentation of the\nEurasian lynx (Lynx lynx). CzechLynx includes more than 30k camera trap images\nannotated with segmentation masks, identity labels, and 20-point skeletons and\ncovers 219 unique individuals across 15 years of systematic monitoring in two\ngeographically distinct regions: Southwest Bohemia and the Western Carpathians.\nTo increase the data variability, we create a complementary synthetic set with\nmore than 100k photorealistic images generated via a Unity-based pipeline and\ndiffusion-driven text-to-texture modeling, covering diverse environments,\nposes, and coat-pattern variations. To allow testing generalization across\nspatial and temporal domains, we define three tailored evaluation\nprotocols/splits: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware\nclosed-set. This dataset is targeted to be instrumental in benchmarking\nstate-of-the-art models and the development of novel methods for not just\nindividual animal re-identification.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-05 12:05:43", "updated": "2025-06-05 12:05:43", "pdf_url": "http://arxiv.org/pdf/2506.04931v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04951v1", "title": "Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations", "authors": ["Igor Meleshin", "Anna Chistyakova", "Anastasia Antsiferova", "Dmitriy Vatolin"], "abstract": "Image Quality Assessment (IQA) models are increasingly relied upon to\nevaluate image quality in real-world systems -- from compression and\nenhancement to generation and streaming. Yet their adoption brings a\nfundamental risk: these models are inherently unstable. Adversarial\nmanipulations can easily fool them, inflating scores and undermining trust.\nTraditionally, such vulnerabilities are addressed through data-driven defenses\n-- adversarial retraining, regularization, or input purification. But what if\nthis is the wrong lens? What if robustness in perceptual models is not\nsomething to learn but something to design? In this work, we propose a\nprovocative idea: robustness as an architectural prior. Rather than training\nmodels to resist perturbations, we reshape their internal structure to suppress\nsensitivity from the ground up. We achieve this by enforcing orthogonal\ninformation flow, constraining the network to norm-preserving operations -- and\nfurther stabilizing the system through pruning and fine-tuning. The result is a\nrobust IQA architecture that withstands adversarial attacks without requiring\nadversarial training or significant changes to the original model. This\napproach suggests a shift in perspective: from optimizing robustness through\ndata to engineering it through design.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-05 12:24:38", "updated": "2025-06-05 12:24:38", "pdf_url": "http://arxiv.org/pdf/2506.04951v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04965v1", "title": "From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation", "authors": ["Adrian Marius Dumitran", "Theodor-Pierre Moroianu", "Vasile Paul Alexe"], "abstract": "This paper presents a comprehensive evaluation of the performance of\nstate-of-the-art Large Language Models (LLMs) on challenging university-level\nalgorithms exams. By testing multiple models on both a Romanian exam and its\nhigh-quality English translation, we analyze LLMs' problem-solving\ncapabilities, consistency, and multilingual performance. Our empirical study\nreveals that the most recent models not only achieve scores comparable to\ntop-performing students but also demonstrate robust reasoning skills on\ncomplex, multi-step algorithmic challenges, even though difficulties remain\nwith graph-based tasks. Building on these findings, we explore the potential of\nLLMs to support educational environments through the generation of high-quality\neditorial content, offering instructors a powerful tool to enhance student\nfeedback. The insights and best practices discussed herein pave the way for\nfurther integration of generative AI in advanced algorithm education.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 12:41:20", "updated": "2025-06-05 12:41:20", "pdf_url": "http://arxiv.org/pdf/2506.04965v1", "comment": "15 pages Pre-print Paper accepted to ITS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04987v1", "title": "A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair", "authors": ["Zanis Ali Khan", "Aayush Garg", "Qiang Tang"], "abstract": "Software vulnerabilities pose significant security threats, requiring\neffective mitigation. While Automated Program Repair (APR) has advanced in\nfixing general bugs, vulnerability patching, a security-critical aspect of APR\nremains underexplored. This study investigates pre-trained language models,\nCodeBERT and CodeT5, for automated vulnerability patching across six datasets\nand four languages. We evaluate their accuracy and generalization to unknown\nvulnerabilities. Results show that while both models face challenges with\nfragmented or sparse context, CodeBERT performs comparatively better in such\nscenarios, whereas CodeT5 excels in capturing complex vulnerability patterns.\nCodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned\nmodels on both in-distribution (trained) and out-of-distribution (unseen)\ndatasets. While fine-tuning improves in-distribution performance, models\nstruggle to generalize to unseen data, highlighting challenges in robust\nvulnerability detection. This study benchmarks model performance, identifies\nlimitations in generalization, and provides actionable insights to advance\nautomated vulnerability patching for real-world security applications.", "categories": ["cs.SE", "cs.AI"], "published": "2025-06-05 13:00:19", "updated": "2025-06-05 13:00:19", "pdf_url": "http://arxiv.org/pdf/2506.04987v1", "comment": "Preprint has been accepted in ARES AI&CCPS (International Workshop on\n  Artificial Intelligence, Cyber and Cyber-Physical Security)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04998v1", "title": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning", "authors": ["Mehdi Azarafza", "Mojtaba Nayyeri", "Faezeh Pasandideh", "Steffen Staab", "Achim Rettberg"], "abstract": "Autonomous UAV operation necessitates reliable mathematical reasoning for\ntasks such as trajectory planning and power management. While traditional\nflight control relies on hardcoded equations, recent Large Language Models\n(LLMs) offer potential for more flexible problem-solving but struggle with\nreliably selecting and applying correct mathematical formulations and executing\nprecise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented\ngeneration framework designed to improve the mathematical reasoning of several\nLLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in\nUAV-specific contexts by providing access to relevant domain literature. To\nconduct an initial assessment, we introduce the UAV-Math-Bench, a small problem\nset comprising 20 UAV-centric mathematical problems across four difficulty\nlevels. Our experiments demonstrate that incorporating retrieval substantially\nincreases exact answer accuracy (achieving up to 75% with o1), reduces\ninstances of incorrect formulation selection (from 25% without RAG to 5% with\nRAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders\nof magnitude for the best-performing models. This pilot study indicates that\nRAG can enable general-purpose LLMs to function as more reliable tools for\nengineering analysis, although direct real-time flight control requires further\ninvestigation and validation on a larger scale. All benchmark data, question\nand answer are publicly available.", "categories": ["cs.AI"], "published": "2025-06-05 13:09:24", "updated": "2025-06-05 13:09:24", "pdf_url": "http://arxiv.org/pdf/2506.04998v1", "comment": "15 pages, 7 figures, 4 appendix subsections", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05014v1", "title": "Towards Reasonable Concept Bottleneck Models", "authors": ["Nektarios Kalampalikis", "Kavya Gupta", "Georgi Vitanov", "Isabel Valera"], "abstract": "In this paper, we propose $\\textbf{C}$oncept $\\textbf{REA}$soning\n$\\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)\nthat: (i) explicitly encodes concept-concept (${\\texttt{C-C}}$) and\nconcept-task (${\\texttt{C$\\rightarrow$Y}}$) relationships to enforce a desired\nmodel reasoning; and (ii) use a regularized side-channel to achieve competitive\ntask performance, while keeping high concept importance. Specifically, CREAM\narchitecturally embeds (bi)directed concept-concept, and concept to task\nrelationships specified by a human expert, while severing undesired information\nflows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates\na black-box side-channel that is regularized to encourage task predictions to\nbe grounded in the relevant concepts, thereby utilizing the side-channel only\nwhen necessary to enhance performance. Our experiments show that: (i) CREAM\nmainly relies on concepts while achieving task performance on par with\nblack-box models; and (ii) the embedded ${\\texttt{C-C}}$ and\n${\\texttt{C$\\rightarrow$Y}}$ relationships ease model interventions and\nmitigate concept leakage.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-06-05 13:22:29", "updated": "2025-06-05 13:22:29", "pdf_url": "http://arxiv.org/pdf/2506.05014v1", "comment": "26 pages, 17 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05020v1", "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System", "authors": ["Haokun Liu", "Zhaoqi Ma", "Yunong Li", "Junichiro Sugihara", "Yicheng Chen", "Jinjie Li", "Moju Zhao"], "abstract": "Heterogeneous multi-robot systems show great potential in complex tasks\nrequiring coordinated hybrid cooperation. However, traditional approaches\nrelying on static models often struggle with task diversity and dynamic\nenvironments. This highlights the need for generalizable intelligence that can\nbridge high-level reasoning with low-level execution across heterogeneous\nagents. To address this, we propose a hierarchical framework integrating a\nprompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision\nLanguage Model (VLM). The LLM performs task decomposition and global semantic\nmap construction, while the VLM extracts task-specified semantic labels and 2D\nspatial information from aerial images to support local planning. Within this\nframework, the aerial robot follows a globally optimized semantic path and\ncontinuously provides bird-view images, guiding the ground robot's local\nsemantic navigation and manipulation, including target-absent scenarios where\nimplicit alignment is maintained. Experiments on a real-world letter-cubes\narrangement task demonstrate the framework's adaptability and robustness in\ndynamic environments. To the best of our knowledge, this is the first\ndemonstration of an aerial-ground heterogeneous system integrating VLM-based\nperception with LLM-driven task reasoning and motion planning.", "categories": ["cs.RO", "cs.AI"], "published": "2025-06-05 13:27:41", "updated": "2025-06-05 13:27:41", "pdf_url": "http://arxiv.org/pdf/2506.05020v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05030v1", "title": "Artificial Intelligence Should Genuinely Support Clinical Reasoning and Decision Making To Bridge the Translational Gap", "authors": ["Kacper Sokol", "James Fackler", "Julia E Vogt"], "abstract": "Artificial intelligence promises to revolutionise medicine, yet its impact\nremains limited because of the pervasive translational gap. We posit that the\nprevailing technology-centric approaches underpin this challenge, rendering\nsuch systems fundamentally incompatible with clinical practice, specifically\ndiagnostic reasoning and decision making. Instead, we propose a novel\nsociotechnical conceptualisation of data-driven support tools designed to\ncomplement doctors' cognitive and epistemic activities. Crucially, it\nprioritises real-world impact over superhuman performance on inconsequential\nbenchmarks.", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "published": "2025-06-05 13:39:37", "updated": "2025-06-05 13:39:37", "pdf_url": "http://arxiv.org/pdf/2506.05030v1", "comment": null, "doi": "10.1038/s41746-025-01725-9", "journal_ref": null}
{"arxiv_id": "2506.05032v1", "title": "Identifying and Understanding Cross-Class Features in Adversarial Training", "authors": ["Zeming Wei", "Yiwen Guo", "Yisen Wang"], "abstract": "Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT.", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "math.OC"], "published": "2025-06-05 13:40:11", "updated": "2025-06-05 13:40:11", "pdf_url": "http://arxiv.org/pdf/2506.05032v1", "comment": "ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05035v1", "title": "TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation", "authors": ["Hyeongwon Jang", "Changhun Kim", "Eunho Yang"], "abstract": "Recent explainable artificial intelligence (XAI) methods for time series\nprimarily estimate point-wise attribution magnitudes, while overlooking the\ndirectional impact on predictions, leading to suboptimal identification of\nsignificant points. Our analysis shows that conventional Integrated Gradients\n(IG) effectively capture critical points with both positive and negative\nimpacts on predictions. However, current evaluation metrics fail to assess this\ncapability, as they inadvertently cancel out opposing feature contributions. To\naddress this limitation, we propose novel evaluation metrics-Cumulative\nPrediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to\nsystematically assess whether attribution methods accurately identify\nsignificant positive and negative points in time series XAI. Under these\nmetrics, conventional IG outperforms recent counterparts. However, directly\napplying IG to time series data may lead to suboptimal outcomes, as generated\npaths ignore temporal relationships and introduce out-of-distribution samples.\nTo overcome these challenges, we introduce TIMING, which enhances IG by\nincorporating temporal awareness while maintaining its theoretical properties.\nExtensive experiments on synthetic and real-world time series benchmarks\ndemonstrate that TIMING outperforms existing time series XAI baselines. Our\ncode is available at https://github.com/drumpt/TIMING.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-05 13:40:40", "updated": "2025-06-05 13:40:40", "pdf_url": "http://arxiv.org/pdf/2506.05035v1", "comment": "ICML 2025 Spotlight Presentation; Code is available at\n  https://github.com/drumpt/TIMING", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05057v1", "title": "TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages", "authors": ["Moshe Ofer", "Orel Zamler", "Amos Azaria"], "abstract": "Large Language Models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages due to limited training data. This paper presents\nTALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource\nLanguages), which integrates an LLM with two bilingual translation models. TALL\ntransforms low-resource inputs into high-resource representations, leveraging\nthe LLM's capabilities while preserving linguistic features through dimension\nalignment layers and custom transformers. Our experiments on Hebrew demonstrate\nsignificant improvements over several baselines, including direct use, naive\ntranslation, and fine-tuning approaches. The architecture employs a\nparameter-efficient strategy, freezing pre-trained components while training\nonly lightweight adapter modules, balancing computational efficiency with\nperformance gains.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 14:02:12", "updated": "2025-06-05 14:02:12", "pdf_url": "http://arxiv.org/pdf/2506.05057v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05068v1", "title": "Does It Make Sense to Speak of Introspection in Large Language Models?", "authors": ["Iulia Com\u015fa", "Murray Shanahan"], "abstract": "Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown ``creative'' writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 14:13:54", "updated": "2025-06-05 14:13:54", "pdf_url": "http://arxiv.org/pdf/2506.05068v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05069v1", "title": "Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation", "authors": ["Keyu Zhao", "Fengli Xu", "Yong Li"], "abstract": "Driven by advances in Large Language Models (LLMs), integrating them into\nrecommendation tasks has gained interest due to their strong semantic\nunderstanding and prompt flexibility. Prior work encoded user-item interactions\nor metadata into prompts for recommendations. In parallel, LLM reasoning,\nboosted by test-time scaling and reinforcement learning, has excelled in fields\nlike mathematics and code, where reasoning traces and correctness signals are\nclear, enabling high performance and interpretability. However, directly\napplying these reasoning methods to recommendation is ineffective because user\nfeedback is implicit and lacks reasoning supervision. To address this, we\npropose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that\nsamples interaction chains from the user-item graph and converts them into\nstructured interaction-of-thoughts via a progressive masked prompting strategy,\nwith each thought representing stepwise reasoning grounded in interaction\ncontext. This allows LLMs to simulate step-by-step decision-making based on\nimplicit patterns. We design a two-stage training pipeline: supervised\nfine-tuning teaches basic reasoning from high-quality traces, and reinforcement\nlearning refines reasoning via reward signals, alleviating sparse explicit\nsupervision. Experiments on three real-world datasets show R2Rec outperforms\nclassical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement\nin HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore,\nthe explicit reasoning chains enhance interpretability by revealing the\ndecision process. Our code is available at:\nhttps://anonymous.4open.science/r/R2Rec-7C5D.", "categories": ["cs.IR", "cs.AI"], "published": "2025-06-05 14:16:44", "updated": "2025-06-05 14:16:44", "pdf_url": "http://arxiv.org/pdf/2506.05069v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05104v1", "title": "Survey on the Evaluation of Generative Models in Music", "authors": ["Alexander Lerch", "Claire Arthur", "Nick Bryan-Kinns", "Corey Ford", "Qianyi Sun", "Ashvala Vinay"], "abstract": "Research on generative systems in music has seen considerable attention and\ngrowth in recent years. A variety of attempts have been made to systematically\nevaluate such systems. We provide an interdisciplinary review of the common\nevaluation targets, methodologies, and metrics for the evaluation of both\nsystem output and model usability, covering subjective and objective\napproaches, qualitative and quantitative approaches, as well as empirical and\ncomputational methods. We discuss the advantages and challenges of such\napproaches from a musicological, an engineering, and an HCI perspective.", "categories": ["cs.SD", "cs.AI", "cs.LG"], "published": "2025-06-05 14:46:04", "updated": "2025-06-05 14:46:04", "pdf_url": "http://arxiv.org/pdf/2506.05104v1", "comment": "Submitted to ACM CSUR, 26-Jun-2024", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05109v1", "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning", "authors": ["Tennison Liu", "Mihaela van der Schaar"], "abstract": "Self-improving agents aim to continuously acquire new capabilities with\nminimal supervision. However, current approaches face two key limitations:\ntheir self-improvement processes are often rigid, fail to generalize across\ntasks domains, and struggle to scale with increasing agent capabilities. We\nargue that effective self-improvement requires intrinsic metacognitive\nlearning, defined as an agent's intrinsic ability to actively evaluate, reflect\non, and adapt its own learning processes. Drawing inspiration from human\nmetacognition, we introduce a formal framework comprising three components:\nmetacognitive knowledge (self-assessment of capabilities, tasks, and learning\nstrategies), metacognitive planning (deciding what and how to learn), and\nmetacognitive evaluation (reflecting on learning experiences to improve future\nlearning). Analyzing existing self-improving agents, we find they rely\npredominantly on extrinsic metacognitive mechanisms, which are fixed,\nhuman-designed loops that limit scalability and adaptability. Examining each\ncomponent, we contend that many ingredients for intrinsic metacognition are\nalready present. Finally, we explore how to optimally distribute metacognitive\nresponsibilities between humans and agents, and robustly evaluate and improve\nintrinsic metacognitive learning, key challenges that must be addressed to\nenable truly sustained, generalized, and aligned self-improvement.", "categories": ["cs.AI"], "published": "2025-06-05 14:53:35", "updated": "2025-06-05 14:53:35", "pdf_url": "http://arxiv.org/pdf/2506.05109v1", "comment": "Published as a conference paper at ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05128v1", "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning", "authors": ["Tanmay Parekh", "Kartik Mehta", "Ninareh Mehrabi", "Kai-Wei Chang", "Nanyun Peng"], "abstract": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 15:16:14", "updated": "2025-06-05 15:16:14", "pdf_url": "http://arxiv.org/pdf/2506.05128v1", "comment": "Submitted at ACL ARR May 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05140v1", "title": "AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models", "authors": ["Chih-Kai Yang", "Neo Ho", "Yi-Jyun Lee", "Hung-yi Lee"], "abstract": "Understanding the internal mechanisms of large audio-language models (LALMs)\nis crucial for interpreting their behavior and improving performance. This work\npresents the first in-depth analysis of how LALMs internally perceive and\nrecognize auditory attributes. By applying vocabulary projection on three\nstate-of-the-art LALMs, we track how attribute information evolves across\nlayers and token positions. We find that attribute information generally\ndecreases with layer depth when recognition fails, and that resolving\nattributes at earlier layers correlates with better accuracy. Moreover, LALMs\nheavily rely on querying auditory inputs for predicting attributes instead of\naggregating necessary information in hidden states at attribute-mentioning\npositions. Based on our findings, we demonstrate a method to enhance LALMs. Our\nresults offer insights into auditory attribute processing, paving the way for\nfuture improvements.", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "published": "2025-06-05 15:22:47", "updated": "2025-06-05 15:22:47", "pdf_url": "http://arxiv.org/pdf/2506.05140v1", "comment": "8 pages, 5 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05154v1", "title": "Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation", "authors": ["Chenyu Lin", "Yilin Wen", "Du Su", "Fei Sun", "Muhan Chen", "Chenfu Bao", "Zhonghou Lv"], "abstract": "Retrieval-augmented generation (RAG) is a mainstream method for improving\nperformance on knowledge-intensive tasks. However,current RAG systems often\nplace too much emphasis on retrieved contexts. This can lead to reliance on\ninaccurate sources and overlook the model's inherent knowledge, especially when\ndealing with misleading or excessive information. To resolve this imbalance, we\npropose Knowledgeable-r1 that using joint sampling and define multi policy\ndistributions in knowledge capability exploration to stimulate large language\nmodels'self-integrated utilization of parametric and contextual knowledge.\nExperiments show that Knowledgeable-r1 significantly enhances robustness and\nreasoning accuracy in both parameters and contextual conflict tasks and general\nRAG tasks, especially outperforming baselines by 17.07% in counterfactual\nscenarios and demonstrating consistent gains across RAG tasks. Our code are\navailable at https://github.com/lcy80366872/ knowledgeable-r1.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-05 15:34:15", "updated": "2025-06-05 15:34:15", "pdf_url": "http://arxiv.org/pdf/2506.05154v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05166v1", "title": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective", "authors": ["Bhavik Chandna", "Zubair Bashir", "Procheta Sen"], "abstract": "Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 15:43:34", "updated": "2025-06-05 15:43:34", "pdf_url": "http://arxiv.org/pdf/2506.05166v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05167v1", "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG", "authors": ["Yeonseok Jeong", "Jinsu Kim", "Dohyeon Lee", "Seung-won Hwang"], "abstract": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\n\\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing\nretrieved documents based on evidentiality, ensuring whether answer generation\nis supported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-05 15:43:49", "updated": "2025-06-05 15:43:49", "pdf_url": "http://arxiv.org/pdf/2506.05167v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05183v1", "title": "TreeRPO: Tree Relative Policy Optimization", "authors": ["Zhicheng Yang", "Zhijiang Guo", "Yinya Huang", "Xiaodan Liang", "Yiwei Wang", "Jing Tang"], "abstract": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-05 15:56:38", "updated": "2025-06-05 15:56:38", "pdf_url": "http://arxiv.org/pdf/2506.05183v1", "comment": "13pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05188v1", "title": "Counterfactual reasoning: an analysis of in-context emergence", "authors": ["Moritz Miller", "Bernhard Sch\u00f6lkopf", "Siyuan Guo"], "abstract": "Large-scale neural language models (LMs) exhibit remarkable performance in\nin-context learning: the ability to learn and reason the input context on the\nfly without parameter update. This work studies in-context counterfactual\nreasoning in language models, that is, to predict the consequences of changes\nunder hypothetical scenarios. We focus on studying a well-defined synthetic\nsetup: a linear regression task that requires noise abduction, where accurate\nprediction is based on inferring and copying the contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning in this controlled setup and provide insights that counterfactual\nreasoning for a broad class of functions can be reduced to a transformation on\nin-context observations; we find self-attention, model depth, and data\ndiversity in pre-training drive performance in Transformers. More\ninterestingly, our findings extend beyond regression tasks and show that\nTransformers can perform noise abduction on sequential data, providing\npreliminary evidence on the potential for counterfactual story generation. Our\ncode is available under\nhttps://github.com/moXmiller/counterfactual-reasoning.git .", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "published": "2025-06-05 16:02:07", "updated": "2025-06-05 16:02:07", "pdf_url": "http://arxiv.org/pdf/2506.05188v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05211v1", "title": "Intentionally Unintentional: GenAI Exceptionalism and the First Amendment", "authors": ["David Atkinson", "Jena D. Hwang", "Jacob Morrison"], "abstract": "This paper challenges the assumption that courts should grant First Amendment\nprotections to outputs from large generative AI models, such as GPT-4 and\nGemini. We argue that because these models lack intentionality, their outputs\ndo not constitute speech as understood in the context of established legal\nprecedent, so there can be no speech to protect. Furthermore, if the model\noutputs are not speech, users cannot claim a First Amendment speech right to\nreceive the outputs. We also argue that extending First Amendment rights to AI\nmodels would not serve the fundamental purposes of free speech, such as\npromoting a marketplace of ideas, facilitating self-governance, or fostering\nself-expression. In fact, granting First Amendment protections to AI models\nwould be detrimental to society because it would hinder the government's\nability to regulate these powerful technologies effectively, potentially\nleading to the unchecked spread of misinformation and other harms.", "categories": ["cs.CY", "cs.AI"], "published": "2025-06-05 16:26:32", "updated": "2025-06-05 16:26:32", "pdf_url": "http://arxiv.org/pdf/2506.05211v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05213v1", "title": "LLM-First Search: Self-Guided Exploration of the Solution Space", "authors": ["Nathan Herr", "Tim Rockt\u00e4schel", "Roberta Raileanu"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-05 16:27:49", "updated": "2025-06-05 16:27:49", "pdf_url": "http://arxiv.org/pdf/2506.05213v1", "comment": "9 main pages, 2 figures, 2 tables, 36 appendix pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05214v1", "title": "Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning", "authors": ["Jingyu Hu", "Hongbo Bo", "Jun Hong", "Xiaowei Liu", "Weiru Liu"], "abstract": "Graph Neural Networks (GNNs) often suffer from degree bias in node\nclassification tasks, where prediction performance varies across nodes with\ndifferent degrees. Several approaches, which adopt Graph Contrastive Learning\n(GCL), have been proposed to mitigate this bias. However, the limited number of\npositive pairs and the equal weighting of all positives and negatives in GCL\nstill lead to low-degree nodes acquiring insufficient and noisy information.\nThis paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to\nmitigate degree bias. It adds more positive pairs by leveraging node labels and\nadaptively weights positive and negative pairs based on their learning\nhardness. In addition, we develop an experimental framework named SHARP to\nextend HAR to a broader range of scenarios. Both our theoretical analysis and\nexperiments validate the effectiveness of SHARP. The experimental results\nacross four datasets show that SHARP achieves better performance against\nbaselines at both global and degree levels.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-05 16:28:12", "updated": "2025-06-05 16:28:12", "pdf_url": "http://arxiv.org/pdf/2506.05214v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05233v1", "title": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training", "authors": ["Johannes von Oswald", "Nino Scherrer", "Seijin Kobayashi", "Luca Versari", "Songlin Yang", "Maximilian Schlegel", "Kaitlin Maile", "Yanick Schimpf", "Oliver Sieberling", "Alexander Meulemans", "Rif A. Saurous", "Guillaume Lajoie", "Charlotte Frenkel", "Razvan Pascanu", "Blaise Ag\u00fcera y Arcas", "Jo\u00e3o Sacramento"], "abstract": "Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-05 16:50:23", "updated": "2025-06-05 16:50:23", "pdf_url": "http://arxiv.org/pdf/2506.05233v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05256v1", "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning", "authors": ["Violet Xiang", "Chase Blagden", "Rafael Rafailov", "Nathan Lile", "Sang Truong", "Chelsea Finn", "Nick Haber"], "abstract": "Large reasoning models (LRMs) achieve higher performance on challenging\nreasoning tasks by generating more tokens at inference time, but this verbosity\noften wastes computation on easy problems. Existing solutions, including\nsupervised finetuning on shorter traces, user-controlled budgets, or RL with\nuniform penalties, either require data curation, manual configuration, or treat\nall problems alike regardless of difficulty. We introduce Adaptive Length\nPenalty (ALP), a reinforcement learning objective tailoring generation length\nto per-prompt solve rate. During training, ALP monitors each prompt's online\nsolve rate through multiple rollouts and adds a differentiable penalty whose\nmagnitude scales inversely with that rate, so confident (easy) prompts incur a\nhigh cost for extra tokens while hard prompts remain unhindered. Posttraining\nDeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly\ndropping performance. Relative to fixed-budget and uniform penalty baselines,\nALP redistributes its reduced budget more intelligently by cutting compute on\neasy prompts and reallocating saved tokens to difficult ones, delivering higher\naccuracy on the hardest problems with higher cost.", "categories": ["cs.AI", "cs.LG"], "published": "2025-06-05 17:17:05", "updated": "2025-06-05 17:17:05", "pdf_url": "http://arxiv.org/pdf/2506.05256v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05265v1", "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams", "authors": ["Mohammed Almutairi"], "abstract": "Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination.", "categories": ["cs.HC", "cs.AI", "cs.MA"], "published": "2025-06-05 17:24:37", "updated": "2025-06-05 17:24:37", "pdf_url": "http://arxiv.org/pdf/2506.05265v1", "comment": "5 pages, UMAP 25, June 16_19, 2025, New York City, NY, USA", "doi": "10.1145/3699682.3727574", "journal_ref": "ACM International Conference on User Modeling, Adaptation and\n  Personalization 2025"}
{"arxiv_id": "2506.05278v1", "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning", "authors": ["Nan Huo", "Jinyang Li", "Bowen Qin", "Ge Qu", "Xiaolong Li", "Xiaodong Li", "Chenhao Ma", "Reynold Cheng"], "abstract": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 17:33:02", "updated": "2025-06-05 17:33:02", "pdf_url": "http://arxiv.org/pdf/2506.05278v1", "comment": "Accepted by ACL 2025 Main", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05281v1", "title": "Fast-DataShapley: Neural Modeling for Training Data Valuation", "authors": ["Haifeng Sun", "Yu Xiong", "Runze Wu", "Xinyu Cai", "Changjie Fan", "Lan Zhang", "Xiang-Yang Li"], "abstract": "The value and copyright of training data are crucial in the artificial\nintelligence industry. Service platforms should protect data providers'\nlegitimate rights and fairly reward them for their contributions. Shapley\nvalue, a potent tool for evaluating contributions, outperforms other methods in\ntheory, but its computational overhead escalates exponentially with the number\nof data providers. Recent works based on Shapley values attempt to mitigate\ncomputation complexity by approximation algorithms. However, they need to\nretrain for each test sample, leading to intolerable costs. We propose\nFast-DataShapley, a one-pass training method that leverages the weighted least\nsquares characterization of the Shapley value to train a reusable explainer\nmodel with real-time reasoning speed. Given new test samples, no retraining is\nrequired to calculate the Shapley values of the training data. Additionally, we\npropose three methods with theoretical guarantees to reduce training overhead\nfrom two aspects: the approximate calculation of the utility function and the\ngroup calculation of the training data. We analyze time complexity to show the\nefficiency of our methods. The experimental evaluations on various image\ndatasets demonstrate superior performance and efficiency compared to baselines.\nSpecifically, the performance is improved to more than 2.5 times, and the\nexplainer's training speed can be increased by two orders of magnitude.", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-05 17:35:46", "updated": "2025-06-05 17:35:46", "pdf_url": "http://arxiv.org/pdf/2506.05281v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05282v1", "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation", "authors": ["Tao Sun", "Liyuan Zhu", "Shengyu Huang", "Shuran Song", "Iro Armeni"], "abstract": "We introduce Rectified Point Flow, a unified parameterization that formulates\npairwise point cloud registration and multi-part shape assembly as a single\nconditional generative problem. Given unposed point clouds, our method learns a\ncontinuous point-wise velocity field that transports noisy points toward their\ntarget positions, from which part poses are recovered. In contrast to prior\nwork that regresses part-wise poses with ad-hoc symmetry handling, our method\nintrinsically learns assembly symmetries without symmetry labels. Together with\na self-supervised encoder focused on overlapping points, our method achieves a\nnew state-of-the-art performance on six benchmarks spanning pairwise\nregistration and shape assembly. Notably, our unified formulation enables\neffective joint training on diverse datasets, facilitating the learning of\nshared geometric priors and consequently boosting accuracy. Project page:\nhttps://rectified-pointflow.github.io/.", "categories": ["cs.CV", "cs.AI", "cs.RO"], "published": "2025-06-05 17:36:03", "updated": "2025-06-05 17:36:03", "pdf_url": "http://arxiv.org/pdf/2506.05282v1", "comment": "Project page: https://rectified-pointflow.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05295v1", "title": "Sample Complexity and Representation Ability of Test-time Scaling Paradigms", "authors": ["Baihe Huang", "Shanda Li", "Tianhao Wu", "Yiming Yang", "Ameet Talwalkar", "Kannan Ramchandran", "Michael I. Jordan", "Jiantao Jiao"], "abstract": "Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-06-05 17:48:19", "updated": "2025-06-05 17:48:19", "pdf_url": "http://arxiv.org/pdf/2506.05295v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05296v1", "title": "Control Tax: The Price of Keeping AI in Check", "authors": ["Mikhail Terekhov", "Zhen Ning David Liu", "Caglar Gulcehre", "Samuel Albanie"], "abstract": "The rapid integration of agentic AI into high-stakes real-world applications\nrequires robust oversight mechanisms. The emerging field of AI Control (AIC)\naims to provide such an oversight mechanism, but practical adoption depends\nheavily on implementation overhead. To study this problem better, we introduce\nthe notion of Control tax -- the operational and financial cost of integrating\ncontrol measures into AI pipelines. Our work makes three key contributions to\nthe field of AIC: (1) we introduce a theoretical framework that quantifies the\nControl Tax and maps classifier performance to safety assurances; (2) we\nconduct comprehensive evaluations of state-of-the-art language models in\nadversarial settings, where attacker models insert subtle backdoors into code\nwhile monitoring models attempt to detect these vulnerabilities; and (3) we\nprovide empirical financial cost estimates for control protocols and develop\noptimized monitoring strategies that balance safety and cost-effectiveness\nwhile accounting for practical constraints like auditing budgets. Our framework\nenables practitioners to make informed decisions by systematically connecting\nsafety guarantees with their costs, advancing AIC through principled economic\nfeasibility assessment across different deployment contexts.", "categories": ["cs.AI", "cs.LG"], "published": "2025-06-05 17:48:39", "updated": "2025-06-05 17:48:39", "pdf_url": "http://arxiv.org/pdf/2506.05296v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05305v1", "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback", "authors": ["Deepak Pandita", "Tharindu Cyril Weerasooriya", "Ankit Parag Shah", "Christopher M. Homan", "Wei Wei"], "abstract": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 17:52:30", "updated": "2025-06-05 17:52:30", "pdf_url": "http://arxiv.org/pdf/2506.05305v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05309v1", "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games", "authors": ["Niv Eckhaus", "Uri Berger", "Gabriel Stanovsky"], "abstract": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.", "categories": ["cs.MA", "cs.AI", "cs.CL"], "published": "2025-06-05 17:53:44", "updated": "2025-06-05 17:53:44", "pdf_url": "http://arxiv.org/pdf/2506.05309v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05314v1", "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models", "authors": ["Taha Entesari", "Arman Hatami", "Rinat Khaziev", "Anil Ramakrishna", "Mahyar Fazlyab"], "abstract": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 17:55:23", "updated": "2025-06-05 17:55:23", "pdf_url": "http://arxiv.org/pdf/2506.05314v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05316v1", "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay", "authors": ["Yifan Sun", "Jingyan Shen", "Yibin Wang", "Tianyu Chen", "Zhendong Wang", "Mingyuan Zhou", "Huan Zhang"], "abstract": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-05 17:55:43", "updated": "2025-06-05 17:55:43", "pdf_url": "http://arxiv.org/pdf/2506.05316v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05340v1", "title": "Exploring Diffusion Transformer Designs via Grafting", "authors": ["Keshigeyan Chandrasegaran", "Michael Poli", "Daniel Y. Fu", "Dongjun Kim", "Lea M. Hadzic", "Manling Li", "Agrim Gupta", "Stefano Massaroli", "Azalia Mirhoseini", "Juan Carlos Niebles", "Stefano Ermon", "Li Fei-Fei"], "abstract": "Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu", "categories": ["cs.LG", "cs.AI"], "published": "2025-06-05 17:59:40", "updated": "2025-06-05 17:59:40", "pdf_url": "http://arxiv.org/pdf/2506.05340v1", "comment": "22 pages; Project website: https://grafting.stanford.edu", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05341v1", "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning", "authors": ["Xingjian Ran", "Yixuan Li", "Linning Xu", "Mulin Yu", "Bo Dai"], "abstract": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-05 17:59:42", "updated": "2025-06-05 17:59:42", "pdf_url": "http://arxiv.org/pdf/2506.05341v1", "comment": "Project Page: https://directlayout.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05342v1", "title": "Refer to Anything with Vision-Language Prompts", "authors": ["Shengcao Cao", "Zijun Wei", "Jason Kuen", "Kangning Liu", "Lingzhi Zhang", "Jiuxiang Gu", "HyunJoon Jung", "Liang-Yan Gui", "Yu-Xiong Wang"], "abstract": "Recent image segmentation models have advanced to segment images into\nhigh-quality masks for visual entities, and yet they cannot provide\ncomprehensive semantic understanding for complex queries based on both language\nand vision. This limitation reduces their effectiveness in applications that\nrequire user-friendly interactions driven by vision-language prompts. To bridge\nthis gap, we introduce a novel task of omnimodal referring expression\nsegmentation (ORES). In this task, a model produces a group of masks based on\narbitrary prompts specified by text only or text plus reference visual\nentities. To address this new challenge, we propose a novel framework to \"Refer\nto Any Segmentation Mask Group\" (RAS), which augments segmentation models with\ncomplex multimodal interactions and comprehension via a mask-centric large\nmultimodal model. For training and benchmarking ORES models, we create datasets\nMaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by\ntext and reference entities. Through extensive evaluation, we demonstrate\nsuperior performance of RAS on our new ORES task, as well as classic referring\nexpression segmentation (RES) and generalized referring expression segmentation\n(GRES) tasks. Project page: https://Ref2Any.github.io.", "categories": ["cs.CV", "cs.AI"], "published": "2025-06-05 17:59:51", "updated": "2025-06-05 17:59:51", "pdf_url": "http://arxiv.org/pdf/2506.05342v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04521v1", "title": "Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation", "authors": ["Di Wu", "Seth Aycock", "Christof Monz"], "abstract": "Large Language Models (LLMs) demonstrate strong reasoning capabilities for\nmany tasks, often by explicitly decomposing the task via Chain-of-Thought (CoT)\nreasoning. Recent work on LLM-based translation designs hand-crafted prompts to\ndecompose translation, or trains models to incorporate intermediate\nsteps.~\\textit{Translating Step-by-step}~\\citep{briakou2024translating}, for\ninstance, introduces a multi-step prompt with decomposition and refinement of\ntranslation with LLMs, which achieved state-of-the-art results on WMT24. In\nthis work, we scrutinise this strategy's effectiveness. Empirically, we find no\nclear evidence that performance gains stem from explicitly decomposing the\ntranslation process, at least for the models on test; and we show that simply\nprompting LLMs to ``translate again'' yields even better results than\nhuman-like step-by-step prompting. Our analysis does not rule out the role of\nreasoning, but instead invites future work exploring the factors for CoT's\neffectiveness in the context of translation.", "categories": ["cs.CL"], "published": "2025-06-05 00:04:39", "updated": "2025-06-05 00:04:39", "pdf_url": "http://arxiv.org/pdf/2506.04521v1", "comment": "16 pages, 16 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04527v1", "title": "Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit and Explicit Grapheme Conditioning", "authors": ["Hien Ohnaka", "Yuma Shirahata", "Byeongseon Park", "Ryuichi Yamamoto"], "abstract": "We propose a model to obtain phonemic and prosodic labels of speech that are\ncoherent with graphemes. Unlike previous methods that simply fine-tune a\npre-trained ASR model with the labels, the proposed model conditions the label\ngeneration on corresponding graphemes by two methods: 1) Add implicit grapheme\nconditioning through prompt encoder using pre-trained BERT features. 2)\nExplicitly prune the label hypotheses inconsistent with the grapheme during\ninference. These methods enable obtaining parallel data of speech, the labels,\nand graphemes, which is applicable to various downstream tasks such as\ntext-to-speech and accent estimation from text. Experiments showed that the\nproposed method significantly improved the consistency between graphemes and\nthe predicted labels. Further, experiments on accent estimation task confirmed\nthat the created parallel data by the proposed method effectively improve the\nestimation accuracy.", "categories": ["cs.SD", "cs.CL", "eess.AS"], "published": "2025-06-05 00:24:00", "updated": "2025-06-05 00:24:00", "pdf_url": "http://arxiv.org/pdf/2506.04527v1", "comment": "5 pages, 2 figures, and 4 tables, accepted to INTERSPEECH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04534v1", "title": "Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs", "authors": ["William Sheffield", "Kanishka Misra", "Valentina Pyatkin", "Ashwini Deo", "Kyle Mahowald", "Junyi Jessy Li"], "abstract": "Discourse particles are crucial elements that subtly shape the meaning of\ntext. These words, often polyfunctional, give rise to nuanced and often quite\ndisparate semantic/discourse effects, as exemplified by the diverse uses of the\nparticle \"just\" (e.g., exclusive, temporal, emphatic). This work investigates\nthe capacity of LLMs to distinguish the fine-grained senses of English \"just\",\na well-studied example in formal semantics, using data meticulously created and\nlabeled by expert linguists. Our findings reveal that while LLMs exhibit some\nability to differentiate between broader categories, they struggle to fully\ncapture more subtle nuances, highlighting a gap in their understanding of\ndiscourse particles.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 00:59:05", "updated": "2025-06-05 00:59:05", "pdf_url": "http://arxiv.org/pdf/2506.04534v1", "comment": "To be published in Findings of The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025). The main paper is 5\n  pages and contains 3 figures and 1 table. In total, the paper is 12 pages and\n  contains 8 figures and 5 tables (References + Appendix)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04535v1", "title": "BSBench: will your LLM find the largest prime number?", "authors": ["K. O. T. Erziev"], "abstract": "We propose that benchmarking LLMs on questions which have no reasonable\nanswer actually isn't as silly as it sounds. We also present a benchmark that\nallows such testing and a method to modify the existing datasets, and discover\nthat existing models demonstrate a performance far from the perfect on such\nquestions. Our code and data artifacts are available at\nhttps://github.com/L3G5/impossible-bench", "categories": ["cs.CL"], "published": "2025-06-05 00:59:16", "updated": "2025-06-05 00:59:16", "pdf_url": "http://arxiv.org/pdf/2506.04535v1", "comment": "7 + 2 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04557v1", "title": "SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?", "authors": ["Senyu Li", "Jiayi Wang", "Felermino D. M. A. Ali", "Colin Cherry", "Daniel Deutsch", "Eleftheria Briakou", "Rui Sousa-Silva", "Henrique Lopes Cardoso", "Pontus Stenetorp", "David Ifeoluwa Adelani"], "abstract": "Evaluating machine translation (MT) quality for under-resourced African\nlanguages remains a significant challenge, as existing metrics often suffer\nfrom limited language coverage and poor performance in low-resource settings.\nWhile recent efforts, such as AfriCOMET, have addressed some of the issues,\nthey are still constrained by small evaluation sets, a lack of publicly\navailable training data tailored to African languages, and inconsistent\nperformance in extremely low-resource scenarios. In this work, we introduce\nSSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 13\nAfrican language pairs from the News domain, with over 63,000 sentence-level\nannotations from a diverse set of MT systems. Based on this data, we develop\nSSA-COMET and SSA-COMET-QE, improved reference-based and reference-free\nevaluation metrics. We also benchmark prompting-based approaches using\nstate-of-the-art LLMs like GPT-4o and Claude. Our experimental results show\nthat SSA-COMET models significantly outperform AfriCOMET and are competitive\nwith the strongest LLM (Gemini 2.5 Pro) evaluated in our study, particularly on\nlow-resource languages such as Twi, Luo, and Yoruba. All resources are released\nunder open licenses to support future research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 02:16:56", "updated": "2025-06-05 02:16:56", "pdf_url": "http://arxiv.org/pdf/2506.04557v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04566v1", "title": "Clustering and Median Aggregation Improve Differentially Private Inference", "authors": ["Kareem Amin", "Salman Avestimehr", "Sara Babakniya", "Alex Bie", "Weiwei Kong", "Natalia Ponomareva", "Umar Syed"], "abstract": "Differentially private (DP) language model inference is an approach for\ngenerating private synthetic text. A sensitive input example is used to prompt\nan off-the-shelf large language model (LLM) to produce a similar example.\nMultiple examples can be aggregated together to formally satisfy the DP\nguarantee.\n  Prior work creates inference batches by sampling sensitive inputs uniformly\nat random. We show that uniform sampling degrades the quality of privately\ngenerated text, especially when the sensitive examples concern heterogeneous\ntopics.\n  We remedy this problem by clustering the input data before selecting\ninference batches. Next, we observe that clustering also leads to more similar\nnext-token predictions across inferences. We use this insight to introduce a\nnew algorithm that aggregates next token statistics by privately computing\nmedians instead of averages. This approach leverages the fact that the median\nhas decreased local sensitivity when next token predictions are similar,\nallowing us to state a data-dependent and ex-post DP guarantee about the\nprivacy properties of this algorithm. Finally, we demonstrate improvements in\nterms of representativeness metrics (e.g., MAUVE) as well as downstream task\nperformance. We show that our method produces high-quality synthetic data at\nsignificantly lower privacy cost than a previous state-of-the-art method.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "published": "2025-06-05 02:34:50", "updated": "2025-06-05 02:34:50", "pdf_url": "http://arxiv.org/pdf/2506.04566v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04572v1", "title": "Demonstrations of Integrity Attacks in Multi-Agent Systems", "authors": ["Can Zheng", "Yuhan Cao", "Xiaoning Dong", "Tianxing He"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding, code generation, and complex planning.\nSimultaneously, Multi-Agent Systems (MAS) have garnered attention for their\npotential to enable cooperation among distributed agents. However, from a\nmulti-party perspective, MAS could be vulnerable to malicious agents that\nexploit the system to serve self-interests without disrupting its core\nfunctionality. This work explores integrity attacks where malicious agents\nemploy subtle prompt manipulation to bias MAS operations and gain various\nbenefits. Four types of attacks are examined: \\textit{Scapegoater}, who\nmisleads the system monitor to underestimate other agents' contributions;\n\\textit{Boaster}, who misleads the system monitor to overestimate their own\nperformance; \\textit{Self-Dealer}, who manipulates other agents to adopt\ncertain tools; and \\textit{Free-Rider}, who hands off its own task to others.\nWe demonstrate that strategically crafted prompts can introduce systematic\nbiases in MAS behavior and executable instructions, enabling malicious agents\nto effectively mislead evaluation systems and manipulate collaborative agents.\nFurthermore, our attacks can bypass advanced LLM-based monitors, such as\nGPT-4o-mini and o3-mini, highlighting the limitations of current detection\nmechanisms. Our findings underscore the critical need for MAS architectures\nwith robust security protocols and content validation mechanisms, alongside\nmonitoring systems capable of comprehensive risk scenario assessment.", "categories": ["cs.CL"], "published": "2025-06-05 02:44:49", "updated": "2025-06-05 02:44:49", "pdf_url": "http://arxiv.org/pdf/2506.04572v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04574v1", "title": "Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis", "authors": ["Dimitris Vamvourellis", "Dhagash Mehta"], "abstract": "We investigate the effectiveness of large language models (LLMs), including\nreasoning-based and non-reasoning models, in performing zero-shot financial\nsentiment analysis. Using the Financial PhraseBank dataset annotated by domain\nexperts, we evaluate how various LLMs and prompting strategies align with\nhuman-labeled sentiment in a financial context. We compare three proprietary\nLLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that\nsimulate System 1 (fast and intuitive) or System 2 (slow and deliberate)\nthinking and benchmark them against two smaller models (FinBERT-Prosus,\nFinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest\nthat reasoning, either through prompting or inherent model design, does not\nimprove performance on this task. Surprisingly, the most accurate and\nhuman-aligned combination of model and method was GPT-4o without any\nChain-of-Thought (CoT) prompting. We further explore how performance is\nimpacted by linguistic complexity and annotation agreement levels, uncovering\nthat reasoning may introduce overthinking, leading to suboptimal predictions.\nThis suggests that for financial sentiment classification, fast, intuitive\n\"System 1\"-like thinking aligns more closely with human judgment compared to\n\"System 2\"-style slower, deliberative reasoning simulated by reasoning models\nor CoT prompting. Our results challenge the default assumption that more\nreasoning always leads to better LLM decisions, particularly in high-stakes\nfinancial applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 02:47:23", "updated": "2025-06-05 02:47:23", "pdf_url": "http://arxiv.org/pdf/2506.04574v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04575v1", "title": "Are LLMs Reliable Translators of Logical Reasoning Across Lexically Diversified Contexts?", "authors": ["Qingchuan Li", "Jiatong Li", "Zirui Liu", "Mingyue Cheng", "Yuting Zeng", "Qi Liu", "Tongxuan Liu"], "abstract": "Neuro-symbolic approaches combining large language models (LLMs) with solvers\nexcels in logical reasoning problems need long reasoning chains. In this\nparadigm, LLMs serve as translators, converting natural language reasoning\nproblems into formal logic formulas. Then reliable symbolic solvers return\ncorrect solutions. Despite their success, we find that LLMs, as translators,\nstruggle to handle lexical diversification, a common linguistic phenomenon,\nindicating that LLMs as logic translators are unreliable in real-world\nscenarios. Moreover, existing logical reasoning benchmarks lack lexical\ndiversity, failing to challenge LLMs' ability to translate such text and thus\nobscuring this issue. In this work, we propose SCALe, a benchmark designed to\naddress this significant gap through **logic-invariant lexical\ndiversification**. By using LLMs to transform original benchmark datasets into\nlexically diversified but logically equivalent versions, we evaluate LLMs'\nability to consistently map diverse expressions to uniform logical symbols on\nthese new datasets. Experiments using SCALe further confirm that current LLMs\nexhibit deficiencies in this capability. Building directly on the deficiencies\nidentified through our benchmark, we propose a new method, MenTaL, to address\nthis limitation. This method guides LLMs to first construct a table unifying\ndiverse expressions before performing translation. Applying MenTaL through\nin-context learning and supervised fine-tuning (SFT) significantly improves the\nperformance of LLM translators on lexically diversified text. Our code is now\navailable at https://github.com/wufeiwuwoshihua/LexicalDiver.", "categories": ["cs.CL"], "published": "2025-06-05 02:49:36", "updated": "2025-06-05 02:49:36", "pdf_url": "http://arxiv.org/pdf/2506.04575v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04579v1", "title": "Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching", "authors": ["Jianfei Zhang", "Bei Li", "Jun Bai", "Rumei Li", "Yanmeng Wang", "Chenghua Lin", "Wenge Rong"], "abstract": "In-Context Learning (ICL) empowers Large Language Models (LLMs) for rapid\ntask adaptation without Fine-Tuning (FT), but its reliance on demonstration\nselection remains a critical challenge. While many-shot ICL shows promising\nperformance through scaled demonstrations, the selection method for many-shot\ndemonstrations remains limited to random selection in existing work. Since the\nconventional instance-level retrieval is not suitable for many-shot scenarios,\nwe hypothesize that the data requirements for in-context learning and\nfine-tuning are analogous. To this end, we introduce a novel gradient matching\napproach that selects demonstrations by aligning fine-tuning gradients between\nthe entire training set of the target task and the selected examples, so as to\napproach the learning effect on the entire training set within the selected\nexamples. Through gradient matching on relatively small models, e.g.,\nQwen2.5-3B or Llama3-8B, our method consistently outperforms random selection\non larger LLMs from 4-shot to 128-shot scenarios across 9 diverse datasets. For\ninstance, it surpasses random selection by 4% on Qwen2.5-72B and Llama3-70B,\nand by around 2% on 5 closed-source LLMs. This work unlocks more reliable and\neffective many-shot ICL, paving the way for its broader application.", "categories": ["cs.CL"], "published": "2025-06-05 02:57:05", "updated": "2025-06-05 02:57:05", "pdf_url": "http://arxiv.org/pdf/2506.04579v1", "comment": "accepted to the ACL2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04583v1", "title": "SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing", "authors": ["Hongjun Liu", "Yilun Zhao", "Arman Cohan", "Chen Zhao"], "abstract": "Automatic fact-checking has recently received more attention as a means of\ncombating misinformation. Despite significant advancements, fact-checking\nsystems based on retrieval-augmented language models still struggle to tackle\nadversarial claims, which are intentionally designed by humans to challenge\nfact-checking systems. To address these challenges, we propose a training-free\nmethod designed to rephrase the original claim, making it easier to locate\nsupporting evidence. Our modular framework, SUCEA, decomposes the task into\nthree steps: 1) Claim Segmentation and Decontextualization that segments\nadversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval\nand Claim Editing that iteratively retrieves evidence and edits the subclaim\nbased on the retrieved evidence; 3) Evidence Aggregation and Label Prediction\nthat aggregates all retrieved evidence and predicts the entailment label.\nExperiments on two challenging fact-checking datasets demonstrate that our\nframework significantly improves on both retrieval and entailment label\naccuracy, outperforming four strong claim-decomposition-based baselines.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 02:58:15", "updated": "2025-06-05 02:58:15", "pdf_url": "http://arxiv.org/pdf/2506.04583v1", "comment": "16 pages, 10 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04585v1", "title": "MuSciClaims: Multimodal Scientific Claim Verification", "authors": ["Yash Kumar Lal", "Manikanta Bandham", "Mohammad Saqib Hasan", "Apoorva Kashi", "Mahnaz Koupaee", "Niranjan Balasubramanian"], "abstract": "Assessing scientific claims requires identifying, extracting, and reasoning\nwith multimodal data expressed in information-rich figures in scientific\nliterature. Despite the large body of work in scientific QA, figure captioning,\nand other multimodal reasoning tasks over chart-based data, there are no\nreadily usable multimodal benchmarks that directly test claim verification\nabilities. To remedy this gap, we introduce a new benchmark MuSciClaims\naccompanied by diagnostics tasks. We automatically extract supported claims\nfrom scientific articles, which we manually perturb to produce contradicted\nclaims. The perturbations are designed to test for a specific set of claim\nverification capabilities. We also introduce a suite of diagnostic tasks that\nhelp understand model failures. Our results show most vision-language models\nare poor (~0.3-0.5 F1), with even the best model only achieving 0.77 F1. They\nare also biased towards judging claims as supported, likely misunderstanding\nnuanced perturbations within the claims. Our diagnostics show models are bad at\nlocalizing correct evidence within figures, struggle with aggregating\ninformation across modalities, and often fail to understand basic components of\nthe figure.", "categories": ["cs.CL"], "published": "2025-06-05 02:59:51", "updated": "2025-06-05 02:59:51", "pdf_url": "http://arxiv.org/pdf/2506.04585v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04586v1", "title": "LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models", "authors": ["Wen Ding", "Fan Qian"], "abstract": "We introduce LESS (Large Language Model Enhanced Semi-supervised Learning), a\nversatile framework that leverages Large Language Models (LLMs) to correct\npseudo labels generated from in-the-wild data. Within the LESS framework,\npseudo-labeled text from Automatic Speech Recognition (ASR) or Automatic Speech\nTranslation (AST) of the unsupervised data is refined by an LLM, and augmented\nby a data filtering strategy to optimize LLM knowledge transfer efficiency.\nExperiments on both Mandarin ASR and Spanish-to-English AST tasks show that\nLESS achieves a notable absolute WER reduction of 3.77% on the Wenet Speech\ntest set, as well as BLEU scores of 34.0 and 64.7 on Callhome and Fisher test\nsets respectively. These results validate the adaptability of LESS across\ndifferent languages, tasks, and domains. Ablation studies conducted with\nvarious LLMs and prompt configurations provide novel insights into leveraging\nLLM-derived knowledge for speech processing applications.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-05 03:00:04", "updated": "2025-06-05 03:00:04", "pdf_url": "http://arxiv.org/pdf/2506.04586v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04592v1", "title": "Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification", "authors": ["Chengwu Liu", "Ye Yuan", "Yichun Yin", "Yan Xu", "Xin Xu", "Zaoyu Chen", "Yasheng Wang", "Lifeng Shang", "Qun Liu", "Ming Zhang"], "abstract": "Chain-of-Thought (CoT) prompting has become the de facto method to elicit\nreasoning capabilities from large language models (LLMs). However, to mitigate\nhallucinations in CoT that are notoriously difficult to detect, current methods\nsuch as process reward models (PRMs) or self-consistency operate as opaque\nboxes and do not provide checkable evidence for their judgments, possibly\nlimiting their effectiveness. To address this issue, we draw inspiration from\nthe idea that \"the gold standard for supporting a mathematical claim is to\nprovide a proof\". We propose a retrospective, step-aware formal verification\nframework $Safe$. Rather than assigning arbitrary scores, we strive to\narticulate mathematical claims in formal mathematical language Lean 4 at each\nreasoning step and provide formal proofs to identify hallucinations. We\nevaluate our framework $Safe$ across multiple language models and various\nmathematical datasets, demonstrating a significant performance improvement\nwhile offering interpretable and verifiable evidence. We also propose\n$FormalStep$ as a benchmark for step correctness theorem proving with $30,809$\nformal statements. To the best of our knowledge, our work represents the first\nendeavor to utilize formal mathematical language Lean 4 for verifying natural\nlanguage content generated by LLMs, aligning with the reason why formal\nmathematical languages were created in the first place: to provide a robust\nfoundation for hallucination-prone human-written proofs.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 03:16:08", "updated": "2025-06-05 03:16:08", "pdf_url": "http://arxiv.org/pdf/2506.04592v1", "comment": "Accepted in ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04603v1", "title": "A MISMATCHED Benchmark for Scientific Natural Language Inference", "authors": ["Firoz Shaik", "Mobashir Sadat", "Nikita Gautam", "Doina Caragea", "Cornelia Caragea"], "abstract": "Scientific Natural Language Inference (NLI) is the task of predicting the\nsemantic relation between a pair of sentences extracted from research articles.\nExisting datasets for this task are derived from various computer science (CS)\ndomains, whereas non-CS domains are completely ignored. In this paper, we\nintroduce a novel evaluation benchmark for scientific NLI, called MISMATCHED.\nThe new MISMATCHED benchmark covers three non-CS domains-PSYCHOLOGY,\nENGINEERING, and PUBLIC HEALTH, and contains 2,700 human annotated sentence\npairs. We establish strong baselines on MISMATCHED using both Pre-trained Small\nLanguage Models (SLMs) and Large Language Models (LLMs). Our best performing\nbaseline shows a Macro F1 of only 78.17% illustrating the substantial headroom\nfor future improvements. In addition to introducing the MISMATCHED benchmark,\nwe show that incorporating sentence pairs having an implicit scientific NLI\nrelation between them in model training improves their performance on\nscientific NLI. We make our dataset and code publicly available on GitHub.", "categories": ["cs.CL"], "published": "2025-06-05 03:40:57", "updated": "2025-06-05 03:40:57", "pdf_url": "http://arxiv.org/pdf/2506.04603v1", "comment": "Accepted to Findings of ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04611v1", "title": "Revisiting Test-Time Scaling: A Survey and a Diversity-Aware Method for Efficient Reasoning", "authors": ["Ho-Lam Chung", "Teng-Yun Hsiao", "Hsiao-Ying Huang", "Chunerh Cho", "Jian-Ren Lin", "Zhang Ziwei", "Yun-Nung Chen"], "abstract": "Test-Time Scaling (TTS) improves the reasoning performance of Large Language\nModels (LLMs) by allocating additional compute during inference. We conduct a\nstructured survey of TTS methods and categorize them into sampling-based,\nsearch-based, and trajectory optimization strategies. We observe that\nreasoning-optimized models often produce less diverse outputs, which limits TTS\neffectiveness. To address this, we propose ADAPT (A Diversity Aware Prefix\nfine-Tuning), a lightweight method that applies prefix tuning with a\ndiversity-focused data strategy. Experiments on mathematical reasoning tasks\nshow that ADAPT reaches 80% accuracy using eight times less compute than strong\nbaselines. Our findings highlight the essential role of generative diversity in\nmaximizing TTS effectiveness.", "categories": ["cs.CL"], "published": "2025-06-05 04:02:17", "updated": "2025-06-05 04:02:17", "pdf_url": "http://arxiv.org/pdf/2506.04611v1", "comment": "emnlp 2025 submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04616v1", "title": "Subjective Perspectives within Learned Representations Predict High-Impact Innovation", "authors": ["Likun Cao", "Rui Pan", "James Evans"], "abstract": "Existing studies of innovation emphasize the power of social structures to\nshape innovation capacity. Emerging machine learning approaches, however,\nenable us to model innovators' personal perspectives and interpersonal\ninnovation opportunities as a function of their prior trajectories of\nexperience. We theorize then quantify subjective perspectives and innovation\nopportunities based on innovator positions within the geometric space of\nconcepts inscribed by dynamic language representations. Using data on millions\nof scientists, inventors, writers, entrepreneurs, and Wikipedia contributors\nacross the creative domains of science, technology, film, entrepreneurship, and\nWikipedia, here we show that measured subjective perspectives anticipate what\nideas individuals and groups creatively attend to and successfully combine in\nfuture. When perspective and background diversity are decomposed as the angular\ndifference between collaborators' perspectives on their creation and between\ntheir experiences, the former consistently anticipates creative achievement\nwhile the latter portends its opposite, across all cases and time periods\nexamined. We analyze a natural experiment and simulate creative collaborations\nbetween AI (large language model) agents designed with various perspective and\nbackground diversity, which are consistent with our observational findings. We\nexplore mechanisms underlying these findings and identify how successful\ncollaborators leverage common language to weave together diverse experience\nobtained through trajectories of prior work that converge to provoke one\nanother and innovate. We explore the importance of these findings for team\nassembly and research policy.", "categories": ["cs.CL", "stat.AP", "stat.ML"], "published": "2025-06-05 04:18:53", "updated": "2025-06-05 04:18:53", "pdf_url": "http://arxiv.org/pdf/2506.04616v1", "comment": "107 pages, 20 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04624v1", "title": "Static Word Embeddings for Sentence Semantic Representation", "authors": ["Takashi Wada", "Yuki Hirakawa", "Ryotaro Shimizu", "Takahiro Kawashima", "Yuki Saito"], "abstract": "We propose new static word embeddings optimised for sentence semantic\nrepresentation. We first extract word embeddings from a pre-trained Sentence\nTransformer, and improve them with sentence-level principal component analysis,\nfollowed by either knowledge distillation or contrastive learning. During\ninference, we represent sentences by simply averaging word embeddings, which\nrequires little computational cost. We evaluate models on both monolingual and\ncross-lingual tasks and show that our model substantially outperforms existing\nstatic models on sentence semantic tasks, and even rivals a basic Sentence\nTransformer model (SimCSE) on some data sets. Lastly, we perform a variety of\nanalyses and show that our method successfully removes word embedding\ncomponents that are irrelevant to sentence semantics, and adjusts the vector\nnorms based on the influence of words on sentence semantics.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 04:33:10", "updated": "2025-06-05 04:33:10", "pdf_url": "http://arxiv.org/pdf/2506.04624v1", "comment": "15 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04625v1", "title": "Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning", "authors": ["Zhiyuan Ma", "Jiayu Liu", "Xianzhen Luo", "Zhenya Huang", "Qingfu Zhu", "Wanxiang Che"], "abstract": "Empowering large language models (LLMs) with effective tool utilization\ncapabilities is crucial for enabling AI agents to solve complex problems.\nHowever, current models face two major limitations: (1) unreliable tool\nplanning and invocation due to low-quality instruction datasets (e.g.,\nwidespread hallucinated API calls), and (2) weak tool reflection abilities\n(over 90% of errors cannot be corrected) resulting from static imitation\nlearning. To address these critical limitations, we propose Tool-MVR, a novel\nTool-Augmented LLM that achieves comprehensive System 2 reasoning through two\nkey innovations. Specifically, we first introduce Multi-Agent Meta-Verification\n(MAMV), a systematic pipeline that rigorously validates APIs, queries, and\nreasoning trajectories to construct ToolBench-V, a new high-quality instruction\ndataset that addresses the limitation of unreliable tool planning and\ninvocation. Second, we propose Exploration-based Reflection Learning (EXPLORE),\nwhich enhances tool reflection capabilities by leveraging tool feedback through\na dynamic \"Error -> Reflection -> Correction\" learning paradigm, resulting in\nour reflection dataset ToolBench-R and addressing the critical weakness in tool\nreflection. Finally, we obtain Tool-MVR by finetuning open-source LLMs (e.g.,\nQwen-7B) on both ToolBench-V and ToolBench-R. Our experiments demonstrate that\nTool-MVR achieves state-of-the-art performance on StableToolBench, surpassing\nboth ToolLLM (by 23.9%) and GPT-4 (by 15.3%) while reducing API calls by 31.4%,\nwith strong generalization capabilities across unseen tools and scenarios.\nAdditionally, on our proposed RefineToolBench, the first benchmark specifically\ndesigned to evaluate tool reflection capabilities, Tool-MVR achieves a 58.9%\nerror correction rate, significantly outperforming ToolLLM's 9.1%.", "categories": ["cs.CL"], "published": "2025-06-05 04:35:49", "updated": "2025-06-05 04:35:49", "pdf_url": "http://arxiv.org/pdf/2506.04625v1", "comment": "Accepted at the Research Track of KDD 2025", "doi": "10.1145/3711896.3736835", "journal_ref": null}
{"arxiv_id": "2506.04635v1", "title": "ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual Speech Recognition", "authors": ["Thai-Binh Nguyen", "Thi Van Nguyen", "Quoc Truong Do", "Chi Mai Luong"], "abstract": "Audio-Visual Speech Recognition (AVSR) has gained significant attention\nrecently due to its robustness against noise, which often challenges\nconventional speech recognition systems that rely solely on audio features.\nDespite this advantage, AVSR models remain limited by the scarcity of extensive\ndatasets, especially for most languages beyond English. Automated data\ncollection offers a promising solution. This work presents a practical approach\nto generate AVSR datasets from raw video, refining existing techniques for\nimproved efficiency and accessibility. We demonstrate its broad applicability\nby developing a baseline AVSR model for Vietnamese. Experiments show the\nautomatically collected dataset enables a strong baseline, achieving\ncompetitive performance with robust ASR in clean conditions and significantly\noutperforming them in noisy environments like cocktail parties. This efficient\nmethod provides a pathway to expand AVSR to more languages, particularly\nunder-resourced ones.", "categories": ["cs.CL", "cs.CV"], "published": "2025-06-05 05:13:01", "updated": "2025-06-05 05:13:01", "pdf_url": "http://arxiv.org/pdf/2506.04635v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04642v1", "title": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache Compression and Mean-centering", "authors": ["Vinay Joshi", "Pratik Prabhanjan Brahma", "Zicheng Liu", "Emad Barsoum"], "abstract": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts.", "categories": ["cs.CL"], "published": "2025-06-05 05:23:38", "updated": "2025-06-05 05:23:38", "pdf_url": "http://arxiv.org/pdf/2506.04642v1", "comment": "ACL-2025 industry-track accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04649v1", "title": "Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents", "authors": ["Juhyun Oh", "Eunsu Kim", "Alice Oh"], "abstract": "Real-world planning problems require constant adaptation to changing\nrequirements and balancing of competing constraints. However, current\nbenchmarks for evaluating LLMs' planning capabilities primarily focus on\nstatic, single-turn scenarios. We introduce Flex-TravelPlanner, a benchmark\nthat evaluates language models' ability to reason flexibly in dynamic planning\nscenarios. Building on the TravelPlanner dataset~\\citep{xie2024travelplanner},\nwe introduce two novel evaluation settings: (1) sequential constraint\nintroduction across multiple turns, and (2) scenarios with explicitly\nprioritized competing constraints. Our analysis of GPT-4o and Llama 3.1 70B\nreveals several key findings: models' performance on single-turn tasks poorly\npredicts their ability to adapt plans across multiple turns; constraint\nintroduction order significantly affects performance; and models struggle with\nconstraint prioritization, often incorrectly favoring newly introduced lower\npriority preferences over existing higher-priority constraints. These findings\nhighlight the importance of evaluating LLMs in more realistic, dynamic planning\nscenarios and suggest specific directions for improving model performance on\ncomplex planning tasks. The code and dataset for our framework are publicly\navailable at https://github.com/juhyunohh/FlexTravelBench.", "categories": ["cs.CL"], "published": "2025-06-05 05:31:50", "updated": "2025-06-05 05:31:50", "pdf_url": "http://arxiv.org/pdf/2506.04649v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04652v1", "title": "EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label Speech Emotion Recognition", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Yu-Hsuan Li Liang", "Hung-yi Lee"], "abstract": "Speech emotion recognition (SER) systems often exhibit gender bias. However,\nthe effectiveness and robustness of existing debiasing methods in such\nmulti-label scenarios remain underexplored. To address this gap, we present\nEMO-Debias, a large-scale comparison of 13 debiasing methods applied to\nmulti-label SER. Our study encompasses techniques from pre-processing,\nregularization, adversarial learning, biased learners, and distributionally\nrobust optimization. Experiments conducted on acted and naturalistic emotion\ndatasets, using WavLM and XLSR representations, evaluate each method under\nconditions of gender imbalance. Our analysis quantifies the trade-offs between\nfairness and accuracy, identifying which approaches consistently reduce gender\nperformance gaps without compromising overall model performance. The findings\nprovide actionable insights for selecting effective debiasing strategies and\nhighlight the impact of dataset distributions.", "categories": ["eess.AS", "cs.CL"], "published": "2025-06-05 05:48:31", "updated": "2025-06-05 05:48:31", "pdf_url": "http://arxiv.org/pdf/2506.04652v1", "comment": "8 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04679v1", "title": "Normative Conflicts and Shallow AI Alignment", "authors": ["Rapha\u00ebl Milli\u00e8re"], "abstract": "The progress of AI systems such as large language models (LLMs) raises\nincreasingly pressing concerns about their safe deployment. This paper examines\nthe value alignment problem for LLMs, arguing that current alignment strategies\nare fundamentally inadequate to prevent misuse. Despite ongoing efforts to\ninstill norms such as helpfulness, honesty, and harmlessness in LLMs through\nfine-tuning based on human preferences, they remain vulnerable to adversarial\nattacks that exploit conflicts between these norms. I argue that this\nvulnerability reflects a fundamental limitation of existing alignment methods:\nthey reinforce shallow behavioral dispositions rather than endowing LLMs with a\ngenuine capacity for normative deliberation. Drawing from on research in moral\npsychology, I show how humans' ability to engage in deliberative reasoning\nenhances their resilience against similar adversarial tactics. LLMs, by\ncontrast, lack a robust capacity to detect and rationally resolve normative\nconflicts, leaving them susceptible to manipulation; even recent advances in\nreasoning-focused LLMs have not addressed this vulnerability. This ``shallow\nalignment'' problem carries significant implications for AI safety and\nregulation, suggesting that current approaches are insufficient for mitigating\npotential harms posed by increasingly capable AI systems.", "categories": ["cs.CL"], "published": "2025-06-05 06:57:28", "updated": "2025-06-05 06:57:28", "pdf_url": "http://arxiv.org/pdf/2506.04679v1", "comment": "Published in Philosophical Studies", "doi": "10.1007/s11098-025-02347-3", "journal_ref": "Milliere, R. (2025). Normative conflicts and shallow AI alignment.\n  Philosophical Studies, 1-44"}
{"arxiv_id": "2506.04681v1", "title": "Urania: Differentially Private Insights into AI Use", "authors": ["Daogao Liu", "Edith Cohen", "Badih Ghazi", "Peter Kairouz", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi", "Adam Sealfon", "Da Yu", "Chiyuan Zhang"], "abstract": "We introduce $Urania$, a novel framework for generating insights about LLM\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\nframework employs a private clustering mechanism and innovative keyword\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\napproaches. By leveraging DP tools such as clustering, partition selection, and\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair\nsimilarity, and LLM-based metrics, benchmarking against a non-private\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\nempirical privacy evaluation that demonstrates the enhanced robustness of our\nDP pipeline. The results show the framework's ability to extract meaningful\nconversational insights while maintaining stringent user privacy, effectively\nbalancing data utility with privacy preservation.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CY"], "published": "2025-06-05 07:00:31", "updated": "2025-06-05 07:00:31", "pdf_url": "http://arxiv.org/pdf/2506.04681v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04688v1", "title": "MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models", "authors": ["Gio Paik", "Geewook Kim", "Jinbae Im"], "abstract": "This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-06-05 07:11:36", "updated": "2025-06-05 07:11:36", "pdf_url": "http://arxiv.org/pdf/2506.04688v1", "comment": "ACL Findings 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04689v1", "title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models", "authors": ["Thao Nguyen", "Yang Li", "Olga Golovneva", "Luke Zettlemoyer", "Sewoong Oh", "Ludwig Schmidt", "Xian Li"], "abstract": "Scaling laws predict that the performance of large language models improves\nwith increasing model size and data size. In practice, pre-training has been\nrelying on massive web crawls, using almost all data sources publicly available\non the internet so far. However, this pool of natural data does not grow at the\nsame rate as the compute supply. Furthermore, the availability of high-quality\ntexts is even more limited: data filtering pipelines often remove up to 99% of\nthe initial web scrapes to achieve state-of-the-art. To address the \"data wall\"\nof pre-training scaling, our work explores ways to transform and recycle data\ndiscarded in existing filtering processes. We propose REWIRE, REcycling the Web\nwith guIded REwrite, a method to enrich low-quality documents so that they\ncould become useful for training. This in turn allows us to increase the\nrepresentation of synthetic data in the final pre-training set. Experiments at\n1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw\ntexts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points\nimprovement respectively across 22 diverse tasks, compared to training on only\nfiltered web data. Training on the raw-synthetic data mix is also more\neffective than having access to 2x web data. Through further analysis, we\ndemonstrate that about 82% of the mixed in texts come from transforming\nlower-quality documents that would otherwise be discarded. REWIRE also\noutperforms related approaches of generating synthetic data, including\nWikipedia-style paraphrasing, question-answer synthesizing and knowledge\nextraction. These results suggest that recycling web texts holds the potential\nfor being a simple and effective approach for scaling pre-training data.", "categories": ["cs.CL", "cs.LG"], "published": "2025-06-05 07:12:12", "updated": "2025-06-05 07:12:12", "pdf_url": "http://arxiv.org/pdf/2506.04689v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04693v1", "title": "Cracking the Code: Enhancing Implicit Hate Speech Detection through Coding Classification", "authors": ["Lu Wei", "Liangzhi Li", "Tong Xiang", "Xiao Liu", "Noa Garcia"], "abstract": "The internet has become a hotspot for hate speech (HS), threatening societal\nharmony and individual well-being. While automatic detection methods perform\nwell in identifying explicit hate speech (ex-HS), they struggle with more\nsubtle forms, such as implicit hate speech (im-HS). We tackle this problem by\nintroducing a new taxonomy for im-HS detection, defining six encoding\nstrategies named codetypes. We present two methods for integrating codetypes\ninto im-HS detection: 1) prompting large language models (LLMs) directly to\nclassify sentences based on generated responses, and 2) using LLMs as encoders\nwith codetypes embedded during the encoding process. Experiments show that the\nuse of codetypes improves im-HS detection in both Chinese and English datasets,\nvalidating the effectiveness of our approach across different languages.", "categories": ["cs.CL"], "published": "2025-06-05 07:15:21", "updated": "2025-06-05 07:15:21", "pdf_url": "http://arxiv.org/pdf/2506.04693v1", "comment": "Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025),\n  112-126", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04708v1", "title": "Accelerated Test-Time Scaling with Model-Free Speculative Sampling", "authors": ["Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "abstract": "Language models have demonstrated remarkable capabilities in reasoning tasks\nthrough test-time scaling techniques like best-of-N sampling and tree search.\nHowever, these approaches often demand substantial computational resources,\ncreating a critical trade-off between performance and efficiency. We introduce\nSTAND (STochastic Adaptive N-gram Drafting), a novel model-free speculative\ndecoding approach that leverages the inherent redundancy in reasoning\ntrajectories to achieve significant acceleration without compromising accuracy.\nOur analysis reveals that reasoning paths frequently reuse similar reasoning\npatterns, enabling efficient model-free token prediction without requiring\nseparate draft models. By introducing stochastic drafting and preserving\nprobabilistic information through a memory-efficient logit-based N-gram module,\ncombined with optimized Gumbel-Top-K sampling and data-driven tree\nconstruction, STAND significantly improves token acceptance rates. Extensive\nevaluations across multiple models and reasoning tasks (AIME-2024,\nGPQA-Diamond, and LiveCodeBench) demonstrate that STAND reduces inference\nlatency by 60-65% compared to standard autoregressive decoding while\nmaintaining accuracy. Furthermore, STAND outperforms state-of-the-art\nspeculative decoding methods by 14-28% in throughput and shows strong\nperformance even in single-trajectory scenarios, reducing inference latency by\n48-58%. As a model-free approach, STAND can be applied to any existing language\nmodel without additional training, being a powerful plug-and-play solution for\naccelerating language model reasoning.", "categories": ["cs.CL"], "published": "2025-06-05 07:31:18", "updated": "2025-06-05 07:31:18", "pdf_url": "http://arxiv.org/pdf/2506.04708v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04711v1", "title": "LLM-based phoneme-to-grapheme for phoneme-based speech recognition", "authors": ["Te Ma", "Min Bi", "Saierdaer Yusuyin", "Hao Huang", "Zhijian Ou"], "abstract": "In automatic speech recognition (ASR), phoneme-based multilingual\npre-training and crosslingual fine-tuning is attractive for its high data\nefficiency and competitive results compared to subword-based models. However,\nWeighted Finite State Transducer (WFST) based decoding is limited by its\ncomplex pipeline and inability to leverage large language models (LLMs).\nTherefore, we propose LLM-based phoneme-to-grapheme (LLM-P2G) decoding for\nphoneme-based ASR, consisting of speech-to-phoneme (S2P) and\nphoneme-to-grapheme (P2G). A challenge is that there seems to have information\nloss in cascading S2P and P2G. To address this challenge, we propose two\ntraining strategies: data augmentation with noisy phonemes (DANP), and\nrandomized top-$K$ marginalized (TKM) training and decoding. Our experimental\nresults show that LLM-P2G outperforms WFST-based systems in crosslingual ASR\nfor Polish and German, by relative WER reductions of 3.6% and 6.9%\nrespectively.", "categories": ["cs.SD", "cs.CL", "eess.AS"], "published": "2025-06-05 07:35:55", "updated": "2025-06-05 07:35:55", "pdf_url": "http://arxiv.org/pdf/2506.04711v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04714v1", "title": "IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech translation", "authors": ["Bhavana Akkiraju", "Aishwarya Pothula", "Santosh Kesiraju", "Anil Kumar Vuppala"], "abstract": "This paper presents the submission of IIITH-BUT to the IWSLT 2025 shared task\non speech translation for the low-resource Bhojpuri-Hindi language pair. We\nexplored the impact of hyperparameter optimisation and data augmentation\ntechniques on the performance of the SeamlessM4T model fine-tuned for this\nspecific task. We systematically investigated a range of hyperparameters\nincluding learning rate schedules, number of update steps, warm-up steps, label\nsmoothing, and batch sizes; and report their effect on translation quality. To\naddress data scarcity, we applied speed perturbation and SpecAugment and\nstudied their effect on translation quality. We also examined the use of\ncross-lingual signal through joint training with Marathi and Bhojpuri speech\ndata. Our experiments reveal that careful selection of hyperparameters and the\napplication of simple yet effective augmentation techniques significantly\nimprove performance in low-resource settings. We also analysed the translation\nhypotheses to understand various kinds of errors that impacted the translation\nquality in terms of BLEU.", "categories": ["cs.CL", "eess.AS"], "published": "2025-06-05 07:38:01", "updated": "2025-06-05 07:38:01", "pdf_url": "http://arxiv.org/pdf/2506.04714v1", "comment": "Paper is accepted to IWSLT2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04721v1", "title": "SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat", "authors": ["Yuru Jiang", "Wenxuan Ding", "Shangbin Feng", "Greg Durrett", "Yulia Tsvetkov"], "abstract": "We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs\nthrough competition and combat. To complement a single model's lack of\ndiversity in generation and biases in evaluation, multiple LLMs form a \"sparta\ntribe\" to compete against each other in fulfilling instructions while serving\nas judges for the competition of others. For each iteration, one instruction\nand two models are selected for a duel, the other models evaluate the two\nresponses, and their evaluation scores are aggregated through a adapted\nelo-ranking based reputation system, where winners/losers of combat gain/lose\nweight in evaluating others. The peer-evaluated combat results then become\npreference pairs where the winning response is preferred over the losing one,\nand all models learn from these preferences at the end of each iteration.\nSPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative\nand collective competition process. Extensive experiments demonstrate that\nSPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines\nacross 10 out of 12 tasks and datasets with 7.0% average improvement. Further\nanalysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen\ntasks and leverages the expertise diversity of participating models to produce\nmore logical, direct and informative outputs.", "categories": ["cs.CL"], "published": "2025-06-05 07:51:23", "updated": "2025-06-05 07:51:23", "pdf_url": "http://arxiv.org/pdf/2506.04721v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04734v1", "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design", "authors": ["Lin Sun", "Weihong Lin", "Jinzhu Wu", "Yongfu Zhu", "Xiaoqi Jian", "Guangxiang Zhao", "Change Jia", "Linglin Zhang", "Sai-er Hu", "Yuhan Wu", "Xiangzheng Zhang"], "abstract": "Reasoning models represented by the Deepseek-R1-Distill series have been\nwidely adopted by the open-source community due to their strong performance in\nmathematics, science, programming, and other domains. However, our study\nreveals that their benchmark evaluation results are subject to significant\nfluctuations caused by various factors. Subtle differences in evaluation\nconditions can lead to substantial variations in results. Similar phenomena are\nobserved in other open-source inference models fine-tuned based on the\nDeepseek-R1-Distill series, as well as in the QwQ-32B model, making their\nclaimed performance improvements difficult to reproduce reliably. Therefore, we\nadvocate for the establishment of a more rigorous paradigm for model\nperformance evaluation and present our empirical assessments of the\nDeepseek-R1-Distill series models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-05 08:09:11", "updated": "2025-06-05 08:09:11", "pdf_url": "http://arxiv.org/pdf/2506.04734v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04739v1", "title": "Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection", "authors": ["Ziyi Zhou", "Xiaoming Zhang", "Litian Zhang", "Yibo Zhang", "Zhenyu Guan", "Chaozhuo Li", "Philip S. Yu"], "abstract": "The widespread dissemination of fake news on social media has significantly\nimpacted society, resulting in serious consequences. Conventional deep learning\nmethodologies employing small language models (SLMs) suffer from extensive\nsupervised training requirements and difficulties adapting to evolving news\nenvironments due to data scarcity and distribution shifts. Large language\nmodels (LLMs), despite robust zero-shot capabilities, fall short in accurately\ndetecting fake news owing to outdated knowledge and the absence of suitable\ndemonstrations. In this paper, we propose a novel Continuous Collaborative\nEmergent Fake News Detection (C$^2$EFND) framework to address these challenges.\nThe C$^2$EFND framework strategically leverages both LLMs' generalization power\nand SLMs' classification expertise via a multi-round collaborative learning\nframework. We further introduce a lifelong knowledge editing module based on a\nMixture-of-Experts architecture to incrementally update LLMs and a replay-based\ncontinue learning method to ensure SLMs retain prior knowledge without\nretraining entirely. Extensive experiments on Pheme and Twitter16 datasets\ndemonstrate that C$^2$EFND significantly outperforms existed methods,\neffectively improving detection accuracy and adaptability in continuous\nemergent fake news scenarios.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 08:17:55", "updated": "2025-06-05 08:17:55", "pdf_url": "http://arxiv.org/pdf/2506.04739v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04760v1", "title": "Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "abstract": "Large Language Models (LLMs) have shown potential in generating hypothetical\ndocuments for query expansion, thereby enhancing information retrieval\nperformance. However, the efficacy of this method is highly dependent on the\nquality of the generated documents, which often requires complex prompt\nstrategies and the integration of advanced dense retrieval techniques. This can\nbe both costly and computationally intensive. To mitigate these limitations, we\nexplore the use of zero-shot LLM-based query expansion to improve sparse\nretrieval, particularly for learned sparse retrievers. We introduce a novel\nfusion ranking framework, Exp4Fuse, which enhances the performance of sparse\nretrievers through an indirect application of zero-shot LLM-based query\nexpansion. Exp4Fuse operates by simultaneously considering two retrieval\nroutes-one based on the original query and the other on the LLM-augmented\nquery. It then generates two ranked lists using a sparse retriever and fuses\nthem using a modified reciprocal rank fusion method. We conduct extensive\nevaluations of Exp4Fuse against leading LLM-based query expansion methods and\nadvanced retrieval techniques on three MS MARCO-related datasets and seven\nlow-resource datasets. Experimental results reveal that Exp4Fuse not only\nsurpasses existing LLM-based query expansion methods in enhancing sparse\nretrievers but also, when combined with advanced sparse retrievers, achieves\nSOTA results on several benchmarks. This highlights the superior performance\nand effectiveness of Exp4Fuse in improving query expansion for sparse\nretrieval.", "categories": ["cs.IR", "cs.CL"], "published": "2025-06-05 08:44:34", "updated": "2025-06-05 08:44:34", "pdf_url": "http://arxiv.org/pdf/2506.04760v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04762v1", "title": "GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "abstract": "Large language models (LLMs)-based query expansion for information retrieval\naugments queries with generated hypothetical documents with LLMs. However, its\nperformance relies heavily on the scale of the language models (LMs),\nnecessitating larger, more advanced LLMs. This approach is costly,\ncomputationally intensive, and often has limited accessibility. To address\nthese limitations, we introduce GOLFer - Smaller LMs-Generated Documents\nHallucination Filter & Combiner - a novel method leveraging smaller open-source\nLMs for query expansion. GOLFer comprises two modules: a hallucination filter\nand a documents combiner. The former detects and removes non-factual and\ninconsistent sentences in generated documents, a common issue with smaller LMs,\nwhile the latter combines the filtered content with the query using a weight\nvector to balance their influence. We evaluate GOLFer alongside dominant\nLLM-based query expansion methods on three web search and ten low-resource\ndatasets. Experimental results demonstrate that GOLFer consistently outperforms\nother methods using smaller LMs, and maintains competitive performance against\nmethods using large-size LLMs, demonstrating its effectiveness.", "categories": ["cs.IR", "cs.CL"], "published": "2025-06-05 08:45:48", "updated": "2025-06-05 08:45:48", "pdf_url": "http://arxiv.org/pdf/2506.04762v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04772v1", "title": "Identifying Reliable Evaluation Metrics for Scientific Text Revision", "authors": ["L\u00e9ane Jourdan", "Florian Boudin", "Richard Dufour", "Nicolas Hernandez"], "abstract": "Evaluating text revision in scientific writing remains a challenge, as\ntraditional metrics such as ROUGE and BERTScore primarily focus on similarity\nrather than capturing meaningful improvements. In this work, we analyse and\nidentify the limitations of these metrics and explore alternative evaluation\nmethods that better align with human judgments. We first conduct a manual\nannotation study to assess the quality of different revisions. Then, we\ninvestigate reference-free evaluation metrics from related NLP domains.\nAdditionally, we examine LLM-as-a-judge approaches, analysing their ability to\nassess revisions with and without a gold reference. Our results show that LLMs\neffectively assess instruction-following but struggle with correctness, while\ndomain-specific metrics provide complementary insights. We find that a hybrid\napproach combining LLM-as-a-judge evaluation and task-specific metrics offers\nthe most reliable assessment of revision quality.", "categories": ["cs.CL"], "published": "2025-06-05 09:00:23", "updated": "2025-06-05 09:00:23", "pdf_url": "http://arxiv.org/pdf/2506.04772v1", "comment": "Accepted to ACL 2025 main", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04774v1", "title": "Fine-Grained Interpretation of Political Opinions in Large Language Models", "authors": ["Jingyu Hu", "Mengyue Yang", "Mengnan Du", "Weiru Liu"], "abstract": "Studies of LLMs' political opinions mainly rely on evaluations of their\nopen-ended responses. Recent work indicates that there is a misalignment\nbetween LLMs' responses and their internal intentions. This motivates us to\nprobe LLMs' internal mechanisms and help uncover their internal political\nstates. Additionally, we found that the analysis of LLMs' political opinions\noften relies on single-axis concepts, which can lead to concept confounds. In\nthis work, we extend the single-axis to multi-dimensions and apply\ninterpretable representation engineering techniques for more transparent LLM\npolitical concept learning. Specifically, we designed a four-dimensional\npolitical learning framework and constructed a corresponding dataset for\nfine-grained political concept vector learning. These vectors can be used to\ndetect and intervene in LLM internals. Experiments are conducted on eight\nopen-source LLMs with three representation engineering techniques. Results show\nthese vectors can disentangle political concept confounds. Detection tasks\nvalidate the semantic meaning of the vectors and show good generalization and\nrobustness in OOD settings. Intervention Experiments show these vectors can\nintervene in LLMs to generate responses with different political leanings.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 09:06:59", "updated": "2025-06-05 09:06:59", "pdf_url": "http://arxiv.org/pdf/2506.04774v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04779v1", "title": "MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning Benchmark", "authors": ["Dingdong Wang", "Jincenzi Wu", "Junan Li", "Dongchao Yang", "Xueyuan Chen", "Tianhua Zhang", "Helen Meng"], "abstract": "Speech inherently contains rich acoustic information that extends far beyond\nthe textual language. In real-world spoken language understanding, effective\ninterpretation often requires integrating semantic meaning (e.g., content),\nparalinguistic features (e.g., emotions, speed, pitch) and phonological\ncharacteristics (e.g., prosody, intonation, rhythm), which are embedded in\nspeech. While recent multimodal Speech Large Language Models (SpeechLLMs) have\ndemonstrated remarkable capabilities in processing audio information, their\nability to perform fine-grained perception and complex reasoning in natural\nspeech remains largely unexplored. To address this gap, we introduce MMSU, a\ncomprehensive benchmark designed specifically for understanding and reasoning\nin spoken language. MMSU comprises 5,000 meticulously curated\naudio-question-answer triplets across 47 distinct tasks. To ground our\nbenchmark in linguistic theory, we systematically incorporate a wide range of\nlinguistic phenomena, including phonetics, prosody, rhetoric, syntactics,\nsemantics, and paralinguistics. Through a rigorous evaluation of 14 advanced\nSpeechLLMs, we identify substantial room for improvement in existing models,\nhighlighting meaningful directions for future optimization. MMSU establishes a\nnew standard for comprehensive assessment of spoken language understanding,\nproviding valuable insights for developing more sophisticated human-AI speech\ninteraction systems. MMSU benchmark is available at\nhttps://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available\nat https://github.com/dingdongwang/MMSU_Bench.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-05 09:09:36", "updated": "2025-06-05 09:09:36", "pdf_url": "http://arxiv.org/pdf/2506.04779v1", "comment": "MMSU benchmark is available at\n  https://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available\n  at https://github.com/dingdongwang/MMSU_Bench", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04788v1", "title": "Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques", "authors": ["Jisu An", "Junseok Lee", "Jeoungeun Lee", "Yongseok Son"], "abstract": "The rapid progress of Multimodal Large Language Models(MLLMs) has transformed\nthe AI landscape. These models combine pre-trained LLMs with various modality\nencoders. This integration requires a systematic understanding of how different\nmodalities connect to the language backbone. Our survey presents an LLM-centric\nanalysis of current approaches. We examine methods for transforming and\naligning diverse modal inputs into the language embedding space. This addresses\na significant gap in existing literature. We propose a classification framework\nfor MLLMs based on three key dimensions. First, we examine architectural\nstrategies for modality integration. This includes both the specific\nintegration mechanisms and the fusion level. Second, we categorize\nrepresentation learning techniques as either joint or coordinate\nrepresentations. Third, we analyze training paradigms, including training\nstrategies and objective functions. By examining 125 MLLMs developed between\n2021 and 2025, we identify emerging patterns in the field. Our taxonomy\nprovides researchers with a structured overview of current integration\ntechniques. These insights aim to guide the development of more robust\nmultimodal integration strategies for future models built on pre-trained\nfoundations.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 09:14:41", "updated": "2025-06-05 09:14:41", "pdf_url": "http://arxiv.org/pdf/2506.04788v1", "comment": "18 pages, 3 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04810v1", "title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study", "authors": ["Yujun Zhou", "Jiayi Ye", "Zipeng Ling", "Yufei Han", "Yue Huang", "Haomin Zhuang", "Zhenwen Liang", "Kehan Guo", "Taicheng Guo", "Xiangqi Wang", "Xiangliang Zhang"], "abstract": "Logical reasoning is a core capability for many applications of large\nlanguage models (LLMs), yet existing benchmarks often rely solely on\nfinal-answer accuracy, failing to capture the quality and structure of the\nreasoning process. We propose FineLogic, a fine-grained evaluation framework\nthat assesses logical reasoning across three dimensions: overall benchmark\naccuracy, stepwise soundness, and representation-level alignment. In addition,\nto better understand how reasoning capabilities emerge, we conduct a\ncomprehensive study on the effects of supervision format during fine-tuning. We\nconstruct four supervision styles (one natural language and three symbolic\nvariants) and train LLMs under each. Our findings reveal that natural language\nsupervision yields strong generalization even on out-of-distribution and\nlong-context tasks, while symbolic reasoning styles promote more structurally\nsound and atomic inference chains. Further, our representation-level probing\nshows that fine-tuning primarily improves reasoning behaviors through\nstep-by-step generation, rather than enhancing shortcut prediction or\ninternalized correctness. Together, our framework and analysis provide a more\nrigorous and interpretable lens for evaluating and improving logical reasoning\nin LLMs.", "categories": ["cs.CL", "cs.AI", "cs.LO"], "published": "2025-06-05 09:34:12", "updated": "2025-06-05 09:34:12", "pdf_url": "http://arxiv.org/pdf/2506.04810v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04811v1", "title": "Design of intelligent proofreading system for English translation based on CNN and BERT", "authors": ["Feijun Liu", "Huifeng Wang", "Kun Wang", "Yizhen Wang"], "abstract": "Since automatic translations can contain errors that require substantial\nhuman post-editing, machine translation proofreading is essential for improving\nquality. This paper proposes a novel hybrid approach for robust proofreading\nthat combines convolutional neural networks (CNN) with Bidirectional Encoder\nRepresentations from Transformers (BERT). In order to extract semantic\ninformation from phrases and expressions, CNN uses a variety of convolution\nkernel filters to capture local n-gram patterns. In the meanwhile, BERT creates\ncontext-rich representations of whole sequences by utilizing stacked\nbidirectional transformer encoders. Using BERT's attention processes, the\nintegrated error detection component relates tokens to spot translation\nirregularities including word order problems and omissions. The correction\nmodule then uses parallel English-German alignment and GRU decoder models in\nconjunction with translation memory to propose logical modifications that\nmaintain original meaning. A unified end-to-end training process optimized for\npost-editing performance is applied to the whole pipeline. The multi-domain\ncollection of WMT and the conversational dialogues of Open-Subtitles are two of\nthe English-German parallel corpora used to train the model. Multiple loss\nfunctions supervise detection and correction capabilities. Experiments attain a\n90% accuracy, 89.37% F1, and 16.24% MSE, exceeding recent proofreading\ntechniques by over 10% overall. Comparative benchmarking demonstrates\nstate-of-the-art performance in identifying and coherently rectifying\nmistranslations and omissions.", "categories": ["cs.CL"], "published": "2025-06-05 09:34:42", "updated": "2025-06-05 09:34:42", "pdf_url": "http://arxiv.org/pdf/2506.04811v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04822v1", "title": "Evaluating Vision-Language and Large Language Models for Automated Student Assessment in Indonesian Classrooms", "authors": ["Nurul Aisyah", "Muhammad Dehan Al Kautsar", "Arif Hidayat", "Raqib Chowdhury", "Fajri Koto"], "abstract": "Although vision-language and large language models (VLM and LLM) offer\npromising opportunities for AI-driven educational assessment, their\neffectiveness in real-world classroom settings, particularly in\nunderrepresented educational contexts, remains underexplored. In this study, we\nevaluated the performance of a state-of-the-art VLM and several LLMs on 646\nhandwritten exam responses from grade 4 students in six Indonesian schools,\ncovering two subjects: Mathematics and English. These sheets contain more than\n14K student answers that span multiple choice, short answer, and essay\nquestions. Assessment tasks include grading these responses and generating\npersonalized feedback. Our findings show that the VLM often struggles to\naccurately recognize student handwriting, leading to error propagation in\ndownstream LLM grading. Nevertheless, LLM-generated feedback retains some\nutility, even when derived from imperfect input, although limitations in\npersonalization and contextual relevance persist.", "categories": ["cs.CL"], "published": "2025-06-05 09:41:09", "updated": "2025-06-05 09:41:09", "pdf_url": "http://arxiv.org/pdf/2506.04822v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04824v1", "title": "A Reasoning-Based Approach to Cryptic Crossword Clue Solving", "authors": ["Martin Andrews", "Sam Witteveen"], "abstract": "Cryptic crossword clues are challenging language tasks for which new test\nsets are released daily by major newspapers on a global basis. Each cryptic\nclue contains both the definition of the answer to be placed in the crossword\ngrid (in common with regular crosswords), and 'wordplay' that proves that the\nanswer is correct (i.e. a human solver can be confident that an answer is\ncorrect without needing crossing words as confirmation). This work describes an\nLLM-based reasoning system built from open-licensed components that solves\ncryptic clues by (i) hypothesising answers; (ii) proposing wordplay\nexplanations; and (iii) using a verifier system that operates on codified\nreasoning steps. Overall, this system establishes a new state-of-the-art\nperformance on the challenging Cryptonite dataset of clues from The Times and\nThe Telegraph newspapers in the UK. Because each proved solution is expressed\nin Python, interpretable wordplay reasoning for proven answers is available for\ninspection.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 09:43:28", "updated": "2025-06-05 09:43:28", "pdf_url": "http://arxiv.org/pdf/2506.04824v1", "comment": "9 page paper plus Appendices. Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04831v1", "title": "From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health Trajectories with LLMs", "authors": ["Chantal Pellegrini", "Ege \u00d6zsoy", "David Bani-Harouni", "Matthias Keicher", "Nassir Navab"], "abstract": "Healthcare systems face significant challenges in managing and interpreting\nvast, heterogeneous patient data for personalized care. Existing approaches\noften focus on narrow use cases with a limited feature space, overlooking the\ncomplex, longitudinal interactions needed for a holistic understanding of\npatient health. In this work, we propose a novel approach to patient pathway\nmodeling by transforming diverse electronic health record (EHR) data into a\nstructured representation and designing a holistic pathway prediction model,\nEHR2Path, optimized to predict future health trajectories. Further, we\nintroduce a novel summary mechanism that embeds long-term temporal context into\ntopic-specific summary tokens, improving performance over text-only models,\nwhile being much more token-efficient. EHR2Path demonstrates strong performance\nin both next time-step prediction and longitudinal simulation, outperforming\ncompetitive baselines. It enables detailed simulations of patient trajectories,\ninherently targeting diverse evaluation tasks, such as forecasting vital signs,\nlab test results, or length-of-stay, opening a path towards predictive and\npersonalized healthcare.", "categories": ["cs.LG", "cs.CL"], "published": "2025-06-05 09:54:01", "updated": "2025-06-05 09:54:01", "pdf_url": "http://arxiv.org/pdf/2506.04831v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04832v1", "title": "Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models", "authors": ["Changyue Wang", "Weihang Su", "Qingyao Ai", "Yiqun Liu"], "abstract": "Large Reasoning Models (LRMs) extend large language models with explicit,\nmulti-step reasoning traces to enhance transparency and performance on complex\ntasks. However, these reasoning traces can be redundant or logically\ninconsistent, making them a new source of hallucination that is difficult to\ndetect. Existing hallucination detection methods focus primarily on\nanswer-level uncertainty and often fail to detect hallucinations or logical\ninconsistencies arising from the model's reasoning trace. This oversight is\nparticularly problematic for LRMs, where the explicit thinking trace is not\nonly an important support to the model's decision-making process but also a key\nsource of potential hallucination. To this end, we propose RACE (Reasoning and\nAnswer Consistency Evaluation), a novel framework specifically tailored for\nhallucination detection in LRMs. RACE operates by extracting essential\nreasoning steps and computing four diagnostic signals: inter-sample consistency\nof reasoning traces, entropy-based answer uncertainty, semantic alignment\nbetween reasoning and answers, and internal coherence of reasoning. This joint\nanalysis enables fine-grained hallucination detection even when the final\nanswer appears correct. Experiments across datasets and different LLMs\ndemonstrate that RACE outperforms existing hallucination detection baselines,\noffering a robust and generalizable solution for evaluating LRMs. Our code is\navailable at: https://github.com/bebr2/RACE.", "categories": ["cs.CL"], "published": "2025-06-05 09:54:04", "updated": "2025-06-05 09:54:04", "pdf_url": "http://arxiv.org/pdf/2506.04832v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04848v1", "title": "MockConf: A Student Interpretation Dataset: Analysis, Word- and Span-level Alignment and Baselines", "authors": ["D\u00e1vid Javorsk\u00fd", "Ond\u0159ej Bojar", "Fran\u00e7ois Yvon"], "abstract": "In simultaneous interpreting, an interpreter renders a source speech into\nanother language with a very short lag, much sooner than sentences are\nfinished. In order to understand and later reproduce this dynamic and complex\ntask automatically, we need dedicated datasets and tools for analysis,\nmonitoring, and evaluation, such as parallel speech corpora, and tools for\ntheir automatic annotation. Existing parallel corpora of translated texts and\nassociated alignment algorithms hardly fill this gap, as they fail to model\nlong-range interactions between speech segments or specific types of\ndivergences (e.g., shortening, simplification, functional generalization)\nbetween the original and interpreted speeches. In this work, we introduce\nMockConf, a student interpreting dataset that was collected from Mock\nConferences run as part of the students' curriculum. This dataset contains 7\nhours of recordings in 5 European languages, transcribed and aligned at the\nlevel of spans and words. We further implement and release InterAlign, a modern\nweb-based annotation tool for parallel word and span annotations on long\ninputs, suitable for aligning simultaneous interpreting. We propose metrics for\nthe evaluation and a baseline for automatic alignment. Dataset and tools are\nreleased to the community.", "categories": ["cs.CL"], "published": "2025-06-05 10:16:15", "updated": "2025-06-05 10:16:15", "pdf_url": "http://arxiv.org/pdf/2506.04848v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04851v1", "title": "Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights", "authors": ["Giorgio Biancini", "Alessio Ferrato", "Carla Limongelli"], "abstract": "Integrating Artificial Intelligence (AI) in educational settings has brought\nnew learning approaches, transforming the practices of both students and\neducators. Among the various technologies driving this transformation, Large\nLanguage Models (LLMs) have emerged as powerful tools for creating educational\nmaterials and question answering, but there are still space for new\napplications. Educators commonly use Multiple-Choice Questions (MCQs) to assess\nstudent knowledge, but manually generating these questions is\nresource-intensive and requires significant time and cognitive effort. In our\nopinion, LLMs offer a promising solution to these challenges. This paper\npresents a novel comparative analysis of three widely known LLMs - Llama 2,\nMistral, and GPT-3.5 - to explore their potential for creating informative and\nchallenging MCQs. In our approach, we do not rely on the knowledge of the LLM,\nbut we inject the knowledge into the prompt to contrast the hallucinations,\ngiving the educators control over the test's source text, too. Our experiment\ninvolving 21 educators shows that GPT-3.5 generates the most effective MCQs\nacross several known metrics. Additionally, it shows that there is still some\nreluctance to adopt AI in the educational field. This study sheds light on the\npotential of LLMs to generate MCQs and improve the educational experience,\nproviding valuable insights for the future.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 10:21:49", "updated": "2025-06-05 10:21:49", "pdf_url": "http://arxiv.org/pdf/2506.04851v1", "comment": "Copyright ACM 2024. This is the author's version of the work. It is\n  posted here for your personal use. Not for redistribution. The definitive\n  Version of Record was published in Adjunct Proceedings of the 32nd ACM\n  Conference on User Modeling, Adaptation and Personalization (UMAP Adjunct\n  '24), http://dx.doi.org/10.1145/3631700.3665233", "doi": "10.1145/3631700.3665233", "journal_ref": null}
{"arxiv_id": "2506.04855v1", "title": "Prompting LLMs: Length Control for Isometric Machine Translation", "authors": ["D\u00e1vid Javorsk\u00fd", "Ond\u0159ej Bojar", "Fran\u00e7ois Yvon"], "abstract": "In this study, we explore the effectiveness of isometric machine translation\nacross multiple language pairs (En$\\to$De, En$\\to$Fr, and En$\\to$Es) under the\nconditions of the IWSLT Isometric Shared Task 2022. Using eight open-source\nlarge language models (LLMs) of varying sizes, we investigate how different\nprompting strategies, varying numbers of few-shot examples, and demonstration\nselection influence translation quality and length control. We discover that\nthe phrasing of instructions, when aligned with the properties of the provided\ndemonstrations, plays a crucial role in controlling the output length. Our\nexperiments show that LLMs tend to produce shorter translations only when\npresented with extreme examples, while isometric demonstrations often lead to\nthe models disregarding length constraints. While few-shot prompting generally\nenhances translation quality, further improvements are marginal across 5, 10,\nand 20-shot settings. Finally, considering multiple outputs allows to notably\nimprove overall tradeoff between the length and quality, yielding\nstate-of-the-art performance for some language pairs.", "categories": ["cs.CL"], "published": "2025-06-05 10:24:08", "updated": "2025-06-05 10:24:08", "pdf_url": "http://arxiv.org/pdf/2506.04855v1", "comment": "Accepted to IWSLT 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04887v1", "title": "Evaluating the Effectiveness of Linguistic Knowledge in Pretrained Language Models: A Case Study of Universal Dependencies", "authors": ["Wenxi Li"], "abstract": "Universal Dependencies (UD), while widely regarded as the most successful\nlinguistic framework for cross-lingual syntactic representation, remains\nunderexplored in terms of its effectiveness. This paper addresses this gap by\nintegrating UD into pretrained language models and assesses if UD can improve\ntheir performance on a cross-lingual adversarial paraphrase identification\ntask. Experimental results show that incorporation of UD yields significant\nimprovements in accuracy and $F_1$ scores, with average gains of 3.85\\% and\n6.08\\% respectively. These enhancements reduce the performance gap between\npretrained models and large language models in some language pairs, and even\noutperform the latter in some others. Furthermore, the UD-based similarity\nscore between a given language and English is positively correlated to the\nperformance of models in that language. Both findings highlight the validity\nand potential of UD in out-of-domain tasks.", "categories": ["cs.CL"], "published": "2025-06-05 11:10:14", "updated": "2025-06-05 11:10:14", "pdf_url": "http://arxiv.org/pdf/2506.04887v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04894v1", "title": "ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests", "authors": ["Shiyi Xu", "Yiwen Hu", "Yingqian Min", "Zhipeng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "abstract": "With the significant progress of large reasoning models in complex coding and\nreasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are\ninsufficient to evaluate the coding capabilities of large language models\n(LLMs) in real competition environments. Moreover, current evaluation metrics\nsuch as Pass@K fail to capture the reflective abilities of reasoning models. To\naddress these challenges, we propose \\textbf{ICPC-Eval}, a top-level\ncompetitive coding benchmark designed to probing the frontiers of LLM\nreasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent\nICPC contests held in various regions of the world, offering three key\ncontributions: 1) A challenging realistic ICPC competition scenario, featuring\na problem type and difficulty distribution consistent with actual contests. 2)\nA robust test case generation method and a corresponding local evaluation\ntoolkit, enabling efficient and accurate local evaluation. 3) An effective\ntest-time scaling evaluation metric, Refine@K, which allows iterative repair of\nsolutions based on execution feedback. The results underscore the significant\nchallenge in evaluating complex reasoning abilities: top-tier reasoning models\nlike DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their\nin-context reasoning potential when compared to non-reasoning counterparts.\nFurthermore, despite recent advancements in code generation, these models still\nlag behind top-performing human teams. We release the benchmark at:\nhttps://github.com/RUCAIBox/Slow_Thinking_with_LLMs", "categories": ["cs.CL"], "published": "2025-06-05 11:20:37", "updated": "2025-06-05 11:20:37", "pdf_url": "http://arxiv.org/pdf/2506.04894v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04907v1", "title": "Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots", "authors": ["Alex Pan", "Mary-Anne Williams"], "abstract": "Large Language Models (LLMs), whilst great at extracting facts from text,\nstruggle with nested narrative reasoning. Existing long context and multi-hop\nQA benchmarks inadequately test this, lacking realistic distractors or failing\nto decouple context length from reasoning complexity, masking a fundamental LLM\nlimitation. We introduce Verbose ListOps, a novel benchmark that\nprogrammatically transposes ListOps computations into lengthy, coherent\nstories. This uniquely forces internal computation and state management of\nnested reasoning problems by withholding intermediate results, and offers\nfine-grained controls for both narrative size \\emph{and} reasoning difficulty.\nWhilst benchmarks like LongReason (2025) advance approaches for synthetically\nexpanding the context size of multi-hop QA problems, Verbose ListOps pinpoints\na specific LLM vulnerability: difficulty in state management for nested\nsub-reasoning amongst semantically-relevant, distracting narrative. Our\nexperiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse\nin performance on Verbose ListOps at modest (~10k token) narrative lengths,\ndespite effortlessly solving raw ListOps equations. Addressing this failure is\nparamount for real-world text interpretation which requires identifying key\nreasoning points, tracking conceptual intermediate results, and filtering\nirrelevant information. Verbose ListOps, and its extensible generation\nframework thus enables targeted reasoning enhancements beyond mere\ncontext-window expansion; a critical step to automating the world's knowledge\nwork.", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "published": "2025-06-05 11:41:05", "updated": "2025-06-05 11:41:05", "pdf_url": "http://arxiv.org/pdf/2506.04907v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04909v1", "title": "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models", "authors": ["Kai Wang", "Yihao Zhang", "Meng Sun"], "abstract": "The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "published": "2025-06-05 11:44:19", "updated": "2025-06-05 11:44:19", "pdf_url": "http://arxiv.org/pdf/2506.04909v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04913v1", "title": "Dissecting Long Reasoning Models: An Empirical Study", "authors": ["Yongyu Mu", "Jiali Zeng", "Bei Li", "Xinyan Guan", "Fandong Meng", "Jie Zhou", "Tong Xiao", "Jingbo Zhu"], "abstract": "Despite recent progress in training long-context reasoning models via\nreinforcement learning (RL), several open questions and counterintuitive\nbehaviors remain. This work focuses on three key aspects: (1) We systematically\nanalyze the roles of positive and negative samples in RL, revealing that\npositive samples mainly facilitate data fitting, whereas negative samples\nsignificantly enhance generalization and robustness. Interestingly, training\nsolely on negative samples can rival standard RL training performance. (2) We\nidentify substantial data inefficiency in group relative policy optimization,\nwhere over half of the samples yield zero advantage. To address this, we\nexplore two straightforward strategies, including relative length rewards and\noffline sample injection, to better leverage these data and enhance reasoning\nefficiency and capability. (3) We investigate unstable performance across\nvarious reasoning models and benchmarks, attributing instability to uncertain\nproblems with ambiguous outcomes, and demonstrate that multiple evaluation runs\nmitigate this issue.", "categories": ["cs.LG", "cs.CL"], "published": "2025-06-05 11:47:10", "updated": "2025-06-05 11:47:10", "pdf_url": "http://arxiv.org/pdf/2506.04913v1", "comment": "Working in process", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04915v1", "title": "A Practitioner's Guide to Building ASR Models for Low-Resource Languages: A Case Study on Scottish Gaelic", "authors": ["Ond\u0159ej Klejch", "William Lamb", "Peter Bell"], "abstract": "An effective approach to the development of ASR systems for low-resource\nlanguages is to fine-tune an existing multilingual end-to-end model. When the\noriginal model has been trained on large quantities of data from many\nlanguages, fine-tuning can be effective with limited training data, even when\nthe language in question was not present in the original training data. The\nfine-tuning approach has been encouraged by the availability of public-domain\nE2E models and is widely believed to lead to state-of-the-art results. This\npaper, however, challenges that belief. We show that an approach combining\nhybrid HMMs with self-supervised models can yield substantially better\nperformance with limited training data. This combination allows better\nutilisation of all available speech and text data through continued\nself-supervised pre-training and semi-supervised training. We benchmark our\napproach on Scottish Gaelic, achieving WER reductions of 32% relative over our\nbest fine-tuned Whisper model.", "categories": ["cs.CL", "eess.AS"], "published": "2025-06-05 11:52:08", "updated": "2025-06-05 11:52:08", "pdf_url": "http://arxiv.org/pdf/2506.04915v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04920v1", "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback", "authors": ["Junior Cedric Tonga", "KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Fajri Koto", "Ekaterina Kochmar"], "abstract": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 11:53:04", "updated": "2025-06-05 11:53:04", "pdf_url": "http://arxiv.org/pdf/2506.04920v1", "comment": "Preprint, in submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04929v1", "title": "ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT", "authors": ["Miko\u0142aj Pokrywka", "Wojciech Kusa", "Mieszko Rutkowski", "Miko\u0142aj Koszowski"], "abstract": "Neural Machine Translation (NMT) has improved translation by using\nTransformer-based models, but it still struggles with word ambiguity and\ncontext. This problem is especially important in domain-specific applications,\nwhich often have problems with unclear sentences or poor data quality. Our\nresearch explores how adding information to models can improve translations in\nthe context of e-commerce data. To this end we create ConECT -- a new\nCzech-to-Polish e-commerce product translation dataset coupled with images and\nproduct metadata consisting of 11,400 sentence pairs. We then investigate and\ncompare different methods that are applicable to context-aware translation. We\ntest a vision-language model (VLM), finding that visual context aids\ntranslation quality. Additionally, we explore the incorporation of contextual\ninformation into text-to-text models, such as the product's category path or\nimage descriptions. The results of our study demonstrate that the incorporation\nof contextual information leads to an improvement in the quality of machine\ntranslation. We make the new dataset publicly available.", "categories": ["cs.CL"], "published": "2025-06-05 12:02:01", "updated": "2025-06-05 12:02:01", "pdf_url": "http://arxiv.org/pdf/2506.04929v1", "comment": "Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04965v1", "title": "From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation", "authors": ["Adrian Marius Dumitran", "Theodor-Pierre Moroianu", "Vasile Paul Alexe"], "abstract": "This paper presents a comprehensive evaluation of the performance of\nstate-of-the-art Large Language Models (LLMs) on challenging university-level\nalgorithms exams. By testing multiple models on both a Romanian exam and its\nhigh-quality English translation, we analyze LLMs' problem-solving\ncapabilities, consistency, and multilingual performance. Our empirical study\nreveals that the most recent models not only achieve scores comparable to\ntop-performing students but also demonstrate robust reasoning skills on\ncomplex, multi-step algorithmic challenges, even though difficulties remain\nwith graph-based tasks. Building on these findings, we explore the potential of\nLLMs to support educational environments through the generation of high-quality\neditorial content, offering instructors a powerful tool to enhance student\nfeedback. The insights and best practices discussed herein pave the way for\nfurther integration of generative AI in advanced algorithm education.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 12:41:20", "updated": "2025-06-05 12:41:20", "pdf_url": "http://arxiv.org/pdf/2506.04965v1", "comment": "15 pages Pre-print Paper accepted to ITS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04981v1", "title": "Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining and Data Filtering", "authors": ["Andres Carofilis", "Pradeep Rangappa", "Srikanth Madikeri", "Shashi Kumar", "Sergio Burdisso", "Jeena Prakash", "Esau Villatoro-Tello", "Petr Motlicek", "Bidisha Sharma", "Kadri Hacioglu", "Shankar Venkatesan", "Saurabh Vyas", "Andreas Stolcke"], "abstract": "Fine-tuning pretrained ASR models for specific domains is challenging when\nlabeled data is scarce. But unlabeled audio and labeled data from related\ndomains are often available. We propose an incremental semi-supervised learning\npipeline that first integrates a small in-domain labeled set and an auxiliary\ndataset from a closely related domain, achieving a relative improvement of 4%\nover no auxiliary data. Filtering based on multi-model consensus or named\nentity recognition (NER) is then applied to select and iteratively refine\npseudo-labels, showing slower performance saturation compared to random\nselection. Evaluated on the multi-domain Wow call center and Fisher English\ncorpora, it outperforms single-step fine-tuning. Consensus-based filtering\noutperforms other methods, providing up to 22.3% relative improvement on Wow\nand 24.8% on Fisher over single-step fine-tuning with random selection. NER is\nthe second-best filter, providing competitive performance at a lower\ncomputational cost.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-05 12:53:20", "updated": "2025-06-05 12:53:20", "pdf_url": "http://arxiv.org/pdf/2506.04981v1", "comment": "Accepted at Interspeech 2025, Netherlands", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.04997v1", "title": "Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings", "authors": ["Yubo Ma", "Jinsong Li", "Yuhang Zang", "Xiaobao Wu", "Xiaoyi Dong", "Pan Zhang", "Yuhang Cao", "Haodong Duan", "Jiaqi Wang", "Yixin Cao", "Aixin Sun"], "abstract": "Despite the strong performance of ColPali/ColQwen2 in Visualized Document\nRetrieval (VDR), it encodes each page into multiple patch-level embeddings and\nleads to excessive memory usage. This empirical study investigates methods to\nreduce patch embeddings per page at minimum performance degradation. We\nevaluate two token-reduction strategies: token pruning and token merging.\nRegarding token pruning, we surprisingly observe that a simple random strategy\noutperforms other sophisticated pruning methods, though still far from\nsatisfactory. Further analysis reveals that pruning is inherently unsuitable\nfor VDR as it requires removing certain page embeddings without query-specific\ninformation. Turning to token merging (more suitable for VDR), we search for\nthe optimal combinations of merging strategy across three dimensions and\ndevelop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance\nwith only 11.8% of original memory usage, and preserves 94.6% effectiveness at\n2.8% memory footprint. We expect our empirical findings and resulting\nLight-ColPali/ColQwen2 offer valuable insights and establish a competitive\nbaseline for future research towards efficient VDR.", "categories": ["cs.IR", "cs.CL"], "published": "2025-06-05 13:06:01", "updated": "2025-06-05 13:06:01", "pdf_url": "http://arxiv.org/pdf/2506.04997v1", "comment": "Accepted by ACL 2025 findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05000v1", "title": "SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View", "authors": ["Yongjie Xiao", "Hongru Liang", "Peixin Qin", "Yao Zhang", "Wenqiang Lei"], "abstract": "Despite the great potential of large language models(LLMs) in machine\ncomprehension, it is still disturbing to fully count on them in real-world\nscenarios. This is probably because there is no rational explanation for\nwhether the comprehension process of LLMs is aligned with that of experts. In\nthis paper, we propose SCOP to carefully examine how LLMs perform during the\ncomprehension process from a cognitive view. Specifically, it is equipped with\na systematical definition of five requisite skills during the comprehension\nprocess, a strict framework to construct testing data for these skills, and a\ndetailed analysis of advanced open-sourced and closed-sourced LLMs using the\ntesting data. With SCOP, we find that it is still challenging for LLMs to\nperform an expert-level comprehension process. Even so, we notice that LLMs\nshare some similarities with experts, e.g., performing better at comprehending\nlocal information than global information. Further analysis reveals that LLMs\ncan be somewhat unreliable -- they might reach correct answers through flawed\ncomprehension processes. Based on SCOP, we suggest that one direction for\nimproving LLMs is to focus more on the comprehension process, ensuring all\ncomprehension skills are thoroughly developed during training.", "categories": ["cs.CL"], "published": "2025-06-05 13:10:24", "updated": "2025-06-05 13:10:24", "pdf_url": "http://arxiv.org/pdf/2506.05000v1", "comment": "arXiv admin note: text overlap with arXiv:2004.14535 by other authors", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05010v1", "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development", "authors": ["Zhenran Xu", "Xue Yang", "Yiyu Wang", "Qingli Hu", "Zijiao Wu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "abstract": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.", "categories": ["cs.CL", "cs.CV"], "published": "2025-06-05 13:20:50", "updated": "2025-06-05 13:20:50", "pdf_url": "http://arxiv.org/pdf/2506.05010v1", "comment": "ACL 2025 Demo. Github: https://github.com/AIDC-AI/ComfyUI-Copilot", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05017v1", "title": "Controlling Summarization Length Through EOS Token Weighting", "authors": ["Zeno Belligoli", "Emmanouil Stergiadis", "Eran Fainman", "Ilya Gusev"], "abstract": "Controlling the length of generated text can be crucial in various\ntext-generation tasks, including summarization. Existing methods often require\ncomplex model alterations, limiting compatibility with pre-trained models. We\naddress these limitations by developing a simple approach for controlling the\nlength of automatic text summaries by increasing the importance of correctly\npredicting the EOS token in the cross-entropy loss computation. The proposed\nmethodology is agnostic to architecture and decoding algorithms and orthogonal\nto other inference-time techniques to control the generation length. We tested\nit with encoder-decoder and modern GPT-style LLMs, and show that this method\ncan control generation length, often without affecting the quality of the\nsummary.", "categories": ["cs.CL", "cs.LG"], "published": "2025-06-05 13:25:28", "updated": "2025-06-05 13:25:28", "pdf_url": "http://arxiv.org/pdf/2506.05017v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05038v1", "title": "Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers", "authors": ["Yutao Hou", "Zeguan Xiao", "Fei Yu", "Yihan Jiang", "Xuetao Wei", "Hailiang Huang", "Yun Chen", "Guanhua Chen"], "abstract": "Large language models (LLMs) have achieved distinguished performance on\nvarious reasoning-intensive tasks. However, LLMs might still face the\nchallenges of robustness issues and fail unexpectedly in some simple reasoning\ntasks. Previous works evaluate the LLM robustness with hand-crafted templates\nor a limited set of perturbation rules, indicating potential data contamination\nin pre-training or fine-tuning datasets. In this work, inspired by stress\ntesting in software engineering, we propose a novel framework, Automatic\nRobustness Checker (AR-Checker), to generate mathematical problem variants that\nmaintain the semantic meanings of the original one but might fail the LLMs. The\nAR-Checker framework generates mathematical problem variants through\nmulti-round parallel streams of LLM-based rewriting and verification. Our\nframework can generate benchmark variants dynamically for each LLM, thus\nminimizing the risk of data contamination. Experiments on GSM8K and MATH-500\ndemonstrate the strong performance of AR-Checker on mathematical tasks. We also\nevaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro,\nand CommonsenseQA, where it also achieves strong performance, further proving\nthe effectiveness of AR-Checker.", "categories": ["cs.CL"], "published": "2025-06-05 13:42:39", "updated": "2025-06-05 13:42:39", "pdf_url": "http://arxiv.org/pdf/2506.05038v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05057v1", "title": "TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages", "authors": ["Moshe Ofer", "Orel Zamler", "Amos Azaria"], "abstract": "Large Language Models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages due to limited training data. This paper presents\nTALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource\nLanguages), which integrates an LLM with two bilingual translation models. TALL\ntransforms low-resource inputs into high-resource representations, leveraging\nthe LLM's capabilities while preserving linguistic features through dimension\nalignment layers and custom transformers. Our experiments on Hebrew demonstrate\nsignificant improvements over several baselines, including direct use, naive\ntranslation, and fine-tuning approaches. The architecture employs a\nparameter-efficient strategy, freezing pre-trained components while training\nonly lightweight adapter modules, balancing computational efficiency with\nperformance gains.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 14:02:12", "updated": "2025-06-05 14:02:12", "pdf_url": "http://arxiv.org/pdf/2506.05057v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05062v1", "title": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech Evaluation", "authors": ["Noy Sternlicht", "Ariel Gera", "Roy Bar-Haim", "Tom Hope", "Noam Slonim"], "abstract": "We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that have previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task.", "categories": ["cs.CL"], "published": "2025-06-05 14:06:51", "updated": "2025-06-05 14:06:51", "pdf_url": "http://arxiv.org/pdf/2506.05062v1", "comment": "Code: https://github.com/noy-sternlicht/Debatable-Intelligence", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05068v1", "title": "Does It Make Sense to Speak of Introspection in Large Language Models?", "authors": ["Iulia Com\u015fa", "Murray Shanahan"], "abstract": "Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown ``creative'' writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 14:13:54", "updated": "2025-06-05 14:13:54", "pdf_url": "http://arxiv.org/pdf/2506.05068v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05070v1", "title": "RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation", "authors": ["Tianjiao Li", "Mengran Yu", "Chenyu Shi", "Yanjun Zhao", "Xiaojing Liu", "Qiang Zhang", "Qi Zhang", "Xuanjing Huang", "Jiayin Wang"], "abstract": "Large language models (LLMs) possess strong multilingual capabilities, and\ncombining Reinforcement Learning from Human Feedback (RLHF) with translation\ntasks has shown great potential. However, we observe that this paradigm\nperforms unexpectedly poorly when applied to colloquial subtitle translation\ntasks. In this work, we investigate this issue and find that the offline reward\nmodel (RM) gradually diverges from the online LLM due to distributional shift,\nultimately leading to undesirable training outcomes. To address this, we\npropose RIVAL, an adversarial training framework that formulates the process as\na min-max game between the RM and the LLM. RIVAL iteratively updates the both\nmodels, with the RM trained to distinguish strong from weak translations\n(qualitative preference reward), and the LLM trained to enhance its translation\nfor closing this gap. To stabilize training and improve generalizability, we\nalso incorporate quantitative preference reward (e.g., BLEU) into the RM,\nenabling reference-free quality modeling aligned with human evaluation. Through\nextensive experiments, we demonstrate that the proposed adversarial training\nframework significantly improves upon translation baselines.", "categories": ["cs.CL"], "published": "2025-06-05 14:18:21", "updated": "2025-06-05 14:18:21", "pdf_url": "http://arxiv.org/pdf/2506.05070v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05073v1", "title": "Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation", "authors": ["Soumitra Ghosh", "Gopendra Vikram Singh", "Shambhavi", "Sabarna Choudhury", "Asif Ekbal"], "abstract": "Self-harm detection on social media is critical for early intervention and\nmental health support, yet remains challenging due to the subtle,\ncontext-dependent nature of such expressions. Identifying self-harm intent aids\nsuicide prevention by enabling timely responses, but current large language\nmodels (LLMs) struggle to interpret implicit cues in casual language and\nemojis. This work enhances LLMs' comprehension of self-harm by distinguishing\nintent through nuanced language-emoji interplay. We present the Centennial\nEmoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with\ncontextual self-harm interpretations and the Self-Harm Identification aNd\nintent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering\ndetailed annotations for self-harm labels, casual mentions (CMs), and serious\nintents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)\nfine-tunes LLMs for multi-task learning: self-harm detection (primary) and\nCM/SI span detection (auxiliary); c) generates explainable rationales for\nself-harm predictions. We evaluate the framework on three state-of-the-art\nLLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and\nfine-tuned scenarios. By coupling intent differentiation with contextual cues,\nour approach commendably enhances LLM performance in both detection and\nexplanation tasks, effectively addressing the inherent ambiguity in self-harm\nsignals. The SHINES dataset, CESM-100 and codebase are publicly available at:\nhttps://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .", "categories": ["cs.CL"], "published": "2025-06-05 14:19:48", "updated": "2025-06-05 14:19:48", "pdf_url": "http://arxiv.org/pdf/2506.05073v1", "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025 Main)", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05080v1", "title": "Parking, Perception, and Retail: Street-Level Determinants of Community Vitality in Harbin", "authors": ["HaoTian Lan"], "abstract": "The commercial vitality of community-scale streets in Chinese cities is\nshaped by complex interactions between vehicular accessibility, environmental\nquality, and pedestrian perception. This study proposes an interpretable,\nimage-based framework to examine how street-level features -- including parked\nvehicle density, greenery, cleanliness, and street width -- impact retail\nperformance and user satisfaction in Harbin, China. Leveraging street view\nimagery and a multimodal large language model (VisualGLM-6B), we construct a\nCommunity Commercial Vitality Index (CCVI) from Meituan and Dianping data and\nanalyze its relationship with spatial attributes extracted via GPT-4-based\nperception modeling. Our findings reveal that while moderate vehicle presence\nmay enhance commercial access, excessive on-street parking -- especially in\nnarrow streets -- erodes walkability and reduces both satisfaction and\nshop-level pricing. In contrast, streets with higher perceived greenery and\ncleanliness show significantly greater satisfaction scores but only weak\nassociations with pricing. Street width moderates the effects of vehicle\npresence, underscoring the importance of spatial configuration. These results\ndemonstrate the value of integrating AI-assisted perception with urban\nmorphological analysis to capture non-linear and context-sensitive drivers of\ncommercial success. This study advances both theoretical and methodological\nfrontiers by highlighting the conditional role of vehicle activity in\nneighborhood commerce and demonstrating the feasibility of multimodal AI for\nperceptual urban diagnostics. The implications extend to urban design, parking\nmanagement, and scalable planning tools for community revitalization.", "categories": ["cs.CL", "cs.CV"], "published": "2025-06-05 14:28:48", "updated": "2025-06-05 14:28:48", "pdf_url": "http://arxiv.org/pdf/2506.05080v1", "comment": "22 pages,5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05087v1", "title": "Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics", "authors": ["HaoTian Lan"], "abstract": "While objective street metrics derived from imagery or GIS have become\nstandard in urban analytics, they remain insufficient to capture subjective\nperceptions essential to inclusive urban design. This study introduces a novel\nMultimodal Street Evaluation Framework (MSEF) that fuses a vision transformer\n(VisualGLM-6B) with a large language model (GPT-4), enabling interpretable\ndual-output assessment of streetscapes. Leveraging over 15,000 annotated\nstreet-view images from Harbin, China, we fine-tune the framework using LoRA\nand P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1\nscore of 0.84 on objective features and 89.3 percent agreement with aggregated\nresident perceptions, validated across stratified socioeconomic geographies.\nBeyond classification accuracy, MSEF captures context-dependent contradictions:\nfor instance, informal commerce boosts perceived vibrancy while simultaneously\nreducing pedestrian comfort. It also identifies nonlinear and semantically\ncontingent patterns -- such as the divergent perceptual effects of\narchitectural transparency across residential and commercial zones -- revealing\nthe limits of universal spatial heuristics. By generating natural-language\nrationales grounded in attention mechanisms, the framework bridges sensory data\nwith socio-affective inference, enabling transparent diagnostics aligned with\nSDG 11. This work offers both methodological innovation in urban perception\nmodeling and practical utility for planning systems seeking to reconcile\ninfrastructural precision with lived experience.", "categories": ["cs.CV", "cs.CL"], "published": "2025-06-05 14:34:04", "updated": "2025-06-05 14:34:04", "pdf_url": "http://arxiv.org/pdf/2506.05087v1", "comment": "24 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05107v1", "title": "CL-ISR: A Contrastive Learning and Implicit Stance Reasoning Framework for Misleading Text Detection on Social Media", "authors": ["Tianyi Huang", "Zikun Cui", "Cuiqianhe Du", "Chia-En Chiang"], "abstract": "Misleading text detection on social media platforms is a critical research\narea, as these texts can lead to public misunderstanding, social panic and even\neconomic losses. This paper proposes a novel framework - CL-ISR (Contrastive\nLearning and Implicit Stance Reasoning), which combines contrastive learning\nand implicit stance reasoning, to improve the detection accuracy of misleading\ntexts on social media. First, we use the contrastive learning algorithm to\nimprove the model's learning ability of semantic differences between truthful\nand misleading texts. Contrastive learning could help the model to better\ncapture the distinguishing features between different categories by\nconstructing positive and negative sample pairs. This approach enables the\nmodel to capture distinguishing features more effectively, particularly in\nlinguistically complicated situations. Second, we introduce the implicit stance\nreasoning module, to explore the potential stance tendencies in the text and\ntheir relationships with related topics. This method is effective for\nidentifying content that misleads through stance shifting or emotional\nmanipulation, because it can capture the implicit information behind the text.\nFinally, we integrate these two algorithms together to form a new framework,\nCL-ISR, which leverages the discriminative power of contrastive learning and\nthe interpretive depth of stance reasoning to significantly improve detection\neffect.", "categories": ["cs.CL"], "published": "2025-06-05 14:52:28", "updated": "2025-06-05 14:52:28", "pdf_url": "http://arxiv.org/pdf/2506.05107v1", "comment": "6 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05121v1", "title": "The NTNU System at the S&I Challenge 2025 SLA Open Track", "authors": ["Hong-Yun Lin", "Tien-Hong Lo", "Yu-Hsuan Fang", "Jhen-Ke Lin", "Chung-Chun Wang", "Hao-Chien Lu", "Berlin Chen"], "abstract": "A recent line of research on spoken language assessment (SLA) employs neural\nmodels such as BERT and wav2vec 2.0 (W2V) to evaluate speaking proficiency\nacross linguistic and acoustic modalities. Although both models effectively\ncapture features relevant to oral competence, each exhibits modality-specific\nlimitations. BERT-based methods rely on ASR transcripts, which often fail to\ncapture prosodic and phonetic cues for SLA. In contrast, W2V-based methods\nexcel at modeling acoustic features but lack semantic interpretability. To\novercome these limitations, we propose a system that integrates W2V with Phi-4\nmultimodal large language model (MLLM) through a score fusion strategy. The\nproposed system achieves a root mean square error (RMSE) of 0.375 on the\nofficial test set of the Speak & Improve Challenge 2025, securing second place\nin the competition. For comparison, the RMSEs of the top-ranked, third-ranked,\nand official baseline systems are 0.364, 0.384, and 0.444, respectively.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-06-05 15:09:23", "updated": "2025-06-05 15:09:23", "pdf_url": "http://arxiv.org/pdf/2506.05121v1", "comment": "submitted to the ISCA SLaTE-2025 Workshop", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05128v1", "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning", "authors": ["Tanmay Parekh", "Kartik Mehta", "Ninareh Mehrabi", "Kai-Wei Chang", "Nanyun Peng"], "abstract": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 15:16:14", "updated": "2025-06-05 15:16:14", "pdf_url": "http://arxiv.org/pdf/2506.05128v1", "comment": "Submitted at ACL ARR May 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05136v1", "title": "Information Locality as an Inductive Bias for Neural Language Models", "authors": ["Taiga Someya", "Anej Svete", "Brian DuSell", "Timothy J. O'Donnell", "Mario Giulianelli", "Ryan Cotterell"], "abstract": "Inductive biases are inherent in every machine learning system, shaping how\nmodels generalize from finite data. In the case of neural language models\n(LMs), debates persist as to whether these biases align with or diverge from\nhuman processing constraints. To address this issue, we propose a quantitative\nframework that allows for controlled investigations into the nature of these\nbiases. Within our framework, we introduce $m$-local entropy$\\unicode{x2013}$an\ninformation-theoretic measure derived from average lossy-context\nsurprisal$\\unicode{x2013}$that captures the local uncertainty of a language by\nquantifying how effectively the $m-1$ preceding symbols disambiguate the next\nsymbol. In experiments on both perturbed natural language corpora and languages\ndefined by probabilistic finite-state automata (PFSAs), we show that languages\nwith higher $m$-local entropy are more difficult for Transformer and LSTM LMs\nto learn. These results suggest that neural LMs, much like humans, are highly\nsensitive to the local statistical structure of a language.", "categories": ["cs.CL"], "published": "2025-06-05 15:21:05", "updated": "2025-06-05 15:21:05", "pdf_url": "http://arxiv.org/pdf/2506.05136v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05140v1", "title": "AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models", "authors": ["Chih-Kai Yang", "Neo Ho", "Yi-Jyun Lee", "Hung-yi Lee"], "abstract": "Understanding the internal mechanisms of large audio-language models (LALMs)\nis crucial for interpreting their behavior and improving performance. This work\npresents the first in-depth analysis of how LALMs internally perceive and\nrecognize auditory attributes. By applying vocabulary projection on three\nstate-of-the-art LALMs, we track how attribute information evolves across\nlayers and token positions. We find that attribute information generally\ndecreases with layer depth when recognition fails, and that resolving\nattributes at earlier layers correlates with better accuracy. Moreover, LALMs\nheavily rely on querying auditory inputs for predicting attributes instead of\naggregating necessary information in hidden states at attribute-mentioning\npositions. Based on our findings, we demonstrate a method to enhance LALMs. Our\nresults offer insights into auditory attribute processing, paving the way for\nfuture improvements.", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "published": "2025-06-05 15:22:47", "updated": "2025-06-05 15:22:47", "pdf_url": "http://arxiv.org/pdf/2506.05140v1", "comment": "8 pages, 5 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05142v1", "title": "Do Large Language Models Judge Error Severity Like Humans?", "authors": ["Diege Sun", "Guanyi Chen", "Fan Zhao", "Xiaorong Cheng", "Tingting He"], "abstract": "Large Language Models (LLMs) are increasingly used as automated evaluators in\nnatural language generation, yet it remains unclear whether they can accurately\nreplicate human judgments of error severity. In this study, we systematically\ncompare human and LLM assessments of image descriptions containing controlled\nsemantic errors. We extend the experimental framework of van Miltenburg et al.\n(2020) to both unimodal (text-only) and multimodal (text + image) settings,\nevaluating four error types: age, gender, clothing type, and clothing colour.\nOur findings reveal that humans assign varying levels of severity to different\nerror types, with visual context significantly amplifying perceived severity\nfor colour and type errors. Notably, most LLMs assign low scores to gender\nerrors but disproportionately high scores to colour errors, unlike humans, who\njudge both as highly severe but for different reasons. This suggests that these\nmodels may have internalised social norms influencing gender judgments but lack\nthe perceptual grounding to emulate human sensitivity to colour, which is\nshaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,\nreplicates the human-like ranking of error severity, but it fails to\ndistinguish between error types as clearly as humans. Surprisingly,\nDeepSeek-V3, a unimodal LLM, achieves the highest alignment with human\njudgments across both unimodal and multimodal conditions, outperforming even\nstate-of-the-art multimodal models.", "categories": ["cs.CL"], "published": "2025-06-05 15:24:33", "updated": "2025-06-05 15:24:33", "pdf_url": "http://arxiv.org/pdf/2506.05142v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05146v1", "title": "CIVET: Systematic Evaluation of Understanding in VLMs", "authors": ["Massimo Rizzoli", "Simone Alghisi", "Olha Khomyn", "Gabriel Roccabruna", "Seyed Mahed Mousavi", "Giuseppe Riccardi"], "abstract": "While Vision-Language Models (VLMs) have achieved competitive performance in\nvarious tasks, their comprehension of the underlying structure and semantics of\na scene remains understudied. To investigate the understanding of VLMs, we\nstudy their capability regarding object properties and relations in a\ncontrolled and interpretable manner. To this scope, we introduce CIVET, a novel\nand extensible framework for systematiC evaluatIon Via controllEd sTimuli.\nCIVET addresses the lack of standardized systematic evaluation for assessing\nVLMs' understanding, enabling researchers to test hypotheses with statistical\nrigor. With CIVET, we evaluate five state-of-the-art VLMs on exhaustive sets of\nstimuli, free from annotation noise, dataset-specific biases, and uncontrolled\nscene complexity. Our findings reveal that 1) current VLMs can accurately\nrecognize only a limited set of basic object properties; 2) their performance\nheavily depends on the position of the object in the scene; 3) they struggle to\nunderstand basic relations among objects. Furthermore, a comparative evaluation\nwith human annotators reveals that VLMs still fall short of achieving\nhuman-level accuracy.", "categories": ["cs.CV", "cs.CL"], "published": "2025-06-05 15:27:16", "updated": "2025-06-05 15:27:16", "pdf_url": "http://arxiv.org/pdf/2506.05146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05154v1", "title": "Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation", "authors": ["Chenyu Lin", "Yilin Wen", "Du Su", "Fei Sun", "Muhan Chen", "Chenfu Bao", "Zhonghou Lv"], "abstract": "Retrieval-augmented generation (RAG) is a mainstream method for improving\nperformance on knowledge-intensive tasks. However,current RAG systems often\nplace too much emphasis on retrieved contexts. This can lead to reliance on\ninaccurate sources and overlook the model's inherent knowledge, especially when\ndealing with misleading or excessive information. To resolve this imbalance, we\npropose Knowledgeable-r1 that using joint sampling and define multi policy\ndistributions in knowledge capability exploration to stimulate large language\nmodels'self-integrated utilization of parametric and contextual knowledge.\nExperiments show that Knowledgeable-r1 significantly enhances robustness and\nreasoning accuracy in both parameters and contextual conflict tasks and general\nRAG tasks, especially outperforming baselines by 17.07% in counterfactual\nscenarios and demonstrating consistent gains across RAG tasks. Our code are\navailable at https://github.com/lcy80366872/ knowledgeable-r1.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-05 15:34:15", "updated": "2025-06-05 15:34:15", "pdf_url": "http://arxiv.org/pdf/2506.05154v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05166v1", "title": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective", "authors": ["Bhavik Chandna", "Zubair Bashir", "Procheta Sen"], "abstract": "Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 15:43:34", "updated": "2025-06-05 15:43:34", "pdf_url": "http://arxiv.org/pdf/2506.05166v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05167v1", "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG", "authors": ["Yeonseok Jeong", "Jinsu Kim", "Dohyeon Lee", "Seung-won Hwang"], "abstract": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\n\\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing\nretrieved documents based on evidentiality, ensuring whether answer generation\nis supported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-06-05 15:43:49", "updated": "2025-06-05 15:43:49", "pdf_url": "http://arxiv.org/pdf/2506.05167v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05176v1", "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models", "authors": ["Yanzhao Zhang", "Mingxin Li", "Dingkun Long", "Xin Zhang", "Huan Lin", "Baosong Yang", "Pengjun Xie", "An Yang", "Dayiheng Liu", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "abstract": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.", "categories": ["cs.CL"], "published": "2025-06-05 15:49:48", "updated": "2025-06-05 15:49:48", "pdf_url": "http://arxiv.org/pdf/2506.05176v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05188v1", "title": "Counterfactual reasoning: an analysis of in-context emergence", "authors": ["Moritz Miller", "Bernhard Sch\u00f6lkopf", "Siyuan Guo"], "abstract": "Large-scale neural language models (LMs) exhibit remarkable performance in\nin-context learning: the ability to learn and reason the input context on the\nfly without parameter update. This work studies in-context counterfactual\nreasoning in language models, that is, to predict the consequences of changes\nunder hypothetical scenarios. We focus on studying a well-defined synthetic\nsetup: a linear regression task that requires noise abduction, where accurate\nprediction is based on inferring and copying the contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning in this controlled setup and provide insights that counterfactual\nreasoning for a broad class of functions can be reduced to a transformation on\nin-context observations; we find self-attention, model depth, and data\ndiversity in pre-training drive performance in Transformers. More\ninterestingly, our findings extend beyond regression tasks and show that\nTransformers can perform noise abduction on sequential data, providing\npreliminary evidence on the potential for counterfactual story generation. Our\ncode is available under\nhttps://github.com/moXmiller/counterfactual-reasoning.git .", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "published": "2025-06-05 16:02:07", "updated": "2025-06-05 16:02:07", "pdf_url": "http://arxiv.org/pdf/2506.05188v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05205v1", "title": "RELIC: Evaluating Compositional Instruction Following via Language Recognition", "authors": ["Jackson Petty", "Michael Y. Hu", "Wentao Wang", "Shauli Ravfogel", "William Merrill", "Tal Linzen"], "abstract": "Large language models (LLMs) are increasingly expected to perform tasks based\nonly on a specification of the task provided in context, without examples of\ninputs and outputs; this ability is referred to as instruction following. We\nintroduce the Recognition of Languages In-Context (RELIC) framework to evaluate\ninstruction following using language recognition: the task of determining if a\nstring is generated by formal grammar. Unlike many standard evaluations of\nLLMs' ability to use their context, this task requires composing together a\nlarge number of instructions (grammar productions) retrieved from the context.\nBecause the languages are synthetic, the task can be increased in complexity as\nLLMs' skills improve, and new instances can be automatically generated,\nmitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and\nfind that their accuracy can be reliably predicted from the complexity of the\ngrammar and the individual example strings, and that even the most advanced\nLLMs currently available show near-chance performance on more complex grammars\nand samples, in line with theoretical expectations. We also use RELIC to\ndiagnose how LLMs attempt to solve increasingly difficult reasoning tasks,\nfinding that as the complexity of the language recognition task increases,\nmodels switch to relying on shallow heuristics instead of following complex\ninstructions.", "categories": ["cs.CL"], "published": "2025-06-05 16:17:24", "updated": "2025-06-05 16:17:24", "pdf_url": "http://arxiv.org/pdf/2506.05205v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05209v1", "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text", "authors": ["Nikhil Kandpal", "Brian Lester", "Colin Raffel", "Sebastian Majstorovic", "Stella Biderman", "Baber Abbasi", "Luca Soldaini", "Enrico Shippole", "A. Feder Cooper", "Aviya Skowron", "John Kirchenbauer", "Shayne Longpre", "Lintang Sutawika", "Alon Albalak", "Zhenlin Xu", "Guilherme Penedo", "Loubna Ben Allal", "Elie Bakouch", "John David Pressman", "Honglu Fan", "Dashiell Stander", "Guangyu Song", "Aaron Gokaslan", "Tom Goldstein", "Brian R. Bartoldson", "Bhavya Kailkhura", "Tyler Murray"], "abstract": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.", "categories": ["cs.CL", "cs.LG"], "published": "2025-06-05 16:21:30", "updated": "2025-06-05 16:21:30", "pdf_url": "http://arxiv.org/pdf/2506.05209v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05213v1", "title": "LLM-First Search: Self-Guided Exploration of the Solution Space", "authors": ["Nathan Herr", "Tim Rockt\u00e4schel", "Roberta Raileanu"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.", "categories": ["cs.AI", "cs.CL"], "published": "2025-06-05 16:27:49", "updated": "2025-06-05 16:27:49", "pdf_url": "http://arxiv.org/pdf/2506.05213v1", "comment": "9 main pages, 2 figures, 2 tables, 36 appendix pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05214v1", "title": "Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning", "authors": ["Jingyu Hu", "Hongbo Bo", "Jun Hong", "Xiaowei Liu", "Weiru Liu"], "abstract": "Graph Neural Networks (GNNs) often suffer from degree bias in node\nclassification tasks, where prediction performance varies across nodes with\ndifferent degrees. Several approaches, which adopt Graph Contrastive Learning\n(GCL), have been proposed to mitigate this bias. However, the limited number of\npositive pairs and the equal weighting of all positives and negatives in GCL\nstill lead to low-degree nodes acquiring insufficient and noisy information.\nThis paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to\nmitigate degree bias. It adds more positive pairs by leveraging node labels and\nadaptively weights positive and negative pairs based on their learning\nhardness. In addition, we develop an experimental framework named SHARP to\nextend HAR to a broader range of scenarios. Both our theoretical analysis and\nexperiments validate the effectiveness of SHARP. The experimental results\nacross four datasets show that SHARP achieves better performance against\nbaselines at both global and degree levels.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-05 16:28:12", "updated": "2025-06-05 16:28:12", "pdf_url": "http://arxiv.org/pdf/2506.05214v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05227v1", "title": "Improving Low-Resource Morphological Inflection via Self-Supervised Objectives", "authors": ["Adam Wiemerslage", "Katharina von der Wense"], "abstract": "Self-supervised objectives have driven major advances in NLP by leveraging\nlarge-scale unlabeled data, but such resources are scarce for many of the\nworld's languages. Surprisingly, they have not been explored much for\ncharacter-level tasks, where smaller amounts of data have the potential to be\nbeneficial. We investigate the effectiveness of self-supervised auxiliary tasks\nfor morphological inflection -- a character-level task highly relevant for\nlanguage documentation -- in extremely low-resource settings, training\nencoder-decoder transformers for 19 languages and 13 auxiliary objectives.\nAutoencoding yields the best performance when unlabeled data is very limited,\nwhile character masked language modeling (CMLM) becomes more effective as data\navailability increases. Though objectives with stronger inductive biases\ninfluence model predictions intuitively, they rarely outperform standard CMLM.\nHowever, sampling masks based on known morpheme boundaries consistently\nimproves performance, highlighting a promising direction for low-resource\nmorphological modeling.", "categories": ["cs.CL"], "published": "2025-06-05 16:42:45", "updated": "2025-06-05 16:42:45", "pdf_url": "http://arxiv.org/pdf/2506.05227v1", "comment": "ACL 2025 main", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05229v1", "title": "Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts", "authors": ["Danil Sivtsov", "Ivan Rodkin", "Gleb Kuzmin", "Yuri Kuratov", "Ivan Oseledets"], "abstract": "Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications.", "categories": ["cs.LG", "cs.CL"], "published": "2025-06-05 16:43:48", "updated": "2025-06-05 16:43:48", "pdf_url": "http://arxiv.org/pdf/2506.05229v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05233v1", "title": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training", "authors": ["Johannes von Oswald", "Nino Scherrer", "Seijin Kobayashi", "Luca Versari", "Songlin Yang", "Maximilian Schlegel", "Kaitlin Maile", "Yanick Schimpf", "Oliver Sieberling", "Alexander Meulemans", "Rif A. Saurous", "Guillaume Lajoie", "Charlotte Frenkel", "Razvan Pascanu", "Blaise Ag\u00fcera y Arcas", "Jo\u00e3o Sacramento"], "abstract": "Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-05 16:50:23", "updated": "2025-06-05 16:50:23", "pdf_url": "http://arxiv.org/pdf/2506.05233v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05235v1", "title": "Towards a Unified System of Representation for Continuity and Discontinuity in Natural Language", "authors": ["Ratna Kandala", "Prakash Mondal"], "abstract": "Syntactic discontinuity is a grammatical phenomenon in which a constituent is\nsplit into more than one part because of the insertion of an element which is\nnot part of the constituent. This is observed in many languages across the\nworld such as Turkish, Russian, Japanese, Warlpiri, Navajo, Hopi, Dyirbal,\nYidiny etc. Different formalisms/frameworks in current linguistic theory\napproach the problem of discontinuous structures in different ways. Each\nframework/formalism has widely been viewed as an independent and non-converging\nsystem of analysis. In this paper, we propose a unified system of\nrepresentation for both continuity and discontinuity in structures of natural\nlanguages by taking into account three formalisms, in particular, Phrase\nStructure Grammar (PSG) for its widely used notion of constituency, Dependency\nGrammar (DG) for its head-dependent relations, and Categorial Grammar (CG) for\nits focus on functor-argument relations. We attempt to show that discontinuous\nexpressions as well as continuous structures can be analysed through a unified\nmathematical derivation incorporating the representations of linguistic\nstructure in these three grammar formalisms.", "categories": ["cs.CL"], "published": "2025-06-05 16:54:41", "updated": "2025-06-05 16:54:41", "pdf_url": "http://arxiv.org/pdf/2506.05235v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05243v1", "title": "CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection", "authors": ["Ron Eliav", "Arie Cattan", "Eran Hirsch", "Shahaf Bassan", "Elias Stengel-Eskin", "Mohit Bansal", "Ido Dagan"], "abstract": "A common approach to hallucination detection casts it as a natural language\ninference (NLI) task, often using LLMs to classify whether the generated text\nis entailed by corresponding reference texts. Since entailment classification\nis a complex reasoning task, one would expect that LLMs could benefit from\ngenerating an explicit reasoning process, as in CoT reasoning or the explicit\n``thinking'' of recent reasoning models. In this work, we propose that guiding\nsuch models to perform a systematic and comprehensive reasoning process -- one\nthat both decomposes the text into smaller facts and also finds evidence in the\nsource for each fact -- allows models to execute much finer-grained and\naccurate entailment decisions, leading to increased performance. To that end,\nwe define a 3-step reasoning process, consisting of (i) claim decomposition,\n(ii) sub-claim attribution and entailment classification, and (iii) aggregated\nclassification, showing that such guided reasoning indeed yields improved\nhallucination detection. Following this reasoning framework, we introduce an\nanalysis scheme, consisting of several metrics that measure the quality of the\nintermediate reasoning steps, which provided additional empirical evidence for\nthe improved quality of our guided reasoning scheme.", "categories": ["cs.CL"], "published": "2025-06-05 17:02:52", "updated": "2025-06-05 17:02:52", "pdf_url": "http://arxiv.org/pdf/2506.05243v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05278v1", "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning", "authors": ["Nan Huo", "Jinyang Li", "Bowen Qin", "Ge Qu", "Xiaolong Li", "Xiaodong Li", "Chenhao Ma", "Reynold Cheng"], "abstract": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-06-05 17:33:02", "updated": "2025-06-05 17:33:02", "pdf_url": "http://arxiv.org/pdf/2506.05278v1", "comment": "Accepted by ACL 2025 Main", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05305v1", "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback", "authors": ["Deepak Pandita", "Tharindu Cyril Weerasooriya", "Ankit Parag Shah", "Christopher M. Homan", "Wei Wei"], "abstract": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 17:52:30", "updated": "2025-06-05 17:52:30", "pdf_url": "http://arxiv.org/pdf/2506.05305v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05309v1", "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games", "authors": ["Niv Eckhaus", "Uri Berger", "Gabriel Stanovsky"], "abstract": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.", "categories": ["cs.MA", "cs.AI", "cs.CL"], "published": "2025-06-05 17:53:44", "updated": "2025-06-05 17:53:44", "pdf_url": "http://arxiv.org/pdf/2506.05309v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05314v1", "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models", "authors": ["Taha Entesari", "Arman Hatami", "Rinat Khaziev", "Anil Ramakrishna", "Mahyar Fazlyab"], "abstract": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-06-05 17:55:23", "updated": "2025-06-05 17:55:23", "pdf_url": "http://arxiv.org/pdf/2506.05314v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05316v1", "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay", "authors": ["Yifan Sun", "Jingyan Shen", "Yibin Wang", "Tianyu Chen", "Zhendong Wang", "Mingyuan Zhou", "Huan Zhang"], "abstract": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-06-05 17:55:43", "updated": "2025-06-05 17:55:43", "pdf_url": "http://arxiv.org/pdf/2506.05316v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05332v1", "title": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding", "authors": ["Jingyang Lin", "Jialian Wu", "Ximeng Sun", "Ze Wang", "Jiang Liu", "Yusheng Su", "Xiaodong Yu", "Hao Chen", "Jiebo Luo", "Zicheng Liu", "Emad Barsoum"], "abstract": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.", "categories": ["cs.CV", "cs.CL"], "published": "2025-06-05 17:59:04", "updated": "2025-06-05 17:59:04", "pdf_url": "http://arxiv.org/pdf/2506.05332v1", "comment": "Project page: https://videomarathon.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05333v1", "title": "Kinetics: Rethinking Test-Time Scaling Laws", "authors": ["Ranajoy Sadhukhan", "Zhuoming Chen", "Haizhong Zheng", "Yang Zhou", "Emma Strubell", "Beidi Chen"], "abstract": "We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nfor realizing the full potential of test-time scaling because, unlike training,\nwhere parameter scaling saturates, test-time accuracy continues to improve\nthrough increased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics.", "categories": ["cs.LG", "cs.CL"], "published": "2025-06-05 17:59:24", "updated": "2025-06-05 17:59:24", "pdf_url": "http://arxiv.org/pdf/2506.05333v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05334v1", "title": "Search Arena: Analyzing Search-Augmented LLMs", "authors": ["Mihran Miroyan", "Tsung-Han Wu", "Logan King", "Tianle Li", "Jiayi Pan", "Xinyan Hu", "Wei-Lin Chiang", "Anastasios N. Angelopoulos", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "abstract": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.", "categories": ["cs.CL", "cs.IR", "cs.LG"], "published": "2025-06-05 17:59:26", "updated": "2025-06-05 17:59:26", "pdf_url": "http://arxiv.org/pdf/2506.05334v1", "comment": "Preprint. Code: https://github.com/lmarena/search-arena. Dataset:\n  https://huggingface.co/datasets/lmarena-ai/search-arena-24k", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05339v1", "title": "Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models", "authors": ["Anirudh Bharadwaj", "Chaitanya Malaviya", "Nitish Joshi", "Mark Yatskar"], "abstract": "Language models serve as proxies for human preference judgements in alignment\nand evaluation, yet they exhibit systematic miscalibration, prioritizing\nsuperficial patterns over substantive qualities. This bias manifests as\noverreliance on features like length, structure, and style, leading to issues\nlike reward hacking and unreliable evaluations. Evidence suggests these biases\noriginate in artifacts in human training data. In this work, we systematically\ninvestigate the relationship between training data biases and preference model\nmiscalibration across five idiosyncratic features of language model\ngenerations: length, structure, jargon, sycophancy and vagueness. Using\ncontrolled counterfactual pairs, we first quantify the extent to which\npreference models favor responses with magnified biases (skew), finding this\npreference occurs in >60% of instances, and model preferences show high\nmiscalibration (~40%) compared to human preferences. Notably, bias features\nonly show mild negative correlations to human preference labels (mean r_human =\n-0.12) but show moderately strong positive correlations with labels from a\nstrong reward model (mean r_model = +0.36), suggesting that models may overrely\non spurious cues. To mitigate these issues, we propose a simple post-training\nmethod based on counterfactual data augmentation (CDA) using synthesized\ncontrastive examples. Finetuning models with CDA reduces average miscalibration\nfrom 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%,\nwhile maintaining overall RewardBench performance, showing that targeted\ndebiasing is effective for building reliable preference models.", "categories": ["cs.CL"], "published": "2025-06-05 17:59:32", "updated": "2025-06-05 17:59:32", "pdf_url": "http://arxiv.org/pdf/2506.05339v1", "comment": "Code and data available at\n  https://github.com/anirudhb123/preference-model-biases", "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05345v1", "title": "Inference-Time Hyper-Scaling with KV Cache Compression", "authors": ["Adrian \u0141a\u0144cucki", "Konrad Staniszewski", "Piotr Nawrot", "Edoardo M. Ponti"], "abstract": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.", "categories": ["cs.LG", "cs.CL"], "published": "2025-06-05 17:59:55", "updated": "2025-06-05 17:59:55", "pdf_url": "http://arxiv.org/pdf/2506.05345v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2506.05346v1", "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets", "authors": ["Lei Hsiung", "Tianyu Pang", "Yung-Chen Tang", "Linyue Song", "Tsung-Yi Ho", "Pin-Yu Chen", "Yaoqing Yang"], "abstract": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.", "categories": ["cs.CR", "cs.CL", "cs.LG"], "published": "2025-06-05 17:59:55", "updated": "2025-06-05 17:59:55", "pdf_url": "http://arxiv.org/pdf/2506.05346v1", "comment": "Project Page: https://hsiung.cc/llm-similarity-risk/", "doi": null, "journal_ref": null}
