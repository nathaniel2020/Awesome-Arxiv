{"arxiv_id": "2505.12581v1", "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Lu\u00eds Carbonera"], "abstract": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-19 00:03:57", "updated": "2025-05-19 00:03:57", "pdf_url": "http://arxiv.org/pdf/2505.12581v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12583v1", "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "abstract": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-19 00:11:42", "updated": "2025-05-19 00:11:42", "pdf_url": "http://arxiv.org/pdf/2505.12583v1", "comment": "Accepted to IJCAI 2025 Survey Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12585v1", "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization", "authors": ["En Yu", "Jie Lu", "Xiaoyu Yang", "Guangquan Zhang", "Zhen Fang"], "abstract": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 00:38:18", "updated": "2025-05-19 00:38:18", "pdf_url": "http://arxiv.org/pdf/2505.12585v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12594v1", "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "abstract": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 01:14:57", "updated": "2025-05-19 01:14:57", "pdf_url": "http://arxiv.org/pdf/2505.12594v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12623v1", "title": "Lightweight and Effective Preference Construction in PIBT for Large-Scale Multi-Agent Pathfinding", "authors": ["Keisuke Okumura", "Hiroki Nagai"], "abstract": "PIBT is a computationally lightweight algorithm that can be applied to a\nvariety of multi-agent pathfinding (MAPF) problems, generating the next\ncollision-free locations of agents given another. Because of its simplicity and\nscalability, it is becoming a popular underlying scheme for recent large-scale\nMAPF methods involving several hundreds or thousands of agents. Vanilla PIBT\nmakes agents behave greedily towards their assigned goals, while agents\ntypically have multiple best actions, since the graph shortest path is not\nalways unique. Consequently, tiebreaking about how to choose between these\nactions significantly affects resulting solutions. This paper studies two\nsimple yet effective techniques for tiebreaking in PIBT, without compromising\nits computational advantage. The first technique allows an agent to\nintelligently dodge another, taking into account whether each action will\nhinder the progress of the next timestep. The second technique is to learn,\nthrough multiple PIBT runs, how an action causes regret in others and to use\nthis information to minimise regret collectively. Our empirical results\ndemonstrate that these techniques can reduce the solution cost of one-shot MAPF\nand improve the throughput of lifelong MAPF. For instance, in densely populated\none-shot cases, the combined use of these tiebreaks achieves improvements of\naround 10-20% in sum-of-costs, without significantly compromising the speed of\na PIBT-based planner.", "categories": ["cs.MA", "cs.AI"], "published": "2025-05-19 02:12:29", "updated": "2025-05-19 02:12:29", "pdf_url": "http://arxiv.org/pdf/2505.12623v1", "comment": "To be presented at SoCS-25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12626v1", "title": "scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data", "authors": ["Ping Xu", "Zhiyuan Ning", "Pengjiang Li", "Wenhao Liu", "Pengyang Wang", "Jiaxu Cui", "Yuanchun Zhou", "Pengfei Wang"], "abstract": "Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell\nclustering playing a key role in identifying cell types and marker genes.\nRecent advances, especially graph neural networks (GNNs)-based methods, have\nsignificantly improved clustering performance. However, the analysis of\nscRNA-seq data remains challenging due to noise, sparsity, and high\ndimensionality. Compounding these challenges, GNNs often suffer from\nover-smoothing, limiting their ability to capture complex biological\ninformation. In response, we propose scSiameseClu, a novel Siamese Clustering\nframework for interpreting single-cell RNA-seq data, comprising of 3 key steps:\n(1) Dual Augmentation Module, which applies biologically informed perturbations\nto the gene expression matrix and cell graph relationships to enhance\nrepresentation robustness; (2) Siamese Fusion Module, which combines\ncross-correlation refinement and adaptive information fusion to capture complex\ncellular relationships while mitigating over-smoothing; and (3) Optimal\nTransport Clustering, which utilizes Sinkhorn distance to efficiently align\ncluster assignments with predefined proportions while maintaining balance.\nComprehensive evaluations on seven real-world datasets demonstrate\nthat~\\methodname~outperforms state-of-the-art methods in single-cell\nclustering, cell type annotation, and cell type classification, providing a\npowerful tool for scRNA-seq data interpretation.", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "published": "2025-05-19 02:17:09", "updated": "2025-05-19 02:17:09", "pdf_url": "http://arxiv.org/pdf/2505.12626v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12630v1", "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "abstract": "All-in-one image restoration aims to recover clear images from various\ndegradation types and levels with a unified model. Nonetheless, the significant\nvariations among degradation types present challenges for training a universal\nmodel, often resulting in task interference, where the gradient update\ndirections of different tasks may diverge due to shared parameters. To address\nthis issue, motivated by the routing strategy, we propose DFPIR, a novel\nall-in-one image restorer that introduces Degradation-aware Feature\nPerturbations(DFP) to adjust the feature space to align with the unified\nparameter space. In this paper, the feature perturbations primarily include\nchannel-wise perturbations and attention-wise perturbations. Specifically,\nchannel-wise perturbations are implemented by shuffling the channels in\nhigh-dimensional space guided by degradation types, while attention-wise\nperturbations are achieved through selective masking in the attention space. To\nachieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)\nto implement these two functions, positioned between the encoding and decoding\nstages of the encoder-decoder architecture. Extensive experimental results\ndemonstrate that DFPIR achieves state-of-the-art performance on several\nall-in-one image restoration tasks including image denoising, image dehazing,\nimage deraining, motion deblurring, and low-light image enhancement. Our codes\nare available at https://github.com/TxpHome/DFPIR.", "categories": ["cs.CV", "cs.AI", "I.4.5"], "published": "2025-05-19 02:37:11", "updated": "2025-05-19 02:37:11", "pdf_url": "http://arxiv.org/pdf/2505.12630v1", "comment": "Accepted to CVPR 2025. 8 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12632v1", "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 02:39:03", "updated": "2025-05-19 02:39:03", "pdf_url": "http://arxiv.org/pdf/2505.12632v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12638v1", "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "abstract": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome.", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.LG"], "published": "2025-05-19 02:45:42", "updated": "2025-05-19 02:45:42", "pdf_url": "http://arxiv.org/pdf/2505.12638v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12641v1", "title": "Single Image Reflection Removal via inter-layer Complementarity", "authors": ["Yue Huang", "Zi'ang Li", "Tianle Hu", "Jie Wen", "Guanbin Li", "Jinglin Zhang", "Guoxu Zhou", "Xiaozhao Fang"], "abstract": "Although dual-stream architectures have achieved remarkable success in single\nimage reflection removal, they fail to fully exploit inter-layer\ncomplementarity in their physical modeling and network design, which limits the\nquality of image separation. To address this fundamental limitation, we propose\ntwo targeted improvements to enhance dual-stream architectures: First, we\nintroduce a novel inter-layer complementarity model where low-frequency\ncomponents extracted from the residual layer interact with the transmission\nlayer through dual-stream architecture to enhance inter-layer complementarity.\nMeanwhile, high-frequency components from the residual layer provide inverse\nmodulation to both streams, improving the detail quality of the transmission\nlayer. Second, we propose an efficient inter-layer complementarity attention\nmechanism which first cross-reorganizes dual streams at the channel level to\nobtain reorganized streams with inter-layer complementary structures, then\nperforms attention computation on the reorganized streams to achieve better\ninter-layer separation, and finally restores the original stream structure for\noutput. Experimental results demonstrate that our method achieves\nstate-of-the-art separation quality on multiple public datasets while\nsignificantly reducing both computational cost and model complexity.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 02:50:15", "updated": "2025-05-19 02:50:15", "pdf_url": "http://arxiv.org/pdf/2505.12641v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12650v1", "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "abstract": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 03:04:50", "updated": "2025-05-19 03:04:50", "pdf_url": "http://arxiv.org/pdf/2505.12650v1", "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12651v1", "title": "$\\texttt{DIAMONDs}$: A Dataset for $\\mathbb{D}$ynamic $\\mathbb{I}$nformation $\\mathbb{A}$nd $\\mathbb{M}$ental modeling $\\mathbb{O}$f $\\mathbb{N}$umeric $\\mathbb{D}$iscussions", "authors": ["Sayontan Ghosh", "Mahnaz Koupaee", "Yash Kumar Lal", "Pegah Alipoormolabashi", "Mohammad Saqib Hasan", "Jun Seok Kang", "Niranjan Balasubramanian"], "abstract": "Understanding multiparty conversations demands robust Theory of Mind (ToM)\ncapabilities, including the ability to track dynamic information, manage\nknowledge asymmetries, and distinguish relevant information across extended\nexchanges. To advance ToM evaluation in such settings, we present a carefully\ndesigned scalable methodology for generating high-quality benchmark\nconversation-question pairs with these characteristics. Using this methodology,\nwe create $\\texttt{DIAMONDs}$, a new conversational QA dataset covering common\nbusiness, financial or other group interactions. In these goal-oriented\nconversations, participants often have to track certain numerical quantities\n(say $\\textit{expected profit}$) of interest that can be derived from other\nvariable quantities (like $\\textit{marketing expenses, expected sales,\nsalary}$, etc.), whose values also change over the course of the conversation.\n$\\texttt{DIAMONDs}$ questions pose simple numerical reasoning problems over\nsuch quantities of interest (e.g., $\\textit{funds required for charity events,\nexpected company profit next quarter}$, etc.) in the context of the information\nexchanged in conversations. This allows for precisely evaluating ToM\ncapabilities for carefully tracking and reasoning over participants' knowledge\nstates.\n  Our evaluation of state-of-the-art language models reveals significant\nchallenges in handling participant-centric reasoning, specifically in\nsituations where participants have false beliefs. Models also struggle with\nconversations containing distractors and show limited ability to identify\nscenarios with insufficient information. These findings highlight current\nmodels' ToM limitations in handling real-world multi-party conversations.", "categories": ["cs.AI"], "published": "2025-05-19 03:05:13", "updated": "2025-05-19 03:05:13", "pdf_url": "http://arxiv.org/pdf/2505.12651v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12654v1", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "abstract": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:08:30", "updated": "2025-05-19 03:08:30", "pdf_url": "http://arxiv.org/pdf/2505.12654v1", "comment": "Accepected by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12655v1", "title": "Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models", "authors": ["Yisheng Zhong", "Yizhu Wen", "Junfeng Guo", "Mehran Kafai", "Heng Huang", "Hanqing Guo", "Zhuangdi Zhu"], "abstract": "Protecting cyber Intellectual Property (IP) such as web content is an\nincreasingly critical concern. The rise of large language models (LLMs) with\nonline retrieval capabilities presents a double-edged sword that enables\nconvenient access to information but often undermines the rights of original\ncontent creators. As users increasingly rely on LLM-generated responses, they\ngradually diminish direct engagement with original information sources,\nsignificantly reducing the incentives for IP creators to contribute, and\nleading to a saturating cyberspace with more AI-generated content. In response,\nwe propose a novel defense framework that empowers web content creators to\nsafeguard their web-based IP from unauthorized LLM real-time extraction by\nleveraging the semantic understanding capability of LLMs themselves. Our method\nfollows principled motivations and effectively addresses an intractable\nblack-box optimization problem. Real-world experiments demonstrated that our\nmethods improve defense success rates from 2.5% to 88.6% on different LLMs,\noutperforming traditional defenses such as configuration-based restrictions.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 03:14:08", "updated": "2025-05-19 03:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12655v1", "comment": "13 pages, 13 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12662v1", "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "abstract": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:25:18", "updated": "2025-05-19 03:25:18", "pdf_url": "http://arxiv.org/pdf/2505.12662v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12664v1", "title": "Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design", "authors": ["Ziqing Xing", "Zhaoyang Zhang", "Zirui Chen", "Hongning Ruan", "Zhaohui Yang"], "abstract": "In this paper, we incorporate physical knowledge into learning-based\nhigh-precision target sensing using the multi-view channel state information\n(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kind\nof multi-view sensing problem can be naturally cast into a conditional\ngeneration framework. To this end, we design a bipartite neural network\narchitecture, the first part of which uses an elaborately designed encoder to\nfuse the latent target features embedded in the multi-view CSI, and then the\nsecond uses them as conditioning inputs of a powerful generative model to guide\nthe target's reconstruction. Specifically, the encoder is designed to capture\nthe physical correlation between the CSI and the target, and also be adaptive\nto the numbers and positions of BS-UE pairs. Therein the view-specific nature\nof CSI is assimilated by introducing a spatial positional embedding scheme,\nwhich exploits the structure of electromagnetic(EM)-wave propagation channels.\nFinally, a conditional diffusion model with a weighted loss is employed to\ngenerate the target's point cloud from the fused features. Extensive numerical\nresults demonstrate that the proposed generative multi-view (Gen-MV) sensing\nframework exhibits excellent flexibility and significant performance\nimprovement on the reconstruction quality of target's shape and EM properties.", "categories": ["eess.SP", "cs.AI", "cs.LG"], "published": "2025-05-19 03:27:24", "updated": "2025-05-19 03:27:24", "pdf_url": "http://arxiv.org/pdf/2505.12664v1", "comment": "submitted to IEEE Transactions on Wireless Communications", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12669v1", "title": "Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment", "authors": ["Abhinaba Roy", "Geeta Puri", "Dorien Herremans"], "abstract": "We present Text2midi-InferAlign, a novel technique for improving symbolic\nmusic generation at inference time. Our method leverages text-to-audio\nalignment and music structural alignment rewards during inference to encourage\nthe generated music to be consistent with the input caption. Specifically, we\nintroduce two objectives scores: a text-audio consistency score that measures\nrhythmic alignment between the generated music and the original text caption,\nand a harmonic consistency score that penalizes generated music containing\nnotes inconsistent with the key. By optimizing these alignment-based objectives\nduring the generation process, our model produces symbolic music that is more\nclosely tied to the input captions, thereby improving the overall quality and\ncoherence of the generated compositions. Our approach can extend any existing\nautoregressive model without requiring further training or fine-tuning. We\nevaluate our work on top of Text2midi - an existing text-to-midi generation\nmodel, demonstrating significant improvements in both objective and subjective\nevaluation metrics.", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68T07", "I.2.1"], "published": "2025-05-19 03:36:06", "updated": "2025-05-19 03:36:06", "pdf_url": "http://arxiv.org/pdf/2505.12669v1", "comment": "7 pages, 1 figure, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12680v1", "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "abstract": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 03:56:05", "updated": "2025-05-19 03:56:05", "pdf_url": "http://arxiv.org/pdf/2505.12680v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12684v1", "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "abstract": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants.", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.SI"], "published": "2025-05-19 04:06:32", "updated": "2025-05-19 04:06:32", "pdf_url": "http://arxiv.org/pdf/2505.12684v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12692v1", "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "abstract": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 04:32:02", "updated": "2025-05-19 04:32:02", "pdf_url": "http://arxiv.org/pdf/2505.12692v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12701v1", "title": "Counterfactual Explanations for Continuous Action Reinforcement Learning", "authors": ["Shuyang Dong", "Shangtong Zhang", "Lu Feng"], "abstract": "Reinforcement Learning (RL) has shown great promise in domains like\nhealthcare and robotics but often struggles with adoption due to its lack of\ninterpretability. Counterfactual explanations, which address \"what if\"\nscenarios, provide a promising avenue for understanding RL decisions but remain\nunderexplored for continuous action spaces. We propose a novel approach for\ngenerating counterfactual explanations in continuous action RL by computing\nalternative action sequences that improve outcomes while minimizing deviations\nfrom the original sequence. Our approach leverages a distance metric for\ncontinuous actions and accounts for constraints such as adhering to predefined\npolicies in specific states. Evaluations in two RL domains, Diabetes Control\nand Lunar Lander, demonstrate the effectiveness, efficiency, and generalization\nof our approach, enabling more interpretable and trustworthy RL applications.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 04:41:54", "updated": "2025-05-19 04:41:54", "pdf_url": "http://arxiv.org/pdf/2505.12701v1", "comment": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12705v1", "title": "DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories", "authors": ["Joel Jang", "Seonghyeon Ye", "Zongyu Lin", "Jiannan Xiang", "Johan Bjorck", "Yu Fang", "Fengyuan Hu", "Spencer Huang", "Kaushil Kundalia", "Yen-Chen Lin", "Loic Magne", "Ajay Mandlekar", "Avnish Narayan", "You Liang Tan", "Guanzhi Wang", "Jing Wang", "Qi Wang", "Yinzhen Xu", "Xiaohui Zeng", "Kaiyuan Zheng", "Ruijie Zheng", "Ming-Yu Liu", "Luke Zettlemoyer", "Dieter Fox", "Jan Kautz", "Scott Reed", "Yuke Zhu", "Linxi Fan"], "abstract": "We introduce DreamGen, a simple yet highly effective 4-stage pipeline for\ntraining robot policies that generalize across behaviors and environments\nthrough neural trajectories - synthetic robot data generated from video world\nmodels. DreamGen leverages state-of-the-art image-to-video generative models,\nadapting them to the target robot embodiment to produce photorealistic\nsynthetic videos of familiar or novel tasks in diverse environments. Since\nthese models generate only videos, we recover pseudo-action sequences using\neither a latent action model or an inverse-dynamics model (IDM). Despite its\nsimplicity, DreamGen unlocks strong behavior and environment generalization: a\nhumanoid robot can perform 22 new behaviors in both seen and unseen\nenvironments, while requiring teleoperation data from only a single\npick-and-place task in one environment. To evaluate the pipeline\nsystematically, we introduce DreamGen Bench, a video generation benchmark that\nshows a strong correlation between benchmark performance and downstream policy\nsuccess. Our work establishes a promising new axis for scaling robot learning\nwell beyond manual data collection.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-19 04:55:39", "updated": "2025-05-19 04:55:39", "pdf_url": "http://arxiv.org/pdf/2505.12705v1", "comment": "See website for videos:\n  https://research.nvidia.com/labs/gear/dreamgen", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12707v1", "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI", "authors": ["Yingchen He", "Christian D. Weilbach", "Martyna E. Wojciechowska", "Yuxuan Zhang", "Frank Wood"], "abstract": "Advances in deep generative modelling have made it increasingly plausible to\ntrain human-level embodied agents. Yet progress has been limited by the absence\nof large-scale, real-time, multi-modal, and socially interactive datasets that\nreflect the sensory-motor complexity of natural environments. To address this,\nwe present PLAICraft, a novel data collection platform and dataset capturing\nmultiplayer Minecraft interactions across five time-aligned modalities: video,\ngame output audio, microphone input audio, mouse, and keyboard actions. Each\nmodality is logged with millisecond time precision, enabling the study of\nsynchronous, embodied behaviour in a rich, open-ended world. The dataset\ncomprises over 10,000 hours of gameplay from more than 10,000 global\nparticipants.\\footnote{We have done a privacy review for the public release of\nan initial 200-hour subset of the dataset, with plans to release most of the\ndataset over time.} Alongside the dataset, we provide an evaluation suite for\nbenchmarking model capabilities in object recognition, spatial awareness,\nlanguage grounding, and long-term memory. PLAICraft opens a path toward\ntraining and evaluating agents that act fluently and purposefully in real time,\npaving the way for truly embodied artificial intelligence.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "published": "2025-05-19 05:00:47", "updated": "2025-05-19 05:00:47", "pdf_url": "http://arxiv.org/pdf/2505.12707v1", "comment": "9 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12711v1", "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining", "authors": ["Qichen Sun", "Zhengrui Guo", "Rui Peng", "Hao Chen", "Jinzhuo Wang"], "abstract": "Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 05:07:34", "updated": "2025-05-19 05:07:34", "pdf_url": "http://arxiv.org/pdf/2505.12711v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12716v1", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "abstract": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 05:16:21", "updated": "2025-05-19 05:16:21", "pdf_url": "http://arxiv.org/pdf/2505.12716v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12731v1", "title": "Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps", "authors": ["Jie Ou", "Jinyu Guo", "Shuaihong Jiang", "Zhaokun Wang", "Libo Qin", "Shunyu Yao", "Wenhong Tian"], "abstract": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.", "categories": ["cs.AI"], "published": "2025-05-19 05:39:38", "updated": "2025-05-19 05:39:38", "pdf_url": "http://arxiv.org/pdf/2505.12731v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12734v1", "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "abstract": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences.", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "published": "2025-05-19 05:47:13", "updated": "2025-05-19 05:47:13", "pdf_url": "http://arxiv.org/pdf/2505.12734v1", "comment": "14 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12737v1", "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning", "authors": ["Hongjoon Ahn", "Heewoong Choi", "Jisu Han", "Taesup Moon"], "abstract": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical\nlearning paradigm where goal-reaching policies are trained from abundant\nunlabeled (reward-free) datasets without additional environment interaction.\nHowever, offline GCRL still struggles with long-horizon tasks, even with recent\nadvances that employ hierarchical policy structures, such as HIQL. By\nidentifying the root cause of this challenge, we observe the following\ninsights: First, performance bottlenecks mainly stem from the high-level\npolicy's inability to generate appropriate subgoals. Second, when learning the\nhigh-level policy in the long-horizon regime, the sign of the advantage signal\nfrequently becomes incorrect. Thus, we argue that improving the value function\nto produce a clear advantage signal for learning the high-level policy is\nessential. In this paper, we propose a simple yet effective solution:\nOption-aware Temporally Abstracted value learning, dubbed OTA, which\nincorporates temporal abstraction into the temporal-difference learning\nprocess. By modifying the value update to be option-aware, the proposed\nlearning scheme contracts the effective horizon length, enabling better\nadvantage estimates even in long-horizon regimes. We experimentally show that\nthe high-level policy extracted using the OTA value function achieves strong\nperformance on complex tasks from OGBench, a recently proposed offline GCRL\nbenchmark, including maze navigation and visual robotic manipulation\nenvironments.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 05:51:11", "updated": "2025-05-19 05:51:11", "pdf_url": "http://arxiv.org/pdf/2505.12737v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12738v1", "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "abstract": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs.", "categories": ["cs.LG", "cs.AI", "cs.SI"], "published": "2025-05-19 05:53:25", "updated": "2025-05-19 05:53:25", "pdf_url": "http://arxiv.org/pdf/2505.12738v1", "comment": "18 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12741v1", "title": "Dense Communication between Language Models", "authors": ["Shiguang Wu", "Yaqing Wang", "Quanming Yao"], "abstract": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility.", "categories": ["cs.AI"], "published": "2025-05-19 05:56:06", "updated": "2025-05-19 05:56:06", "pdf_url": "http://arxiv.org/pdf/2505.12741v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12744v1", "title": "Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation", "authors": ["Weiliang Tang", "Dong Jing", "Jia-Hui Pan", "Zhiwu Lu", "Yun-Hui Liu", "Li Erran Li", "Mingyu Ding", "Chi-Wing Fu"], "abstract": "Recent Large Multimodal Models have demonstrated remarkable reasoning\ncapabilities, especially in solving complex mathematical problems and realizing\naccurate spatial perception. Our key insight is that these emerging abilities\ncan naturally extend to robotic manipulation by enabling LMMs to directly infer\nthe next goal in language via reasoning, rather than relying on a separate\naction head. However, this paradigm meets two main challenges: i) How to make\nLMMs understand the spatial action space, and ii) How to fully exploit the\nreasoning capacity of LMMs in solving these tasks. To tackle the former\nchallenge, we propose a novel task formulation, which inputs the current states\nof object parts and the gripper, and reformulates rotation by a new axis\nrepresentation instead of traditional Euler angles. This representation is more\ncompatible with spatial reasoning and easier to interpret within a unified\nlanguage space. For the latter challenge, we design a pipeline to utilize\ncutting-edge LMMs to generate a small but high-quality reasoning dataset of\nmulti-round dialogues that successfully solve manipulation tasks for supervised\nfine-tuning. Then, we perform reinforcement learning by trial-and-error\ninteractions in simulation to further enhance the model's reasoning abilities\nfor robotic manipulation. Our resulting reasoning model built upon a 7B\nbackbone, named ReasonManip, demonstrates three notable advantages driven by\nits system-2 level reasoning capabilities: i) exceptional generalizability to\nout-of-distribution environments, objects, and tasks; ii) inherent sim-to-real\ntransfer ability enabled by the unified language representation shared across\ndomains; iii) transparent interpretability connecting high-level reasoning and\nlow-level control. Extensive experiments demonstrate the effectiveness of the\nproposed paradigm and its potential to advance LMM-driven robotic manipulation.", "categories": ["cs.AI"], "published": "2025-05-19 06:00:14", "updated": "2025-05-19 06:00:14", "pdf_url": "http://arxiv.org/pdf/2505.12744v1", "comment": "17 pages, 16 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12745v1", "title": "PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization", "authors": ["Dong Kyu Cho", "Inwoo Hwang", "Sanghack Lee"], "abstract": "Data augmentation is a popular tool for single source domain generalization,\nwhich expands the source domain by generating simulated ones, improving\ngeneralization on unseen target domains. In this work, we show that the\nperformance of such augmentation-based methods in the target domains\nuniversally fluctuates during training, posing challenges in model selection\nunder realistic scenarios. We argue that the fluctuation stems from the\ninability of the model to accumulate the knowledge learned from diverse\naugmentations, exacerbating feature distortion during training. Based on this\nobservation, we propose a novel generalization method, coined Parameter-Space\nEnsemble with Entropy Regularization (PEER), that uses a proxy model to learn\nthe augmented data on behalf of the main model. The main model is updated by\naveraging its parameters with the proxy model, progressively accumulating\nknowledge over the training steps. Maximizing the mutual information between\nthe output representations of the two models guides the learning process of the\nproxy model, mitigating feature distortion during training. Experimental\nresults demonstrate the effectiveness of PEER in reducing the OOD performance\nfluctuation and enhancing generalization across various datasets, including\nPACS, Digits, Office-Home, and VLCS. Notably, our method with simple random\naugmentation achieves state-of-the-art performance, surpassing prior approaches\non sDG that utilize complex data augmentation strategies.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 06:01:11", "updated": "2025-05-19 06:01:11", "pdf_url": "http://arxiv.org/pdf/2505.12745v1", "comment": "21 pages, 9 figures, Accepted at CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12746v1", "title": "Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs", "authors": ["Haruka Asanuma", "Naoko Koide-Majima", "Ken Nakamura", "Takato Horii", "Shinji Nishimoto", "Masafumi Oizumi"], "abstract": "Recent studies have revealed that human emotions exhibit a high-dimensional,\ncomplex structure. A full capturing of this complexity requires new approaches,\nas conventional models that disregard high dimensionality risk overlooking key\nnuances of human emotions. Here, we examined the extent to which the latest\ngeneration of rapidly evolving Multimodal Large Language Models (MLLMs) capture\nthese high-dimensional, intricate emotion structures, including capabilities\nand limitations. Specifically, we compared self-reported emotion ratings from\nparticipants watching videos with model-generated estimates (e.g., Gemini or\nGPT). We evaluated performance not only at the individual video level but also\nfrom emotion structures that account for inter-video relationships. At the\nlevel of simple correlation between emotion structures, our results\ndemonstrated strong similarity between human and model-inferred emotion\nstructures. To further explore whether the similarity between humans and models\nis at the signle item level or the coarse-categorical level, we applied Gromov\nWasserstein Optimal Transport. We found that although performance was not\nnecessarily high at the strict, single-item level, performance across video\ncategories that elicit similar emotions was substantial, indicating that the\nmodel could infer human emotional experiences at the category level. Our\nresults suggest that current state-of-the-art MLLMs broadly capture the complex\nhigh-dimensional emotion structures at the category level, as well as their\napparent limitations in accurately capturing entire structures at the\nsingle-item level.", "categories": ["cs.AI", "I.2.7; I.2.10; I.5.1"], "published": "2025-05-19 06:03:22", "updated": "2025-05-19 06:03:22", "pdf_url": "http://arxiv.org/pdf/2505.12746v1", "comment": "25 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12748v1", "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "abstract": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-05-19 06:08:53", "updated": "2025-05-19 06:08:53", "pdf_url": "http://arxiv.org/pdf/2505.12748v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12750v1", "title": "Malware families discovery via Open-Set Recognition on Android manifest permissions", "authors": ["Filippo Leveni", "Matteo Mistura", "Francesco Iubatti", "Carmine Giangregorio", "Nicol\u00f2 Pastore", "Cesare Alippi", "Giacomo Boracchi"], "abstract": "Malware are malicious programs that are grouped into families based on their\npenetration technique, source code, and other characteristics. Classifying\nmalware programs into their respective families is essential for building\neffective defenses against cyber threats. Machine learning models have a huge\npotential in malware detection on mobile devices, as malware families can be\nrecognized by classifying permission data extracted from Android manifest\nfiles. Still, the malware classification task is challenging due to the\nhigh-dimensional nature of permission data and the limited availability of\ntraining samples. In particular, the steady emergence of new malware families\nmakes it impossible to acquire a comprehensive training set covering all the\nmalware classes. In this work, we present a malware classification system that,\non top of classifying known malware, detects new ones. In particular, we\ncombine an open-set recognition technique developed within the computer vision\ncommunity, namely MaxLogit, with a tree-based Gradient Boosting classifier,\nwhich is particularly effective in classifying high-dimensional data. Our\nsolution turns out to be very practical, as it can be seamlessly employed in a\nstandard classification workflow, and efficient, as it adds minimal\ncomputational overhead. Experiments on public and proprietary datasets\ndemonstrate the potential of our solution, which has been deployed in a\nbusiness environment.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "published": "2025-05-19 06:19:54", "updated": "2025-05-19 06:19:54", "pdf_url": "http://arxiv.org/pdf/2505.12750v1", "comment": "Submitted to European Conference on Artificial Intelligence (ECAI\n  2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12751v1", "title": "Structure-based Anomaly Detection and Clustering", "authors": ["Filippo Leveni"], "abstract": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system.", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "published": "2025-05-19 06:20:00", "updated": "2025-05-19 06:20:00", "pdf_url": "http://arxiv.org/pdf/2505.12751v1", "comment": "Doctoral dissertation at Politecnico di Milano", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12761v1", "title": "Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding", "authors": ["Donghwa Shin", "Edwin Zhang"], "abstract": "Transformers have recently gained popularity in time series forecasting due\nto their ability to capture long-term dependencies. However, many existing\nmodels focus only on capturing temporal dependencies while omitting intricate\nrelationships between variables. Recent models have tried tackling this by\nexplicitly modeling both cross-time and cross-variate dependencies through a\nsequential or unified attention mechanism, but they are entirely channel\ndependent (CD) across all layers, making them potentially susceptible to\noverfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),\na lightweight CD module that injects cross-variate context into\nchannel-independent (CI) models by simply modifying the patch embedding\nprocess. We achieve this by adding a learnable positional encoding and a\nlightweight router-attention block to the vanilla patch embedding layer. We\nthen integrate CVPE into Time-LLM, a multimodal CI forecasting model, to\ndemonstrate its effectiveness in capturing cross-variate dependencies and\nenhance the CI model's performance. Extensive experimental results on seven\nreal-world datasets show that our enhanced Time-LLM outperforms the original\nbaseline model simply by incorporating the CVPE module, with no other changes.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 06:41:14", "updated": "2025-05-19 06:41:14", "pdf_url": "http://arxiv.org/pdf/2505.12761v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12762v1", "title": "IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment", "authors": ["Chenlin Ming", "Chendi Qu", "Mengzhang Cai", "Qizhi Pei", "Zhuoshi Pan", "Yu Li", "Xiaoming Duan", "Lijun Wu", "Conghui He"], "abstract": "Large Language Models (LLMs) have achieved impressive performance through\nSupervised Fine-tuning (SFT) on diverse instructional datasets. When training\non multiple capabilities simultaneously, the mixture training dataset, governed\nby volumes of data from different domains, is a critical factor that directly\nimpacts the final model's performance. Unlike many studies that focus on\nenhancing the quality of training datasets through data selection methods, few\nworks explore the intricate relationship between the compositional quantity of\nmixture training datasets and the emergent capabilities of LLMs. Given the\navailability of a high-quality multi-domain training dataset, understanding the\nimpact of data from each domain on the model's overall capabilities is crucial\nfor preparing SFT data and training a well-balanced model that performs\neffectively across diverse domains. In this work, we introduce IDEAL, an\ninnovative data equilibrium adaptation framework designed to effectively\noptimize volumes of data from different domains within mixture SFT datasets,\nthereby enhancing the model's alignment and performance across multiple\ncapabilities. IDEAL employs a gradient-based approach to iteratively refine the\ntraining data distribution, dynamically adjusting the volumes of\ndomain-specific data based on their impact on downstream task performance. By\nleveraging this adaptive mechanism, IDEAL ensures a balanced dataset\ncomposition, enabling the model to achieve robust generalization and consistent\nproficiency across diverse tasks. Experiments across different capabilities\ndemonstrate that IDEAL outperforms conventional uniform data allocation\nstrategies, achieving a comprehensive improvement of approximately 7% in\nmulti-task evaluation scores.", "categories": ["cs.AI"], "published": "2025-05-19 06:42:44", "updated": "2025-05-19 06:42:44", "pdf_url": "http://arxiv.org/pdf/2505.12762v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12763v1", "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "abstract": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 06:43:08", "updated": "2025-05-19 06:43:08", "pdf_url": "http://arxiv.org/pdf/2505.12763v1", "comment": "Accepted to ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12767v1", "title": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates", "authors": ["Danqing Chen", "Tobias Ladner", "Ahmed Rayen Mhadhbi", "Matthias Althoff"], "abstract": "As large language models become integral to high-stakes applications,\nensuring their robustness and fairness is critical. Despite their success,\nlarge language models remain vulnerable to adversarial attacks, where small\nperturbations, such as synonym substitutions, can alter model predictions,\nposing risks in fairness-critical areas, such as gender bias mitigation, and\nsafety-critical areas, such as toxicity detection. While formal verification\nhas been explored for neural networks, its application to large language models\nremains limited. This work presents a holistic verification framework to\ncertify the robustness of transformer-based language models, with a focus on\nensuring gender fairness and consistent outputs across different gender-related\nterms. Furthermore, we extend this methodology to toxicity detection, offering\nformal guarantees that adversarially manipulated toxic inputs are consistently\ndetected and appropriately censored, thereby ensuring the reliability of\nmoderation systems. By formalizing robustness within the embedding space, this\nwork strengthens the reliability of language models in ethical AI deployment\nand content moderation.", "categories": ["cs.AI"], "published": "2025-05-19 06:46:17", "updated": "2025-05-19 06:46:17", "pdf_url": "http://arxiv.org/pdf/2505.12767v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12774v1", "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "abstract": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation.", "categories": ["cs.GR", "cs.AI", "cs.CV"], "published": "2025-05-19 07:02:12", "updated": "2025-05-19 07:02:12", "pdf_url": "http://arxiv.org/pdf/2505.12774v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12781v1", "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:10:42", "updated": "2025-05-19 07:10:42", "pdf_url": "http://arxiv.org/pdf/2505.12781v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12788v1", "title": "Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs", "authors": ["Zhongni Hou", "Miao Su", "Xiaolong Jin", "Zixuan Li", "Long Bai", "Jiafeng Guo", "Xueqi Cheng"], "abstract": "Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of\n(subject, predicate, object, timestamp) to describe temporal facts, have\nattracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditional\nTKGs by utilizing n-tuples to incorporate auxiliary elements alongside core\nelements (i.e., subject, predicate, and object) of facts, so as to represent\nthem in a more fine-grained manner. Reasoning over N-TKGs aims to predict\npotential future facts based on historical ones. However, existing N-TKG\nreasoning methods often lack explainability due to their black-box nature.\nTherefore, we introduce a new Reinforcement Learning-based method, named\nMT-Path, which leverages the temporal information to traverse historical\nn-tuples and construct a temporal reasoning path. Specifically, in order to\nintegrate the information encapsulated within n-tuples, i.e., the\nentity-irrelevant information within the predicate, the information about core\nelements, and the complete information about the entire n-tuples, MT-Path\nutilizes a mixture policy-driven action selector, which bases on three\nlow-level policies, namely, the predicate-focused policy, the\ncore-element-focused policy and the whole-fact-focused policy. Further, MT-Path\nutilizes an auxiliary element-aware GCN to capture the rich semantic\ndependencies among facts, thereby enabling the agent to gain a deep\nunderstanding of each n-tuple. Experimental results demonstrate the\neffectiveness and the explainability of MT-Path.", "categories": ["cs.AI"], "published": "2025-05-19 07:20:33", "updated": "2025-05-19 07:20:33", "pdf_url": "http://arxiv.org/pdf/2505.12788v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12795v1", "title": "FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities", "authors": ["Shibo Hong", "Jiahao Ying", "Haiyuan Liang", "Mengdi Zhang", "Jun Kuang", "Jiazheng Zhang", "Yixin Cao"], "abstract": "Evaluating the open-ended outputs of large language models (LLMs) has become\na bottleneck as model capabilities, task diversity, and modality coverage\nrapidly expand. Existing \"LLM-as-a-Judge\" evaluators are typically narrow in a\nfew tasks, aspects, or modalities, and easily suffer from low consistency. In\nthis paper, we argue that explicit, fine-grained aspect specification is the\nkey to both generalizability and objectivity in automated evaluation. To do so,\nwe introduce a hierarchical aspect taxonomy spanning 112 aspects that unifies\nevaluation across four representative settings - Natural Language Generation,\nImage Understanding, Image Generation, and Interleaved Text-and-Image\nGeneration. Building on this taxonomy, we create FRAbench, a benchmark\ncomprising 60.4k pairwise samples with 325k aspect-level labels obtained from a\ncombination of human and LLM annotations. FRAbench provides the first\nlarge-scale, multi-modal resource for training and meta-evaluating fine-grained\nLMM judges. Leveraging FRAbench, we develop GenEval, a fine-grained evaluator\ngeneralizable across tasks and modalities. Experiments show that GenEval (i)\nattains high agreement with GPT-4o and expert annotators, (ii) transfers\nrobustly to unseen tasks and modalities, and (iii) reveals systematic\nweaknesses of current LMMs on evaluation.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-19 07:29:26", "updated": "2025-05-19 07:29:26", "pdf_url": "http://arxiv.org/pdf/2505.12795v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12800v1", "title": "OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching", "authors": ["Hieu-Nghia Huynh-Nguyen", "Ngoc Son Nguyen", "Huynh Nguyen Dang", "Thieu Vo", "Truong-Son Hy", "Van Nguyen"], "abstract": "Text-to-speech (TTS) systems have seen significant advancements in recent\nyears, driven by improvements in deep learning and neural network\narchitectures. Viewing the output speech as a data distribution, previous\napproaches often employ traditional speech representations, such as waveforms\nor spectrograms, within the Flow Matching framework. However, these methods\nhave limitations, including overlooking various speech attributes and incurring\nhigh computational costs due to additional constraints introduced during\ntraining. To address these challenges, we introduce OZSpeech, the first TTS\nmethod to explore optimal transport conditional flow matching with one-step\nsampling and a learned prior as the condition, effectively disregarding\npreceding states and reducing the number of sampling steps. Our approach\noperates on disentangled, factorized components of speech in token format,\nenabling accurate modeling of each speech attribute, which enhances the TTS\nsystem's ability to precisely clone the prompt speech. Experimental results\nshow that our method achieves promising performance over existing methods in\ncontent accuracy, naturalness, prosody generation, and speaker style\npreservation. Audio samples are available at our demo page\nhttps://ozspeech.github.io/OZSpeech_Web/.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 07:31:55", "updated": "2025-05-19 07:31:55", "pdf_url": "http://arxiv.org/pdf/2505.12800v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12805v1", "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA", "authors": ["Seanie Lee", "Sangwoo Park", "Dong Bok Lee", "Dominik Wagner", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "abstract": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable\nlow-rank matrices into frozen pre-trained weights, is widely used for efficient\nfine-tuning of language models in federated learning (FL). However, when\ncombined with differentially private stochastic gradient descent (DP-SGD), LoRA\nfaces substantial noise amplification: DP-SGD perturbs per-sample gradients,\nand the matrix multiplication of the LoRA update ($BA$) intensifies this\neffect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model\nexpressiveness, often resulting in suboptimal adaptation. To address this, we\npropose FedSVD, a simple yet effective method that introduces a global\nreparameterization based on singular value decomposition (SVD). In our\napproach, each client optimizes only the $B$ matrix and transmits it to the\nserver. The server aggregates the $B$ matrices, computes the product $BA$ using\nthe previous $A$, and refactorizes the result via SVD. This yields a new\nadaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an\nupdated $B$ containing the remaining SVD components. This reparameterization\navoids quadratic noise amplification, while allowing $A$ to better capture the\nprincipal directions of the aggregate updates. Moreover, the orthonormal\nstructure of $A$ bounds the gradient norms of $B$ and preserves more signal\nunder DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD\nconsistently improves stability and performance across a variety of privacy\nsettings and benchmarks, outperforming relevant baselines under both private\nand non-private regimes.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 07:32:56", "updated": "2025-05-19 07:32:56", "pdf_url": "http://arxiv.org/pdf/2505.12805v1", "comment": "preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12811v1", "title": "Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning", "authors": ["Wei-Chen Liao", "Ti-Rong Wu", "I-Chen Wu"], "abstract": "Multi-agent reinforcement Learning (MARL) is often challenged by the sight\nrange dilemma, where agents either receive insufficient or excessive\ninformation from their environment. In this paper, we propose a novel method,\ncalled Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes\nan Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight\nrange during training. Experiment results show several advantages of using DSR.\nFirst, we demonstrate using DSR achieves better performance in three common\nMARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse\n(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show\nthat DSR consistently improves performance across multiple MARL algorithms,\nincluding QMIX and MAPPO. Third, DSR offers suitable sight ranges for different\ntraining steps, thereby accelerating the training process. Finally, DSR\nprovides additional interpretability by indicating the optimal sight range used\nduring training. Unlike existing methods that rely on global information or\ncommunication mechanisms, our approach operates solely based on the individual\nsight ranges of agents. This approach offers a practical and efficient solution\nto the sight range dilemma, making it broadly applicable to real-world complex\nenvironments.", "categories": ["cs.MA", "cs.AI", "cs.LG"], "published": "2025-05-19 07:40:42", "updated": "2025-05-19 07:40:42", "pdf_url": "http://arxiv.org/pdf/2505.12811v1", "comment": "Accepted at AAMAS 2025. The compiled PDF includes the appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12814v1", "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "abstract": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:45:09", "updated": "2025-05-19 07:45:09", "pdf_url": "http://arxiv.org/pdf/2505.12814v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12815v1", "title": "Learning in Chaos: Efficient Autoscaling and Self-healing for Distributed Training at the Edge", "authors": ["Wenjiao Feng", "Rongxing Xiao", "Zonghang Li", "Hongfang Yu", "Gang Sun", "Long Luo", "Mohsen Guizani", "Qirong Ho"], "abstract": "Frequent node and link changes in edge AI clusters disrupt distributed\ntraining, while traditional checkpoint-based recovery and cloud-centric\nautoscaling are too slow for scale-out and ill-suited to chaotic and\nself-governed edge. This paper proposes Chaos, a resilient and scalable edge\ndistributed training system with built-in self-healing and autoscaling. It\nspeeds up scale-out by using multi-neighbor replication with fast shard\nscheduling, allowing a new node to pull the latest training state from nearby\nneighbors in parallel while balancing the traffic load between them. It also\nuses a cluster monitor to track resource and topology changes to assist\nscheduler decisions, and handles scaling events through peer negotiation\nprotocols, enabling fully self-governed autoscaling without a central admin.\nExtensive experiments show that Chaos consistently achieves much lower\nscale-out delays than Pollux, EDL, and Autoscaling, and handles scale-in,\nconnect-link, and disconnect-link events within 1 millisecond, making it\nsmoother to handle node joins, exits, and failures. It also delivers the lowest\nidle time, showing superior resource use and scalability as the cluster grows.", "categories": ["cs.DC", "cs.AI", "68T99", "I.2.11"], "published": "2025-05-19 07:52:17", "updated": "2025-05-19 07:52:17", "pdf_url": "http://arxiv.org/pdf/2505.12815v1", "comment": "13 pages, 16 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12821v1", "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "abstract": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:03:38", "updated": "2025-05-19 08:03:38", "pdf_url": "http://arxiv.org/pdf/2505.12821v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12822v1", "title": "Emergent Specialization: Rare Token Neurons in Language Models", "authors": ["Jing Liu", "Haozheng Wang", "Yueheng Li"], "abstract": "Large language models struggle with representing and generating rare tokens\ndespite their importance in specialized domains. In this study, we identify\nneuron structures with exceptionally strong influence on language model's\nprediction of rare tokens, termed as rare token neurons, and investigate the\nmechanism for their emergence and behavior. These neurons exhibit a\ncharacteristic three-phase organization (plateau, power-law, and rapid decay)\nthat emerges dynamically during training, evolving from a homogeneous initial\nstate to a functionally differentiated architecture. In the activation space,\nrare token neurons form a coordinated subnetwork that selectively co-activates\nwhile avoiding co-activation with other neurons. This functional specialization\npotentially correlates with the development of heavy-tailed weight\ndistributions, suggesting a statistical mechanical basis for emergent\nspecialization.", "categories": ["cs.AI"], "published": "2025-05-19 08:05:13", "updated": "2025-05-19 08:05:13", "pdf_url": "http://arxiv.org/pdf/2505.12822v1", "comment": "9 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12833v1", "title": "Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs", "authors": ["Zhuo Yang", "Lingli Ge", "Dong Han", "Tianfan Fu", "Yuqiang Li"], "abstract": "Many real-world scientific and industrial applications require the\noptimization of expensive black-box functions. Bayesian Optimization (BO)\nprovides an effective framework for such problems. However, traditional BO\nmethods are prone to get trapped in local optima and often lack interpretable\ninsights. To address this issue, this paper designs Reasoning BO, a novel\nframework that leverages reasoning models to guide the sampling process in BO\nwhile incorporating multi-agent systems and knowledge graphs for online\nknowledge accumulation. By integrating the reasoning and contextual\nunderstanding capabilities of Large Language Models (LLMs), we can provide\nstrong guidance to enhance the BO process. As the optimization progresses,\nReasoning BO provides real-time sampling recommendations along with critical\ninsights grounded in plausible scientific theories, aiding in the discovery of\nsuperior solutions within the search space. We systematically evaluate our\napproach across 10 diverse tasks encompassing synthetic mathematical functions\nand complex real-world applications. The framework demonstrates its capability\nto progressively refine sampling strategies through real-time insights and\nhypothesis evolution, effectively identifying higher-performing regions of the\nsearch space for focused exploration. This process highlights the powerful\nreasoning and context-learning abilities of LLMs in optimization scenarios. For\nexample, in the Direct Arylation task, our method increased the yield to 60.7%,\nwhereas traditional BO achieved only a 25.2% yield. Furthermore, our\ninvestigation reveals that smaller LLMs, when fine-tuned through reinforcement\nlearning, can attain comparable performance to their larger counterparts. This\nenhanced reasoning capability paves the way for more efficient automated\nscientific experimentation while maintaining computational feasibility.", "categories": ["cs.AI"], "published": "2025-05-19 08:20:40", "updated": "2025-05-19 08:20:40", "pdf_url": "http://arxiv.org/pdf/2505.12833v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12837v1", "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "abstract": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:25:21", "updated": "2025-05-19 08:25:21", "pdf_url": "http://arxiv.org/pdf/2505.12837v1", "comment": "20 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12843v1", "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF", "authors": ["Kangwen Zhao", "Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "abstract": "Reinforcement Learning from Human Feedback relies on reward models to align\nlarge language models with human preferences. However, RLHF often suffers from\nreward hacking, wherein policy learning exploits flaws in the trained reward\nmodel to maximize reward scores without genuinely aligning with human\npreferences. A significant example of such reward hacking is length bias, where\nreward models usually favor longer responses irrespective of actual response\nquality. Previous works on length bias have notable limitations, these\napproaches either mitigate bias without characterizing the bias form, or simply\nassume a linear length-reward relation. To accurately model the intricate\nnature of length bias and facilitate more effective bias mitigation, we propose\nFiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a\nframework that autonomously learns and corrects underlying bias patterns. Our\napproach consists of three stages: First, we train a standard reward model\nwhich inherently contains length bias. Next, we deploy a lightweight fitting\nmodel to explicitly capture the non-linear relation between length and reward.\nFinally, we incorporate this learned relation into the reward model to debias.\nExperimental results demonstrate that FiMi-RM achieves a more balanced\nlength-reward distribution. Furthermore, when applied to alignment algorithms,\nour debiased reward model improves length-controlled win rate and reduces\nverbosity without compromising its performance.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 08:29:28", "updated": "2025-05-19 08:29:28", "pdf_url": "http://arxiv.org/pdf/2505.12843v1", "comment": "Due to the word limit for arXiv abstract, the abstract here has been\n  abridged compared to the one in the PDF", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12844v1", "title": "AGI-Elo: How Far Are We From Mastering A Task?", "authors": ["Shuo Sun", "Yimin Zhao", "Christina Dao Wen Lee", "Jiawei Sun", "Chengran Yuan", "Zefan Huang", "Dongen Li", "Justin KW Yeoh", "Alok Prakash", "Thomas W. Malone", "Marcelo H. Ang Jr"], "abstract": "As the field progresses toward Artificial General Intelligence (AGI), there\nis a pressing need for more comprehensive and insightful evaluation frameworks\nthat go beyond aggregate performance metrics. This paper introduces a unified\nrating system that jointly models the difficulty of individual test cases and\nthe competency of AI models (or humans) across vision, language, and action\ndomains. Unlike existing metrics that focus solely on models, our approach\nallows for fine-grained, difficulty-aware evaluations through competitive\ninteractions between models and tasks, capturing both the long-tail\ndistribution of real-world challenges and the competency gap between current\nmodels and full task mastery. We validate the generalizability and robustness\nof our system through extensive experiments on multiple established datasets\nand models across distinct AGI domains. The resulting rating distributions\noffer novel perspectives and interpretable insights into task difficulty, model\nprogression, and the outstanding challenges that remain on the path to\nachieving full AGI task mastery.", "categories": ["cs.AI", "cs.RO"], "published": "2025-05-19 08:30:13", "updated": "2025-05-19 08:30:13", "pdf_url": "http://arxiv.org/pdf/2505.12844v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12845v1", "title": "Multi-Level Aware Preference Learning: Enhancing RLHF for Complex Multi-Instruction Tasks", "authors": ["Ruopei Sun", "Jianfeng Cai", "Jinhua Zhu", "Kangwen Zhao", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "abstract": "RLHF has emerged as a predominant approach for aligning artificial\nintelligence systems with human preferences, demonstrating exceptional and\nmeasurable efficacy in instruction following tasks; however, it exhibits\ninsufficient compliance capabilities when confronted with complex\nmulti-instruction tasks. Conventional approaches rely heavily on human\nannotation or more sophisticated large language models, thereby introducing\nsubstantial resource expenditure or potential bias concerns. Meanwhile,\nalternative synthetic methods that augment standard preference datasets often\ncompromise the model's semantic quality. Our research identifies a critical\noversight in existing techniques, which predominantly focus on comparing\nresponses while neglecting valuable latent signals embedded within prompt\ninputs, and which only focus on preference disparities at the intra-sample\nlevel, while neglecting to account for the inter-sample level preference\ndifferentials that exist among preference data. To leverage these previously\nneglected indicators, we propose a novel Multi-level Aware Preference Learning\n(MAPL) framework, capable of enhancing multi-instruction capabilities.\nSpecifically, for any given response in original preference data pairs, we\nconstruct varied prompts with a preference relation under different conditions,\nin order to learn intra-sample level preference disparities. Furthermore, for\nany given original preference pair, we synthesize multi-instruction preference\npairs to capture preference discrepancies at the inter-sample level. Building\non the two datasets constructed above, we consequently devise two sophisticated\ntraining objective functions. Subsequently, our framework integrates seamlessly\ninto both Reward Modeling and Direct Preference Optimization paradigms. Through\nrigorous evaluation across multiple benchmarks, we empirically validate the\nefficacy of our framework.", "categories": ["cs.AI"], "published": "2025-05-19 08:33:11", "updated": "2025-05-19 08:33:11", "pdf_url": "http://arxiv.org/pdf/2505.12845v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12851v1", "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting", "authors": ["Yanhua Wen", "Lu Ai", "Gang Liu", "Chuang Li", "Jianhao Wei"], "abstract": "Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 08:39:07", "updated": "2025-05-19 08:39:07", "pdf_url": "http://arxiv.org/pdf/2505.12851v1", "comment": "14 pages, 5 figures, BlockSys2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12863v1", "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "abstract": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation.", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "published": "2025-05-19 08:46:45", "updated": "2025-05-19 08:46:45", "pdf_url": "http://arxiv.org/pdf/2505.12863v1", "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12864v1", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr\u00fcwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "abstract": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "published": "2025-05-19 08:48:12", "updated": "2025-05-19 08:48:12", "pdf_url": "http://arxiv.org/pdf/2505.12864v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12869v1", "title": "Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption", "authors": ["Koki Wakiyama", "Tomohiro I", "Hiroshi Sakamoto"], "abstract": "Feature selection is a technique that extracts a meaningful subset from a set\nof features in training data. When the training data is large-scale,\nappropriate feature selection enables the removal of redundant features, which\ncan improve generalization performance, accelerate the training process, and\nenhance the interpretability of the model. This study proposes a\nprivacy-preserving computation model for feature selection. Generally, when the\ndata owner and analyst are the same, there is no need to conceal the private\ninformation. However, when they are different parties or when multiple owners\nexist, an appropriate privacy-preserving framework is required. Although\nvarious private feature selection algorithms, they all require two or more\ncomputing parties and do not guarantee security in environments where no\nexternal party can be fully trusted. To address this issue, we propose the\nfirst outsourcing algorithm for feature selection using fully homomorphic\nencryption. Compared to a prior two-party algorithm, our result improves the\ntime and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n\ndenote the number of features and data samples, respectively. We also\nimplemented the proposed algorithm and conducted comparative experiments with\nthe naive one. The experimental result shows the efficiency of our method even\nwith small datasets.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 08:55:56", "updated": "2025-05-19 08:55:56", "pdf_url": "http://arxiv.org/pdf/2505.12869v1", "comment": "14 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12871v1", "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-19 08:57:08", "updated": "2025-05-19 08:57:08", "pdf_url": "http://arxiv.org/pdf/2505.12871v1", "comment": "To appear at ICML 25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12872v1", "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging", "authors": ["Maytus Piriyajitakonkij", "Rujikorn Charakorn", "Weicheng Tao", "Wei Pan", "Mingfei Sun", "Cheston Tan", "Mengmi Zhang"], "abstract": "Early cavemen relied on gestures, vocalizations, and simple signals to\ncoordinate, plan, avoid predators, and share resources. Today, humans\ncollaborate using complex languages to achieve remarkable results. What drives\nthis evolution in communication? How does language emerge, adapt, and become\nvital for teamwork? Understanding the origins of language remains a challenge.\nA leading hypothesis in linguistics and anthropology posits that language\nevolved to meet the ecological and social demands of early human cooperation.\nLanguage did not arise in isolation, but through shared survival goals.\nInspired by this view, we investigate the emergence of language in multi-agent\nForaging Games. These environments are designed to reflect the cognitive and\necological constraints believed to have influenced the evolution of\ncommunication. Agents operate in a shared grid world with only partial\nknowledge about other agents and the environment, and must coordinate to\ncomplete games like picking up high-value targets or executing temporally\nordered actions. Using end-to-end deep reinforcement learning, agents learn\nboth actions and communication strategies from scratch. We find that agents\ndevelop communication protocols with hallmark features of natural language:\narbitrariness, interchangeability, displacement, cultural transmission, and\ncompositionality. We quantify each property and analyze how different factors,\nsuch as population size and temporal dependencies, shape specific aspects of\nthe emergent language. Our framework serves as a platform for studying how\nlanguage can evolve from partial observability, temporal reasoning, and\ncooperative goals in embodied multi-agent settings. We will release all data,\ncode, and models publicly.", "categories": ["cs.AI", "cs.LG", "cs.MA"], "published": "2025-05-19 08:57:30", "updated": "2025-05-19 08:57:30", "pdf_url": "http://arxiv.org/pdf/2505.12872v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12880v1", "title": "AdS-GNN -- a Conformally Equivariant Graph Neural Network", "authors": ["Maksim Zhdanov", "Nabil Iqbal", "Erik Bekkers", "Patrick Forr\u00e9"], "abstract": "Conformal symmetries, i.e.\\ coordinate transformations that preserve angles,\nplay a key role in many fields, including physics, mathematics, computer vision\nand (geometric) machine learning. Here we build a neural network that is\nequivariant under general conformal transformations. To achieve this, we lift\ndata from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to\nexploit a known correspondence between conformal transformations of flat space\nand isometric transformations on the AdS space. We then build upon the fact\nthat such isometric transformations have been extensively studied on general\ngeometries in the geometric deep learning literature. We employ message-passing\nlayers conditioned on the proper distance, yielding a computationally efficient\nframework. We validate our model on tasks from computer vision and statistical\nphysics, demonstrating strong performance, improved generalization capacities,\nand the ability to extract conformal data such as scaling dimensions from the\ntrained network.", "categories": ["cs.LG", "cs.AI", "hep-th"], "published": "2025-05-19 09:08:52", "updated": "2025-05-19 09:08:52", "pdf_url": "http://arxiv.org/pdf/2505.12880v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12882v1", "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "abstract": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 09:10:55", "updated": "2025-05-19 09:10:55", "pdf_url": "http://arxiv.org/pdf/2505.12882v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12884v1", "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "abstract": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-19 09:11:54", "updated": "2025-05-19 09:11:54", "pdf_url": "http://arxiv.org/pdf/2505.12884v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12886v1", "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "categories": ["cs.AI", "cs.CL", "cs.CY"], "published": "2025-05-19 09:16:40", "updated": "2025-05-19 09:16:40", "pdf_url": "http://arxiv.org/pdf/2505.12886v1", "comment": "25 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12891v1", "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "abstract": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 09:22:02", "updated": "2025-05-19 09:22:02", "pdf_url": "http://arxiv.org/pdf/2505.12891v1", "comment": "First version. There are still some examples to be added into the\n  appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12894v1", "title": "HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Keke Tang", "Chao Gao", "Zhen Wang"], "abstract": "Hypergraphs offer superior modeling capabilities for social networks,\nparticularly in capturing group phenomena that extend beyond pairwise\ninteractions in rumor propagation. Existing approaches in rumor source\ndetection predominantly focus on dyadic interactions, which inadequately\naddress the complexity of more intricate relational structures. In this study,\nwe present a novel approach for Source Detection in Hypergraphs (HyperDet) via\nInteractive Relationship Construction and Feature-rich Attention Fusion.\nSpecifically, our methodology employs an Interactive Relationship Construction\nmodule to accurately model both the static topology and dynamic interactions\namong users, followed by the Feature-rich Attention Fusion module, which\nautonomously learns node features and discriminates between nodes using a\nself-attention mechanism, thereby effectively learning node representations\nunder the framework of accurately modeled higher-order relationships. Extensive\nexperimental validation confirms the efficacy of our HyperDet approach,\nshowcasing its superiority relative to current state-of-the-art methods.", "categories": ["cs.SI", "cs.AI"], "published": "2025-05-19 09:27:46", "updated": "2025-05-19 09:27:46", "pdf_url": "http://arxiv.org/pdf/2505.12894v1", "comment": "Accepted by IJCAI25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12900v1", "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "abstract": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation.", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "published": "2025-05-19 09:35:58", "updated": "2025-05-19 09:35:58", "pdf_url": "http://arxiv.org/pdf/2505.12900v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12903v1", "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach", "authors": ["Shiao Wang", "Xiao Wang", "Liye Jin", "Bo Jiang", "Lin Zhu", "Lan Chen", "Yonghong Tian", "Bin Luo"], "abstract": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras\ncoupled with computationally intensive deep neural network architectures to\nachieve effective tracking. However, such frame-based methods inherently face\nchallenges in achieving low-latency performance and often fail in\nresource-constrained environments. Visual object tracking using bio-inspired\nevent cameras has emerged as a promising research direction in recent years,\noffering distinct advantages for low-latency applications. In this paper, we\npropose a novel Slow-Fast Tracking paradigm that flexibly adapts to different\noperational requirements, termed SFTrack. The proposed framework supports two\ncomplementary modes, i.e., a high-precision slow tracker for scenarios with\nsufficient computational resources, and an efficient fast tracker tailored for\nlatency-aware, resource-constrained environments. Specifically, our framework\nfirst performs graph-based representation learning from\nhigh-temporal-resolution event streams, and then integrates the learned\ngraph-structured information into two FlashAttention-based vision backbones,\nyielding the slow and fast trackers, respectively. The fast tracker achieves\nlow latency through a lightweight network design and by producing multiple\nbounding box outputs in a single forward pass. Finally, we seamlessly combine\nboth trackers via supervised fine-tuning and further enhance the fast tracker's\nperformance through a knowledge distillation strategy. Extensive experiments on\npublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate the\neffectiveness and efficiency of our proposed method across different real-world\nscenarios. The source code has been released on\nhttps://github.com/Event-AHU/SlowFast_Event_Track.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 09:37:23", "updated": "2025-05-19 09:37:23", "pdf_url": "http://arxiv.org/pdf/2505.12903v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12904v1", "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning", "authors": ["Hilde I. Hummel", "Arwin Gansekoele", "Sandjai Bhulai", "Rob van der Mei"], "abstract": "The increasing level of sound pollution in marine environments poses an\nincreased threat to ocean health, making it crucial to monitor underwater\nnoise. By monitoring this noise, the sources responsible for this pollution can\nbe mapped. Monitoring is performed by passively listening to these sounds. This\ngenerates a large amount of data records, capturing a mix of sound sources such\nas ship activities and marine mammal vocalizations. Although machine learning\noffers a promising solution for automatic sound classification, current\nstate-of-the-art methods implement supervised learning. This requires a large\namount of high-quality labeled data that is not publicly available. In\ncontrast, a massive amount of lower-quality unlabeled data is publicly\navailable, offering the opportunity to explore unsupervised learning\ntechniques. This research explores this possibility by implementing an\nunsupervised Contrastive Learning approach. Here, a Conformer-based encoder is\noptimized by the so-called Variance-Invariance-Covariance Regularization loss\nfunction on these lower-quality unlabeled data and the translation to the\nlabeled data is made. Through classification tasks involving recognizing ship\ntypes and marine mammal vocalizations, our method demonstrates to produce\nrobust and generalized embeddings. This shows to potential of unsupervised\nmethods for various automatic underwater acoustic analysis tasks.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 09:37:46", "updated": "2025-05-19 09:37:46", "pdf_url": "http://arxiv.org/pdf/2505.12904v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12908v1", "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "abstract": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 09:44:01", "updated": "2025-05-19 09:44:01", "pdf_url": "http://arxiv.org/pdf/2505.12908v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12909v1", "title": "Sinusoidal Initialization, Time for a New Start", "authors": ["Alberto Fern\u00e1ndez-Hern\u00e1ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ort\u00ed"], "abstract": "Initialization plays a critical role in Deep Neural Network training,\ndirectly influencing convergence, stability, and generalization. Common\napproaches such as Glorot and He initializations rely on randomness, which can\nproduce uneven weight distributions across layer connections. In this paper, we\nintroduce the Sinusoidal initialization, a novel deterministic method that\nemploys sinusoidal functions to construct structured weight matrices expressly\nto improve the spread and balance of weights throughout the network while\nsimultaneously fostering a more uniform, well-conditioned distribution of\nneuron activation states from the very first forward pass. Because Sinusoidal\ninitialization begins with weights and activations that are already evenly and\nefficiently utilized, it delivers consistently faster convergence, greater\ntraining stability, and higher final accuracy across a wide range of models,\nincluding convolutional neural networks, vision transformers, and large\nlanguage models. On average, our experiments show an increase of 4.8 % in final\nvalidation accuracy and 20.9 % in convergence speed. By replacing randomness\nwith structure, this initialization provides a stronger and more reliable\nfoundation for Deep Learning systems.", "categories": ["cs.LG", "cs.AI", "I.2; G.3; I.2.6"], "published": "2025-05-19 09:45:18", "updated": "2025-05-19 09:45:18", "pdf_url": "http://arxiv.org/pdf/2505.12909v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12910v1", "title": "SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Chao Gao", "Zhen Wang", "Keke Tang"], "abstract": "Source detection on graphs has demonstrated high efficacy in identifying\nrumor origins. Despite advances in machine learning-based methods, many fail to\ncapture intrinsic dynamics of rumor propagation. In this work, we present\nSourceDetMamba: A Graph-aware State Space Model for Source Detection in\nSequential Hypergraphs, which harnesses the recent success of the state space\nmodel Mamba, known for its superior global modeling capabilities and\ncomputational efficiency, to address this challenge. Specifically, we first\nemploy hypergraphs to model high-order interactions within social networks.\nSubsequently, temporal network snapshots generated during the propagation\nprocess are sequentially fed in reverse order into Mamba to infer underlying\npropagation dynamics. Finally, to empower the sequential model to effectively\ncapture propagation patterns while integrating structural information, we\npropose a novel graph-aware state update mechanism, wherein the state of each\nnode is propagated and refined by both temporal dependencies and topological\ncontext. Extensive evaluations on eight datasets demonstrate that\nSourceDetMamba consistently outperforms state-of-the-art approaches.", "categories": ["cs.SI", "cs.AI"], "published": "2025-05-19 09:45:27", "updated": "2025-05-19 09:45:27", "pdf_url": "http://arxiv.org/pdf/2505.12910v1", "comment": "Accepted by IJCAI25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12920v1", "title": "PyFCG: Fluid Construction Grammar in Python", "authors": ["Paul Van Eecke", "Katrien Beuls"], "abstract": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-19 10:00:01", "updated": "2025-05-19 10:00:01", "pdf_url": "http://arxiv.org/pdf/2505.12920v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12923v1", "title": "The Traitors: Deception and Trust in Multi-Agent Language Model Simulations", "authors": ["Pedro M. P. Curvo"], "abstract": "As AI systems increasingly assume roles where trust and alignment with human\nvalues are essential, understanding when and why they engage in deception has\nbecome a critical research priority. We introduce The Traitors, a multi-agent\nsimulation framework inspired by social deduction games, designed to probe\ndeception, trust formation, and strategic communication among large language\nmodel (LLM) agents under asymmetric information. A minority of agents the\ntraitors seek to mislead the majority, while the faithful must infer hidden\nidentities through dialogue and reasoning. Our contributions are: (1) we ground\nthe environment in formal frameworks from game theory, behavioral economics,\nand social cognition; (2) we develop a suite of evaluation metrics capturing\ndeception success, trust dynamics, and collective inference quality; (3) we\nimplement a fully autonomous simulation platform where LLMs reason over\npersistent memory and evolving social dynamics, with support for heterogeneous\nagent populations, specialized traits, and adaptive behaviors. Our initial\nexperiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model)\nreveal a notable asymmetry: advanced models like GPT-4o demonstrate superior\ndeceptive capabilities yet exhibit disproportionate vulnerability to others'\nfalsehoods. This suggests deception skills may scale faster than detection\nabilities. Overall, The Traitors provides a focused, configurable testbed for\ninvestigating LLM behavior in socially nuanced interactions. We position this\nwork as a contribution toward more rigorous research on deception mechanisms,\nalignment challenges, and the broader social reliability of AI systems.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-19 10:01:35", "updated": "2025-05-19 10:01:35", "pdf_url": "http://arxiv.org/pdf/2505.12923v1", "comment": "9 main pages, 31 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12925v1", "title": "CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive Programming", "authors": ["Han Deng", "Yuan Meng", "Shixiang Tang", "Wanli Ouyang", "Xinzhu Ma"], "abstract": "Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem --\nsimilar question retrieval -- to address this issue. Due to the lack of both\ndata and models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code and\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate and Simplified-to-Full), built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. In addition, we develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Code and data are available at: https://github.com/coldchair/CPRet", "categories": ["cs.SE", "cs.AI", "cs.IR", "H.3.3"], "published": "2025-05-19 10:07:51", "updated": "2025-05-19 10:07:51", "pdf_url": "http://arxiv.org/pdf/2505.12925v1", "comment": "main 9 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12929v1", "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "abstract": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:14:08", "updated": "2025-05-19 10:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12929v1", "comment": "24 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12938v1", "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "abstract": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 10:22:04", "updated": "2025-05-19 10:22:04", "pdf_url": "http://arxiv.org/pdf/2505.12938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12942v1", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "abstract": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:29:32", "updated": "2025-05-19 10:29:32", "pdf_url": "http://arxiv.org/pdf/2505.12942v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12944v1", "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "abstract": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "published": "2025-05-19 10:31:30", "updated": "2025-05-19 10:31:30", "pdf_url": "http://arxiv.org/pdf/2505.12944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12951v1", "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "abstract": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 10:44:49", "updated": "2025-05-19 10:44:49", "pdf_url": "http://arxiv.org/pdf/2505.12951v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12960v1", "title": "Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory", "authors": ["Chengping He", "Mingrui Jiang", "Keyi Shan", "Szu-Hao Yang", "Zefan Li", "Shengbo Wang", "Giacomo Pedretti", "Jim Ignowski", "Can Li"], "abstract": "Brain-inspired computing aims to mimic cognitive functions like associative\nmemory, the ability to recall complete patterns from partial cues. Memristor\ntechnology offers promising hardware for such neuromorphic systems due to its\npotential for efficient in-memory analog computing. Hopfield Neural Networks\n(HNNs) are a classic model for associative memory, but implementations on\nconventional hardware suffer from efficiency bottlenecks, while prior\nmemristor-based HNNs faced challenges with vulnerability to hardware defects\ndue to offline training, limited storage capacity, and difficulty processing\nanalog patterns. Here we introduce and experimentally demonstrate on integrated\nmemristor hardware a new hardware-adaptive learning algorithm for associative\nmemories that significantly improves defect tolerance and capacity, and\nnaturally extends to scalable multilayer architectures capable of handling both\nbinary and continuous patterns. Our approach achieves 3x effective capacity\nunder 50% device faults compared to state-of-the-art methods. Furthermore, its\nextension to multilayer architectures enables superlinear capacity scaling\n(\\(\\propto N^{1.49}\\ for binary patterns) and effective recalling of continuous\npatterns (\\propto N^{1.74}\\ scaling), as compared to linear capacity scaling\nfor previous HNNs. It also provides flexibility to adjust capacity by tuning\nhidden neurons for the same-sized patterns. By leveraging the massive\nparallelism of the hardware enabled by synchronous updates, it reduces energy\nby 8.8x and latency by 99.7% for 64-dimensional patterns over asynchronous\nschemes, with greater improvements at scale. This promises the development of\nmore reliable memristor-based associative memory systems and enables new\napplications research due to the significantly improved capacity, efficiency,\nand flexibility.", "categories": ["cs.LG", "cs.AI", "cs.ET"], "published": "2025-05-19 10:55:09", "updated": "2025-05-19 10:55:09", "pdf_url": "http://arxiv.org/pdf/2505.12960v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12963v1", "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies", "authors": ["Maksim I. Ivanov", "Olga E. Mendybaeva", "Yuri E. Karyakin", "Igor N. Glukhikh", "Aleksey V. Lebedev"], "abstract": "This article explores the use of artificial intelligence for the diagnosis of\npathologies of the temporomandibular joint (TMJ), in particular, for the\nsegmentation of the articular disc on MRI images. The relevance of the work is\ndue to the high prevalence of TMJ pathologies, as well as the need to improve\nthe accuracy and speed of diagnosis in medical institutions. During the study,\nthe existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,\nare not suitable for studying the articular disc due to the orientation towards\nbone structures. To solve the problem, an original dataset was collected from\n94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the\namount of data, augmentation methods were used. After that, the models of\nU-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and\ncompared. The evaluation was carried out according to the Dice Score,\nPrecision, Sensitivity, Specificity, and Mean Average Precision metrics. The\nresults confirm the potential of using the Roboflow model for segmentation of\nthe temporomandibular joint. In the future, it is planned to develop an\nalgorithm for measuring the distance between the jaws and determining the\nposition of the articular disc, which will improve the diagnosis of TMJ\npathologies.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-19 10:58:02", "updated": "2025-05-19 10:58:02", "pdf_url": "http://arxiv.org/pdf/2505.12963v1", "comment": "10 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12966v1", "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection", "authors": ["Zihan Xiong", "Xiaohua Wu", "Lei Chen", "Fangqi Lou"], "abstract": "Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 11:01:49", "updated": "2025-05-19 11:01:49", "pdf_url": "http://arxiv.org/pdf/2505.12966v1", "comment": "9 pages,ICMR accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12981v1", "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "abstract": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation.", "categories": ["cs.CR", "cs.AI", "cs.HC"], "published": "2025-05-19 11:17:46", "updated": "2025-05-19 11:17:46", "pdf_url": "http://arxiv.org/pdf/2505.12981v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12983v1", "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:18:54", "updated": "2025-05-19 11:18:54", "pdf_url": "http://arxiv.org/pdf/2505.12983v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12992v1", "title": "Fractured Chain-of-Thought Reasoning", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "published": "2025-05-19 11:30:41", "updated": "2025-05-19 11:30:41", "pdf_url": "http://arxiv.org/pdf/2505.12992v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12996v1", "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:34:47", "updated": "2025-05-19 11:34:47", "pdf_url": "http://arxiv.org/pdf/2505.12996v1", "comment": "12 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13010v1", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "abstract": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-19 11:54:39", "updated": "2025-05-19 11:54:39", "pdf_url": "http://arxiv.org/pdf/2505.13010v1", "comment": "7 pages, 5 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13011v1", "title": "Unveiling and Steering Connectome Organization with Interpretable Latent Variables", "authors": ["Yubin Li", "Xingyu Liu", "Guozhang Chen"], "abstract": "The brain's intricate connectome, a blueprint for its function, presents\nimmense complexity, yet it arises from a compact genetic code, hinting at\nunderlying low-dimensional organizational principles. This work bridges\nconnectomics and representation learning to uncover these principles. We\npropose a framework that combines subgraph extraction from the Drosophila\nconnectome, FlyWire, with a generative model to derive interpretable\nlow-dimensional representations of neural circuitry. Crucially, an\nexplainability module links these latent dimensions to specific structural\nfeatures, offering insights into their functional relevance. We validate our\napproach by demonstrating effective graph reconstruction and, significantly,\nthe ability to manipulate these latent codes to controllably generate\nconnectome subgraphs with predefined properties. This research offers a novel\ntool for understanding brain architecture and a potential avenue for designing\nbio-inspired artificial neural networks.", "categories": ["cs.AI"], "published": "2025-05-19 11:54:40", "updated": "2025-05-19 11:54:40", "pdf_url": "http://arxiv.org/pdf/2505.13011v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13023v1", "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "abstract": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "published": "2025-05-19 12:07:29", "updated": "2025-05-19 12:07:29", "pdf_url": "http://arxiv.org/pdf/2505.13023v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13025v1", "title": "LiBOG: Lifelong Learning for Black-Box Optimizer Generation", "authors": ["Jiyuan Pei", "Yi Mei", "Jialin Liu", "Mengjie Zhang"], "abstract": "Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in\nautomating the configuration and generation of black-box optimizers,\nsignificantly reducing the human effort required for optimizer design and\ndiscovering optimizers with higher performance than classic human-designed\noptimizers. However, existing MetaBBO methods conduct one-off training under\nthe assumption that a stationary problem distribution with extensive and\nrepresentative training problem samples is pre-available. This assumption is\noften impractical in real-world scenarios, where diverse problems following\nshifting distribution continually arise. Consequently, there is a pressing need\nfor methods that can continuously learn from new problems encountered\non-the-fly and progressively enhance their capabilities. In this work, we\nexplore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a\nnovel approach designed to learn from sequentially encountered problems and\ngenerate high-performance optimizers for Black-Box Optimization (BBO). LiBOG\nconsolidates knowledge both across tasks and within tasks to mitigate\ncatastrophic forgetting. Extensive experiments demonstrate LiBOG's\neffectiveness in learning to generate high-performance optimizers in a lifelong\nlearning manner, addressing catastrophic forgetting while maintaining\nplasticity to learn new tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:09:25", "updated": "2025-05-19 12:09:25", "pdf_url": "http://arxiv.org/pdf/2505.13025v1", "comment": "Accepted at IJCAI 2025. To appear", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13026v1", "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "abstract": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:10:17", "updated": "2025-05-19 12:10:17", "pdf_url": "http://arxiv.org/pdf/2505.13026v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13028v1", "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "authors": ["Sayon Palit", "Daniel Woods"], "abstract": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "published": "2025-05-19 12:12:00", "updated": "2025-05-19 12:12:00", "pdf_url": "http://arxiv.org/pdf/2505.13028v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13031v1", "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "authors": ["Yicheng Xiao", "Lin Song", "Yukang Chen", "Yingmin Luo", "Yuxin Chen", "Yukang Gan", "Wei Huang", "Xiu Li", "Xiaojuan Qi", "Ying Shan"], "abstract": "Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\n\\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}.", "categories": ["cs.AI"], "published": "2025-05-19 12:17:04", "updated": "2025-05-19 12:17:04", "pdf_url": "http://arxiv.org/pdf/2505.13031v1", "comment": "Code: https://github.com/EasonXiao-888/MindOmni", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13033v1", "title": "TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis", "authors": ["Vijay Ekambaram", "Subodh Kumar", "Arindam Jati", "Sumanta Mukherjee", "Tomoya Sakai", "Pankaj Dayama", "Wesley M. Gifford", "Jayant Kalagnanam"], "abstract": "The rise of time-series pre-trained models has advanced temporal\nrepresentation learning, but current state-of-the-art models are often\nlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compact\ntime-series pre-trained models with only 1M parameters, specialized to perform\nstrongly across classification, anomaly detection, imputation, and retrieval\ntasks. TSPulse introduces innovations at both the architecture and task levels.\nAt the architecture level, it employs a dual-space masked reconstruction,\nlearning from both time and frequency domains to capture complementary signals.\nThis is further enhanced by a dual-embedding disentanglement, generating both\ndetailed embeddings for fine-grained analysis and high-level semantic\nembeddings for broader task understanding. Notably, TSPulse's semantic\nembeddings are robust to shifts in time, magnitude, and noise, which is\nimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,\na fine-tuning component enabling task-specific feature attention. It also\nintroduces a multi-head triangulation technique that correlates deviations from\nmultiple prediction heads, enhancing anomaly detection by fusing complementary\nmodel outputs. Additionally, a hybrid mask pretraining is proposed to improves\nzero-shot imputation by reducing pre-training bias. These architecture and task\ninnovations collectively contribute to TSPulse's significant performance gains:\n5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly\ndetection leaderboard, +50% in zero-shot imputation, and +25% in time-series\nretrieval. Remarkably, these results are achieved with just 1M parameters,\nmaking TSPulse 10-100X smaller than existing pre-trained models. Its efficiency\nenables GPU-free inference and rapid pre-training, setting a new standard for\nefficient time-series pre-trained models. Models will be open-sourced soon.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:18:53", "updated": "2025-05-19 12:18:53", "pdf_url": "http://arxiv.org/pdf/2505.13033v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13036v1", "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "authors": ["Sai Koneru", "Maike Z\u00fcfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "abstract": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:21:29", "updated": "2025-05-19 12:21:29", "pdf_url": "http://arxiv.org/pdf/2505.13036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13043v1", "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation", "authors": ["Hao-Ran Yang", "Xiaohui Chen", "Chuan-Xian Ren"], "abstract": "Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 12:33:52", "updated": "2025-05-19 12:33:52", "pdf_url": "http://arxiv.org/pdf/2505.13043v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13044v1", "title": "CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents", "authors": ["Rebecca Westh\u00e4u\u00dfer", "Frederik Berenz", "Wolfgang Minker", "Sebastian Zepf"], "abstract": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) and are a powerful enabler for interactive systems. However,\nthey still face challenges in long-term interactions that require adaptation\ntowards the user as well as contextual knowledge and understanding of the\never-changing environment. To overcome these challenges, holistic memory\nmodeling is required to efficiently retrieve and store relevant information\nacross interaction sessions for suitable responses. Cognitive AI, which aims to\nsimulate the human thought process in a computerized model, highlights\ninteresting aspects, such as thoughts, memory mechanisms, and decision-making,\nthat can contribute towards improved memory modeling for LLMs. Inspired by\nthese cognitive AI principles, we propose our memory framework CAIM. CAIM\nconsists of three modules: 1.) The Memory Controller as the central decision\nunit; 2.) the Memory Retrieval, which filters relevant data for interaction\nupon request; and 3.) the Post-Thinking, which maintains the memory storage. We\ncompare CAIM against existing approaches, focusing on metrics such as retrieval\naccuracy, response correctness, contextual coherence, and memory storage. The\nresults demonstrate that CAIM outperforms baseline frameworks across different\nmetrics, highlighting its context-awareness and potential to improve long-term\nhuman-AI interactions.", "categories": ["cs.AI", "cs.HC"], "published": "2025-05-19 12:33:52", "updated": "2025-05-19 12:33:52", "pdf_url": "http://arxiv.org/pdf/2505.13044v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13053v1", "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "abstract": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:42:23", "updated": "2025-05-19 12:42:23", "pdf_url": "http://arxiv.org/pdf/2505.13053v1", "comment": "currently under review at Frontiers in Communication", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13073v1", "title": "Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion", "authors": ["Dengfeng Liu", "Jucai Zhai", "Xiaoguang Jiang", "Ziqun Li", "Qianjin Yu", "Feng Liu", "Rui Ye", "Huang Liu", "Zhiguo Yang", "Yongsheng Du", "Fang Tan"], "abstract": "Code completion technology based on large language model has significantly\nimproved the development efficiency of programmers. However, in practical\napplications, there remains a gap between current commonly used code completion\nevaluation metrics and users' actual perception. To address this issue, we\npropose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,\nfrom the perspective of probabilistic modeling. Furthermore, to tackle the lack\nof effective structural semantic modeling and cross-module dependency\ninformation in LLMs for repository-level code completion scenarios, we propose\na data processing method based on a Structure-Preserving and\nSemantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis\nand experimental validation, we demonstrate the superiority of the proposed\nevaluation metrics in terms of user perception consistency, as well as the\neffectiveness of the data processing method in enhancing model performance.", "categories": ["cs.SE", "cs.AI"], "published": "2025-05-19 13:09:32", "updated": "2025-05-19 13:09:32", "pdf_url": "http://arxiv.org/pdf/2505.13073v1", "comment": "14 pages,8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13076v1", "title": "The Hidden Dangers of Browsing AI Agents", "authors": ["Mykyta Mudryi", "Markiyan Chaklosh", "Grzegorz W\u00f3jcik"], "abstract": "Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 13:10:29", "updated": "2025-05-19 13:10:29", "pdf_url": "http://arxiv.org/pdf/2505.13076v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13077v1", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "abstract": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 13:11:28", "updated": "2025-05-19 13:11:28", "pdf_url": "http://arxiv.org/pdf/2505.13077v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13079v1", "title": "Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "abstract": "Transferring linguistic knowledge from a pretrained language model (PLM) to\nacoustic feature learning has proven effective in enhancing end-to-end\nautomatic speech recognition (E2E-ASR). However, aligning representations\nbetween linguistic and acoustic modalities remains a challenge due to inherent\nmodality gaps. Optimal transport (OT) has shown promise in mitigating these\ngaps by minimizing the Wasserstein distance (WD) between linguistic and\nacoustic feature distributions. However, previous OT-based methods overlook\nstructural relationships, treating feature vectors as unordered sets. To\naddress this, we propose Graph Matching Optimal Transport (GM-OT), which models\nlinguistic and acoustic sequences as structured graphs. Nodes represent feature\nembeddings, while edges capture temporal and sequential relationships. GM-OT\nminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)\n(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)\nformulation. This enables structured alignment and more efficient knowledge\ntransfer compared to existing OT-based approaches. Theoretical analysis further\nshows that prior OT-based methods in linguistic knowledge transfer can be\nviewed as a special case within our GM-OT framework. We evaluate GM-OT on\nMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge\ntransfer. Experimental results demonstrate significant performance gains over\nstate-of-the-art models, validating the effectiveness of our approach.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-19 13:13:18", "updated": "2025-05-19 13:13:18", "pdf_url": "http://arxiv.org/pdf/2505.13079v1", "comment": "To appear in Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13082v1", "title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers", "authors": ["Kyeongman Park", "Seongho Joo", "Kyomin Jung"], "abstract": "We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 13:13:46", "updated": "2025-05-19 13:13:46", "pdf_url": "http://arxiv.org/pdf/2505.13082v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13087v1", "title": "Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings", "authors": ["Adrien Lagesse", "Marc Lelarge"], "abstract": "We propose a novel benchmarking methodology for graph neural networks (GNNs)\nbased on the graph alignment problem, a combinatorial optimization task that\ngeneralizes graph isomorphism by aligning two unlabeled graphs to maximize\noverlapping edges. We frame this problem as a self-supervised learning task and\npresent several methods to generate graph alignment datasets using synthetic\nrandom graphs and real-world graph datasets from multiple domains. For a given\ngraph dataset, we generate a family of graph alignment datasets with increasing\ndifficulty, allowing us to rank the performance of various architectures. Our\nexperiments indicate that anisotropic graph neural networks outperform standard\nconvolutional architectures. To further demonstrate the utility of the graph\nalignment task, we show its effectiveness for unsupervised GNN pre-training,\nwhere the learned node embeddings outperform other positional encodings on\nthree molecular regression tasks and achieve state-of-the-art results on the\nPCQM4Mv2 dataset with significantly fewer parameters. To support\nreproducibility and further research, we provide an open-source Python package\nto generate graph alignment datasets and benchmark new GNN architectures.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 13:22:17", "updated": "2025-05-19 13:22:17", "pdf_url": "http://arxiv.org/pdf/2505.13087v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13094v1", "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech Separation", "authors": ["Guo Chen", "Kai Li", "Runxuan Yang", "Xiaolin Hu"], "abstract": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 13:25:51", "updated": "2025-05-19 13:25:51", "pdf_url": "http://arxiv.org/pdf/2505.13094v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13098v1", "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "abstract": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.", "categories": ["cs.AI", "cs.CL", "cs.DB"], "published": "2025-05-19 13:29:27", "updated": "2025-05-19 13:29:27", "pdf_url": "http://arxiv.org/pdf/2505.13098v1", "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13101v1", "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "authors": ["Shaowu Wu", "Liting Zeng", "Wei Lu", "Xiangyang Luo"], "abstract": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 13:31:48", "updated": "2025-05-19 13:31:48", "pdf_url": "http://arxiv.org/pdf/2505.13101v1", "comment": "10 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13102v1", "title": "Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast", "authors": ["Ji Qi", "Tam Thuc Do", "Mingxiao Liu", "Zhuoshi Pan", "Yuzhe Li", "Gene Cheung", "H. Vicky Zhao"], "abstract": "To forecast traffic with both spatial and temporal dimensions, we unroll a\nmixed-graph-based optimization algorithm into a lightweight and interpretable\ntransformer-like neural net. Specifically, we construct two graphs: an\nundirected graph $\\mathcal{G}^u$ capturing spatial correlations across\ngeography, and a directed graph $\\mathcal{G}^d$ capturing sequential\nrelationships over time. We formulate a prediction problem for the future\nsamples of signal $\\mathbf{x}$, assuming it is \"smooth\" with respect to both\n$\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and\n$\\ell_1$-norm variational terms to quantify and promote signal smoothness\n(low-frequency reconstruction) on a directed graph. We construct an iterative\nalgorithm based on alternating direction method of multipliers (ADMM), and\nunroll it into a feed-forward network for data-driven parameter learning. We\ninsert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$, which\nare akin to the self-attention mechanism in classical transformers. Experiments\nshow that our unrolled networks achieve competitive traffic forecast\nperformance as state-of-the-art prediction schemes, while reducing parameter\ncounts drastically. Our code is available in\nhttps://github.com/SingularityUndefined/Unrolling-GSP-STForecast.", "categories": ["cs.LG", "cs.AI", "eess.SP"], "published": "2025-05-19 13:32:34", "updated": "2025-05-19 13:32:34", "pdf_url": "http://arxiv.org/pdf/2505.13102v1", "comment": "19 pages, 5 figures, 8 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13109v1", "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "abstract": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 13:36:45", "updated": "2025-05-19 13:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13109v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13115v1", "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "abstract": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-19 13:46:35", "updated": "2025-05-19 13:46:35", "pdf_url": "http://arxiv.org/pdf/2505.13115v1", "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13116v1", "title": "Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data", "authors": ["Kathrin Lammers", "Valerie Vaquet", "Barbara Hammer"], "abstract": "As machine learning is increasingly applied in an online fashion to deal with\nevolving data streams, the fairness of these algorithms is a matter of growing\nethical and legal concern. In many use cases, class imbalance in the data also\nneeds to be dealt with to ensure predictive performance. Current fairness-aware\nstream learners typically attempt to solve these issues through in- or\npost-processing by focusing on optimizing one specific discrimination metric,\naddressing class imbalance in a separate processing step. While C-SMOTE is a\nhighly effective model-agnostic pre-processing approach to mitigate class\nimbalance, as a side effect of this method, algorithmic bias is often\nintroduced.\n  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant -\nas a pre-processing approach to simultaneously address the class imbalance and\nfairness concerns by employing situation testing and balancing\nfairness-relevant groups during oversampling. Unlike other fairness-aware\nstream learners, CFSMOTE is not optimizing for only one specific fairness\nmetric, therefore avoiding potentially problematic trade-offs. Our experiments\nshow significant improvement on several common group fairness metrics in\ncomparison to vanilla C-SMOTE while maintaining competitive performance, also\nin comparison to other fairness-aware algorithms.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 13:46:47", "updated": "2025-05-19 13:46:47", "pdf_url": "http://arxiv.org/pdf/2505.13116v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13118v1", "title": "Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals", "authors": ["Marouane Il Idrissi", "Agathe Fernandes Machado", "Ewen Gallic", "Arthur Charpentier"], "abstract": "Cooperative game theory methods, notably Shapley values, have significantly\nenhanced machine learning (ML) interpretability. However, existing explainable\nAI (XAI) frameworks mainly attribute average model predictions, overlooking\npredictive uncertainty. This work addresses that gap by proposing a novel,\nmodel-agnostic uncertainty attribution (UA) method grounded in conformal\nprediction (CP). By defining cooperative games where CP interval\nproperties-such as width and bounds-serve as value functions, we systematically\nattribute predictive uncertainty to input features. Extending beyond the\ntraditional Shapley values, we use the richer class of Harsanyi allocations,\nand in particular the proportional Shapley values, which distribute attribution\nproportionally to feature importance. We propose a Monte Carlo approximation\nmethod with robust statistical guarantees to address computational feasibility,\nsignificantly improving runtime efficiency. Our comprehensive experiments on\nsynthetic benchmarks and real-world datasets demonstrate the practical utility\nand interpretative depth of our approach. By combining cooperative game theory\nand conformal prediction, we offer a rigorous, flexible toolkit for\nunderstanding and communicating predictive uncertainty in high-stakes ML\napplications.", "categories": ["cs.AI", "cs.LG", "stat.ML"], "published": "2025-05-19 13:49:05", "updated": "2025-05-19 13:49:05", "pdf_url": "http://arxiv.org/pdf/2505.13118v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13122v1", "title": "When majority rules, minority loses: bias amplification of gradient descent", "authors": ["Fran\u00e7ois Bachoc", "J\u00e9r\u00f4me Bolte", "Ryan Boustany", "Jean-Michel Loubes"], "abstract": "Despite growing empirical evidence of bias amplification in machine learning,\nits theoretical foundations remain poorly understood. We develop a formal\nframework for majority-minority learning tasks, showing how standard training\ncan favor majority groups and produce stereotypical predictors that neglect\nminority-specific features. Assuming population and variance imbalance, our\nanalysis reveals three key findings: (i) the close proximity between\n``full-data'' and stereotypical predictors, (ii) the dominance of a region\nwhere training the entire model tends to merely learn the majority traits, and\n(iii) a lower bound on the additional training required. Our results are\nillustrated through experiments in deep learning for tabular and image\nclassification tasks.", "categories": ["cs.LG", "cs.AI", "math.OC"], "published": "2025-05-19 13:51:49", "updated": "2025-05-19 13:51:49", "pdf_url": "http://arxiv.org/pdf/2505.13122v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13123v1", "title": "Just Dance with $\u03c0$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "abstract": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 13:51:57", "updated": "2025-05-19 13:51:57", "pdf_url": "http://arxiv.org/pdf/2505.13123v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13124v1", "title": "$\u03bc$PC: Scaling Predictive Coding to 100+ Layer Networks", "authors": ["Francesco Innocenti", "El Mehdi Achour", "Christopher L. Buckley"], "abstract": "The biological implausibility of backpropagation (BP) has motivated many\nalternative, brain-inspired algorithms that attempt to rely only on local\ninformation, such as predictive coding (PC) and equilibrium propagation.\nHowever, these algorithms have notoriously struggled to train very deep\nnetworks, preventing them from competing with BP in large-scale settings.\nIndeed, scaling PC networks (PCNs) has recently been posed as a challenge for\nthe community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can\nbe trained reliably using a Depth-$\\mu$P parameterisation (Yang et al., 2023;\nBordelon et al., 2023) which we call \"$\\mu$PC\". Through an extensive analysis\nof the scaling behaviour of PCNs, we reveal several pathologies that make\nstandard PCNs difficult to train at large depths. We then show that, despite\naddressing only some of these instabilities, $\\mu$PC allows stable training of\nvery deep (up to 128-layer) residual networks on simple classification tasks\nwith competitive performance and little tuning compared to current benchmarks.\nMoreover, $\\mu$PC enables zero-shot transfer of both weight and activity\nlearning rates across widths and depths. Our results have implications for\nother local algorithms and could be extended to convolutional and transformer\narchitectures. Code for $\\mu$PC is made available as part of a JAX library for\nPCNs at https://github.com/thebuckleylab/jpc (Innocenti et al., 2024).", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.6"], "published": "2025-05-19 13:54:29", "updated": "2025-05-19 13:54:29", "pdf_url": "http://arxiv.org/pdf/2505.13124v1", "comment": "34 pages, 41 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13126v1", "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "abstract": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 13:58:15", "updated": "2025-05-19 13:58:15", "pdf_url": "http://arxiv.org/pdf/2505.13126v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13130v1", "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach", "authors": ["Muhammad Awais Amin", "Adama Ilboudo", "Abdul Samad bin Shahid", "Amjad Ali", "Waqas Haider Khan Bangyal"], "abstract": "One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:00:10", "updated": "2025-05-19 14:00:10", "pdf_url": "http://arxiv.org/pdf/2505.13130v1", "comment": null, "doi": "10.63075/2jepm102", "journal_ref": "Annual Methodological Archive Research Review 3 (05) (2025),\n  296-321"}
{"arxiv_id": "2505.13136v1", "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "abstract": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 14:07:20", "updated": "2025-05-19 14:07:20", "pdf_url": "http://arxiv.org/pdf/2505.13136v1", "comment": "under review @ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13144v1", "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning", "authors": ["Dongsu Lee", "Minhae Kwon"], "abstract": "The goal of offline reinforcement learning (RL) is to extract a\nhigh-performance policy from the fixed datasets, minimizing performance\ndegradation due to out-of-distribution (OOD) samples. Offline model-based RL\n(MBRL) is a promising approach that ameliorates OOD issues by enriching\nstate-action transitions with augmentations synthesized via a learned dynamics\nmodel. Unfortunately, seminal offline MBRL methods often struggle in\nsparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL\nframework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),\nthat generates augmented transitions in a temporally structured latent space\nrather than in raw state space. To model long-horizon behavior, TempDATA learns\na latent abstraction that captures a temporal distance from both trajectory and\ntransition levels of state space. Our experiments confirm that TempDATA\noutperforms previous offline MBRL methods and achieves matching or surpassing\nthe performance of diffusion-based trajectory augmentation and goal-conditioned\nRL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-19 14:11:14", "updated": "2025-05-19 14:11:14", "pdf_url": "http://arxiv.org/pdf/2505.13144v1", "comment": "2025 ICML", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13156v1", "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "abstract": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:17:37", "updated": "2025-05-19 14:17:37", "pdf_url": "http://arxiv.org/pdf/2505.13156v1", "comment": "23 pages, 4 figures, and 1 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13157v1", "title": "Role-Playing Evaluation for Large Language Models", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "abstract": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:18:16", "updated": "2025-05-19 14:18:16", "pdf_url": "http://arxiv.org/pdf/2505.13157v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13175v1", "title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment", "authors": ["Siming Sun", "Kai Zhang", "Xuejun Jiang", "Wenchao Meng", "Qinmin Yang"], "abstract": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting.", "categories": ["cs.AI"], "published": "2025-05-19 14:30:41", "updated": "2025-05-19 14:30:41", "pdf_url": "http://arxiv.org/pdf/2505.13175v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13176v1", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "abstract": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:30:46", "updated": "2025-05-19 14:30:46", "pdf_url": "http://arxiv.org/pdf/2505.13176v1", "comment": "Accepted by ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13180v1", "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models", "authors": ["Matteo Merler", "Nicola Dainese", "Minttu Alakuijala", "Giovanni Bonetta", "Pietro Ferrazzi", "Yu Tian", "Bernardo Magnini", "Pekka Marttinen"], "abstract": "Integrating Large Language Models with symbolic planners is a promising\ndirection for obtaining verifiable and grounded plans compared to planning in\nnatural language, with recent works extending this idea to visual domains using\nVision-Language Models (VLMs). However, rigorous comparison between\nVLM-grounded symbolic approaches and methods that plan directly with a VLM has\nbeen hindered by a lack of common environments, evaluation protocols and model\ncoverage. We introduce ViPlan, the first open-source benchmark for Visual\nPlanning with symbolic predicates and VLMs. ViPlan features a series of\nincreasingly challenging tasks in two domains: a visual variant of the classic\nBlocksworld planning problem and a simulated household robotics environment. We\nbenchmark nine open-source VLM families across multiple sizes, along with\nselected closed models, evaluating both VLM-grounded symbolic planning and\nusing the models directly to propose actions. We find symbolic planning to\noutperform direct VLM planning in Blocksworld, where accurate image grounding\nis crucial, whereas the opposite is true in the household robotics tasks, where\ncommonsense knowledge and the ability to recover from errors are beneficial.\nFinally, we show that across most models and methods, there is no significant\nbenefit to using Chain-of-Thought prompting, suggesting that current VLMs still\nstruggle with visual reasoning.", "categories": ["cs.AI"], "published": "2025-05-19 14:38:15", "updated": "2025-05-19 14:38:15", "pdf_url": "http://arxiv.org/pdf/2505.13180v1", "comment": "9 pages, 5 figures and 1 table in the main text; 43 pages, 9 figures\n  and 16 tables including supplementary material", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13182v1", "title": "Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping", "authors": ["Jianfeng Xu"], "abstract": "[Objective] This study focuses on addressing the current lack of a unified\nformal theoretical framework in machine learning, as well as the deficiencies\nin interpretability and ethical safety assurance. [Methods] A formal\ninformation model is first constructed, utilizing sets of well-formed formulas\nto explicitly define the ontological states and carrier mappings of typical\ncomponents in machine learning. Learnable and processable predicates, along\nwith learning and processing functions, are introduced to analyze the logical\ndeduction and constraint rules of the causal chains within models. [Results] A\nmeta-framework for machine learning theory (MLT-MF) is established. Based on\nthis framework, universal definitions for model interpretability and ethical\nsafety are proposed. Furthermore, three key theorems are proved: the\nequivalence of model interpretability and information recoverability, the\nassurance of ethical safety, and the estimation of generalization error.\n[Limitations] The current framework assumes ideal conditions with noiseless\ninformation-enabling mappings and primarily targets model learning and\nprocessing logic in static scenarios. It does not yet address information\nfusion and conflict resolution across ontological spaces in multimodal or\nmulti-agent systems. [Conclusions] This work overcomes the limitations of\nfragmented research and provides a unified theoretical foundation for\nsystematically addressing the critical challenges currently faced in machine\nlearning.", "categories": ["cs.LO", "cs.AI"], "published": "2025-05-19 14:39:41", "updated": "2025-05-19 14:39:41", "pdf_url": "http://arxiv.org/pdf/2505.13182v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13188v1", "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns", "authors": ["Juntian Zhu", "Miguel de Carvalho", "Zhouwang Yang", "Fengxiang He"], "abstract": "An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-19 14:45:58", "updated": "2025-05-19 14:45:58", "pdf_url": "http://arxiv.org/pdf/2505.13188v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13191v1", "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision", "authors": ["Pengcheng Pan", "Yonekura Shogo", "Yasuo Kuniyoshi"], "abstract": "Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:48:36", "updated": "2025-05-19 14:48:36", "pdf_url": "http://arxiv.org/pdf/2505.13191v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13192v1", "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "authors": ["Christoph J\u00fcrgen Hemmer", "Daniel Durstewitz"], "abstract": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.", "categories": ["cs.LG", "cs.AI", "math.DS", "nlin.CD"], "published": "2025-05-19 14:49:10", "updated": "2025-05-19 14:49:10", "pdf_url": "http://arxiv.org/pdf/2505.13192v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13195v1", "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities", "authors": ["Lili Zhang", "Haomiaomiao Wang", "Long Cheng", "Libao Deng", "Tomas Ward"], "abstract": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch.", "categories": ["cs.AI"], "published": "2025-05-19 14:50:44", "updated": "2025-05-19 14:50:44", "pdf_url": "http://arxiv.org/pdf/2505.13195v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13196v1", "title": "A Physics-Inspired Optimizer: Velocity Regularized Adam", "authors": ["Pranav Vaidhyanathan", "Lucas Schorling", "Natalia Ares", "Michael A. Osborne"], "abstract": "We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer\nfor training deep neural networks that draws on ideas from quartic terms for\nkinetic energy with its stabilizing effects on various system dynamics.\nPrevious algorithms, including the ubiquitous Adam, operate at the so called\nadaptive edge of stability regime during training leading to rapid oscillations\nand slowed convergence of loss. However, VRAdam adds a higher order penalty on\nthe learning rate based on the velocity such that the algorithm automatically\nslows down whenever weight updates become large. In practice, we observe that\nthe effective dynamic learning rate shrinks in high-velocity regimes, damping\noscillations and allowing for a more aggressive base step size when necessary\nwithout divergence. By combining this velocity-based regularizer for global\ndamping with per-parameter scaling of Adam to create a hybrid optimizer, we\ndemonstrate that VRAdam consistently exceeds the performance against standard\noptimizers including AdamW. We benchmark various tasks such as image\nclassification, language modeling, image generation and generative modeling\nusing diverse architectures and training methodologies including Convolutional\nNeural Networks (CNNs), Transformers, and GFlowNets.", "categories": ["cs.LG", "cs.AI", "quant-ph"], "published": "2025-05-19 14:51:40", "updated": "2025-05-19 14:51:40", "pdf_url": "http://arxiv.org/pdf/2505.13196v1", "comment": "L. Schorling and P. Vaidhyanathan contributed equally to this work.\n  20 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13201v1", "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "abstract": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:54:04", "updated": "2025-05-19 14:54:04", "pdf_url": "http://arxiv.org/pdf/2505.13201v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13208v1", "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "abstract": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.", "categories": ["quant-ph", "cs.AI", "cs.CL"], "published": "2025-05-19 14:57:53", "updated": "2025-05-19 14:57:53", "pdf_url": "http://arxiv.org/pdf/2505.13208v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13210v1", "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "abstract": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:58:44", "updated": "2025-05-19 14:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13210v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13211v1", "title": "MAGI-1: Autoregressive Video Generation at Scale", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "abstract": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:58:50", "updated": "2025-05-19 14:58:50", "pdf_url": "http://arxiv.org/pdf/2505.13211v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "published": "2025-05-19 15:09:23", "updated": "2025-05-19 15:09:23", "pdf_url": "http://arxiv.org/pdf/2505.13227v1", "comment": "49 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13232v1", "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "abstract": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-19 15:15:35", "updated": "2025-05-19 15:15:35", "pdf_url": "http://arxiv.org/pdf/2505.13232v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13246v1", "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "abstract": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.", "categories": ["cs.AI", "cs.HC"], "published": "2025-05-19 15:28:10", "updated": "2025-05-19 15:28:10", "pdf_url": "http://arxiv.org/pdf/2505.13246v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13253v1", "title": "Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic", "authors": ["Lennart R\u00f6stel", "Dominik Winkelbauer", "Johannes Pitz", "Leon Sievers", "Berthold B\u00e4uml"], "abstract": "In-hand manipulation and grasping are fundamental yet often separately\naddressed tasks in robotics. For deriving in-hand manipulation policies,\nreinforcement learning has recently shown great success. However, the derived\ncontrollers are not yet useful in real-world scenarios because they often\nrequire a human operator to place the objects in suitable initial (grasping)\nstates. Finding stable grasps that also promote the desired in-hand\nmanipulation goal is an open problem. In this work, we propose a method for\nbridging this gap by leveraging the critic network of a reinforcement learning\nagent trained for in-hand manipulation to score and select initial grasps. Our\nexperiments show that this method significantly increases the success rate of\nin-hand manipulation without requiring additional training. We also present an\nimplementation of a full grasp manipulation pipeline on a real-world system,\nenabling autonomous grasping and reorientation even of unwieldy objects.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-19 15:36:34", "updated": "2025-05-19 15:36:34", "pdf_url": "http://arxiv.org/pdf/2505.13253v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13257v1", "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "authors": ["Zilu Tang", "Afra Feyza Aky\u00fcrek", "Ekin Aky\u00fcrek", "Derry Wijaya"], "abstract": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:39:48", "updated": "2025-05-19 15:39:48", "pdf_url": "http://arxiv.org/pdf/2505.13257v1", "comment": "9 pages, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13264v1", "title": "Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty", "authors": ["Carlos Rodriguez-Pardo", "Louis Daumas", "Leonardo Chiani", "Massimo Tavoni"], "abstract": "Climate-economic modeling under uncertainty presents significant\ncomputational challenges that may limit policymakers' ability to address\nclimate change effectively. This paper explores neural network-based approaches\nfor solving high-dimensional optimal control problems arising from models that\nincorporate ambiguity aversion in climate mitigation decisions. We develop a\ncontinuous-time endogenous-growth economic model that accounts for multiple\nmitigation pathways, including emission-free capital and carbon intensity\nreductions. Given the inherent complexity and high dimensionality of these\nmodels, traditional numerical methods become computationally intractable. We\nbenchmark several neural network architectures against finite-difference\ngenerated solutions, evaluating their ability to capture the dynamic\ninteractions between uncertainty, technology transitions, and optimal climate\npolicy. Our findings demonstrate that appropriate neural architecture selection\nsignificantly impacts both solution accuracy and computational efficiency when\nmodeling climate-economic systems under uncertainty. These methodological\nadvances enable more sophisticated modeling of climate policy decisions,\nallowing for better representation of technology transitions and\nuncertainty-critical elements for developing effective mitigation strategies in\nthe face of climate change.", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF", "math.AP", "68T07 (Primary) 35Q91, 91B76 (Secondary)", "I.2.1; I.5.1; J.4"], "published": "2025-05-19 15:46:12", "updated": "2025-05-19 15:46:12", "pdf_url": "http://arxiv.org/pdf/2505.13264v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13268v1", "title": "Representation of perceived prosodic similarity of conversational feedback", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "abstract": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:47:51", "updated": "2025-05-19 15:47:51", "pdf_url": "http://arxiv.org/pdf/2505.13268v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13273v1", "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "abstract": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-19 15:53:32", "updated": "2025-05-19 15:53:32", "pdf_url": "http://arxiv.org/pdf/2505.13273v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13280v1", "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "authors": ["Elias Collaert", "Abel Rodr\u00edguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "abstract": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "published": "2025-05-19 16:04:43", "updated": "2025-05-19 16:04:43", "pdf_url": "http://arxiv.org/pdf/2505.13280v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13287v1", "title": "Level Generation with Quantum Reservoir Computing", "authors": ["Jo\u00e3o S. Ferreira", "Pierre Fromholz", "Hari Shaji", "James R. Wootton"], "abstract": "Reservoir computing is a form of machine learning particularly suited for\ntime series analysis, including forecasting predictions. We take an\nimplementation of \\emph{quantum} reservoir computing that was initially\ndesigned to generate variants of musical scores and adapt it to create levels\nof Super Mario Bros. Motivated by our analysis of these levels, we develop a\nnew Roblox \\textit{obby} where the courses can be generated in real time on\nsuperconducting qubit hardware, and investigate some of the constraints placed\nby such real-time generation.", "categories": ["cs.AI", "quant-ph"], "published": "2025-05-19 16:09:30", "updated": "2025-05-19 16:09:30", "pdf_url": "http://arxiv.org/pdf/2505.13287v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13291v1", "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Micha\u0142 Wili\u0144ski", "Gus Welter", "Artur Dubrawski"], "abstract": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 16:11:23", "updated": "2025-05-19 16:11:23", "pdf_url": "http://arxiv.org/pdf/2505.13291v1", "comment": "Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13292v1", "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs", "authors": ["Huaiying Luo", "Cheng Ji"], "abstract": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 16:14:27", "updated": "2025-05-19 16:14:27", "pdf_url": "http://arxiv.org/pdf/2505.13292v1", "comment": "Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13307v1", "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-19 16:25:55", "updated": "2025-05-19 16:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13307v1", "comment": "Manuscript", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13308v1", "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "abstract": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 16:26:02", "updated": "2025-05-19 16:26:02", "pdf_url": "http://arxiv.org/pdf/2505.13308v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13315v1", "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "authors": ["Reza T. Batley", "Sourav Saha"], "abstract": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.", "categories": ["cs.LG", "cs.AI", "cs.MS"], "published": "2025-05-19 16:29:07", "updated": "2025-05-19 16:29:07", "pdf_url": "http://arxiv.org/pdf/2505.13315v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13316v1", "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "abstract": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 16:29:12", "updated": "2025-05-19 16:29:12", "pdf_url": "http://arxiv.org/pdf/2505.13316v1", "comment": "6 pages, 5 figures, accepted at ICME 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13324v1", "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "abstract": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI.", "categories": ["stat.ML", "cs.AI", "cs.LG", "econ.EM", "stat.ME"], "published": "2025-05-19 16:34:36", "updated": "2025-05-19 16:34:36", "pdf_url": "http://arxiv.org/pdf/2505.13324v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13329v1", "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications", "authors": ["Fr\u00e9d\u00e9ric Berdoz", "Dustin Brunner", "Yann Vonlanthen", "Roger Wattenhofer"], "abstract": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future.", "categories": ["cs.CY", "cs.AI", "cs.CR"], "published": "2025-05-19 16:38:06", "updated": "2025-05-19 16:38:06", "pdf_url": "http://arxiv.org/pdf/2505.13329v1", "comment": "This is the extended version of the paper, accepted at IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "abstract": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "categories": ["cs.CL", "cs.AI", "eess.AS"], "published": "2025-05-19 16:47:46", "updated": "2025-05-19 16:47:46", "pdf_url": "http://arxiv.org/pdf/2505.13338v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13339v1", "title": "OPA-Pack: Object-Property-Aware Robotic Bin Packing", "authors": ["Jia-Hui Pan", "Yeok Tatt Cheah", "Zhengzhe Liu", "Ka-Hei Hui", "Xiaojie Gao", "Pheng-Ann Heng", "Yun-Hui Liu", "Chi-Wing Fu"], "abstract": "Robotic bin packing aids in a wide range of real-world scenarios such as\ne-commerce and warehouses. Yet, existing works focus mainly on considering the\nshape of objects to optimize packing compactness and neglect object properties\nsuch as fragility, edibility, and chemistry that humans typically consider when\npacking objects. This paper presents OPA-Pack (Object-Property-Aware Packing\nframework), the first framework that equips the robot with object property\nconsiderations in planning the object packing. Technical-wise, we develop a\nnovel object property recognition scheme with retrieval-augmented generation\nand chain-of-thought reasoning, and build a dataset with object property\nannotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to\njointly separate incompatible object pairs and reduce pressure on fragile\nobjects, while compacting the packing. Further, OPA-Net consists of a property\nembedding layer to encode the property of candidate objects to be packed,\ntogether with a fragility heightmap and an avoidance heightmap to keep track of\nthe packed objects. Then, we design a reward function and adopt a deep\nQ-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack\ngreatly improves the accuracy of separating incompatible object pairs (from 52%\nto 95%) and largely reduces pressure on fragile objects (by 29.4%), while\nmaintaining good packing compactness. Besides, we demonstrate the effectiveness\nof OPA-Pack on a real packing platform, showcasing its practicality in\nreal-world scenarios.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-19 16:48:14", "updated": "2025-05-19 16:48:14", "pdf_url": "http://arxiv.org/pdf/2505.13339v1", "comment": "Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13344v1", "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "abstract": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 16:50:26", "updated": "2025-05-19 16:50:26", "pdf_url": "http://arxiv.org/pdf/2505.13344v1", "comment": "https://berkegokmen1.github.io/RoPECraft/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13346v1", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "abstract": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 16:50:35", "updated": "2025-05-19 16:50:35", "pdf_url": "http://arxiv.org/pdf/2505.13346v1", "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13355v1", "title": "Multi-Armed Bandits Meet Large Language Models", "authors": ["Djallel Bouneffouf", "Raphael Feraud"], "abstract": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.", "categories": ["cs.AI"], "published": "2025-05-19 16:57:57", "updated": "2025-05-19 16:57:57", "pdf_url": "http://arxiv.org/pdf/2505.13355v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13358v1", "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "abstract": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 16:59:47", "updated": "2025-05-19 16:59:47", "pdf_url": "http://arxiv.org/pdf/2505.13358v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13372v1", "title": "Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning", "authors": ["Irene Brugnara", "Alessandro Valentini", "Andrea Micheli"], "abstract": "Recent work investigated the use of Reinforcement Learning (RL) for the\nsynthesis of heuristic guidance to improve the performance of temporal planners\nwhen a domain is fixed and a set of training problems (not plans) is given. The\nidea is to extract a heuristic from the value function of a particular\n(possibly infinite-state) MDP constructed over the training problems.\n  In this paper, we propose an evolution of this learning and planning\nframework that focuses on exploiting the information provided by symbolic\nheuristics during both the RL and planning phases. First, we formalize\ndifferent reward schemata for the synthesis and use symbolic heuristics to\nmitigate the problems caused by the truncation of episodes needed to deal with\nthe potentially infinite MDP. Second, we propose learning a residual of an\nexisting symbolic heuristic, which is a \"correction\" of the heuristic value,\ninstead of eagerly learning the whole heuristic from scratch. Finally, we use\nthe learned heuristic in combination with a symbolic heuristic using a\nmultiple-queue planning approach to balance systematic search with imperfect\nlearned information. We experimentally compare all the approaches, highlighting\ntheir strengths and weaknesses and significantly advancing the state of the art\nfor this planning and learning schema.", "categories": ["cs.AI"], "published": "2025-05-19 17:19:13", "updated": "2025-05-19 17:19:13", "pdf_url": "http://arxiv.org/pdf/2505.13372v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13379v1", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:24:16", "updated": "2025-05-19 17:24:16", "pdf_url": "http://arxiv.org/pdf/2505.13379v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13380v1", "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:24:26", "updated": "2025-05-19 17:24:26", "pdf_url": "http://arxiv.org/pdf/2505.13380v1", "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13381v1", "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "abstract": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms.", "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.7; H.5.2"], "published": "2025-05-19 17:25:07", "updated": "2025-05-19 17:25:07", "pdf_url": "http://arxiv.org/pdf/2505.13381v1", "comment": "10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM\n  Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy", "doi": "10.1145/3698205.3729542", "journal_ref": null}
{"arxiv_id": "2505.13388v1", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:29:03", "updated": "2025-05-19 17:29:03", "pdf_url": "http://arxiv.org/pdf/2505.13388v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13391v1", "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "authors": ["Miko\u0142aj Ma\u0142ki\u0144ski", "Jacek Ma\u0144dziuk"], "abstract": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods.", "categories": ["cs.AI", "cs.CV", "cs.LG"], "published": "2025-05-19 17:32:07", "updated": "2025-05-19 17:32:07", "pdf_url": "http://arxiv.org/pdf/2505.13391v1", "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13393v1", "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "authors": ["Christopher K. Frantz"], "abstract": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool.", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "published": "2025-05-19 17:33:15", "updated": "2025-05-19 17:33:15", "pdf_url": "http://arxiv.org/pdf/2505.13393v1", "comment": "24 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13400v1", "title": "Robin: A multi-agent system for automating scientific discovery", "authors": ["Ali Essam Ghareeb", "Benjamin Chang", "Ludovico Mitchener", "Angela Yiu", "Caralyn J. Szostkiewicz", "Jon M. Laurent", "Muhammed T. Razzak", "Andrew D. White", "Michaela M. Hinks", "Samuel G. Rodriques"], "abstract": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.", "categories": ["cs.AI", "cs.MA", "q-bio.QM"], "published": "2025-05-19 17:36:17", "updated": "2025-05-19 17:36:17", "pdf_url": "http://arxiv.org/pdf/2505.13400v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13406v1", "title": "AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database", "authors": ["Rong Bian", "Yu Geng", "Zijian Yang", "Bing Cheng"], "abstract": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM.", "categories": ["cs.AI"], "published": "2025-05-19 17:41:29", "updated": "2025-05-19 17:41:29", "pdf_url": "http://arxiv.org/pdf/2505.13406v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13408v1", "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "abstract": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:44:26", "updated": "2025-05-19 17:44:26", "pdf_url": "http://arxiv.org/pdf/2505.13408v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13417v1", "title": "AdaptThink: Reasoning Models Can Learn When to Think", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "abstract": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:50:52", "updated": "2025-05-19 17:50:52", "pdf_url": "http://arxiv.org/pdf/2505.13417v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13425v1", "title": "Learnware of Language Models: Specialized Small Language Models Can Do Big", "authors": ["Zhi-Hao Tan", "Zi-Chen Zhao", "Hao-Yu Shi", "Xin-Yu Zhang", "Peng Tan", "Yang Yu", "Zhi-Hua Zhou"], "abstract": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 17:54:35", "updated": "2025-05-19 17:54:35", "pdf_url": "http://arxiv.org/pdf/2505.13425v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13427v1", "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-19 17:55:08", "updated": "2025-05-19 17:55:08", "pdf_url": "http://arxiv.org/pdf/2505.13427v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13437v1", "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "abstract": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 17:58:11", "updated": "2025-05-19 17:58:11", "pdf_url": "http://arxiv.org/pdf/2505.13437v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 17:58:44", "updated": "2025-05-19 17:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13439v1", "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "abstract": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 17:59:01", "updated": "2025-05-19 17:59:01", "pdf_url": "http://arxiv.org/pdf/2505.13439v1", "comment": "24 pages, 13 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:59:31", "updated": "2025-05-19 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.13445v1", "comment": "code available at https://github.com/xyliu-cs/RISE", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "abstract": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:59:58", "updated": "2025-05-19 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.13448v1", "comment": "10 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12584v1", "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "abstract": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations.", "categories": ["cs.CL"], "published": "2025-05-19 00:14:43", "updated": "2025-05-19 00:14:43", "pdf_url": "http://arxiv.org/pdf/2505.12584v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12587v1", "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "abstract": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 00:50:49", "updated": "2025-05-19 00:50:49", "pdf_url": "http://arxiv.org/pdf/2505.12587v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12592v1", "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "abstract": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts.", "categories": ["cs.CL"], "published": "2025-05-19 01:08:26", "updated": "2025-05-19 01:08:26", "pdf_url": "http://arxiv.org/pdf/2505.12592v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12594v1", "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "abstract": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 01:14:57", "updated": "2025-05-19 01:14:57", "pdf_url": "http://arxiv.org/pdf/2505.12594v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12616v1", "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "authors": ["Shujauddin Syed", "Ted Pedersen"], "abstract": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios.", "categories": ["cs.CL", "68T50"], "published": "2025-05-19 01:58:22", "updated": "2025-05-19 01:58:22", "pdf_url": "http://arxiv.org/pdf/2505.12616v1", "comment": "SemEval-2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12621v1", "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "authors": ["Jo\u00e3o Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "abstract": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-19 02:08:20", "updated": "2025-05-19 02:08:20", "pdf_url": "http://arxiv.org/pdf/2505.12621v1", "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12625v1", "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "abstract": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment.", "categories": ["cs.CL", "cs.CR", "cs.LG"], "published": "2025-05-19 02:16:56", "updated": "2025-05-19 02:16:56", "pdf_url": "http://arxiv.org/pdf/2505.12625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12629v1", "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "abstract": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 02:35:53", "updated": "2025-05-19 02:35:53", "pdf_url": "http://arxiv.org/pdf/2505.12629v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12632v1", "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 02:39:03", "updated": "2025-05-19 02:39:03", "pdf_url": "http://arxiv.org/pdf/2505.12632v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12636v1", "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "abstract": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here.", "categories": ["cs.CL"], "published": "2025-05-19 02:44:57", "updated": "2025-05-19 02:44:57", "pdf_url": "http://arxiv.org/pdf/2505.12636v1", "comment": "Accepted by ACL 2025 main", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12654v1", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "abstract": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:08:30", "updated": "2025-05-19 03:08:30", "pdf_url": "http://arxiv.org/pdf/2505.12654v1", "comment": "Accepected by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12662v1", "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "abstract": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:25:18", "updated": "2025-05-19 03:25:18", "pdf_url": "http://arxiv.org/pdf/2505.12662v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12680v1", "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "abstract": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 03:56:05", "updated": "2025-05-19 03:56:05", "pdf_url": "http://arxiv.org/pdf/2505.12680v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12692v1", "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "abstract": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 04:32:02", "updated": "2025-05-19 04:32:02", "pdf_url": "http://arxiv.org/pdf/2505.12692v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12716v1", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "abstract": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 05:16:21", "updated": "2025-05-19 05:16:21", "pdf_url": "http://arxiv.org/pdf/2505.12716v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12717v1", "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "abstract": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks.", "categories": ["cs.CL"], "published": "2025-05-19 05:18:58", "updated": "2025-05-19 05:18:58", "pdf_url": "http://arxiv.org/pdf/2505.12717v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12718v1", "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "abstract": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content.", "categories": ["cs.CL", "cs.HC"], "published": "2025-05-19 05:19:26", "updated": "2025-05-19 05:19:26", "pdf_url": "http://arxiv.org/pdf/2505.12718v1", "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12723v1", "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "abstract": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages.", "categories": ["cs.CL"], "published": "2025-05-19 05:25:29", "updated": "2025-05-19 05:25:29", "pdf_url": "http://arxiv.org/pdf/2505.12723v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12727v1", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "abstract": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma.", "categories": ["cs.CL", "cs.CY", "cs.HC"], "published": "2025-05-19 05:31:42", "updated": "2025-05-19 05:31:42", "pdf_url": "http://arxiv.org/pdf/2505.12727v1", "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12763v1", "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "abstract": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 06:43:08", "updated": "2025-05-19 06:43:08", "pdf_url": "http://arxiv.org/pdf/2505.12763v1", "comment": "Accepted to ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12768v1", "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "abstract": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set.", "categories": ["cs.CL"], "published": "2025-05-19 06:46:47", "updated": "2025-05-19 06:46:47", "pdf_url": "http://arxiv.org/pdf/2505.12768v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12781v1", "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:10:42", "updated": "2025-05-19 07:10:42", "pdf_url": "http://arxiv.org/pdf/2505.12781v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12792v1", "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "abstract": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies.", "categories": ["cs.CL"], "published": "2025-05-19 07:24:35", "updated": "2025-05-19 07:24:35", "pdf_url": "http://arxiv.org/pdf/2505.12792v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12808v1", "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "abstract": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 07:34:25", "updated": "2025-05-19 07:34:25", "pdf_url": "http://arxiv.org/pdf/2505.12808v1", "comment": "20 pages, ongoing work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12814v1", "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "abstract": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:45:09", "updated": "2025-05-19 07:45:09", "pdf_url": "http://arxiv.org/pdf/2505.12814v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12821v1", "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "abstract": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:03:38", "updated": "2025-05-19 08:03:38", "pdf_url": "http://arxiv.org/pdf/2505.12821v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12831v1", "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "abstract": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP.", "categories": ["cs.CL"], "published": "2025-05-19 08:19:27", "updated": "2025-05-19 08:19:27", "pdf_url": "http://arxiv.org/pdf/2505.12831v1", "comment": "ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12835v1", "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "abstract": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-19 08:21:20", "updated": "2025-05-19 08:21:20", "pdf_url": "http://arxiv.org/pdf/2505.12835v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12837v1", "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "abstract": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:25:21", "updated": "2025-05-19 08:25:21", "pdf_url": "http://arxiv.org/pdf/2505.12837v1", "comment": "20 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12842v1", "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "abstract": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 08:29:05", "updated": "2025-05-19 08:29:05", "pdf_url": "http://arxiv.org/pdf/2505.12842v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12859v1", "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "abstract": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge.", "categories": ["cs.CL"], "published": "2025-05-19 08:43:54", "updated": "2025-05-19 08:43:54", "pdf_url": "http://arxiv.org/pdf/2505.12859v1", "comment": "To be presented a ACL 2025, Main, Long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12864v1", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr\u00fcwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "abstract": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "published": "2025-05-19 08:48:12", "updated": "2025-05-19 08:48:12", "pdf_url": "http://arxiv.org/pdf/2505.12864v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12871v1", "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-19 08:57:08", "updated": "2025-05-19 08:57:08", "pdf_url": "http://arxiv.org/pdf/2505.12871v1", "comment": "To appear at ICML 25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12886v1", "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "categories": ["cs.AI", "cs.CL", "cs.CY"], "published": "2025-05-19 09:16:40", "updated": "2025-05-19 09:16:40", "pdf_url": "http://arxiv.org/pdf/2505.12886v1", "comment": "25 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12888v1", "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "abstract": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines.", "categories": ["cs.CL"], "published": "2025-05-19 09:18:19", "updated": "2025-05-19 09:18:19", "pdf_url": "http://arxiv.org/pdf/2505.12888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12891v1", "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "abstract": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 09:22:02", "updated": "2025-05-19 09:22:02", "pdf_url": "http://arxiv.org/pdf/2505.12891v1", "comment": "First version. There are still some examples to be added into the\n  appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12896v1", "title": "On the Thinking-Language Modeling Gap in Large Language Models", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "abstract": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks.", "categories": ["cs.CL", "cs.LG", "stat.ML"], "published": "2025-05-19 09:31:52", "updated": "2025-05-19 09:31:52", "pdf_url": "http://arxiv.org/pdf/2505.12896v1", "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12900v1", "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "abstract": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation.", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "published": "2025-05-19 09:35:58", "updated": "2025-05-19 09:35:58", "pdf_url": "http://arxiv.org/pdf/2505.12900v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12920v1", "title": "PyFCG: Fluid Construction Grammar in Python", "authors": ["Paul Van Eecke", "Katrien Beuls"], "abstract": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-19 10:00:01", "updated": "2025-05-19 10:00:01", "pdf_url": "http://arxiv.org/pdf/2505.12920v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12929v1", "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "abstract": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:14:08", "updated": "2025-05-19 10:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12929v1", "comment": "24 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12938v1", "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "abstract": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 10:22:04", "updated": "2025-05-19 10:22:04", "pdf_url": "http://arxiv.org/pdf/2505.12938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12942v1", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "abstract": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:29:32", "updated": "2025-05-19 10:29:32", "pdf_url": "http://arxiv.org/pdf/2505.12942v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12949v1", "title": "Neural Morphological Tagging for Nguni Languages", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "abstract": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages.", "categories": ["cs.CL"], "published": "2025-05-19 10:41:47", "updated": "2025-05-19 10:41:47", "pdf_url": "http://arxiv.org/pdf/2505.12949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12950v1", "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "abstract": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE.", "categories": ["cs.CL"], "published": "2025-05-19 10:42:36", "updated": "2025-05-19 10:42:36", "pdf_url": "http://arxiv.org/pdf/2505.12950v1", "comment": "14 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12964v1", "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "abstract": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master.", "categories": ["cs.CL"], "published": "2025-05-19 11:00:43", "updated": "2025-05-19 11:00:43", "pdf_url": "http://arxiv.org/pdf/2505.12964v1", "comment": "preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12969v1", "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "abstract": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other.", "categories": ["cs.CL"], "published": "2025-05-19 11:04:52", "updated": "2025-05-19 11:04:52", "pdf_url": "http://arxiv.org/pdf/2505.12969v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12970v1", "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "authors": ["Robin Jegan", "Andreas Henrich"], "abstract": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801", "categories": ["cs.CL"], "published": "2025-05-19 11:06:50", "updated": "2025-05-19 11:06:50", "pdf_url": "http://arxiv.org/pdf/2505.12970v1", "comment": "14 pages, 1 figure", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12973v1", "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "abstract": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems.", "categories": ["cs.CL"], "published": "2025-05-19 11:11:12", "updated": "2025-05-19 11:11:12", "pdf_url": "http://arxiv.org/pdf/2505.12973v1", "comment": "8 main body pages, total 25 pages, 15 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12983v1", "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:18:54", "updated": "2025-05-19 11:18:54", "pdf_url": "http://arxiv.org/pdf/2505.12983v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12992v1", "title": "Fractured Chain-of-Thought Reasoning", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "published": "2025-05-19 11:30:41", "updated": "2025-05-19 11:30:41", "pdf_url": "http://arxiv.org/pdf/2505.12992v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12996v1", "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:34:47", "updated": "2025-05-19 11:34:47", "pdf_url": "http://arxiv.org/pdf/2505.12996v1", "comment": "12 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13004v1", "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "abstract": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x.", "categories": ["cs.CL"], "published": "2025-05-19 11:43:37", "updated": "2025-05-19 11:43:37", "pdf_url": "http://arxiv.org/pdf/2505.13004v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13006v1", "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias K\u00e4fer"], "abstract": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions.", "categories": ["cs.CL"], "published": "2025-05-19 11:46:30", "updated": "2025-05-19 11:46:30", "pdf_url": "http://arxiv.org/pdf/2505.13006v1", "comment": "Accepted by NAACL 2025 industry track", "doi": null, "journal_ref": "In Proc. NAACL-HLT 2025 Industry Track, pp. 794-808. Albuquerque,\n  NM, 2025"}
{"arxiv_id": "2505.13010v1", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "abstract": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-19 11:54:39", "updated": "2025-05-19 11:54:39", "pdf_url": "http://arxiv.org/pdf/2505.13010v1", "comment": "7 pages, 5 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13028v1", "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "authors": ["Sayon Palit", "Daniel Woods"], "abstract": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "published": "2025-05-19 12:12:00", "updated": "2025-05-19 12:12:00", "pdf_url": "http://arxiv.org/pdf/2505.13028v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13032v1", "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix", "authors": ["Ziyang Ma", "Yinghao Ma", "Yanqiao Zhu", "Chen Yang", "Yi-Wen Chao", "Ruiyang Xu", "Wenxi Chen", "Yuanzhe Chen", "Zhuo Chen", "Jian Cong", "Kai Li", "Keliang Li", "Siyou Li", "Xinfeng Li", "Xiquan Li", "Zheng Lian", "Yuzhe Liang", "Minghao Liu", "Zhikang Niu", "Tianrui Wang", "Yuping Wang", "Yuxuan Wang", "Yihao Wu", "Guanrou Yang", "Jianwei Yu", "Ruibin Yuan", "Zhisheng Zheng", "Ziya Zhou", "Haina Zhu", "Wei Xue", "Emmanouil Benetos", "Kai Yu", "Eng-Siong Chng", "Xie Chen"], "abstract": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area.", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "published": "2025-05-19 12:18:42", "updated": "2025-05-19 12:18:42", "pdf_url": "http://arxiv.org/pdf/2505.13032v1", "comment": "Open-source at https://github.com/ddlBoJack/MMAR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13034v1", "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "authors": ["M\u00e1rton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "abstract": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models.", "categories": ["cs.CL"], "published": "2025-05-19 12:19:01", "updated": "2025-05-19 12:19:01", "pdf_url": "http://arxiv.org/pdf/2505.13034v1", "comment": "9 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13036v1", "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "authors": ["Sai Koneru", "Maike Z\u00fcfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "abstract": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:21:29", "updated": "2025-05-19 12:21:29", "pdf_url": "http://arxiv.org/pdf/2505.13036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13053v1", "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "abstract": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:42:23", "updated": "2025-05-19 12:42:23", "pdf_url": "http://arxiv.org/pdf/2505.13053v1", "comment": "currently under review at Frontiers in Communication", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13069v1", "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenol\u00e9 Quellec"], "abstract": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability.", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "published": "2025-05-19 13:04:37", "updated": "2025-05-19 13:04:37", "pdf_url": "http://arxiv.org/pdf/2505.13069v1", "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13077v1", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "abstract": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 13:11:28", "updated": "2025-05-19 13:11:28", "pdf_url": "http://arxiv.org/pdf/2505.13077v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13089v1", "title": "Systematic Generalization in Language Models Scales with Information Entropy", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "\u00c9tienne Simon"], "abstract": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization.", "categories": ["cs.CL"], "published": "2025-05-19 13:23:44", "updated": "2025-05-19 13:23:44", "pdf_url": "http://arxiv.org/pdf/2505.13089v1", "comment": "Accepted to ACL 2025: Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13090v1", "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "authors": ["David Stap", "Christof Monz"], "abstract": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity.", "categories": ["cs.CL"], "published": "2025-05-19 13:24:01", "updated": "2025-05-19 13:24:01", "pdf_url": "http://arxiv.org/pdf/2505.13090v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13098v1", "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "abstract": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.", "categories": ["cs.AI", "cs.CL", "cs.DB"], "published": "2025-05-19 13:29:27", "updated": "2025-05-19 13:29:27", "pdf_url": "http://arxiv.org/pdf/2505.13098v1", "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13109v1", "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "abstract": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 13:36:45", "updated": "2025-05-19 13:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13109v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13115v1", "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "abstract": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-19 13:46:35", "updated": "2025-05-19 13:46:35", "pdf_url": "http://arxiv.org/pdf/2505.13115v1", "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13126v1", "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "abstract": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 13:58:15", "updated": "2025-05-19 13:58:15", "pdf_url": "http://arxiv.org/pdf/2505.13126v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13136v1", "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "abstract": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 14:07:20", "updated": "2025-05-19 14:07:20", "pdf_url": "http://arxiv.org/pdf/2505.13136v1", "comment": "under review @ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13141v1", "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "abstract": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English.", "categories": ["cs.CL"], "published": "2025-05-19 14:10:15", "updated": "2025-05-19 14:10:15", "pdf_url": "http://arxiv.org/pdf/2505.13141v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13147v1", "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "abstract": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP.", "categories": ["cs.CL"], "published": "2025-05-19 14:12:05", "updated": "2025-05-19 14:12:05", "pdf_url": "http://arxiv.org/pdf/2505.13147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13156v1", "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "abstract": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:17:37", "updated": "2025-05-19 14:17:37", "pdf_url": "http://arxiv.org/pdf/2505.13156v1", "comment": "23 pages, 4 figures, and 1 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13157v1", "title": "Role-Playing Evaluation for Large Language Models", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "abstract": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:18:16", "updated": "2025-05-19 14:18:16", "pdf_url": "http://arxiv.org/pdf/2505.13157v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13171v1", "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "abstract": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.", "categories": ["cs.CL"], "published": "2025-05-19 14:28:35", "updated": "2025-05-19 14:28:35", "pdf_url": "http://arxiv.org/pdf/2505.13171v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13173v1", "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.", "categories": ["cs.CL", "I.2.7"], "published": "2025-05-19 14:30:10", "updated": "2025-05-19 14:30:10", "pdf_url": "http://arxiv.org/pdf/2505.13173v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13176v1", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "abstract": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:30:46", "updated": "2025-05-19 14:30:46", "pdf_url": "http://arxiv.org/pdf/2505.13176v1", "comment": "Accepted by ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13181v1", "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "abstract": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-19 14:38:59", "updated": "2025-05-19 14:38:59", "pdf_url": "http://arxiv.org/pdf/2505.13181v1", "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13204v1", "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "abstract": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23.", "categories": ["cs.CL"], "published": "2025-05-19 14:55:41", "updated": "2025-05-19 14:55:41", "pdf_url": "http://arxiv.org/pdf/2505.13204v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13208v1", "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "abstract": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.", "categories": ["quant-ph", "cs.AI", "cs.CL"], "published": "2025-05-19 14:57:53", "updated": "2025-05-19 14:57:53", "pdf_url": "http://arxiv.org/pdf/2505.13208v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13210v1", "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "abstract": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:58:44", "updated": "2025-05-19 14:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13210v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13220v1", "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "abstract": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.", "categories": ["cs.CL"], "published": "2025-05-19 15:02:59", "updated": "2025-05-19 15:02:59", "pdf_url": "http://arxiv.org/pdf/2505.13220v1", "comment": "Accepted by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "published": "2025-05-19 15:09:23", "updated": "2025-05-19 15:09:23", "pdf_url": "http://arxiv.org/pdf/2505.13227v1", "comment": "49 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13237v1", "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information", "authors": ["Chih-Kai Yang", "Neo Ho", "Yen-Ting Piao", "Hung-yi Lee"], "abstract": "Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-19 15:20:32", "updated": "2025-05-19 15:20:32", "pdf_url": "http://arxiv.org/pdf/2505.13237v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13244v1", "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "abstract": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 15:24:53", "updated": "2025-05-19 15:24:53", "pdf_url": "http://arxiv.org/pdf/2505.13244v1", "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13251v1", "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "authors": ["Sidney Wong"], "abstract": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide.", "categories": ["cs.CL"], "published": "2025-05-19 15:34:07", "updated": "2025-05-19 15:34:07", "pdf_url": "http://arxiv.org/pdf/2505.13251v1", "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13252v1", "title": "Natural Language Planning via Coding and Inference Scaling", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "abstract": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization.", "categories": ["cs.CL"], "published": "2025-05-19 15:35:17", "updated": "2025-05-19 15:35:17", "pdf_url": "http://arxiv.org/pdf/2505.13252v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13254v1", "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "abstract": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration.", "categories": ["cs.CL"], "published": "2025-05-19 15:38:40", "updated": "2025-05-19 15:38:40", "pdf_url": "http://arxiv.org/pdf/2505.13254v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13257v1", "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "authors": ["Zilu Tang", "Afra Feyza Aky\u00fcrek", "Ekin Aky\u00fcrek", "Derry Wijaya"], "abstract": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:39:48", "updated": "2025-05-19 15:39:48", "pdf_url": "http://arxiv.org/pdf/2505.13257v1", "comment": "9 pages, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13258v1", "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "abstract": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.", "categories": ["cs.CL"], "published": "2025-05-19 15:40:29", "updated": "2025-05-19 15:40:29", "pdf_url": "http://arxiv.org/pdf/2505.13258v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13259v1", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "abstract": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "categories": ["cs.CL"], "published": "2025-05-19 15:41:32", "updated": "2025-05-19 15:41:32", "pdf_url": "http://arxiv.org/pdf/2505.13259v1", "comment": "16 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13268v1", "title": "Representation of perceived prosodic similarity of conversational feedback", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "abstract": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:47:51", "updated": "2025-05-19 15:47:51", "pdf_url": "http://arxiv.org/pdf/2505.13268v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13271v1", "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "abstract": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.", "categories": ["cs.CL"], "published": "2025-05-19 15:52:19", "updated": "2025-05-19 15:52:19", "pdf_url": "http://arxiv.org/pdf/2505.13271v1", "comment": "11 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13282v1", "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "abstract": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.", "categories": ["cs.CL"], "published": "2025-05-19 16:06:13", "updated": "2025-05-19 16:06:13", "pdf_url": "http://arxiv.org/pdf/2505.13282v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13302v1", "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "abstract": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "categories": ["cs.CL"], "published": "2025-05-19 16:20:54", "updated": "2025-05-19 16:20:54", "pdf_url": "http://arxiv.org/pdf/2505.13302v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13307v1", "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-19 16:25:55", "updated": "2025-05-19 16:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13307v1", "comment": "Manuscript", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13308v1", "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "abstract": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 16:26:02", "updated": "2025-05-19 16:26:02", "pdf_url": "http://arxiv.org/pdf/2505.13308v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13312v1", "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility.", "categories": ["cs.CL"], "published": "2025-05-19 16:26:58", "updated": "2025-05-19 16:26:58", "pdf_url": "http://arxiv.org/pdf/2505.13312v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13328v1", "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "abstract": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons.", "categories": ["cs.CL"], "published": "2025-05-19 16:36:13", "updated": "2025-05-19 16:36:13", "pdf_url": "http://arxiv.org/pdf/2505.13328v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "abstract": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "categories": ["cs.CL", "cs.AI", "eess.AS"], "published": "2025-05-19 16:47:46", "updated": "2025-05-19 16:47:46", "pdf_url": "http://arxiv.org/pdf/2505.13338v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13346v1", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "abstract": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 16:50:35", "updated": "2025-05-19 16:50:35", "pdf_url": "http://arxiv.org/pdf/2505.13346v1", "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13348v1", "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "abstract": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks.", "categories": ["cs.CL"], "published": "2025-05-19 16:51:12", "updated": "2025-05-19 16:51:12", "pdf_url": "http://arxiv.org/pdf/2505.13348v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13353v1", "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "authors": ["Adam \u0160torek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "abstract": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation.", "categories": ["cs.CL", "cs.LG", "cs.SE"], "published": "2025-05-19 16:56:31", "updated": "2025-05-19 16:56:31", "pdf_url": "http://arxiv.org/pdf/2505.13353v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13360v1", "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian K\u00e4stner", "Tongshuang Wu"], "abstract": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring.", "categories": ["cs.CL", "cs.SE"], "published": "2025-05-19 17:03:42", "updated": "2025-05-19 17:03:42", "pdf_url": "http://arxiv.org/pdf/2505.13360v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13379v1", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:24:16", "updated": "2025-05-19 17:24:16", "pdf_url": "http://arxiv.org/pdf/2505.13379v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13380v1", "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:24:26", "updated": "2025-05-19 17:24:26", "pdf_url": "http://arxiv.org/pdf/2505.13380v1", "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13388v1", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:29:03", "updated": "2025-05-19 17:29:03", "pdf_url": "http://arxiv.org/pdf/2505.13388v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13393v1", "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "authors": ["Christopher K. Frantz"], "abstract": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool.", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "published": "2025-05-19 17:33:15", "updated": "2025-05-19 17:33:15", "pdf_url": "http://arxiv.org/pdf/2505.13393v1", "comment": "24 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13398v1", "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "abstract": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 17:34:56", "updated": "2025-05-19 17:34:56", "pdf_url": "http://arxiv.org/pdf/2505.13398v1", "comment": "9 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13403v1", "title": "MR. Judge: Multimodal Reasoner as a Judge", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "abstract": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.", "categories": ["cs.CL"], "published": "2025-05-19 17:37:39", "updated": "2025-05-19 17:37:39", "pdf_url": "http://arxiv.org/pdf/2505.13403v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13404v1", "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "abstract": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary", "categories": ["cs.CL", "eess.AS"], "published": "2025-05-19 17:40:58", "updated": "2025-05-19 17:40:58", "pdf_url": "http://arxiv.org/pdf/2505.13404v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13408v1", "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "abstract": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:44:26", "updated": "2025-05-19 17:44:26", "pdf_url": "http://arxiv.org/pdf/2505.13408v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13417v1", "title": "AdaptThink: Reasoning Models Can Learn When to Think", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "abstract": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:50:52", "updated": "2025-05-19 17:50:52", "pdf_url": "http://arxiv.org/pdf/2505.13417v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13418v1", "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "abstract": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 17:51:35", "updated": "2025-05-19 17:51:35", "pdf_url": "http://arxiv.org/pdf/2505.13418v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13430v1", "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "abstract": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-19 17:55:15", "updated": "2025-05-19 17:55:15", "pdf_url": "http://arxiv.org/pdf/2505.13430v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13434v1", "title": "SMOTExT: SMOTE meets Large Language Models", "authors": ["Mateusz Bystro\u0144ski", "Miko\u0142aj Ho\u0142ysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "abstract": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints.", "categories": ["cs.CL"], "published": "2025-05-19 17:57:36", "updated": "2025-05-19 17:57:36", "pdf_url": "http://arxiv.org/pdf/2505.13434v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 17:58:44", "updated": "2025-05-19 17:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13444v1", "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "abstract": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-19 17:59:27", "updated": "2025-05-19 17:59:27", "pdf_url": "http://arxiv.org/pdf/2505.13444v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:59:31", "updated": "2025-05-19 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.13445v1", "comment": "code available at https://github.com/xyliu-cs/RISE", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "abstract": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:59:58", "updated": "2025-05-19 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.13448v1", "comment": "10 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13787v1", "title": "Preference Learning with Lie Detectors can Induce Honesty or Evasion", "authors": ["Chris Cundy", "Adam Gleave"], "abstract": "As AI systems become more capable, deceptive behaviors can undermine\nevaluation and mislead users at deployment. Recent work has shown that lie\ndetectors can accurately classify deceptive behavior, but they are not\ntypically used in the training pipeline due to concerns around contamination\nand objective hacking. We examine these concerns by incorporating a lie\ndetector into the labelling step of LLM post-training and evaluating whether\nthe learned policy is genuinely more honest, or instead learns to fool the lie\ndetector while remaining deceptive. Using DolusChat, a novel 65k-example\ndataset with paired truthful/deceptive responses, we identify three key factors\nthat determine the honesty of learned policies: amount of exploration during\npreference learning, lie detector accuracy, and KL regularization strength. We\nfind that preference learning with lie detectors and GRPO can lead to policies\nwhich evade lie detectors, with deception rates of over 85\\%. However, if the\nlie detector true positive rate (TPR) or KL regularization is sufficiently\nhigh, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)\nconsistently lead to deception rates under 25\\% for realistic TPRs. Our results\nillustrate a more complex picture than previously assumed: depending on the\ncontext, lie-detector-enhanced training can be a powerful tool for scalable\noversight, or a counterproductive method encouraging undetectable misalignment.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 00:31:53", "updated": "2025-05-20 00:31:53", "pdf_url": "http://arxiv.org/pdf/2505.13787v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13792v1", "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "abstract": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 00:49:19", "updated": "2025-05-20 00:49:19", "pdf_url": "http://arxiv.org/pdf/2505.13792v1", "comment": "10 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13794v1", "title": "LLM-based Evaluation Policy Extraction for Ecological Modeling", "authors": ["Qi Cheng", "Licheng Liu", "Qing Zhu", "Runlong Yu", "Zhenong Jin", "Yiqun Xie", "Xiaowei Jia"], "abstract": "Evaluating ecological time series is critical for benchmarking model\nperformance in many important applications, including predicting greenhouse gas\nfluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.\nTraditional numerical metrics (e.g., R-squared, root mean square error) have\nbeen widely used to quantify the similarity between modeled and observed\necosystem variables, but they often fail to capture domain-specific temporal\npatterns critical to ecological processes. As a result, these methods are often\naccompanied by expert visual inspection, which requires substantial human labor\nand limits the applicability to large-scale evaluation. To address these\nchallenges, we propose a novel framework that integrates metric learning with\nlarge language model (LLM)-based natural language policy extraction to develop\ninterpretable evaluation criteria. The proposed method processes pairwise\nannotations and implements a policy optimization mechanism to generate and\ncombine different assessment metrics. The results obtained on multiple datasets\nfor evaluating the predictions of crop gross primary production and carbon\ndioxide flux have confirmed the effectiveness of the proposed method in\ncapturing target assessment preferences, including both synthetically generated\nand expert-annotated model comparisons. The proposed framework bridges the gap\nbetween numerical metrics and expert knowledge while providing interpretable\nevaluation policies that accommodate the diverse needs of different ecosystem\nmodeling studies.", "categories": ["cs.AI"], "published": "2025-05-20 01:02:29", "updated": "2025-05-20 01:02:29", "pdf_url": "http://arxiv.org/pdf/2505.13794v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13805v1", "title": "ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech", "authors": ["Yu Pan", "Yanni Hu", "Yuguang Yang", "Jixun Yao", "Jianhao Ye", "Hongbin Zhou", "Lei Ma", "Jianjun Zhao"], "abstract": "Despite great advances, achieving high-fidelity emotional voice conversion\n(EVC) with flexible and interpretable control remains challenging. This paper\nintroduces ClapFM-EVC, a novel EVC framework capable of generating high-quality\nconverted speech driven by natural language prompts or reference speech with\nadjustable emotion intensity. We first propose EVC-CLAP, an emotional\ncontrastive language-audio pre-training model, guided by natural language\nprompts and categorical labels, to extract and align fine-grained emotional\nelements across speech and text modalities. Then, a FuEncoder with an adaptive\nintensity gate is presented to seamless fuse emotional features with Phonetic\nPosteriorGrams from a pre-trained ASR model. To further improve emotion\nexpressiveness and speech naturalness, we propose a flow matching model\nconditioned on these captured features to reconstruct Mel-spectrogram of source\nspeech. Subjective and objective evaluations validate the effectiveness of\nClapFM-EVC.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 01:34:29", "updated": "2025-05-20 01:34:29", "pdf_url": "http://arxiv.org/pdf/2505.13805v1", "comment": "Accepted by InterSpeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13808v1", "title": "RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework", "authors": ["Faramarz Safi Esfahani", "Ghassan Beydoun", "Morteza Saberi", "Brad McCusker", "Biswajeet Pradhan"], "abstract": "Metaheuristic algorithms are widely used for solving complex optimization\nproblems, yet their effectiveness is often constrained by fixed structures and\nthe need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)\naddresses this limitation by introducing a self-adaptive metaheuristic\nswitching mechanism driven by real-time performance feedback and dynamic\nalgorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)\nand the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select\nand transition between metaheuristic algorithms based on key performance\nindicators, ensuring continuous adaptation. This approach enhances convergence\nspeed, adaptability, and solution quality, outperforming traditional\nmetaheuristics in high-dimensional, dynamic, and multimodal environments.\nExperimental results on benchmark functions demonstrate that PMF significantly\nimproves optimization efficiency by mitigating stagnation and balancing\nexploration-exploitation strategies across various problem landscapes. By\nintegrating AI-driven decision-making and self-correcting mechanisms, PMF paves\nthe way for scalable, intelligent, and autonomous optimization frameworks, with\npromising applications in engineering, logistics, and complex decision-making\nsystems.", "categories": ["cs.NE", "cs.AI"], "published": "2025-05-20 01:41:22", "updated": "2025-05-20 01:41:22", "pdf_url": "http://arxiv.org/pdf/2505.13808v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13814v1", "title": "Articulatory Feature Prediction from Surface EMG during Speech Production", "authors": ["Jihwan Lee", "Kevin Huang", "Kleanthis Avramidis", "Simon Pistrosch", "Monica Gonzalez-Machorro", "Yoonjeong Lee", "Bj\u00f6rn Schuller", "Louis Goldstein", "Shrikanth Narayanan"], "abstract": "We present a model for predicting articulatory features from surface\nelectromyography (EMG) signals during speech production. The proposed model\nintegrates convolutional layers and a Transformer block, followed by separate\npredictors for articulatory features. Our approach achieves a high prediction\ncorrelation of approximately 0.9 for most articulatory features. Furthermore,\nwe demonstrate that these predicted articulatory features can be decoded into\nintelligible speech waveforms. To our knowledge, this is the first method to\ndecode speech waveforms from surface EMG via articulatory features, offering a\nnovel approach to EMG-based speech synthesis. Additionally, we analyze the\nrelationship between EMG electrode placement and articulatory feature\npredictability, providing knowledge-driven insights for optimizing EMG\nelectrode configurations. The source code and decoded speech samples are\npublicly available.", "categories": ["eess.AS", "cs.AI", "cs.SD"], "published": "2025-05-20 01:50:05", "updated": "2025-05-20 01:50:05", "pdf_url": "http://arxiv.org/pdf/2505.13814v1", "comment": "Accepted for Interspeech2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13820v1", "title": "Structured Agent Distillation for Large Language Model", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 02:01:55", "updated": "2025-05-20 02:01:55", "pdf_url": "http://arxiv.org/pdf/2505.13820v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13828v1", "title": "Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models", "authors": ["Kiarash Naghavi Khanghah", "Zhiling Chen", "Lela Romeo", "Qian Yang", "Rajiv Malhotra", "Farhad Imani", "Hongyi Xu"], "abstract": "Additive manufacturing enables the fabrication of complex designs while\nminimizing waste, but faces challenges related to defects and process\nanomalies. This study presents a novel multimodal Retrieval-Augmented\nGeneration-based framework that automates anomaly detection across various\nAdditive Manufacturing processes leveraging retrieved information from\nliterature, including images and descriptive text, rather than training\ndatasets. This framework integrates text and image retrieval from scientific\nliterature and multimodal generation models to perform zero-shot anomaly\nidentification, classification, and explanation generation in a Laser Powder\nBed Fusion setting. The proposed framework is evaluated on four L-PBF\nmanufacturing datasets from Oak Ridge National Laboratory, featuring various\nprinter makes, models, and materials. This evaluation demonstrates the\nframework's adaptability and generalizability across diverse images without\nrequiring additional training. Comparative analysis using Qwen2-VL-2B and\nGPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini\noutperforms Qwen2-VL-2B and proportional random baseline in manufacturing\nanomalies classification. Additionally, the evaluation of the RAG system\nconfirms that incorporating retrieval mechanisms improves average accuracy by\n12% by reducing the risk of hallucination and providing additional information.\nThe proposed framework can be continuously updated by integrating emerging\nresearch, allowing seamless adaptation to the evolving landscape of AM\ntechnologies. This scalable, automated, and zero-shot-capable framework\nstreamlines AM anomaly analysis, enhancing efficiency and accuracy.", "categories": ["cs.AI"], "published": "2025-05-20 02:18:22", "updated": "2025-05-20 02:18:22", "pdf_url": "http://arxiv.org/pdf/2505.13828v1", "comment": "ASME 2025 International Design Engineering Technical Conferences and\n  Computers and Information in Engineering Conference IDETC/CIE2025, August\n  17-20, 2025, Anaheim, CA (IDETC2025-168615)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13831v1", "title": "TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning", "authors": ["Zongyuan Deng", "Yujie Cai", "Qing Liu", "Shiyao Mu", "Bin Lyu", "Zhen Yang"], "abstract": "The selection of base station sites is a critical challenge in 5G network\nplanning, which requires efficient optimization of coverage, cost, user\nsatisfaction, and practical constraints. Traditional manual methods, reliant on\nhuman expertise, suffer from inefficiencies and are limited to an unsatisfied\nplanning-construction consistency. Existing AI tools, despite improving\nefficiency in certain aspects, still struggle to meet the dynamic network\nconditions and multi-objective needs of telecom operators' networks. To address\nthese challenges, we propose TelePlanNet, an AI-driven framework tailored for\nthe selection of base station sites, integrating a three-layer architecture for\nefficient planning and large-scale automation. By leveraging large language\nmodels (LLMs) for real-time user input processing and intent alignment with\nbase station planning, combined with training the planning model using the\nimproved group relative policy optimization (GRPO) reinforcement learning, the\nproposed TelePlanNet can effectively address multi-objective optimization,\nevaluates candidate sites, and delivers practical solutions. Experiments\nresults show that the proposed TelePlanNet can improve the consistency to 78%,\nwhich is superior to the manual methods, providing telecom operators with an\nefficient and scalable tool that significantly advances cellular network\nplanning.", "categories": ["cs.AI", "I.2; I.2.6; C.2.1"], "published": "2025-05-20 02:19:10", "updated": "2025-05-20 02:19:10", "pdf_url": "http://arxiv.org/pdf/2505.13831v1", "comment": "6 pages, 5 figures, 1 table, submitted to IEEE ICCC 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13834v1", "title": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams", "authors": ["Zhi Su", "Yuman Gao", "Emily Lukas", "Yunfei Li", "Jiaze Cai", "Faris Tulbah", "Fei Gao", "Chao Yu", "Zhongyu Li", "Yi Wu", "Koushil Sreenath"], "abstract": "Achieving coordinated teamwork among legged robots requires both fine-grained\nlocomotion control and long-horizon strategic decision-making. Robot soccer\noffers a compelling testbed for this challenge, combining dynamic, competitive,\nand multi-agent interactions. In this work, we present a hierarchical\nmulti-agent reinforcement learning (MARL) framework that enables fully\nautonomous and decentralized quadruped robot soccer. First, a set of highly\ndynamic low-level skills is trained for legged locomotion and ball\nmanipulation, such as walking, dribbling, and kicking. On top of these, a\nhigh-level strategic planning policy is trained with Multi-Agent Proximal\nPolicy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning\nframework allows agents to adapt to diverse opponent strategies and gives rise\nto sophisticated team behaviors, including coordinated passing, interception,\nand dynamic role allocation. With an extensive ablation study, the proposed\nlearning method shows significant advantages in the cooperative and competitive\nmulti-agent soccer game. We deploy the learned policies to real quadruped\nrobots relying solely on onboard proprioception and decentralized localization,\nwith the resulting system supporting autonomous robot-robot and robot-human\nsoccer matches on indoor and outdoor soccer courts.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 02:20:54", "updated": "2025-05-20 02:20:54", "pdf_url": "http://arxiv.org/pdf/2505.13834v1", "comment": "11 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13837v1", "title": "Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements", "authors": ["Gokul Puthumanaillam", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Melkior Ornik"], "abstract": "Robots navigating complex environments must manage uncertainty from sensor\nnoise, environmental changes, and incomplete information, with different tasks\nrequiring varying levels of precision in different areas. For example, precise\nlocalization may be crucial near obstacles but less critical in open spaces. We\npresent GUIDE (Generalized Uncertainty Integration for Decision-Making and\nExecution), a framework that integrates these task-specific requirements into\nnavigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning\nacceptable uncertainty levels to different locations, TSUMs enable robots to\nadapt uncertainty management based on context. When combined with reinforcement\nlearning, GUIDE learns policies that balance task completion and uncertainty\nmanagement without extensive reward engineering. Real-world tests show\nsignificant performance gains over methods lacking task-specific uncertainty\nawareness.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-20 02:23:15", "updated": "2025-05-20 02:23:15", "pdf_url": "http://arxiv.org/pdf/2505.13837v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "abstract": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 02:27:08", "updated": "2025-05-20 02:27:08", "pdf_url": "http://arxiv.org/pdf/2505.13840v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13847v1", "title": "Forensic deepfake audio detection using segmental speech features", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "abstract": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 02:42:46", "updated": "2025-05-20 02:42:46", "pdf_url": "http://arxiv.org/pdf/2505.13847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13851v1", "title": "A Challenge to Build Neuro-Symbolic Video Agents", "authors": ["Sahil Shah", "Harsh Goel", "Sai Shankar Narasimhan", "Minkyu Choi", "S P Sharan", "Oguzhan Akcin", "Sandeep Chinchali"], "abstract": "Modern video understanding systems excel at tasks such as scene\nclassification, object detection, and short video retrieval. However, as video\nanalysis becomes increasingly central to real-world applications, there is a\ngrowing need for proactive video agents for the systems that not only interpret\nvideo streams but also reason about events and take informed actions. A key\nobstacle in this direction is temporal reasoning: while deep learning models\nhave made remarkable progress in recognizing patterns within individual frames\nor short clips, they struggle to understand the sequencing and dependencies of\nevents over time, which is critical for action-driven decision-making.\nAddressing this limitation demands moving beyond conventional deep learning\napproaches. We posit that tackling this challenge requires a neuro-symbolic\nperspective, where video queries are decomposed into atomic events, structured\ninto coherent sequences, and validated against temporal constraints. Such an\napproach can enhance interpretability, enable structured reasoning, and provide\nstronger guarantees on system behavior, all key properties for advancing\ntrustworthy video agents. To this end, we present a grand challenge to the\nresearch community: developing the next generation of intelligent video agents\nthat integrate three core capabilities: (1) autonomous video search and\nanalysis, (2) seamless real-world interaction, and (3) advanced content\ngeneration. By addressing these pillars, we can transition from passive\nperception to intelligent video agents that reason, predict, and act, pushing\nthe boundaries of video understanding.", "categories": ["cs.AI"], "published": "2025-05-20 02:53:21", "updated": "2025-05-20 02:53:21", "pdf_url": "http://arxiv.org/pdf/2505.13851v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13855v1", "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "abstract": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 03:02:05", "updated": "2025-05-20 03:02:05", "pdf_url": "http://arxiv.org/pdf/2505.13855v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13857v1", "title": "Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer", "authors": ["Tian Sun", "Yuqi Chen", "Baihua Zheng", "Weiwei Sun"], "abstract": "In real-world applications, GPS trajectories often suffer from low sampling\nrates, with large and irregular intervals between consecutive GPS points. This\nsparse characteristic presents challenges for their direct use in GPS-based\nsystems. This paper addresses the task of map-constrained trajectory recovery,\naiming to enhance trajectory sampling rates of GPS trajectories. Previous\nstudies commonly adopt a sequence-to-sequence framework, where an encoder\ncaptures the trajectory patterns and a decoder reconstructs the target\ntrajectory. Within this framework, effectively representing the road network\nand extracting relevant trajectory features are crucial for overall\nperformance. Despite advancements in these models, they fail to fully leverage\nthe complex spatio-temporal dynamics present in both the trajectory and the\nroad network.\n  To overcome these limitations, we categorize the spatio-temporal dynamics of\ntrajectory data into two distinct aspects: spatial-temporal traffic dynamics\nand trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for\ntrajectory recovery. To capture spatio-temporal traffic dynamics, we introduce\nPD-GNN, which models periodic patterns and learns topologically aware dynamics\nconcurrently for each road segment. For spatio-temporal trajectory dynamics, we\npresent TedFormer, a time-aware Transformer that incorporates temporal dynamics\nfor each GPS location by integrating closed-form neural ordinary differential\nequations into the attention mechanism. This allows TedFormer to effectively\nhandle irregularly sampled data. Extensive experiments on three real-world\ndatasets demonstrate the superior performance of TedTrajRec. The code is\npublicly available at https://github.com/ysygMhdxw/TEDTrajRec/.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 03:09:17", "updated": "2025-05-20 03:09:17", "pdf_url": "http://arxiv.org/pdf/2505.13857v1", "comment": "Accepted as a journal paper in IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13860v1", "title": "Domain Adaptation of VLM for Soccer Video Understanding", "authors": ["Tiancheng Jiang", "Henry Wang", "Md Sirajus Salekin", "Parmida Atighehchian", "Shinan Zhang"], "abstract": "Vision Language Models (VLMs) have demonstrated strong performance in\nmulti-modal tasks by effectively aligning visual and textual representations.\nHowever, most video understanding VLM research has been domain-agnostic,\nleaving the understanding of their transfer learning capability to specialized\ndomains under-explored. In this work, we address this by exploring the\nadaptability of open-source VLMs to specific domains, and focusing on soccer as\nan initial case study. Our approach uses large-scale soccer datasets and LLM to\ncreate instruction-following data, and use them to iteratively fine-tune the\ngeneral-domain VLM in a curriculum learning fashion (first teaching the model\nkey soccer concepts to then question answering tasks). The final adapted model,\ntrained using a curated dataset of 20k video clips, exhibits significant\nimprovement in soccer-specific tasks compared to the base model, with a 37.5%\nrelative improvement for the visual question-answering task and an accuracy\nimprovement from 11.8% to 63.5% for the downstream soccer action classification\ntask.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 03:12:21", "updated": "2025-05-20 03:12:21", "pdf_url": "http://arxiv.org/pdf/2505.13860v1", "comment": "8 pages, 5 figures, accepted to the 11th IEEE International Workshop\n  on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix\n  included as ancillary PDF", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13872v1", "title": "Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving", "authors": ["Jingzheng Li", "Tiancheng Wang", "Xingyu Peng", "Jiacheng Chen", "Zhijun Chen", "Bing Li", "Xianglong Liu"], "abstract": "Autonomous Driving (AD) systems demand the high levels of safety assurance.\nDespite significant advancements in AD demonstrated on open-source benchmarks\nlike Longest6 and Bench2Drive, existing datasets still lack\nregulatory-compliant scenario libraries for closed-loop testing to\ncomprehensively evaluate the functional safety of AD. Meanwhile, real-world AD\naccidents are underrepresented in current driving datasets. This scarcity leads\nto inadequate evaluation of AD performance, posing risks to safety validation\nand practical deployment. To address these challenges, we propose Safety2Drive,\na safety-critical scenario library designed to evaluate AD systems.\nSafety2Drive offers three key contributions. (1) Safety2Drive comprehensively\ncovers the test items required by standard regulations and contains 70 AD\nfunction test items. (2) Safety2Drive supports the safety-critical scenario\ngeneralization. It has the ability to inject safety threats such as natural\nenvironment corruptions and adversarial attacks cross camera and LiDAR sensors.\n(3) Safety2Drive supports multi-dimensional evaluation. In addition to the\nevaluation of AD systems, it also supports the evaluation of various perception\ntasks, such as object detection and lane detection. Safety2Drive provides a\nparadigm from scenario construction to validation, establishing a standardized\ntest framework for the safe deployment of AD.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 03:27:06", "updated": "2025-05-20 03:27:06", "pdf_url": "http://arxiv.org/pdf/2505.13872v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13873v1", "title": "Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model", "authors": ["Peisong Niu", "Ziqing Ma", "Tian Zhou", "Weiqi Chen", "Lefei Shen", "Rong Jin", "Liang Sun"], "abstract": "Weather forecasting has long posed a significant challenge for humanity.\nWhile recent AI-based models have surpassed traditional numerical weather\nprediction (NWP) methods in global forecasting tasks, overfitting remains a\ncritical issue due to the limited availability of real-world weather data\nspanning only a few decades. Unlike fields like computer vision or natural\nlanguage processing, where data abundance can mitigate overfitting, weather\nforecasting demands innovative strategies to address this challenge with\nexisting data. In this paper, we explore pre-training methods for weather\nforecasting, finding that selecting an appropriately challenging pre-training\ntask introduces locality bias, effectively mitigating overfitting and enhancing\nperformance. We introduce Baguan, a novel data-driven model for medium-range\nweather forecasting, built on a Siamese Autoencoder pre-trained in a\nself-supervised manner and fine-tuned for different lead times. Experimental\nresults show that Baguan outperforms traditional methods, delivering more\naccurate forecasts. Additionally, the pre-trained Baguan demonstrates robust\noverfitting control and excels in downstream tasks, such as\nsubseasonal-to-seasonal (S2S) modeling and regional forecasting, after\nfine-tuning.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 03:29:23", "updated": "2025-05-20 03:29:23", "pdf_url": "http://arxiv.org/pdf/2505.13873v1", "comment": "KDD2025 research track accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13887v1", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "abstract": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 03:48:19", "updated": "2025-05-20 03:48:19", "pdf_url": "http://arxiv.org/pdf/2505.13887v1", "comment": "17 pages, 7 figures, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:2502.17110", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13898v1", "title": "Do Language Models Use Their Depth Efficiently?", "authors": ["R\u00f3bert Csord\u00e1s", "Christopher D. Manning", "Christopher Potts"], "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,\ncomparing the output of the sublayers to the residual stream reveals that\nlayers in the second half contribute much less than those in the first half,\nwith a clear phase transition between the two halves. Second, skipping layers\nin the second half has a much smaller effect on future computations and output\npredictions. Third, for multihop tasks, we are unable to find evidence that\nmodels are using increased depth to compose subresults in examples involving\nmany hops. Fourth, we seek to directly address whether deeper models are using\ntheir additional layers to perform new kinds of computation. To do this, we\ntrain linear maps from the residual stream of a shallow model to a deeper one.\nWe find that layers with the same relative depth map best to each other,\nsuggesting that the larger model simply spreads the same computations out over\nits many layers. All this evidence suggests that deeper models are not using\ntheir depth to learn new kinds of computation, but only using the greater depth\nto perform more fine-grained adjustments to the residual. This may help explain\nwhy increasing scale leads to diminishing returns for stacked Transformer\narchitectures.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "published": "2025-05-20 04:00:56", "updated": "2025-05-20 04:00:56", "pdf_url": "http://arxiv.org/pdf/2505.13898v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13904v1", "title": "Learning to Insert for Constructive Neural Vehicle Routing Solver", "authors": ["Fu Luo", "Xi Lin", "Mengyuan Zhong", "Fei Liu", "Zhenkun Wang", "Jianyong Sun", "Qingfu Zhang"], "abstract": "Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.", "categories": ["cs.LG", "cs.AI", "cs.RO", "math.OC"], "published": "2025-05-20 04:10:50", "updated": "2025-05-20 04:10:50", "pdf_url": "http://arxiv.org/pdf/2505.13904v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13906v1", "title": "XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data", "authors": ["Soyabul Islam Lincoln", "Mirza Mohd Shahriar Maswood"], "abstract": "A common neurodegenerative disease, Alzheimer's disease requires a precise\ndiagnosis and efficient treatment, particularly in light of escalating\nhealthcare expenses and the expanding use of artificial intelligence in medical\ndiagnostics. Many recent studies shows that the combination of brain Magnetic\nResonance Imaging (MRI) and deep neural networks have achieved promising\nresults for diagnosing AD. Using deep convolutional neural networks, this paper\nintroduces a novel deep learning architecture that incorporates multiresidual\nblocks, specialized spatial attention blocks, grouped query attention, and\nmulti-head attention. The study assessed the model's performance on four\npublicly accessible datasets and concentrated on identifying binary and\nmulticlass issues across various categories. This paper also takes into account\nof the explainability of AD's progression and compared with state-of-the-art\nmethods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster\nScore-CAM, and XGRADCAM. Our methodology consistently outperforms current\napproaches, achieving 99.66\\% accuracy in 4-class classification, 99.63\\% in\n3-class classification, and 100\\% in binary classification using Kaggle\ndatasets. For Open Access Series of Imaging Studies (OASIS) datasets the\naccuracies are 99.92\\%, 99.90\\%, and 99.95\\% respectively. The Alzheimer's\nDisease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in\nthree planes (axial, sagittal, and coronal) and a combination of all planes.\nThe study achieved accuracies of 99.08\\% for axis, 99.85\\% for sagittal, 99.5\\%\nfor coronal, and 99.17\\% for all axis, and 97.79\\% and 8.60\\% respectively for\nADNI-2. The network's ability to retrieve important information from MRI images\nis demonstrated by its excellent accuracy in categorizing AD stages.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-20 04:17:28", "updated": "2025-05-20 04:17:28", "pdf_url": "http://arxiv.org/pdf/2505.13906v1", "comment": "20 pages, 12 figures,", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13909v1", "title": "Efficient Agent Training for Computer Use", "authors": ["Yanheng He", "Jiahe Jin", "Pengfei Liu"], "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 04:20:18", "updated": "2025-05-20 04:20:18", "pdf_url": "http://arxiv.org/pdf/2505.13909v1", "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13911v1", "title": "Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation", "authors": ["Ruijie Zhao", "Zuopeng Tan", "Xiao Xue", "Longfei Zhao", "Bing Li", "Zicheng Liao", "Ying Ming", "Jiaru Wang", "Ran Xiao", "Sirong Piao", "Rui Zhao", "Qiqi Xu", "Wei Song"], "abstract": "Pulmonary segment segmentation is crucial for cancer localization and\nsurgical planning. However, the pixel-wise annotation of pulmonary segments is\nlaborious, as the boundaries between segments are indistinguishable in medical\nimages. To this end, we propose a weakly supervised learning (WSL) method,\ntermed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise\nclinical anatomical definition of pulmonary segments to perform pulmonary\nsegment segmentation. Since pulmonary segments reside within the lobes and are\ndetermined by the bronchovascular tree, i.e., artery, airway and vein, the\ndesign of the loss function is founded on two principles. First, segment-level\nlabels are utilized to directly supervise the output of the pulmonary segments,\nensuring that they accurately encompass the appropriate bronchovascular tree.\nSecond, lobe-level supervision indirectly oversees the pulmonary segment,\nensuring their inclusion within the corresponding lobe. Besides, we introduce a\ntwo-stage segmentation strategy that incorporates bronchovascular priori\ninformation. Furthermore, a consistency loss is proposed to enhance the\nsmoothness of segment boundaries, along with an evaluation metric designed to\nmeasure the smoothness of pulmonary segment boundaries. Visual inspection and\nevaluation metrics from experiments conducted on a private dataset demonstrate\nthe effectiveness of our method.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-20 04:23:12", "updated": "2025-05-20 04:23:12", "pdf_url": "http://arxiv.org/pdf/2505.13911v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13914v1", "title": "Parallel Belief Revision via Order Aggregation", "authors": ["Jake Chandler", "Richard Booth"], "abstract": "Despite efforts to better understand the constraints that operate on\nsingle-step parallel (aka \"package\", \"multiple\") revision, very little work has\nbeen carried out on how to extend the model to the iterated case. A recent\npaper by Delgrande & Jin outlines a range of relevant rationality postulates.\nWhile many of these are plausible, they lack an underlying unifying\nexplanation. We draw on recent work on iterated parallel contraction to offer a\ngeneral method for extending serial iterated belief revision operators to\nhandle parallel change. This method, based on a family of order aggregators\nknown as TeamQueue aggregators, provides a principled way to recover the\nindependently plausible properties that can be found in the literature, without\nyielding the more dubious ones.", "categories": ["cs.AI", "I.2.4"], "published": "2025-05-20 04:26:01", "updated": "2025-05-20 04:26:01", "pdf_url": "http://arxiv.org/pdf/2505.13914v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13921v1", "title": "APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight", "authors": ["Wanjing Huang", "Weixiang Yan", "Zhen Zhang", "Ambuj Singh"], "abstract": "Large Language Models (LLMs) demonstrate strong reasoning and task planning\ncapabilities but remain fundamentally limited in physical interaction modeling.\nExisting approaches integrate perception via Vision-Language Models (VLMs) or\nadaptive decision-making through Reinforcement Learning (RL), but they fail to\ncapture dynamic object interactions or require task-specific training, limiting\ntheir real-world applicability. We introduce APEX (Anticipatory\nPhysics-Enhanced Execution), a framework that equips LLMs with physics-driven\nforesight for real-time task planning. APEX constructs structured graphs to\nidentify and model the most relevant dynamic interactions in the environment,\nproviding LLMs with explicit physical state updates. Simultaneously, APEX\nprovides low-latency forward simulations of physically feasible actions,\nallowing LLMs to select optimal strategies based on predictive outcomes rather\nthan static observations. We evaluate APEX on three benchmarks designed to\nassess perception, prediction, and decision-making: (1) Physics Reasoning\nBenchmark, testing causal inference and object motion prediction; (2) Tetris,\nevaluating whether physics-informed prediction enhances decision-making\nperformance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,\nassessing the immediate integration of perception and action feasibility\nanalysis. APEX significantly outperforms standard LLMs and VLM-based models,\ndemonstrating the necessity of explicit physics reasoning for bridging the gap\nbetween language-based intelligence and real-world task execution. The source\ncode and experiment setup are publicly available at\nhttps://github.com/hwj20/APEX_EXP .", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 04:34:58", "updated": "2025-05-20 04:34:58", "pdf_url": "http://arxiv.org/pdf/2505.13921v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13934v1", "title": "RLVR-World: Training World Models with Reinforcement Learning", "authors": ["Jialong Wu", "Shaofeng Yin", "Ningya Feng", "Mingsheng Long"], "abstract": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 05:02:53", "updated": "2025-05-20 05:02:53", "pdf_url": "http://arxiv.org/pdf/2505.13934v1", "comment": "Code is available at project website:\n  https://thuml.github.io/RLVR-World/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13936v1", "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "abstract": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:04:15", "updated": "2025-05-20 05:04:15", "pdf_url": "http://arxiv.org/pdf/2505.13936v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13938v1", "title": "CLEVER: A Curated Benchmark for Formally Verified Code Generation", "authors": ["Amitayush Thakur", "Jasper Lee", "George Tsoukalas", "Meghana Sistla", "Matthew Zhao", "Stefan Zetzche", "Greg Durrett", "Yisong Yue", "Swarat Chaudhuri"], "abstract": "We introduce ${\\rm C{\\small LEVER}}$, a high-quality, curated benchmark of\n161 problems for end-to-end verified code generation in Lean. Each problem\nconsists of (1) the task of generating a specification that matches a held-out\nground-truth specification, and (2) the task of generating a Lean\nimplementation that provably satisfies this specification. Unlike prior\nbenchmarks, ${\\rm C{\\small LEVER}}$ avoids test-case supervision, LLM-generated\nannotations, and specifications that leak implementation logic or allow vacuous\nsolutions. All outputs are verified post-hoc using Lean's type checker to\nensure machine-checkable correctness. We use ${\\rm C{\\small LEVER}}$ to\nevaluate several few-shot and agentic approaches based on state-of-the-art\nlanguage models. These methods all struggle to achieve full verification,\nestablishing it as a challenging frontier benchmark for program synthesis and\nformal reasoning. Our benchmark can be found on\nGitHub(https://github.com/trishullab/clever) as well as\nHuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our\nevaluation code is also available\nonline(https://github.com/trishullab/clever-prover).", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.PL", "cs.SE"], "published": "2025-05-20 05:15:47", "updated": "2025-05-20 05:15:47", "pdf_url": "http://arxiv.org/pdf/2505.13938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13940v1", "title": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery", "authors": ["Kun Li", "Zhennan Wu", "Shoupeng Wang", "Wenbin Hu"], "abstract": "In the field of AI4Science, large-scale language models (LLMs) show great\npotential to parse complex scientific semantics, integrate cross-disciplinary\nknowledge, and assist critical task research. However, in the field of drug\ndiscovery, despite the optimization through professional data pre-training,\ncontext window expansion, and internet search, the existing LLMs are still\nfacing challenges such as massive multi-modal and heterogeneous data\nprocessing, domain knowledge dynamic updating delay, and insufficient\nconfidence in predicting the results of complex computational tasks. To address\nthese challenges, we propose the DrugPilot, an LLM-based agent with\nparameterized reasoning for drug discovery. DrugPilot addresses key limitations\nof traditional end-to-end LLM prediction approaches through its parametric\ninference architecture. This agent system supports major phases of the drug\ndiscovery pipeline, facilitating automated planning and execution of\nmulti-stage research tasks. To address the critical challenge of multi-modal\ndrug data analysis (incorporating both public datasets and user-submitted\ndata), we developed an interactive parameterized memory pool. This innovative\ncomponent standardizes real-world drug data into parametric representations,\nsimultaneously enabling efficient knowledge retrieval in multi-turn dialogue\nwhile mitigating the information loss inherent in text-based data transmission.\nAdditionally, we created a drug instruct dataset across 8 essential drug\ndiscovery tasks for model fine-tuning and evaluation. Based on the Berkeley\nfunction calling evaluation framework, DrugPilot demonstrated the most advanced\ntool calling capabilities on our drug discovery tool instruction dataset,\noutperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves\ntask completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and\nmulti-turn tasks, respectively.", "categories": ["cs.AI", "q-bio.BM"], "published": "2025-05-20 05:18:15", "updated": "2025-05-20 05:18:15", "pdf_url": "http://arxiv.org/pdf/2505.13940v1", "comment": "22 pages, 10 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13941v1", "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "authors": ["Haoyang Fang", "Boran Han", "Nick Erickson", "Xiyuan Zhang", "Su Zhou", "Anirudh Dagar", "Jiani Zhang", "Ali Caner Turkmen", "Cuixiong Hu", "Huzefa Rangwala", "Ying Nian Wu", "Bernie Wang", "George Karypis"], "abstract": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions.", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 05:20:53", "updated": "2025-05-20 05:20:53", "pdf_url": "http://arxiv.org/pdf/2505.13941v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13946v1", "title": "Visual Instruction Bottleneck Tuning", "authors": ["Changdae Oh", "Jiatong Li", "Shawn Im", "Yixuan Li"], "abstract": "Despite widespread adoption, multimodal large language models (MLLMs) suffer\nperformance degradation when encountering unfamiliar queries under distribution\nshifts. Existing methods to improve MLLM generalization typically require\neither more instruction data or larger advanced model architectures, both of\nwhich incur non-trivial human labor or computational costs. In this work, we\ntake an alternative approach to enhance the robustness of MLLMs under\ndistribution shifts, from a representation learning perspective. Inspired by\nthe information bottleneck (IB) principle, we derive a variational lower bound\nof the IB for MLLMs and devise a practical implementation, Visual Instruction\nBottleneck Tuning (Vittle). We then provide a theoretical justification of\nVittle by revealing its connection to an information-theoretic robustness\nmetric of MLLM. Empirical validation of three MLLMs on open-ended and\nclosed-form question answering and object hallucination detection tasks over 45\ndatasets, including 30 shift scenarios, demonstrates that Vittle consistently\nimproves the MLLM's robustness under shifts by pursuing the learning of a\nminimal sufficient representation.", "categories": ["cs.AI"], "published": "2025-05-20 05:24:53", "updated": "2025-05-20 05:24:53", "pdf_url": "http://arxiv.org/pdf/2505.13946v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13948v1", "title": "Memory-Centric Embodied Question Answer", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "abstract": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.", "categories": ["cs.CL", "cs.AI", "cs.MM"], "published": "2025-05-20 05:27:57", "updated": "2025-05-20 05:27:57", "pdf_url": "http://arxiv.org/pdf/2505.13948v1", "comment": "14pages, 7 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13949v1", "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "abstract": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:28:21", "updated": "2025-05-20 05:28:21", "pdf_url": "http://arxiv.org/pdf/2505.13949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13965v1", "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 06:05:56", "updated": "2025-05-20 06:05:56", "pdf_url": "http://arxiv.org/pdf/2505.13965v1", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13969v1", "title": "Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World", "authors": ["Junya Nakanishi", "Jun Baba", "Yuichiro Yoshikawa", "Hiroko Kamide", "Hiroshi Ishiguro"], "abstract": "This paper discusses the functional advantages of the Selection-Broadcast\nCycle structure proposed by Global Workspace Theory (GWT), inspired by human\nconsciousness, particularly focusing on its applicability to artificial\nintelligence and robotics in dynamic, real-time scenarios. While previous\nstudies often examined the Selection and Broadcast processes independently,\nthis research emphasizes their combined cyclic structure and the resulting\nbenefits for real-time cognitive systems. Specifically, the paper identifies\nthree primary benefits: Dynamic Thinking Adaptation, Experience-Based\nAdaptation, and Immediate Real-Time Adaptation. This work highlights GWT's\npotential as a cognitive architecture suitable for sophisticated\ndecision-making and adaptive performance in unsupervised, dynamic environments.\nIt suggests new directions for the development and implementation of robust,\ngeneral-purpose AI and robotics systems capable of managing complex, real-world\ntasks.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 06:07:21", "updated": "2025-05-20 06:07:21", "pdf_url": "http://arxiv.org/pdf/2505.13969v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13971v1", "title": "The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition", "authors": ["Ming Gao", "Shilong Wu", "Hang Chen", "Jun Du", "Chin-Hui Lee", "Shinji Watanabe", "Jingdong Chen", "Siniscalchi Sabato Marco", "Odette Scharenborg"], "abstract": "Meetings are a valuable yet challenging scenario for speech applications due\nto complex acoustic conditions. This paper summarizes the outcomes of the MISP\n2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,\nmulti-device meeting transcription by incorporating video modality alongside\naudio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual\nSpeech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).\nWe present the challenge's objectives, tasks, dataset, baseline systems, and\nsolutions proposed by participants. The best-performing systems achieved\nsignificant improvements over the baseline: the top AVSD model achieved a\nDiarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system\nachieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the\nbest AVDR system achieved a concatenated minimum-permutation Character Error\nRate (cpCER) of 11.56%, improving by 72.49%.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 06:11:51", "updated": "2025-05-20 06:11:51", "pdf_url": "http://arxiv.org/pdf/2505.13971v1", "comment": "Accepted by Interspeech 2025. Camera-ready version", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13973v1", "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "abstract": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 06:12:20", "updated": "2025-05-20 06:12:20", "pdf_url": "http://arxiv.org/pdf/2505.13973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13986v1", "title": "Solving Normalized Cut Problem with Constrained Action Space", "authors": ["Qize Jiang", "Linsey Pang", "Alice Gatti", "Mahima Aggarwa", "Giovanna Vantin", "Xiaosong Ma", "Weiwei Sun", "Sanjay Chawla"], "abstract": "Reinforcement Learning (RL) has emerged as an important paradigm to solve\ncombinatorial optimization problems primarily due to its ability to learn\nheuristics that can generalize across problem instances. However, integrating\nexternal knowledge that will steer combinatorial optimization problem solutions\ntowards domain appropriate outcomes remains an extremely challenging task. In\nthis paper, we propose the first RL solution that uses constrained action\nspaces to guide the normalized cut problem towards pre-defined template\ninstances. Using transportation networks as an example domain, we create a\nWedge and Ring Transformer that results in graph partitions that are shaped in\nform of Wedges and Rings and which are likely to be closer to natural optimal\npartitions. However, our approach is general as it is based on principles that\ncan be generalized to other domains.", "categories": ["math.OC", "cs.AI", "cs.LG", "I.2.8"], "published": "2025-05-20 06:33:39", "updated": "2025-05-20 06:33:39", "pdf_url": "http://arxiv.org/pdf/2505.13986v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13989v1", "title": "When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty", "authors": ["Yanzhe Wen", "Xunkai Li", "Qi Zhang", "Zhu Lei", "Guang Zeng", "Rong-Hua Li", "Guoren Wang"], "abstract": "Recently, large language models (LLMs) have significantly advanced\ntext-attributed graph (TAG) learning. However, existing methods inadequately\nhandle data uncertainty in open-world scenarios, especially concerning limited\nlabeling and unknown-class nodes. Prior solutions typically rely on isolated\nsemantic or structural approaches for unknown-class rejection, lacking\neffective annotation pipelines. To address these limitations, we propose\nOpen-world Graph Assistant (OGA), an LLM-based framework that combines adaptive\nlabel traceability, which integrates semantics and topology for unknown-class\nrejection, and a graph label annotator to enable model updates using newly\nannotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and\npracticality.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 06:37:18", "updated": "2025-05-20 06:37:18", "pdf_url": "http://arxiv.org/pdf/2505.13989v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13994v1", "title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning", "authors": ["Ruiyi Yang", "Hao Xue", "Imran Razzak", "Hakim Hacid", "Flora D. Salim"], "abstract": "Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches.", "categories": ["cs.AI", "cs.IR", "cs.MA"], "published": "2025-05-20 06:44:34", "updated": "2025-05-20 06:44:34", "pdf_url": "http://arxiv.org/pdf/2505.13994v1", "comment": "20 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13995v1", "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "abstract": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-20 06:45:17", "updated": "2025-05-20 06:45:17", "pdf_url": "http://arxiv.org/pdf/2505.13995v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14001v1", "title": "VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change", "authors": ["Sterre Lutz", "Matthijs T. J. Spaan", "Anna Lukina"], "abstract": "Autonomous systems operating in the real world encounter a range of\nuncertainties. Probabilistic neural Lyapunov certification is a powerful\napproach to proving safety of nonlinear stochastic dynamical systems. When\nfaced with changes beyond the modeled uncertainties, e.g., unidentified\nobstacles, probabilistic certificates must be transferred to the new system\ndynamics. However, even when the changes are localized in a known part of the\nstate space, state-of-the-art requires complete re-certification, which is\nparticularly costly for neural certificates. We introduce VeRecycle, the first\nframework to formally reclaim guarantees for discrete-time stochastic dynamical\nsystems. VeRecycle efficiently reuses probabilistic certificates when the\nsystem dynamics deviate only in a given subset of states. We present a general\ntheoretical justification and algorithmic implementation. Our experimental\nevaluation shows scenarios where VeRecycle both saves significant computational\neffort and achieves competitive probabilistic guarantees in compositional\nneural control.", "categories": ["cs.AI", "cs.SY", "eess.SY"], "published": "2025-05-20 06:54:19", "updated": "2025-05-20 06:54:19", "pdf_url": "http://arxiv.org/pdf/2505.14001v1", "comment": "accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14005v1", "title": "Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks", "authors": ["Han Zhang", "Yan Wang", "Guanfeng Liu", "Pengfei Ding", "Huaxiong Wang", "Kwok-Yan Lam"], "abstract": "To enhance the reliability and credibility of graph neural networks (GNNs)\nand improve the transparency of their decision logic, a new field of\nexplainability of GNNs (XGNN) has emerged. However, two major limitations\nseverely degrade the performance and hinder the generalizability of existing\nXGNN methods: they (a) fail to capture the complete decision logic of GNNs\nacross diverse distributions in the entire dataset's sample space, and (b)\nimpose strict prerequisites on edge properties and GNN internal accessibility.\nTo address these limitations, we propose OPEN, a novel c\\textbf{O}mprehensive\nand \\textbf{P}rerequisite-free \\textbf{E}xplainer for G\\textbf{N}Ns. OPEN, as\nthe first work in the literature, can infer and partition the entire dataset's\nsample space into multiple environments, each containing graphs that follow a\ndistinct distribution. OPEN further learns the decision logic of GNNs across\ndifferent distributions by sampling subgraphs from each environment and\nanalyzing their predictions, thus eliminating the need for strict\nprerequisites. Experimental results demonstrate that OPEN captures nearly\ncomplete decision logic of GNNs, outperforms state-of-the-art methods in\nfidelity while maintaining similar efficiency, and enhances robustness in\nreal-world scenarios.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 07:01:47", "updated": "2025-05-20 07:01:47", "pdf_url": "http://arxiv.org/pdf/2505.14005v1", "comment": "Accepted by IJCAI 2025 AI4Tech Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14020v1", "title": "Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning", "authors": ["Hao Dong", "Ziyue Qiao", "Zhiyuan Ning", "Qi Hao", "Yi Du", "Pengyang Wang", "Yuanchun Zhou"], "abstract": "Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs\n(KGs), incorporate the temporal feature to express the transience of knowledge\nby describing when facts occur. TKG extrapolation aims to infer possible future\nfacts based on known history, which has garnered significant attention in\nrecent years. Some existing methods treat TKG as a sequence of independent\nsubgraphs to model temporal evolution patterns, demonstrating impressive\nreasoning performance. However, they still have limitations: 1) In modeling\nsubgraph semantic evolution, they usually neglect the internal structural\ninteractions between subgraphs, which are actually crucial for encoding TKGs.\n2) They overlook the potential smooth features that do not lead to semantic\nchanges, which should be distinguished from the semantic evolution process.\nTherefore, we propose a novel Disentangled Multi-span Evolutionary Network\n(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution\nstrategy that captures local neighbor features while perceiving historical\nneighbor semantic information, thus enabling internal interactions between\nsubgraphs during the evolution process. To maximize the capture of semantic\nchange patterns, we design a disentangle component that adaptively separates\nnodes' active and stable features, used to dynamically control the influence of\nhistorical semantics on future evolution. Extensive experiments conducted on\nfour real-world TKG datasets show that DiMNet demonstrates substantial\nperformance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%\nin MRR.", "categories": ["cs.AI", "cs.IR", "cs.LG"], "published": "2025-05-20 07:22:03", "updated": "2025-05-20 07:22:03", "pdf_url": "http://arxiv.org/pdf/2505.14020v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14024v1", "title": "FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix", "authors": ["Di Wu", "Qian Li", "Heng Yang", "Yong Han"], "abstract": "Federated Learning (FL) enables geographically distributed clients to\ncollaboratively train machine learning models by sharing only their local\nmodels, ensuring data privacy. However, FL is vulnerable to untargeted attacks\nthat aim to degrade the global model's performance on the underlying data\ndistribution. Existing defense mechanisms attempt to improve FL's resilience\nagainst such attacks, but their effectiveness is limited in practical FL\nenvironments due to data heterogeneity. On the contrary, we aim to detect and\nremove the attacks to mitigate their impact. Generalization contribution plays\na crucial role in distinguishing untargeted attacks. Our observations indicate\nthat, with limited data, the divergence between embeddings representing\ndifferent classes provides a better measure of generalization than direct\naccuracy. In light of this, we propose a novel robust aggregation method,\nFedGraM, designed to defend against untargeted attacks in FL. The server\nmaintains an auxiliary dataset containing one sample per class to support\naggregation. This dataset is fed to the local models to extract embeddings.\nThen, the server calculates the norm of the Gram Matrix of the embeddings for\neach local model. The norm serves as an indicator of each model's inter-class\nseparation capability in the embedding space. FedGraM identifies and removes\npotentially malicious models by filtering out those with the largest norms,\nthen averages the remaining local models to form the global model. We conduct\nextensive experiments to evaluate the performance of FedGraM. Our empirical\nresults show that with limited data samples used to construct the auxiliary\ndataset, FedGraM achieves exceptional performance, outperforming\nstate-of-the-art defense methods.", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "published": "2025-05-20 07:26:54", "updated": "2025-05-20 07:26:54", "pdf_url": "http://arxiv.org/pdf/2505.14024v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14027v1", "title": "CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data", "authors": ["Yifan Zeng"], "abstract": "As computer networks proliferate, the gravity of network intrusions has\nescalated, emphasizing the criticality of network intrusion detection systems\nfor safeguarding security. While deep learning models have exhibited promising\nresults in intrusion detection, they face challenges in managing\nhigh-dimensional, complex traffic patterns and imbalanced data categories. This\npaper presents CSAGC-IDS, a network intrusion detection model based on deep\nlearning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced\nconvolutional conditional generative adversarial network that generates\nhigh-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS\nintegrates CSCA-CNN, a convolutional neural network enhanced through cost\nsensitive learning and channel attention mechanism, to extract features from\ncomplex traffic data for precise detection. Experiments conducted on the\nNSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of\n84.52% in five-class classification task, and an accuracy of 91.09% and an F1\nscore of 92.04% in binary classification task.Furthermore, this paper provides\nan interpretability analysis of the proposed model, using SHAP and LIME to\nexplain the decision-making mechanisms of the model.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 07:27:51", "updated": "2025-05-20 07:27:51", "pdf_url": "http://arxiv.org/pdf/2505.14027v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14029v1", "title": "AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards", "authors": ["Laura-Sophia von Hirschhausen", "Jannes S. Magnusson", "Mykyta Kovalenko", "Fredrik Boye", "Tanay Rawat", "Peter Eisert", "Anna Hilsmann", "Sebastian Pretzsch", "Sebastian Bosse"], "abstract": "Deep learning has transformed computer vision for precision agriculture, yet\napple orchard monitoring remains limited by dataset constraints. The lack of\ndiverse, realistic datasets and the difficulty of annotating dense,\nheterogeneous scenes. Existing datasets overlook different growth stages and\nstereo imagery, both essential for realistic 3D modeling of orchards and tasks\nlike fruit localization, yield estimation, and structural analysis. To address\nthese gaps, we present AppleGrowthVision, a large-scale dataset comprising two\nsubsets. The first includes 9,317 high resolution stereo images collected from\na farm in Brandenburg (Germany), covering six agriculturally validated growth\nstages over a full growth cycle. The second subset consists of 1,125 densely\nannotated images from the same farm in Brandenburg and one in Pillnitz\n(Germany), containing a total of 31,084 apple labels. AppleGrowthVision\nprovides stereo-image data with agriculturally validated growth stages,\nenabling precise phenological analysis and 3D reconstructions. Extending\nMinneApple with our data improves YOLOv8 performance by 7.69 % in terms of\nF1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by\n31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy\nusing VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges\nthe gap between agricultural science and computer vision, by enabling the\ndevelopment of robust models for fruit detection, growth modeling, and 3D\nanalysis in precision agriculture. Future work includes improving annotation,\nenhancing 3D reconstruction, and extending multimodal analysis across all\ngrowth stages.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 07:29:22", "updated": "2025-05-20 07:29:22", "pdf_url": "http://arxiv.org/pdf/2505.14029v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14036v1", "title": "Adaptive Cyclic Diffusion for Inference Scaling", "authors": ["Gyubin Lee", "Truong Nhat Nguyen Bao", "Jaesik Yoon", "Dongwoo Lee", "Minsu Kim", "Yoshua Bengio", "Sungjin Ahn"], "abstract": "Diffusion models have demonstrated strong generative capabilities across\ndomains ranging from image synthesis to complex reasoning tasks. However, most\ninference-time scaling methods rely on fixed denoising schedules, limiting\ntheir ability to allocate computation based on instance difficulty or\ntask-specific demands adaptively. We introduce the challenge of adaptive\ninference-time scaling-dynamically adjusting computational effort during\ninference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a\nflexible, search-based inference framework. ABCD refines outputs through\nbi-directional diffusion cycles while adaptively controlling exploration depth\nand termination. It comprises three components: Cyclic Diffusion Search,\nAutomatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.\nExperiments show that ABCD improves performance across diverse tasks while\nmaintaining computational efficiency.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 07:31:38", "updated": "2025-05-20 07:31:38", "pdf_url": "http://arxiv.org/pdf/2505.14036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14038v1", "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data", "authors": ["Xinzhe Zheng", "Sijie Ji", "Jiawei Sun", "Renqi Chen", "Wei Gao", "Mani Srivastava"], "abstract": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 07:36:28", "updated": "2025-05-20 07:36:28", "pdf_url": "http://arxiv.org/pdf/2505.14038v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14045v1", "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 07:43:45", "updated": "2025-05-20 07:43:45", "pdf_url": "http://arxiv.org/pdf/2505.14045v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14057v1", "title": "Field Matters: A lightweight LLM-enhanced Method for CTR Prediction", "authors": ["Yu Cui", "Feng Liu", "Jiawei Chen", "Xingyu Lou", "Changwang Zhang", "Jun Wang", "Yuegang Sun", "Xiaohu Yang", "Can Wang"], "abstract": "Click-through rate (CTR) prediction is a fundamental task in modern\nrecommender systems. In recent years, the integration of large language models\n(LLMs) has been shown to effectively enhance the performance of traditional CTR\nmethods. However, existing LLM-enhanced methods often require extensive\nprocessing of detailed textual descriptions for large-scale instances or\nuser/item entities, leading to substantial computational overhead. To address\nthis challenge, this work introduces LLaCTR, a novel and lightweight\nLLM-enhanced CTR method that employs a field-level enhancement paradigm.\nSpecifically, LLaCTR first utilizes LLMs to distill crucial and lightweight\nsemantic knowledge from small-scale feature fields through self-supervised\nfield-feature fine-tuning. Subsequently, it leverages this field-level semantic\nknowledge to enhance both feature representation and feature interactions. In\nour experiments, we integrate LLaCTR with six representative CTR models across\nfour datasets, demonstrating its superior performance in terms of both\neffectiveness and efficiency compared to existing LLM-enhanced methods. Our\ncode is available at https://anonymous.4open.science/r/LLaCTR-EC46.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-20 08:02:41", "updated": "2025-05-20 08:02:41", "pdf_url": "http://arxiv.org/pdf/2505.14057v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14064v1", "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI", "authors": ["Cosmin I. Bercea", "Jun Li", "Philipp Raffler", "Evamaria O. Riedel", "Lena Schmitzer", "Angela Kurz", "Felix Bitzer", "Paula Ro\u00dfm\u00fcller", "Julian Canisius", "Mirjam L. Beyrle", "Che Liu", "Wenjia Bai", "Bernhard Kainz", "Julia A. Schnabel", "Benedikt Wiestler"], "abstract": "In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously $unknown$ categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present $NOVA$, a challenging, real-life $evaluation-only$\nbenchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\n$extreme$ stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies.", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "published": "2025-05-20 08:10:57", "updated": "2025-05-20 08:10:57", "pdf_url": "http://arxiv.org/pdf/2505.14064v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14072v1", "title": "Personalized Student Knowledge Modeling for Future Learning Resource Prediction", "authors": ["Soroush Hashemifar", "Sherry Sahebi"], "abstract": "Despite advances in deep learning for education, student knowledge tracing\nand behavior modeling face persistent challenges: limited personalization,\ninadequate modeling of diverse learning activities (especially non-assessed\nmaterials), and overlooking the interplay between knowledge acquisition and\nbehavioral patterns. Practical limitations, such as fixed-size sequence\nsegmentation, frequently lead to the loss of contextual information vital for\npersonalized learning. Moreover, reliance on student performance on assessed\nmaterials limits the modeling scope, excluding non-assessed interactions like\nlectures. To overcome these shortcomings, we propose Knowledge Modeling and\nMaterial Prediction (KMaP), a stateful multi-task approach designed for\npersonalized and simultaneous modeling of student knowledge and behavior. KMaP\nemploys clustering-based student profiling to create personalized student\nrepresentations, improving predictions of future learning resource preferences.\nExtensive experiments on two real-world datasets confirm significant behavioral\ndifferences across student clusters and validate the efficacy of the KMaP\nmodel.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-20 08:23:50", "updated": "2025-05-20 08:23:50", "pdf_url": "http://arxiv.org/pdf/2505.14072v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14080v1", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "abstract": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "published": "2025-05-20 08:36:47", "updated": "2025-05-20 08:36:47", "pdf_url": "http://arxiv.org/pdf/2505.14080v1", "comment": null, "doi": "10.1145/3715275.3732112", "journal_ref": "FAccT '25: Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency"}
{"arxiv_id": "2505.14103v1", "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models", "authors": ["Guangke Chen", "Fu Song", "Zhe Zhao", "Xiaojun Jia", "Yang Liu", "Yanchen Qiao", "Weizhe Zhang"], "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak.", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-20 09:10:45", "updated": "2025-05-20 09:10:45", "pdf_url": "http://arxiv.org/pdf/2505.14103v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14106v1", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "abstract": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:13:22", "updated": "2025-05-20 09:13:22", "pdf_url": "http://arxiv.org/pdf/2505.14106v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14107v1", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "abstract": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:14:53", "updated": "2025-05-20 09:14:53", "pdf_url": "http://arxiv.org/pdf/2505.14107v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14117v1", "title": "Collaborative Unlabeled Data Optimization", "authors": ["Xinyi Shang", "Peng Sun", "Fengyuan Liu", "Tao Lin"], "abstract": "This paper pioneers a novel data-centric paradigm to maximize the utility of\nunlabeled data, tackling a critical question: How can we enhance the efficiency\nand sustainability of deep learning training by optimizing the data itself? We\nbegin by identifying three key limitations in existing model-centric\napproaches, all rooted in a shared bottleneck: knowledge extracted from data is\nlocked to model parameters, hindering its reusability and scalability. To this\nend, we propose CoOpt, a highly efficient, parallelized framework for\ncollaborative unlabeled data optimization, thereby effectively encoding\nknowledge into the data itself. By distributing unlabeled data and leveraging\npublicly available task-agnostic models, CoOpt facilitates scalable, reusable,\nand sustainable training pipelines. Extensive experiments across diverse\ndatasets and architectures demonstrate its efficacy and efficiency, achieving\n13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,\nwith training speedups of $1.94 \\times $ and $1.2 \\times$.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:21:40", "updated": "2025-05-20 09:21:40", "pdf_url": "http://arxiv.org/pdf/2505.14117v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14125v1", "title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning", "authors": ["Viet Anh Khoa Tran", "Emre Neftci", "Willem. A. M. Wybo"], "abstract": "Biological brains learn continually from a stream of unlabeled data, while\nintegrating specialized information from sparsely labeled examples without\ncompromising their ability to generalize. Meanwhile, machine learning methods\nare susceptible to catastrophic forgetting in this natural learning setting, as\nsupervised specialist fine-tuning degrades performance on the original task. We\nintroduce task-modulated contrastive learning (TMCL), which takes inspiration\nfrom the biophysical machinery in the neocortex, using predictive coding\nprinciples to integrate top-down information continually and without\nsupervision. We follow the idea that these principles build a view-invariant\nrepresentation space, and that this can be implemented using a contrastive\nloss. Then, whenever labeled samples of a new class occur, new affine\nmodulations are learned that improve separation of the new class from all\nothers, without affecting feedforward weights. By co-opting the view-invariance\nlearning mechanism, we then train feedforward weights to match the unmodulated\nrepresentation of a data sample to its modulated counterparts. This introduces\nmodulation invariance into the representation space, and, by also using past\nmodulations, stabilizes it. Our experiments show improvements in both\nclass-incremental and transfer learning over state-of-the-art unsupervised\napproaches, as well as over comparable supervised approaches, using as few as\n1% of available labels. Taken together, our work suggests that top-down\nmodulations play a crucial role in balancing stability and plasticity.", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "68T05 (primary), 68T07, 68T45 (secondary)", "I.2.6; I.2.10"], "published": "2025-05-20 09:31:57", "updated": "2025-05-20 09:31:57", "pdf_url": "http://arxiv.org/pdf/2505.14125v1", "comment": "33 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14128v1", "title": "A Methodological Framework for Measuring Spatial Labeling Similarity", "authors": ["Yihang Du", "Jiaying Hu", "Suyang Hou", "Yueyang Ding", "Xiaobo Sun"], "abstract": "Spatial labeling assigns labels to specific spatial locations to characterize\ntheir spatial properties and relationships, with broad applications in\nscientific research and practice. Measuring the similarity between two spatial\nlabelings is essential for understanding their differences and the contributing\nfactors, such as changes in location properties or labeling methods. An\nadequate and unbiased measurement of spatial labeling similarity should\nconsider the number of matched labels (label agreement), the topology of\nspatial label distribution, and the heterogeneous impacts of mismatched labels.\nHowever, existing methods often fail to account for all these aspects. To\naddress this gap, we propose a methodological framework to guide the\ndevelopment of methods that meet these requirements. Given two spatial\nlabelings, the framework transforms them into graphs based on location\norganization, labels, and attributes (e.g., location significance). The\ndistributions of their graph attributes are then extracted, enabling an\nefficient computation of distributional discrepancy to reflect the\ndissimilarity level between the two labelings. We further provide a concrete\nimplementation of this framework, termed Spatial Labeling Analogy Metric\n(SLAM), along with an analysis of its theoretical foundation, for evaluating\nspatial labeling results in spatial transcriptomics (ST) \\textit{as per} their\nsimilarity with ground truth labeling. Through a series of carefully designed\nexperimental cases involving both simulated and real ST data, we demonstrate\nthat SLAM provides a comprehensive and accurate reflection of labeling quality\ncompared to other well-established evaluation metrics. Our code is available at\nhttps://github.com/YihDu/SLAM.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:34:03", "updated": "2025-05-20 09:34:03", "pdf_url": "http://arxiv.org/pdf/2505.14128v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14136v1", "title": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging", "authors": ["Ryo Bertolissi", "Jonas H\u00fcbotter", "Ido Hakimi", "Andreas Krause"], "abstract": "Mixture of expert (MoE) models are a promising approach to increasing model\ncapacity without increasing inference cost, and are core components of many\nstate-of-the-art language models. However, current MoE models typically use\nonly few experts due to prohibitive training and inference cost. We propose\nTest-Time Model Merging (TTMM) which scales the MoE paradigm to an order of\nmagnitude more experts and uses model merging to avoid almost any test-time\noverhead. We show that TTMM is an approximation of test-time training (TTT),\nwhich fine-tunes an expert model for each prediction task, i.e., prompt. TTT\nhas recently been shown to significantly improve language models, but is\ncomputationally expensive. We find that performance of TTMM improves with more\nexperts and approaches the performance of TTT. Moreover, we find that with a 1B\nparameter base model, TTMM is more than 100x faster than TTT at test-time by\namortizing the cost of TTT at train-time. Thus, TTMM offers a promising\ncost-effective approach to scale test-time training.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:39:54", "updated": "2025-05-20 09:39:54", "pdf_url": "http://arxiv.org/pdf/2505.14136v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14137v1", "title": "Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games", "authors": ["Vojt\u011bch K\u016fr", "V\u00edt Musil", "Vojt\u011bch \u0158eh\u00e1k"], "abstract": "Adversarial Patrolling games form a subclass of Security games where a\nDefender moves between locations, guarding vulnerable targets. The main\nalgorithmic problem is constructing a strategy for the Defender that minimizes\nthe worst damage an Attacker can cause. We focus on the class of finite-memory\n(also known as regular) Defender's strategies that experimentally outperformed\nother competing classes. A finite-memory strategy can be seen as a positional\nstrategy on a finite set of states. Each state consists of a pair of a location\nand a certain integer value--called memory. Existing algorithms improve the\ntransitional probabilities between the states but require that the available\nmemory size itself is assigned at each location manually. Choosing the right\nmemory assignment is a well-known open and hard problem that hinders the\nusability of finite-memory strategies. We solve this issue by developing a\ngeneral method that iteratively changes the memory assignment. Our algorithm\ncan be used in connection with \\emph{any} black-box strategy optimization tool.\nWe evaluate our method on various experiments and show its robustness by\nsolving instances of various patrolling models.", "categories": ["cs.AI"], "published": "2025-05-20 09:40:53", "updated": "2025-05-20 09:40:53", "pdf_url": "http://arxiv.org/pdf/2505.14137v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14139v1", "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning", "authors": ["Marvin Alles", "Nutan Chen", "Patrick van der Smagt", "Botond Cseke"], "abstract": "The use of guidance to steer sampling toward desired outcomes has been widely\nexplored within diffusion models, especially in applications such as image and\ntrajectory generation. However, incorporating guidance during training remains\nrelatively underexplored. In this work, we introduce energy-guided flow\nmatching, a novel approach that enhances the training of flow models and\neliminates the need for guidance at inference time. We learn a conditional\nvelocity field corresponding to the flow policy by approximating an\nenergy-guided probability path as a Gaussian path. Learning guided trajectories\nis appealing for tasks where the target distribution is defined by a\ncombination of data and an energy function, as in reinforcement learning.\nDiffusion-based policies have recently attracted attention for their expressive\npower and ability to capture multi-modal action distributions. Typically, these\npolicies are optimized using weighted objectives or by back-propagating\ngradients through actions sampled by the policy. As an alternative, we propose\nFlowQ, an offline reinforcement learning algorithm based on energy-guided flow\nmatching. Our method achieves competitive performance while the policy training\ntime is constant in the number of flow sampling steps.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-20 09:43:05", "updated": "2025-05-20 09:43:05", "pdf_url": "http://arxiv.org/pdf/2505.14139v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14140v1", "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning", "authors": ["Qianyue Hao", "Sibo Li", "Jian Yuan", "Yong Li"], "abstract": "Despite rapid advancements in large language models (LLMs), the token-level\nautoregressive nature constrains their complex reasoning capabilities. To\nenhance LLM reasoning, inference-time techniques, including\nChain/Tree/Graph-of-Thought(s), successfully improve the performance, as they\nare fairly cost-effective by guiding reasoning through sophisticated logical\nstructures without modifying LLMs' parameters. However, these manually\npredefined, task-agnostic frameworks are applied uniformly across diverse\ntasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),\nwhere we train a lightweight navigator model with reinforcement learning (RL)\nto adaptively enhance LLM reasoning at inference time. Specifically, we design\nfive basic logic blocks from the perspective of human cognition. During the\nreasoning process, the trained RL navigator dynamically selects the suitable\nlogic blocks and combines them into task-specific logical structures according\nto problem characteristics. Experiments across multiple reasoning benchmarks\n(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)\nillustrate that RLoT outperforms established inference-time techniques by up to\n13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to\nmake sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL\nnavigator demonstrates strong transferability: a model trained on one specific\nLLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is\nopen-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for\nreproducibility.", "categories": ["cs.AI"], "published": "2025-05-20 09:43:33", "updated": "2025-05-20 09:43:33", "pdf_url": "http://arxiv.org/pdf/2505.14140v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14141v1", "title": "Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent", "authors": ["Fanglin Mo", "Junzhe Chen", "Haoxuan Zhu", "Xuming Hu"], "abstract": "Mobile GUI agents execute user commands by directly interacting with the\ngraphical user interface (GUI) of mobile devices, demonstrating significant\npotential to enhance user convenience. However, these agents face considerable\nchallenges in task planning, as they must continuously analyze the GUI and\ngenerate operation instructions step by step. This process often leads to\ndifficulties in making accurate task plans, as GUI agents lack a deep\nunderstanding of how to effectively use the target applications, which can\ncause them to become \"lost\" during task execution. To address the task planning\nissue, we propose SPlanner, a plug-and-play planning module to generate\nexecution plans that guide vision language model(VLMs) in executing tasks. The\nproposed planning module utilizes extended finite state machines (EFSMs) to\nmodel the control logits and configurations of mobile applications. It then\ndecomposes a user instruction into a sequence of primary function modeled in\nEFSMs, and generate the execution path by traversing the EFSMs. We further\nrefine the execution path into a natural language plan using an LLM. The final\nplan is concise and actionable, and effectively guides VLMs to generate\ninteractive GUI actions to accomplish user tasks. SPlanner demonstrates strong\nperformance on dynamic benchmarks reflecting real-world mobile usage. On the\nAndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired\nwith Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point\nimprovement compared to using Qwen2.5-VL-72B without planning assistance.", "categories": ["cs.AI", "I.2.11; H.5.2"], "published": "2025-05-20 09:45:55", "updated": "2025-05-20 09:45:55", "pdf_url": "http://arxiv.org/pdf/2505.14141v1", "comment": "10 pages. Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14143v1", "title": "Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition", "authors": ["Shuo Zhang", "Jinsong Zhang", "Zhejun Zhang", "Lei Li"], "abstract": "Multi-task learning (MTL) enables the efficient transfer of extra knowledge\nacquired from other tasks. The high correlation between multimodal sentiment\nanalysis (MSA) and multimodal emotion recognition (MER) supports their joint\ntraining. However, existing methods primarily employ hard parameter sharing,\nignoring parameter conflicts caused by complex task correlations. In this\npaper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture\nof Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts\nto distinctly model common and unique task characteristics, thereby avoiding\nparameter conflicts. Additionally, inspired by low-rank structures in the\nMixture of Experts (MoE) framework, we design low-rank expert networks to\nreduce parameter and computational overhead as the number of experts increases.\nExtensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that\nMMoLRE achieves state-of-the-art performance on the MSA task and competitive\nresults on the MER task.", "categories": ["cs.AI"], "published": "2025-05-20 09:46:56", "updated": "2025-05-20 09:46:56", "pdf_url": "http://arxiv.org/pdf/2505.14143v1", "comment": "Accepted to ICME 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14146v1", "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "abstract": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 09:53:56", "updated": "2025-05-20 09:53:56", "pdf_url": "http://arxiv.org/pdf/2505.14146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14147v1", "title": "SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning", "authors": ["Xiong Jun Wu", "Zhenduo Zhang", "ZuJie Wen", "Zhiqiang Zhang", "Wang Ren", "Lei Shi", "Cai Chen", "Deng Zhao", "Dingnan Jin", "Qing Cui", "Jun Zhou"], "abstract": "Training large reasoning models (LRMs) with reinforcement learning in STEM\ndomains is hindered by the scarcity of high-quality, diverse, and verifiable\nproblem sets. Existing synthesis methods, such as Chain-of-Thought prompting,\noften generate oversimplified or uncheckable data, limiting model advancement\non complex tasks. To address these challenges, we introduce SHARP, a unified\napproach to Synthesizing High-quality Aligned Reasoning Problems for LRMs\nreinforcement learning with verifiable rewards (RLVR). SHARP encompasses a\nstrategic set of self-alignment principles -- targeting graduate and\nOlympiad-level difficulty, rigorous logical consistency, and unambiguous,\nverifiable answers -- and a structured three-phase framework (Alignment,\nInstantiation, Inference) that ensures thematic diversity and fine-grained\ncontrol over problem generation. We implement SHARP by leveraging a\nstate-of-the-art LRM to infer and verify challenging STEM questions, then\nemploy a reinforcement learning loop to refine the model's reasoning through\nverifiable reward signals. Experiments on benchmarks such as GPQA demonstrate\nthat SHARP-augmented training substantially outperforms existing methods,\nmarkedly improving complex reasoning accuracy and pushing LRM performance\ncloser to expert-level proficiency. Our contributions include the SHARP\nstrategy, framework design, end-to-end implementation, and experimental\nevaluation of its effectiveness in elevating LRM reasoning capabilities.", "categories": ["cs.AI"], "published": "2025-05-20 09:54:42", "updated": "2025-05-20 09:54:42", "pdf_url": "http://arxiv.org/pdf/2505.14147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14148v1", "title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem", "authors": ["Fan Liu", "Zherui Yang", "Cancheng Liu", "Tianrui Song", "Xiaofeng Gao", "Hao Liu"], "abstract": "Mathematical modeling is a cornerstone of scientific discovery and\nengineering practice, enabling the translation of real-world problems into\nformal systems across domains such as physics, biology, and economics. Unlike\nmathematical reasoning, which assumes a predefined formulation, modeling\nrequires open-ended problem analysis, abstraction, and principled\nformalization. While Large Language Models (LLMs) have shown strong reasoning\ncapabilities, they fall short in rigorous model construction, limiting their\nutility in real-world problem-solving. To this end, we formalize the task of\nLLM-powered real-world mathematical modeling, where agents must analyze\nproblems, construct domain-appropriate formulations, and generate complete\nend-to-end solutions. We introduce MM-Bench, a curated benchmark of 111\nproblems from the Mathematical Contest in Modeling (MCM/ICM), spanning the\nyears 2000 to 2025 and across ten diverse domains such as physics, biology, and\neconomics. To tackle this task, we propose MM-Agent, an expert-inspired\nframework that decomposes mathematical modeling into four stages: open-ended\nproblem analysis, structured model formulation, computational problem solving,\nand report generation. Experiments on MM-Bench show that MM-Agent significantly\noutperforms baseline agents, achieving an 11.88\\% improvement over human expert\nsolutions while requiring only 15 minutes and \\$0.88 per task using GPT-4o.\nFurthermore, under official MCM/ICM protocols, MM-Agent assisted two\nundergraduate teams in winning the Finalist Award (\\textbf{top 2.0\\% among\n27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a\nmodeling copilot. Our code is available at\nhttps://github.com/usail-hkust/LLM-MM-Agent", "categories": ["cs.AI"], "published": "2025-05-20 09:55:31", "updated": "2025-05-20 09:55:31", "pdf_url": "http://arxiv.org/pdf/2505.14148v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14156v1", "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "abstract": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "categories": ["cs.CV", "cs.AI", "cs.IR", "I.2; H.3.3"], "published": "2025-05-20 10:05:06", "updated": "2025-05-20 10:05:06", "pdf_url": "http://arxiv.org/pdf/2505.14156v1", "comment": null, "doi": "10.1145/3589334.3645574", "journal_ref": null}
{"arxiv_id": "2505.14157v1", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "abstract": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:05:11", "updated": "2025-05-20 10:05:11", "pdf_url": "http://arxiv.org/pdf/2505.14157v1", "comment": "25 pages, 42 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14163v1", "title": "DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation", "authors": ["He Wang", "Alexander Hanbo Li", "Yiqun Hu", "Sheng Zhang", "Hideo Kobayashi", "Jiani Zhang", "Henry Zhu", "Chung-Wei Hang", "Patrick Ng"], "abstract": "Large language model (LLM) agents have shown promising performance in\ngenerating code for solving complex data science problems. Recent studies\nprimarily focus on enhancing in-context learning through improved search,\nsampling, and planning techniques, while overlooking the importance of the\norder in which problems are tackled during inference. In this work, we develop\na novel inference-time optimization framework, referred to as DSMentor, which\nleverages curriculum learning -- a strategy that introduces simpler task first\nand progressively moves to more complex ones as the learner improves -- to\nenhance LLM agent performance in challenging data science tasks. Our\nmentor-guided framework organizes data science tasks in order of increasing\ndifficulty and incorporates a growing long-term memory to retain prior\nexperiences, guiding the agent's learning progression and enabling more\neffective utilization of accumulated knowledge. We evaluate DSMentor through\nextensive experiments on DSEval and QRData benchmarks. Experiments show that\nDSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval\nand QRData compared to baseline agents. Furthermore, DSMentor demonstrates\nstronger causal reasoning ability, improving the pass rate by 8.8% on the\ncausality problems compared to GPT-4 using Program-of-Thoughts prompts. Our\nwork underscores the importance of developing effective strategies for\naccumulating and utilizing knowledge during inference, mirroring the human\nlearning process and opening new avenues for improving LLM performance through\ncurriculum-based inference optimization.", "categories": ["cs.AI"], "published": "2025-05-20 10:16:21", "updated": "2025-05-20 10:16:21", "pdf_url": "http://arxiv.org/pdf/2505.14163v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14178v1", "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "abstract": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:32:30", "updated": "2025-05-20 10:32:30", "pdf_url": "http://arxiv.org/pdf/2505.14178v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14179v1", "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "abstract": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-20 10:34:45", "updated": "2025-05-20 10:34:45", "pdf_url": "http://arxiv.org/pdf/2505.14179v1", "comment": null, "doi": "10.1016/j.eswa.2024.125529", "journal_ref": "Expert Systems with Applications, 2025"}
{"arxiv_id": "2505.14185v1", "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 10:41:49", "updated": "2025-05-20 10:41:49", "pdf_url": "http://arxiv.org/pdf/2505.14185v1", "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14190v1", "title": "$\u03b1$-GAN by R\u00e9nyi Cross Entropy", "authors": ["Ni Ding", "Miao Qiao", "Jiaxing Xu", "Yiping Ke", "Xiaoyu Zhang"], "abstract": "This paper proposes $\\alpha$-GAN, a generative adversarial network using\nR\\'{e}nyi measures. The value function is formulated, by R\\'{e}nyi cross\nentropy, as an expected certainty measure incurred by the discriminator's soft\ndecision as to where the sample is from, true population or the generator. The\ndiscriminator tries to maximize the R\\'{e}nyi certainty about sample source,\nwhile the generator wants to reduce it by injecting fake samples. This forms a\nmin-max problem with the solution parameterized by the R\\'{e}nyi order\n$\\alpha$. This $\\alpha$-GAN reduces to vanilla GAN at $\\alpha = 1$, where the\nvalue function is exactly the binary cross entropy. The optimization of\n$\\alpha$-GAN is over probability (vector) space. It is shown that the gradient\nis exponentially enlarged when R\\'{e}nyi order is in the range $\\alpha \\in\n(0,1)$. This makes convergence faster, which is verified by experimental\nresults. A discussion shows that choosing $\\alpha \\in (0,1)$ may be able to\nsolve some common problems, e.g., vanishing gradient. A following observation\nreveals that this range has not been fully explored in the existing R\\'{e}nyi\nversion GANs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 10:45:11", "updated": "2025-05-20 10:45:11", "pdf_url": "http://arxiv.org/pdf/2505.14190v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14193v1", "title": "Dynamic Replanning for Improved Public Transport Routing", "authors": ["Abdallah Abuaisha", "Bojie Shen", "Daniel Harabor", "Peter Stuckey", "Mark Wallace"], "abstract": "Delays in public transport are common, often impacting users through\nprolonged travel times and missed transfers. Existing solutions for handling\ndelays remain limited; backup plans based on historical data miss opportunities\nfor earlier arrivals, while snapshot planning accounts for current delays but\nnot future ones. With the growing availability of live delay data, users can\nadjust their journeys in real-time. However, the literature lacks a framework\nthat fully exploits this advantage for system-scale dynamic replanning. To\naddress this, we formalise the dynamic replanning problem in public transport\nrouting and propose two solutions: a \"pull\" approach, where users manually\nrequest replanning, and a novel \"push\" approach, where the server proactively\nmonitors and adjusts journeys. Our experiments show that the push approach\noutperforms the pull approach, achieving significant speedups. The results also\nreveal substantial arrival time savings enabled by dynamic replanning.", "categories": ["cs.AI"], "published": "2025-05-20 10:50:58", "updated": "2025-05-20 10:50:58", "pdf_url": "http://arxiv.org/pdf/2505.14193v1", "comment": "Accepted for publication at IJCAI 2025. 8 pages, 4 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14201v1", "title": "FLASH-D: FlashAttention with Hidden Softmax Division", "authors": ["Kosmas Alexandridis", "Vasileios Titopoulos", "Giorgos Dimitrakopoulos"], "abstract": "The transformer's attention mechanism has revolutionized AI and machine\nlearning, with its efficient computation being crucial to its performance.\nHowever, calculating attention involves matrix operations interspersed with\nsoftmax rescaling, which inherently slows down computation and requires\nprocessing the entire input sequence. Building on online softmax computation,\nFlashAttention integrates softmax calculation with matrix arithmetic, enabling\ntiled computation independent of sequence length. While optimized for GPUs,\nFlashAttention's simplicity makes it amenable to direct hardware acceleration.\nThis work re-evaluates the core FlashAttention kernel, presenting FLASH-D a\nmathematically equivalent, yet simplified, formulation that achieves: (a)\nhiding softmax division within other non-linear function evaluations; (b)\ninherently numerically stable computation of exponentials, eliminating the need\nfor maximum value subtraction; and (c) a reduction in computational cost\nwithout introducing numerical approximations to the FlashAttention kernel.\nImportantly, the essential FlashAttention properties that facilitate efficient\ntiled implementation are fully preserved. Hardware implementation results at\n28nm demonstrate that this proposed formulation achieves a 22.8% reduction in\narea and a 20.3% reduction in power, on average, compared to state-of-the-art\nparallel hardware architectures without any performance penalty.", "categories": ["cs.LG", "cs.AI", "cs.AR"], "published": "2025-05-20 11:01:33", "updated": "2025-05-20 11:01:33", "pdf_url": "http://arxiv.org/pdf/2505.14201v1", "comment": "IEEE/ACM International Symposium on Low Power Electronics and Design\n  (ISLPED) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14206v1", "title": "Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data", "authors": ["Flavio Di Martino", "Franca Delmastro"], "abstract": "The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 11:05:06", "updated": "2025-05-20 11:05:06", "pdf_url": "http://arxiv.org/pdf/2505.14206v1", "comment": "Submitted to ACM Transactions on Computing for Healthcare (ACM\n  HEALTH)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14209v1", "title": "Embedded Mean Field Reinforcement Learning for Perimeter-defense Game", "authors": ["Li Wang", "Xin Yu", "Xuxin Lv", "Gangzheng Ai", "Wenjun Wu"], "abstract": "With the rapid advancement of unmanned aerial vehicles (UAVs) and missile\ntechnologies, perimeter-defense game between attackers and defenders for the\nprotection of critical regions have become increasingly complex and\nstrategically significant across a wide range of domains. However, existing\nstudies predominantly focus on small-scale, simplified two-dimensional\nscenarios, often overlooking realistic environmental perturbations, motion\ndynamics, and inherent heterogeneity--factors that pose substantial challenges\nto real-world applicability. To bridge this gap, we investigate large-scale\nheterogeneous perimeter-defense game in a three-dimensional setting,\nincorporating realistic elements such as motion dynamics and wind fields. We\nderive the Nash equilibrium strategies for both attackers and defenders,\ncharacterize the victory regions, and validate our theoretical findings through\nextensive simulations. To tackle large-scale heterogeneous control challenges\nin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)\nframework. EMFAC leverages representation learning to enable high-level action\naggregation in a mean-field manner, supporting scalable coordination among\ndefenders. Furthermore, we introduce a lightweight agent-level attention\nmechanism based on reward representation, which selectively filters\nobservations and mean-field information to enhance decision-making efficiency\nand accelerate convergence in large-scale tasks. Extensive simulations across\nvarying scales demonstrate the effectiveness and adaptability of EMFAC, which\noutperforms established baselines in both convergence speed and overall\nperformance. To further validate practicality, we test EMFAC in small-scale\nreal-world experiments and conduct detailed analyses, offering deeper insights\ninto the framework's effectiveness in complex scenarios.", "categories": ["cs.AI"], "published": "2025-05-20 11:11:46", "updated": "2025-05-20 11:11:46", "pdf_url": "http://arxiv.org/pdf/2505.14209v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14212v1", "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "abstract": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:16:29", "updated": "2025-05-20 11:16:29", "pdf_url": "http://arxiv.org/pdf/2505.14212v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14216v1", "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "abstract": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 11:22:34", "updated": "2025-05-20 11:22:34", "pdf_url": "http://arxiv.org/pdf/2505.14216v1", "comment": "23 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14217v1", "title": "Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned", "authors": ["Jorge Fabila", "Lidia Garrucho", "V\u00edctor M. Campello", "Carlos Mart\u00edn-Isla", "Karim Lekadir"], "abstract": "This study explores the use of Federated Learning (FL) for tuberculosis (TB)\ndiagnosis using chest X-rays in low-resource settings across Africa. FL allows\nhospitals to collaboratively train AI models without sharing raw patient data,\naddressing privacy concerns and data scarcity that hinder traditional\ncentralized models. The research involved hospitals and research centers in\neight African countries. Most sites used local datasets, while Ghana and The\nGambia used public ones. The study compared locally trained models with a\nfederated model built across all institutions to evaluate FL's real-world\nfeasibility. Despite its promise, implementing FL in sub-Saharan Africa faces\nchallenges such as poor infrastructure, unreliable internet, limited digital\nliteracy, and weak AI regulations. Some institutions were also reluctant to\nshare model updates due to data control concerns. In conclusion, FL shows\nstrong potential for enabling AI-driven healthcare in underserved regions, but\nbroader adoption will require improvements in infrastructure, education, and\nregulatory support.", "categories": ["cs.LG", "cs.AI", "eess.IV"], "published": "2025-05-20 11:23:52", "updated": "2025-05-20 11:23:52", "pdf_url": "http://arxiv.org/pdf/2505.14217v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14226v1", "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "abstract": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:35:25", "updated": "2025-05-20 11:35:25", "pdf_url": "http://arxiv.org/pdf/2505.14226v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14227v1", "title": "VoQA: Visual-only Question Answering", "authors": ["Luyang Jiang", "Jianing An", "Jie Luo", "Wenjun Wu", "Lei Huang"], "abstract": "We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 11:37:49", "updated": "2025-05-20 11:37:49", "pdf_url": "http://arxiv.org/pdf/2505.14227v1", "comment": "18 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14233v1", "title": "Mechanistic Fine-tuning for In-context Learning", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "abstract": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:41:21", "updated": "2025-05-20 11:41:21", "pdf_url": "http://arxiv.org/pdf/2505.14233v1", "comment": "28 pages, 31 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14234v1", "title": "Fast and close Shannon entropy approximation", "authors": ["Illia Horenko", "Davide Bassetti", "Luk\u00e1\u0161 Posp\u00ed\u0161il"], "abstract": "Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy\nare key components in many tools used in physics, information theory, machine\nlearning (ML) and quantum computing. Besides of the significant amounts of SE\ncomputations required in these fields, the singularity of the SE gradient is\none of the central mathematical reason inducing the high cost, frequently low\nrobustness and slow convergence of such tools. Here we propose the Fast Entropy\nApproximation (FEA) - a non-singular rational approximation of Shannon entropy\nand its gradient that achieves a mean absolute error of $10^{-3}$, which is\napproximately $20$ times lower than comparable state-of-the-art methods. FEA\nallows around $50\\%$ faster computation, requiring only $5$ to $6$ elementary\ncomputational operations, as compared to tens of elementary operations behind\nthe fastest entropy computation algorithms with table look-ups, bitshifts, or\nseries approximations. On a set of common benchmarks for the feature selection\nproblem in machine learning, we show that the combined effect of fewer\nelementary operations, low approximation error, and a non-singular gradient\nallows significantly better model quality and enables ML feature extraction\nthat is two to three orders of magnitude faster and computationally cheaper\nwhen incorporating FEA into AI tools.", "categories": ["cs.LG", "cs.AI", "68T01 (Primary) 68Q01, 90C99 (Secondary)"], "published": "2025-05-20 11:41:26", "updated": "2025-05-20 11:41:26", "pdf_url": "http://arxiv.org/pdf/2505.14234v1", "comment": "8 pages, 1 figure", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14235v1", "title": "Toward Embodied AGI: A Review of Embodied AI and the Road Ahead", "authors": ["Yequan Wang", "Aixin Sun"], "abstract": "Artificial General Intelligence (AGI) is often envisioned as inherently\nembodied. With recent advances in robotics and foundational AI models, we stand\nat the threshold of a new era-one marked by increasingly generalized embodied\nAI systems. This paper contributes to the discourse by introducing a systematic\ntaxonomy of Embodied AGI spanning five levels (L1-L5). We review existing\nresearch and challenges at the foundational stages (L1-L2) and outline the key\ncomponents required to achieve higher-level capabilities (L3-L5). Building on\nthese insights and existing technologies, we propose a conceptual framework for\nan L3+ robotic brain, offering both a technical outlook and a foundation for\nfuture exploration.", "categories": ["cs.AI"], "published": "2025-05-20 11:42:26", "updated": "2025-05-20 11:42:26", "pdf_url": "http://arxiv.org/pdf/2505.14235v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14238v1", "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "abstract": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:43:25", "updated": "2025-05-20 11:43:25", "pdf_url": "http://arxiv.org/pdf/2505.14238v1", "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14246v1", "title": "Visual Agentic Reinforcement Fine-Tuning", "authors": ["Ziyu Liu", "Yuhang Zang", "Yushan Zou", "Zijian Liang", "Xiaoyi Dong", "Yuhang Cao", "Haodong Duan", "Dahua Lin", "Jiaqi Wang"], "abstract": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 11:59:25", "updated": "2025-05-20 11:59:25", "pdf_url": "http://arxiv.org/pdf/2505.14246v1", "comment": "project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14252v1", "title": "Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks", "authors": ["Mouad Elaarabi", "Domenico Borzacchiello", "Philippe Le Bot", "Nathan Lauzeral", "Sebastien Comas-Cardona"], "abstract": "In this work, we explore the integration of Sequence Encoding for Online\nParameter Identification with Physics-Informed Neural Networks to create a\nmodel that, once trained, can be utilized for real time applications with\nvariable parameters, boundary conditions, and initial conditions. Recently, the\ncombination of PINNs with Sparse Regression has emerged as a method for\nperforming dynamical system identification through supervised learning and\nsparse regression optimization, while also solving the dynamics using PINNs.\nHowever, this approach can be limited by variations in parameters or boundary\nand initial conditions, requiring retraining of the model whenever changes\noccur. In this work, we introduce an architecture that employs Deep Sets or\nSequence Encoders to encode dynamic parameters, boundary conditions, and\ninitial conditions, using these encoded features as inputs for the PINN,\nenabling the model to adapt to changes in parameters, BCs, and ICs. We apply\nthis approach to three different problems. First, we analyze the Rossler ODE\nsystem, demonstrating the robustness of the model with respect to noise and its\nability to generalize. Next, we explore the model's capability in a 2D\nNavier-Stokes PDE problem involving flow past a cylinder with a parametric\nsinusoidal inlet velocity function, showing that the model can encode pressure\ndata from a few points to identify the inlet velocity profile and utilize\nphysics to compute velocity and pressure throughout the domain. Finally, we\naddress a 1D heat monitoring problem using real data from the heating of glass\nfiber and thermoplastic composite plates.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 12:05:17", "updated": "2025-05-20 12:05:17", "pdf_url": "http://arxiv.org/pdf/2505.14252v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14256v1", "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "abstract": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:09:17", "updated": "2025-05-20 12:09:17", "pdf_url": "http://arxiv.org/pdf/2505.14256v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14260v1", "title": "Speculative Decoding Reimagined for Multimodal Large Language Models", "authors": ["Luxi Lin", "Zhihang Lin", "Zhanpeng Zeng", "Rongrong Ji"], "abstract": "This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 12:12:17", "updated": "2025-05-20 12:12:17", "pdf_url": "http://arxiv.org/pdf/2505.14260v1", "comment": "12 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14268v1", "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:19:10", "updated": "2025-05-20 12:19:10", "pdf_url": "http://arxiv.org/pdf/2505.14268v1", "comment": "16 pages, 14 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14273v1", "title": "X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning", "authors": ["Hiroki Shiraishi", "Hisao Ishibuchi", "Masaya Nakata"], "abstract": "Function approximation is a critical task in various fields. However,\nexisting neural network approaches struggle with locally complex or\ndiscontinuous functions due to their reliance on a single global model covering\nthe entire problem space. We propose X-KAN, a novel method that optimizes\nmultiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary\nrule-based machine learning framework called XCSF. X-KAN combines KAN's high\nexpressiveness with XCSF's adaptive partitioning capability by implementing\nlocal KAN models as rule consequents and defining local regions via rule\nantecedents. Our experimental results on artificial test functions and\nreal-world datasets demonstrate that X-KAN significantly outperforms\nconventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms\nof approximation accuracy. Notably, X-KAN effectively handles functions with\nlocally complex or discontinuous structures that are challenging for\nconventional KAN, using a compact set of rules (average 7.2 $\\pm$ 2.3 rules).\nThese results validate the effectiveness of using KAN as a local model in XCSF,\nwhich evaluates the rule fitness based on both accuracy and generality. Our\nX-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "published": "2025-05-20 12:26:03", "updated": "2025-05-20 12:26:03", "pdf_url": "http://arxiv.org/pdf/2505.14273v1", "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "doi": null, "journal_ref": "34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)"}
{"arxiv_id": "2505.14279v1", "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin M\u00fcnch"], "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:30:46", "updated": "2025-05-20 12:30:46", "pdf_url": "http://arxiv.org/pdf/2505.14279v1", "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14285v1", "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis", "authors": ["Eirini Panteli", "Paulo E. Santos", "Nabil Humphrey"], "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 12:35:43", "updated": "2025-05-20 12:35:43", "pdf_url": "http://arxiv.org/pdf/2505.14285v1", "comment": "8 pages; 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14289v1", "title": "EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection", "authors": ["Yijie Lu", "Tianjie Ju", "Manman Zhao", "Xinbei Ma", "Yuan Guo", "ZhuoSheng Zhang"], "abstract": "As multimodal agents are increasingly trained to operate graphical user\ninterfaces (GUIs) to complete user tasks, they face a growing threat from\nindirect prompt injection, attacks in which misleading instructions are\nembedded into the agent's visual environment, such as popups or chat messages,\nand misinterpreted as part of the intended task. A typical example is\nenvironmental injection, in which GUI elements are manipulated to influence\nagent behavior without directly modifying the user prompt. To address these\nemerging attacks, we propose EVA, a red teaming framework for indirect prompt\ninjection which transforms the attack into a closed loop optimization by\ncontinuously monitoring an agent's attention distribution over the GUI and\nupdating adversarial cues, keywords, phrasing, and layout, in response.\nCompared with prior one shot methods that generate fixed prompts without regard\nfor how the model allocates visual attention, EVA dynamically adapts to\nemerging attention hotspots, yielding substantially higher attack success rates\nand far greater transferability across diverse GUI scenarios. We evaluate EVA\non six widely used generalist and specialist GUI agents in realistic settings\nsuch as popup manipulation, chat based phishing, payments, and email\ncomposition. Experimental results show that EVA substantially improves success\nrates over static baselines. Under goal agnostic constraints, where the\nattacker does not know the agent's task intent, EVA still discovers effective\npatterns. Notably, we find that injection styles transfer well across models,\nrevealing shared behavioral biases in GUI agents. These results suggest that\nevolving indirect prompt injection is a powerful tool not only for red teaming\nagents, but also for uncovering common vulnerabilities in their multimodal\ndecision making.", "categories": ["cs.AI"], "published": "2025-05-20 12:41:05", "updated": "2025-05-20 12:41:05", "pdf_url": "http://arxiv.org/pdf/2505.14289v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14295v1", "title": "Benchmarking data encoding methods in Quantum Machine Learning", "authors": ["Orlane Zang", "Gr\u00e9goire Barru\u00e9", "Tony Quertier"], "abstract": "Data encoding plays a fundamental and distinctive role in Quantum Machine\nLearning (QML). While classical approaches process data directly as vectors,\nQML may require transforming classical data into quantum states through\nencoding circuits, known as quantum feature maps or quantum embeddings. This\nstep leverages the inherently high-dimensional and non-linear nature of Hilbert\nspace, enabling more efficient data separation in complex feature spaces that\nmay be inaccessible to classical methods. This encoding part significantly\naffects the performance of the QML model, so it is important to choose the\nright encoding method for the dataset to be encoded. However, this choice is\ngenerally arbitrary, since there is no \"universal\" rule for knowing which\nencoding to choose based on a specific set of data. There are currently a\nvariety of encoding methods using different quantum logic gates. We studied the\nmost commonly used types of encoding methods and benchmarked them using\ndifferent datasets.", "categories": ["quant-ph", "cs.AI"], "published": "2025-05-20 12:44:14", "updated": "2025-05-20 12:44:14", "pdf_url": "http://arxiv.org/pdf/2505.14295v1", "comment": "30 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14300v1", "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors", "authors": ["Maheep Chaudhary", "Fazl Barez"], "abstract": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 12:49:58", "updated": "2025-05-20 12:49:58", "pdf_url": "http://arxiv.org/pdf/2505.14300v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14312v1", "title": "MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains", "authors": ["Kyungeun Lee", "Moonjung Eo", "Hye-Seung Cho", "Dongmin Kim", "Ye Seul Sim", "Seoyoon Kim", "Min-Kook Suh", "Woohyung Lim"], "abstract": "Despite the widespread use of tabular data in real-world applications, most\nbenchmarks rely on average-case metrics, which fail to reveal how model\nbehavior varies across diverse data regimes. To address this, we propose\nMultiTab, a benchmark suite and evaluation framework for multi-dimensional,\ndata-aware analysis of tabular learning algorithms. Rather than comparing\nmodels only in aggregate, MultiTab categorizes 196 publicly available datasets\nalong key data characteristics, including sample size, label imbalance, and\nfeature interaction, and evaluates 13 representative models spanning a range of\ninductive biases. Our analysis shows that model performance is highly sensitive\nto such regimes: for example, models using sample-level similarity excel on\ndatasets with large sample sizes or high inter-feature correlation, while\nmodels encoding inter-feature dependencies perform best with weakly correlated\nfeatures. These findings reveal that inductive biases do not always behave as\nintended, and that regime-aware evaluation is essential for understanding and\nimproving model behavior. MultiTab enables more principled model design and\noffers practical guidance for selecting models tailored to specific data\ncharacteristics. All datasets, code, and optimization logs are publicly\navailable at https://huggingface.co/datasets/LGAI-DILab/Multitab.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 13:00:43", "updated": "2025-05-20 13:00:43", "pdf_url": "http://arxiv.org/pdf/2505.14312v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14316v1", "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion", "authors": ["Tiehan Cui", "Yanxu Mao", "Peipei Liu", "Congying Liu", "Datao You"], "abstract": "Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 13:03:15", "updated": "2025-05-20 13:03:15", "pdf_url": "http://arxiv.org/pdf/2505.14316v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14330v1", "title": "Handloom Design Generation Using Generative Networks", "authors": ["Rajat Kanti Bhattacharjee", "Meghali Nandi", "Amrit Jha", "Gunajit Kalita", "Ferdous Ahmed Barbhuiya"], "abstract": "This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-20 13:16:55", "updated": "2025-05-20 13:16:55", "pdf_url": "http://arxiv.org/pdf/2505.14330v1", "comment": null, "doi": "10.1109/ICIP40778.2020.9190925", "journal_ref": null}
{"arxiv_id": "2505.14341v1", "title": "Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image", "authors": ["Sifan Li", "Ming Tao", "Hao Zhao", "Ling Shao", "Hao Tang"], "abstract": "Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 13:27:52", "updated": "2025-05-20 13:27:52", "pdf_url": "http://arxiv.org/pdf/2505.14341v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14345v1", "title": "Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights", "authors": ["Aydin Abedinia", "Shima Tabakhi", "Vahid Seydi"], "abstract": "Recent advancements in semi-supervised deep learning have introduced\neffective strategies for leveraging both labeled and unlabeled data to improve\nclassification performance. This work proposes a semi-supervised framework that\nutilizes a distance-based weighting mechanism to prioritize critical training\nsamples based on their proximity to test data. By focusing on the most\ninformative examples, the method enhances model generalization and robustness,\nparticularly in challenging scenarios with noisy or imbalanced datasets.\nBuilding on techniques such as uncertainty consistency and graph-based\nrepresentations, the approach addresses key challenges of limited labeled data\nwhile maintaining scalability. Experiments on twelve benchmark datasets\ndemonstrate significant improvements across key metrics, including accuracy,\nprecision, and recall, consistently outperforming existing methods. This\nframework provides a robust and practical solution for semi-supervised\nlearning, with potential applications in domains such as healthcare and\nsecurity where data limitations pose significant challenges.", "categories": ["cs.LG", "cs.AI", "68T05, 62H30", "I.2.6; I.5.1; I.5.4"], "published": "2025-05-20 13:29:04", "updated": "2025-05-20 13:29:04", "pdf_url": "http://arxiv.org/pdf/2505.14345v1", "comment": "5 pages, 6 figures. This paper has been accepted for publication and\n  oral presentation at the 2025 10th IEEE International Conference on Machine\n  Learning Technologies (ICMLT 2025). The final authenticated version will be\n  available in IEEE Xplore following the conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14349v1", "title": "Upgrading Democracies with Fairer Voting Methods", "authors": ["Evangelos Pournaras", "Srijoni Majumdar", "Thomas Wellings", "Joshua C. Yang", "Fatemeh B. Heravan", "Regula H\u00e4nggli Fricker", "Dirk Helbing"], "abstract": "Voting methods are instrumental design element of democracies. Citizens use\nthem to express and aggregate their preferences to reach a collective decision.\nHowever, voting outcomes can be as sensitive to voting rules as they are to\npeople's voting choices. Despite the significance and inter-disciplinary\nscientific progress on voting methods, several democracies keep relying on\noutdated voting methods that do not fit modern, pluralistic societies well,\nwhile lacking social innovation. Here, we demonstrate how one can upgrade\nreal-world democracies, namely by using alternative preferential voting methods\nsuch as cumulative voting and the method of equal shares designed for a\nproportional representation of voters' preferences. By rigorously assessing a\nnew participatory budgeting approach applied in the city of Aarau, Switzerland,\nwe unravel the striking voting outcomes of fair voting methods: more winning\nprojects with the same budget and broader geographic and preference\nrepresentation of citizens by the elected projects, in particular for voters\nwho used to be under-represented, while promoting novel project ideas. We\nprovide profound causal evidence showing that citizens prefer proportional\nvoting methods, which possess strong legitimacy without the need of very\ntechnical specialized explanations. We also reveal strong underlying democratic\nvalues exhibited by citizens who support fair voting methods such as altruism\nand compromise. These findings come with a global momentum to unleash a new and\nlong-awaited participation blueprint of how to upgrade democracies.", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "cs.MA"], "published": "2025-05-20 13:31:43", "updated": "2025-05-20 13:31:43", "pdf_url": "http://arxiv.org/pdf/2505.14349v1", "comment": "Includes Supplementary Information", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14351v1", "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 13:35:55", "updated": "2025-05-20 13:35:55", "pdf_url": "http://arxiv.org/pdf/2505.14351v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14366v1", "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds", "authors": ["Joel Currie", "Gioele Migno", "Enrico Piacenti", "Maria Elena Giannaccini", "Patric Bach", "Davide De Tommaso", "Agnieszka Wykowska"], "abstract": "We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.", "categories": ["cs.AI", "cs.RO"], "published": "2025-05-20 13:49:09", "updated": "2025-05-20 13:49:09", "pdf_url": "http://arxiv.org/pdf/2505.14366v1", "comment": "Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late\n  Breaking Report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14377v1", "title": "When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making", "authors": ["Ulrike Kuhl", "Annika Bush"], "abstract": "Although the integration of artificial intelligence (AI) into everyday tasks\nimproves efficiency and objectivity, it also risks transmitting bias to human\ndecision-making. In this study, we conducted a controlled experiment that\nsimulated hiring decisions to examine how biased AI recommendations - augmented\nwith or without counterfactual explanations - influence human judgment over\ntime. Participants, acting as hiring managers, completed 60 decision trials\ndivided into a baseline phase without AI, followed by a phase with biased (X)AI\nrecommendations (favoring either male or female candidates), and a final\npost-interaction phase without AI. Our results indicate that the participants\nfollowed the AI recommendations 70% of the time when the qualifications of the\ngiven candidates were comparable. Yet, only a fraction of participants detected\nthe gender bias (8 out of 294). Crucially, exposure to biased AI altered\nparticipants' inherent preferences: in the post-interaction phase,\nparticipants' independent decisions aligned with the bias when no\ncounterfactual explanations were provided before, but reversed the bias when\nexplanations were given. Reported trust did not differ significantly across\nconditions. Confidence varied throughout the study phases after exposure to\nmale-biased AI, indicating nuanced effects of AI bias on decision certainty.\nOur findings point to the importance of calibrating XAI to avoid unintended\nbehavioral shifts in order to safeguard equitable decision-making and prevent\nthe adoption of algorithmic bias.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-20 14:00:28", "updated": "2025-05-20 14:00:28", "pdf_url": "http://arxiv.org/pdf/2505.14377v1", "comment": "Accepted for XAI2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14381v1", "title": "SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation", "authors": ["Yuyang Dong", "Nobuhiro Ueda", "Kriszti\u00e1n Boros", "Daiki Ito", "Takuya Sera", "Masafumi Oyamada"], "abstract": "With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions.", "categories": ["cs.AI"], "published": "2025-05-20 14:03:24", "updated": "2025-05-20 14:03:24", "pdf_url": "http://arxiv.org/pdf/2505.14381v1", "comment": "v1", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14391v1", "title": "Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning", "authors": ["Zhaohui Yang", "Chenghua He", "Xiaowen Shi", "Linjing Li", "Qiyue Yin", "Shihong Deng", "Daxin Jiang"], "abstract": "Many studies focus on data annotation techniques for training effective PRMs.\nHowever, current methods encounter a significant issue when applied to long CoT\nreasoning processes: they tend to focus solely on the first incorrect step and\nall preceding steps, assuming that all subsequent steps are incorrect. These\nmethods overlook the unique self-correction and reflection mechanisms inherent\nin long CoT, where correct reasoning steps may still occur after initial\nreasoning mistakes. To address this issue, we propose a novel data annotation\nmethod for PRMs specifically designed to score the long CoT reasoning process.\nGiven that under the reflection pattern, correct and incorrect steps often\nalternate, we introduce the concepts of Error Propagation and Error Cessation,\nenhancing PRMs' ability to identify both effective self-correction behaviors\nand reasoning based on erroneous steps. Leveraging an LLM-based judger for\nannotation, we collect 1.7 million data samples to train a 7B PRM and evaluate\nit at both solution and step levels. Experimental results demonstrate that\ncompared to existing open-source PRMs and PRMs trained on open-source datasets,\nour PRM achieves superior performance across various metrics, including search\nguidance, BoN, and F1 scores. Compared to widely used MC-based annotation\nmethods, our annotation approach not only achieves higher data efficiency but\nalso delivers superior performance. Detailed analysis is also conducted to\ndemonstrate the stability and generalizability of our method.", "categories": ["cs.AI"], "published": "2025-05-20 14:12:05", "updated": "2025-05-20 14:12:05", "pdf_url": "http://arxiv.org/pdf/2505.14391v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14394v1", "title": "Knowledge Graph Based Repository-Level Code Generation", "authors": ["Mihir Athale", "Vishal Vaddina"], "abstract": "Recent advancements in Large Language Models (LLMs) have transformed code\ngeneration from natural language queries. However, despite their extensive\nknowledge and ability to produce high-quality code, LLMs often struggle with\ncontextual accuracy, particularly in evolving codebases. Current code search\nand retrieval methods frequently lack robustness in both the quality and\ncontextual relevance of retrieved results, leading to suboptimal code\ngeneration. This paper introduces a novel knowledge graph-based approach to\nimprove code search and retrieval leading to better quality of code generation\nin the context of repository-level tasks. The proposed approach represents code\nrepositories as graphs, capturing structural and relational information for\nenhanced context-aware code generation. Our framework employs a hybrid approach\nfor code retrieval to improve contextual relevance, track inter-file modular\ndependencies, generate more robust code and ensure consistency with the\nexisting codebase. We benchmark the proposed approach on the Evolutionary Code\nBenchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,\nand demonstrate that our method significantly outperforms the baseline\napproach. These findings suggest that knowledge graph based code generation\ncould advance robust, context-sensitive coding assistance tools.", "categories": ["cs.AI"], "published": "2025-05-20 14:13:59", "updated": "2025-05-20 14:13:59", "pdf_url": "http://arxiv.org/pdf/2505.14394v1", "comment": "8 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14395v1", "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "abstract": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:14:00", "updated": "2025-05-20 14:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14395v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "authors": ["Ga\u00ebl Gendron", "Jo\u017ee M. Ro\u017eanec", "Michael Witbrock", "Gillian Dobbie"], "abstract": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"], "published": "2025-05-20 14:14:05", "updated": "2025-05-20 14:14:05", "pdf_url": "http://arxiv.org/pdf/2505.14396v1", "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14398v1", "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "abstract": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 14:14:38", "updated": "2025-05-20 14:14:38", "pdf_url": "http://arxiv.org/pdf/2505.14398v1", "comment": "Data and code are available at https://peterbaile.github.io/lag/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14403v1", "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning", "authors": ["Zhaohui Yang", "Shilei Jiang", "Chen Hu", "Linjing Li", "Shihong Deng", "Daxin Jiang"], "abstract": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.", "categories": ["cs.AI"], "published": "2025-05-20 14:16:49", "updated": "2025-05-20 14:16:49", "pdf_url": "http://arxiv.org/pdf/2505.14403v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14412v1", "title": "PRL: Prompts from Reinforcement Learning", "authors": ["Pawe\u0142 Batorski", "Adrian Kosmala", "Paul Swoboda"], "abstract": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 14:26:19", "updated": "2025-05-20 14:26:19", "pdf_url": "http://arxiv.org/pdf/2505.14412v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14419v1", "title": "SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation", "authors": ["Huimin Xu", "Xin Mao", "Feng-Lin Li", "Xiaobao Wu", "Wang Chen", "Wei Zhang", "Anh Tuan Luu"], "abstract": "Process Reward Models (PRMs) have demonstrated promising results in\nmathematical reasoning, but existing process annotation approaches, whether\nthrough human annotations or Monte Carlo simulations, remain computationally\nexpensive. In this paper, we introduce Step COmpression for Process Estimation\n(SCOPE), a novel compression-based approach that significantly reduces\nannotation costs. We first translate natural language reasoning steps into code\nand normalize them through Abstract Syntax Tree, then merge equivalent steps to\nconstruct a prefix tree. Unlike simulation-based methods that waste numerous\nsamples on estimation, SCOPE leverages a compression-based prefix tree where\neach root-to-leaf path serves as a training sample, reducing the complexity\nfrom $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K\nsamples with only 5% of the computational resources required by previous\nmethods. Empirical results demonstrate that PRMs trained on our dataset\nconsistently outperform existing automated annotation approaches on both\nBest-of-N strategy and ProcessBench.", "categories": ["cs.AI"], "published": "2025-05-20 14:31:15", "updated": "2025-05-20 14:31:15", "pdf_url": "http://arxiv.org/pdf/2505.14419v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14428v1", "title": "Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications", "authors": ["Riccardo D'Elia"], "abstract": "The objective of this proposal is to bridge the gap between Deep Learning\n(DL) and System Dynamics (SD) by developing an interpretable neural system\ndynamics framework. While DL excels at learning complex models and making\naccurate predictions, it lacks interpretability and causal reliability.\nTraditional SD approaches, on the other hand, provide transparency and causal\ninsights but are limited in scalability and require extensive domain knowledge.\nTo overcome these limitations, this project introduces a Neural System Dynamics\npipeline, integrating Concept-Based Interpretability, Mechanistic\nInterpretability, and Causal Machine Learning. This framework combines the\npredictive power of DL with the interpretability of traditional SD models,\nresulting in both causal reliability and scalability. The efficacy of the\nproposed pipeline will be validated through real-world applications of the\nEU-funded AutoMoTIF project, which is focused on autonomous multimodal\ntransportation systems. The long-term goal is to collect actionable insights\nthat support the integration of explainability and safety in autonomous\nsystems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 14:38:39", "updated": "2025-05-20 14:38:39", "pdf_url": "http://arxiv.org/pdf/2505.14428v1", "comment": "To be submitted to CEUR-WS.org for publication in the Doctoral\n  Consortium Proceedings of XAI 2025, The World Conference on Explainable\n  Artificial Intelligence", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14435v1", "title": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI", "authors": ["Annika Bush", "Meltem Aksoy", "Markus Pauly", "Greta Ontrup"], "abstract": "As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making.", "categories": ["cs.CY", "cs.AI"], "published": "2025-05-20 14:41:56", "updated": "2025-05-20 14:41:56", "pdf_url": "http://arxiv.org/pdf/2505.14435v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14436v1", "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:42:03", "updated": "2025-05-20 14:42:03", "pdf_url": "http://arxiv.org/pdf/2505.14436v1", "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14442v1", "title": "Creative Preference Optimization", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:43:41", "updated": "2025-05-20 14:43:41", "pdf_url": "http://arxiv.org/pdf/2505.14442v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14451v1", "title": "RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation", "authors": ["Md Atik Ahamed", "Qiang Ye", "Qiang Cheng"], "abstract": "Missing values in high-dimensional, mixed-type datasets pose significant\nchallenges for data imputation, particularly under Missing Not At Random (MNAR)\nmechanisms. Existing methods struggle to integrate local and global data\ncharacteristics, limiting performance in MNAR and high-dimensional settings. We\npropose an innovative framework, RefiDiff, combining local machine learning\npredictions with a novel Mamba-based denoising network capturing\ninterrelationships among distant features and samples. Our approach leverages\npre-refinement for initial warm-up imputations and post-refinement to polish\nresults, enhancing stability and accuracy. By encoding mixed-type data into\nunified tokens, RefiDiff enables robust imputation without architectural or\nhyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods\nacross missing-value settings, excelling in MNAR with a 4x faster training time\nthan SOTA DDPM-based approaches. Extensive evaluations on nine real-world\ndatasets demonstrate its robustness, scalability, and effectiveness in handling\ncomplex missingness patterns.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 14:51:07", "updated": "2025-05-20 14:51:07", "pdf_url": "http://arxiv.org/pdf/2505.14451v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14452v1", "title": "How Managers Perceive AI-Assisted Conversational Training for Workplace Communication", "authors": ["Lance T Wilhelm", "Xiaohan Ding", "Kirk McInnis Knutsen", "Buse Carik", "Eugenia H Rho"], "abstract": "Effective workplace communication is essential for managerial success, yet\nmany managers lack access to tailored and sustained training. Although\nAI-assisted communication systems may offer scalable training solutions, little\nis known about how managers envision the role of AI in helping them improve\ntheir communication skills. To investigate this, we designed a conversational\nrole-play system, CommCoach, as a functional probe to understand how managers\nanticipate using AI to practice their communication skills. Through\nsemi-structured interviews, participants emphasized the value of adaptive,\nlow-risk simulations for practicing difficult workplace conversations. They\nalso highlighted opportunities, including human-AI teaming, transparent and\ncontext-aware feedback, and greater control over AI-generated personas.\nAI-assisted communication training should balance personalization, structured\nlearning objectives, and adaptability to different user styles and contexts.\nHowever, achieving this requires carefully navigating tensions between adaptive\nand consistent AI feedback, realism and potential bias, and the open-ended\nnature of AI conversations versus structured workplace discourse.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-20 14:51:27", "updated": "2025-05-20 14:51:27", "pdf_url": "http://arxiv.org/pdf/2505.14452v1", "comment": "accepted to CUI '25", "doi": "10.1145/3719160.3736639", "journal_ref": null}
{"arxiv_id": "2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "authors": ["Chihan Huang", "Hao Tang"], "abstract": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:52:41", "updated": "2025-05-20 14:52:41", "pdf_url": "http://arxiv.org/pdf/2505.14455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14469v1", "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "abstract": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:05:03", "updated": "2025-05-20 15:05:03", "pdf_url": "http://arxiv.org/pdf/2505.14469v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14479v1", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "abstract": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:13:32", "updated": "2025-05-20 15:13:32", "pdf_url": "http://arxiv.org/pdf/2505.14479v1", "comment": "long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14489v1", "title": "Reasoning Models Better Express Their Confidence", "authors": ["Dongkeun Yoon", "Seungone Kim", "Sohee Yang", "Sunkyoung Kim", "Soyeon Kim", "Yongil Kim", "Eunbi Choi", "Yireun Kim", "Minjoon Seo"], "abstract": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:19:00", "updated": "2025-05-20 15:19:00", "pdf_url": "http://arxiv.org/pdf/2505.14489v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14499v1", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "abstract": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:28:26", "updated": "2025-05-20 15:28:26", "pdf_url": "http://arxiv.org/pdf/2505.14499v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14505v1", "title": "ModRWKV: Transformer Multimodality in Linear Time", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "abstract": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:34:36", "updated": "2025-05-20 15:34:36", "pdf_url": "http://arxiv.org/pdf/2505.14505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14510v1", "title": "BACON: A fully explainable AI model with graded logic for decision making problems", "authors": ["Haishi Bai", "Jozo Dujmovic", "Jianwu Wang"], "abstract": "As machine learning models and autonomous agents are increasingly deployed in\nhigh-stakes, real-world domains such as healthcare, security, finance, and\nrobotics, the need for transparent and trustworthy explanations has become\ncritical. To ensure end-to-end transparency of AI decisions, we need models\nthat are not only accurate but also fully explainable and human-tunable. We\nintroduce BACON, a novel framework for automatically training explainable AI\nmodels for decision making problems using graded logic. BACON achieves high\npredictive accuracy while offering full structural transparency and precise,\nlogic-based symbolic explanations, enabling effective human-AI collaboration\nand expert-guided refinement. We evaluate BACON with a diverse set of\nscenarios: classic Boolean approximation, Iris flower classification, house\npurchasing decisions and breast cancer diagnosis. In each case, BACON provides\nhigh-performance models while producing compact, human-verifiable decision\nlogic. These results demonstrate BACON's potential as a practical and\nprincipled approach for delivering crisp, trustworthy explainable AI.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-20 15:39:05", "updated": "2025-05-20 15:39:05", "pdf_url": "http://arxiv.org/pdf/2505.14510v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14513v1", "title": "Latent Flow Transformer", "authors": ["Yen-Chen Wu", "Feng-Ting Liao", "Meng-Hsi Chen", "Pei-Chen Ho", "Farhang Nabiei", "Da-shan Shiu"], "abstract": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 15:41:05", "updated": "2025-05-20 15:41:05", "pdf_url": "http://arxiv.org/pdf/2505.14513v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14523v1", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "abstract": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-20 15:46:44", "updated": "2025-05-20 15:46:44", "pdf_url": "http://arxiv.org/pdf/2505.14523v1", "comment": "To be published in ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14524v1", "title": "Guarded Query Routing for Large Language Models", "authors": ["Richard \u0160l\u00e9her", "William Brach", "Tibor Sloboda", "Kristi\u00e1n Ko\u0161\u0165\u00e1l", "Lukas Galke"], "abstract": "Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}.", "categories": ["cs.AI"], "published": "2025-05-20 15:46:59", "updated": "2025-05-20 15:46:59", "pdf_url": "http://arxiv.org/pdf/2505.14524v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14526v1", "title": "NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation", "authors": ["Matteo El-Hariry", "Antoine Richard", "Ricard M. Castan", "Luis F. W. Batista", "Matthieu Geist", "Cedric Pradalier", "Miguel Olivares-Mendez"], "abstract": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 15:48:23", "updated": "2025-05-20 15:48:23", "pdf_url": "http://arxiv.org/pdf/2505.14526v1", "comment": "Submitted for publication. Under review (2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14533v1", "title": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers", "authors": ["Mohammad Irfan Uddin", "Nishad Tasnim", "Md Omor Faruk", "Zejian Zhou"], "abstract": "Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 15:52:43", "updated": "2025-05-20 15:52:43", "pdf_url": "http://arxiv.org/pdf/2505.14533v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14539v1", "title": "A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)", "authors": ["Gaia Belardinelli", "Thomas Bolander", "Sebastian Watzl"], "abstract": "In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.", "categories": ["cs.AI"], "published": "2025-05-20 15:56:34", "updated": "2025-05-20 15:56:34", "pdf_url": "http://arxiv.org/pdf/2505.14539v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14544v1", "title": "Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study", "authors": ["Saahil Mahato"], "abstract": "Urban traffic congestion, particularly at intersections, significantly\nimpacts travel time, fuel consumption, and emissions. Traditional fixed-time\nsignal control systems often lack the adaptability to manage dynamic traffic\npatterns effectively. This study explores the application of multi-agent\nreinforcement learning (MARL) to optimize traffic signal coordination across\nmultiple intersections within a simulated environment. Utilizing Pygame, a\nsimulation was developed to model a network of interconnected intersections\nwith randomly generated vehicle flows to reflect realistic traffic variability.\nA decentralized MARL controller was implemented, in which each traffic signal\noperates as an autonomous agent, making decisions based on local observations\nand information from neighboring agents. Performance was evaluated against a\nbaseline fixed-time controller using metrics such as average vehicle wait time\nand overall throughput. The MARL approach demonstrated statistically\nsignificant improvements, including reduced average waiting times and improved\nthroughput. These findings suggest that MARL-based dynamic control strategies\nhold substantial promise for improving urban traffic management efficiency.\nMore research is recommended to address scalability and real-world\nimplementation challenges.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-20 15:59:44", "updated": "2025-05-20 15:59:44", "pdf_url": "http://arxiv.org/pdf/2505.14544v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14549v1", "title": "Can Large Language Models Really Recognize Your Name?", "authors": ["Dzung Pham", "Peter Kairouz", "Niloofar Mireshghallah", "Eugene Bagdasarian", "Chau Minh Pham", "Amir Houmansadr"], "abstract": "Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 16:05:05", "updated": "2025-05-20 16:05:05", "pdf_url": "http://arxiv.org/pdf/2505.14549v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14551v1", "title": "Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains", "authors": ["Petros Drineas", "Rohit Nema", "Rafail Ostrovsky", "Vassilis Zikas"], "abstract": "Reputation systems play an essential role in the Internet era, as they enable\npeople to decide whom to trust, by collecting and aggregating data about users'\nbehavior. Recently, several works proposed the use of reputation for the design\nand scalability improvement of decentralized (blockchain) ledgers; however,\nsuch systems are prone to manipulation and to our knowledge no game-theoretic\ntreatment exists that can support their economic robustness.\n  In this work we put forth a new model for the design of what we call, {\\em\ntrustworthy reputation systems}. Concretely, we describe a class of games,\nwhich we term {\\em trustworthy reputation games}, that enable a set of users to\nreport a function of their beliefs about the trustworthiness of each server in\na set -- i.e., their estimate of the probability that this server will behave\naccording to its specified strategy -- in a way that satisfies the following\nproperties:\n  1. It is $(\\epsilon$-)best response for any rational user in the game to play\na prescribed (truthful) strategy according to their true belief.\n  2. Assuming that the users' beliefs are not too far from the {\\em true}\ntrustworthiness of the servers, playing the above ($\\epsilon-$)Nash equilibrium\nallows anyone who observes the users' strategies to estimate the relative\ntrustworthiness of any two servers.\n  Our utilities and decoding function build on a connection between the well\nknown PageRank algorithm and the problem of trustworthiness discovery, which\ncan be of independent interest. Finally, we show how the above games are\nmotivated by and can be leveraged in proof-of-reputation (PoR) blockchains.", "categories": ["cs.GT", "cs.AI", "cs.CR"], "published": "2025-05-20 16:06:25", "updated": "2025-05-20 16:06:25", "pdf_url": "http://arxiv.org/pdf/2505.14551v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14552v1", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "abstract": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:06:32", "updated": "2025-05-20 16:06:32", "pdf_url": "http://arxiv.org/pdf/2505.14552v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14555v1", "title": "Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting", "authors": ["Yingtao Luo", "Shikai Fang", "Binqing Wu", "Qingsong Wen", "Liang Sun"], "abstract": "Weather forecasting is essential but remains computationally intensive and\nphysically incomplete in traditional numerical weather prediction (NWP)\nmethods. Deep learning (DL) models offer efficiency and accuracy but often\nignore physical laws, limiting interpretability and generalization. We propose\nPhyDL-NWP, a physics-guided deep learning framework that integrates physical\nequations with latent force parameterization into data-driven models. It\npredicts weather variables from arbitrary spatiotemporal coordinates, computes\nphysical terms via automatic differentiation, and uses a physics-informed loss\nto align predictions with governing dynamics. PhyDL-NWP enables resolution-free\ndownscaling by modeling weather as a continuous function and fine-tunes\npre-trained models with minimal overhead, achieving up to 170x faster inference\nwith only 55K parameters. Experiments show that PhyDL-NWP improves both\nforecasting performance and physical consistency.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:13:20", "updated": "2025-05-20 16:13:20", "pdf_url": "http://arxiv.org/pdf/2505.14555v1", "comment": "Published/Accepted in KDD 2025 (February Cycle)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14561v1", "title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification", "authors": ["Theo Lepage", "Reda Dehak"], "abstract": "Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS.", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "published": "2025-05-20 16:19:34", "updated": "2025-05-20 16:19:34", "pdf_url": "http://arxiv.org/pdf/2505.14561v1", "comment": "accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14564v1", "title": "Bellman operator convergence enhancements in reinforcement learning algorithms", "authors": ["David Krame Kadurha", "Domini Jocema Leko Moutouo", "Yae Ulrich Gaba"], "abstract": "This paper reviews the topological groundwork for the study of reinforcement\nlearning (RL) by focusing on the structure of state, action, and policy spaces.\nWe begin by recalling key mathematical concepts such as complete metric spaces,\nwhich form the foundation for expressing RL problems. By leveraging the Banach\ncontraction principle, we illustrate how the Banach fixed-point theorem\nexplains the convergence of RL algorithms and how Bellman operators, expressed\nas operators on Banach spaces, ensure this convergence. The work serves as a\nbridge between theoretical mathematics and practical algorithm design, offering\nnew approaches to enhance the efficiency of RL. In particular, we investigate\nalternative formulations of Bellman operators and demonstrate their impact on\nimproving convergence rates and performance in standard RL environments such as\nMountainCar, CartPole, and Acrobot. Our findings highlight how a deeper\nmathematical understanding of RL can lead to more effective algorithms for\ndecision-making problems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:24:42", "updated": "2025-05-20 16:24:42", "pdf_url": "http://arxiv.org/pdf/2505.14564v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14566v1", "title": "KIPPO: Koopman-Inspired Proximal Policy Optimization", "authors": ["Andrei Cozma", "Landon Harris", "Hairong Qi"], "abstract": "Reinforcement Learning (RL) has made significant strides in various domains,\nand policy gradient methods like Proximal Policy Optimization (PPO) have gained\npopularity due to their balance in performance, training stability, and\ncomputational efficiency. These methods directly optimize policies through\ngradient-based updates. However, developing effective control policies for\nenvironments with complex and non-linear dynamics remains a challenge. High\nvariance in gradient estimates and non-convex optimization landscapes often\nlead to unstable learning trajectories. Koopman Operator Theory has emerged as\na powerful framework for studying non-linear systems through an\ninfinite-dimensional linear operator that acts on a higher-dimensional space of\nmeasurement functions. In contrast with their non-linear counterparts, linear\nsystems are simpler, more predictable, and easier to analyze. In this paper, we\npresent Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an\napproximately linear latent-space representation of the underlying system's\ndynamics while retaining essential features for effective policy learning. This\nis achieved through a Koopman-approximation auxiliary network that can be added\nto the baseline policy optimization algorithms without altering the\narchitecture of the core policy or value function. Extensive experimental\nresults demonstrate consistent improvements over the PPO baseline with 6-60%\nincreased performance while reducing variability by up to 91% when evaluated on\nvarious continuous control tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:25:41", "updated": "2025-05-20 16:25:41", "pdf_url": "http://arxiv.org/pdf/2505.14566v1", "comment": "Accepted for IJCAI 2025. This arXiv submission is the full version of\n  the conference paper, including the appendix and supplementary material\n  omitted from the IJCAI proceedings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "abstract": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 16:28:08", "updated": "2025-05-20 16:28:08", "pdf_url": "http://arxiv.org/pdf/2505.14569v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14599v1", "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "abstract": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 16:49:40", "updated": "2025-05-20 16:49:40", "pdf_url": "http://arxiv.org/pdf/2505.14599v1", "comment": "Accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14603v1", "title": "Towards a Foundation Model for Communication Systems", "authors": ["Davide Buffelli", "Sowmen Das", "Yu-Wei Lin", "Sattar Vakili", "Chien-Yi Wang", "Masoud Attarifar", "Pritthijit Nath", "Da-shan Shiu"], "abstract": "Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.", "categories": ["cs.AI", "cs.LG", "eess.SP"], "published": "2025-05-20 16:52:11", "updated": "2025-05-20 16:52:11", "pdf_url": "http://arxiv.org/pdf/2505.14603v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14604v1", "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning", "authors": ["Haoran Zhao", "Yuchen Yan", "Yongliang Shen", "Haolei Xu", "Wenqi Zhang", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.", "categories": ["cs.AI"], "published": "2025-05-20 16:53:40", "updated": "2025-05-20 16:53:40", "pdf_url": "http://arxiv.org/pdf/2505.14604v1", "comment": "Github:https://github.com/CCAI-Lab/Self-Braking-Tuning; Project:\n  https://CCAI-Lab.github.io/SBT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14608v1", "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "abstract": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:55:44", "updated": "2025-05-20 16:55:44", "pdf_url": "http://arxiv.org/pdf/2505.14608v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14615v1", "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", "authors": ["Anjiang Wei", "Yuheng Wu", "Yingjia Wan", "Tarun Suresh", "Huanmi Tan", "Zhanke Zhou", "Sanmi Koyejo", "Ke Wang", "Alex Aiken"], "abstract": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "published": "2025-05-20 17:00:22", "updated": "2025-05-20 17:00:22", "pdf_url": "http://arxiv.org/pdf/2505.14615v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14625v1", "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 17:16:44", "updated": "2025-05-20 17:16:44", "pdf_url": "http://arxiv.org/pdf/2505.14625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14627v1", "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "authors": ["Ashutosh Adhikari", "Mirella Lapata"], "abstract": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:18:17", "updated": "2025-05-20 17:18:17", "pdf_url": "http://arxiv.org/pdf/2505.14627v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14629v1", "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "abstract": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-20 17:19:57", "updated": "2025-05-20 17:19:57", "pdf_url": "http://arxiv.org/pdf/2505.14629v1", "comment": "Accepted at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "published": "2025-05-20 17:24:09", "updated": "2025-05-20 17:24:09", "pdf_url": "http://arxiv.org/pdf/2505.14633v1", "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14646v1", "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation", "authors": ["Anna C. Doris", "Md Ferdous Alam", "Amin Heyrani Nobari", "Faez Ahmed"], "abstract": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 17:34:44", "updated": "2025-05-20 17:34:44", "pdf_url": "http://arxiv.org/pdf/2505.14646v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "abstract": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-20 17:42:34", "updated": "2025-05-20 17:42:34", "pdf_url": "http://arxiv.org/pdf/2505.14654v1", "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14656v1", "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning", "authors": ["Zihao Zhang", "Fei Liu"], "abstract": "While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch.", "categories": ["cs.AI"], "published": "2025-05-20 17:43:33", "updated": "2025-05-20 17:43:33", "pdf_url": "http://arxiv.org/pdf/2505.14656v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14659v1", "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks", "authors": ["Navneet Kaur", "Lav Gupta"], "abstract": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 17:46:09", "updated": "2025-05-20 17:46:09", "pdf_url": "http://arxiv.org/pdf/2505.14659v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14660v1", "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "authors": ["Ronald Seoh", "Dan Goldwasser"], "abstract": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 17:47:04", "updated": "2025-05-20 17:47:04", "pdf_url": "http://arxiv.org/pdf/2505.14660v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14661v1", "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems", "authors": ["Matthew Russo", "Sivaprasad Sudhir", "Gerardo Vitagliano", "Chunwei Liu", "Tim Kraska", "Samuel Madden", "Michael Cafarella"], "abstract": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.", "categories": ["cs.DB", "cs.AI", "H.2.4; I.2.5"], "published": "2025-05-20 17:49:46", "updated": "2025-05-20 17:49:46", "pdf_url": "http://arxiv.org/pdf/2505.14661v1", "comment": "16 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14664v1", "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings", "authors": ["Yilin Ye", "Junchao Huang", "Xingchen Zeng", "Jiazhi Xia", "Wei Zeng"], "abstract": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "published": "2025-05-20 17:52:03", "updated": "2025-05-20 17:52:03", "pdf_url": "http://arxiv.org/pdf/2505.14664v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:54:54", "updated": "2025-05-20 17:54:54", "pdf_url": "http://arxiv.org/pdf/2505.14667v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "categories": ["cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:55:25", "updated": "2025-05-20 17:55:25", "pdf_url": "http://arxiv.org/pdf/2505.14668v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14673v1", "title": "Training-Free Watermarking for Autoregressive Image Generation", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "abstract": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.", "categories": ["cs.CV", "cs.AI", "cs.CR"], "published": "2025-05-20 17:58:02", "updated": "2025-05-20 17:58:02", "pdf_url": "http://arxiv.org/pdf/2505.14673v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "abstract": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:59:13", "updated": "2025-05-20 17:59:13", "pdf_url": "http://arxiv.org/pdf/2505.14680v1", "comment": "SIGIR 2025 Perspective Paper", "doi": "10.1145/3726302.3730353", "journal_ref": null}
{"arxiv_id": "2505.14681v1", "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "published": "2025-05-20 17:59:16", "updated": "2025-05-20 17:59:16", "pdf_url": "http://arxiv.org/pdf/2505.14681v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14684v1", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 17:59:31", "updated": "2025-05-20 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.14684v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13792v1", "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "abstract": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 00:49:19", "updated": "2025-05-20 00:49:19", "pdf_url": "http://arxiv.org/pdf/2505.13792v1", "comment": "10 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13820v1", "title": "Structured Agent Distillation for Large Language Model", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 02:01:55", "updated": "2025-05-20 02:01:55", "pdf_url": "http://arxiv.org/pdf/2505.13820v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "abstract": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 02:27:08", "updated": "2025-05-20 02:27:08", "pdf_url": "http://arxiv.org/pdf/2505.13840v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13844v1", "title": "Improve Language Model and Brain Alignment via Associative Memory", "authors": ["Congchi Yin", "Yongpeng Zhang", "Xuyun Wen", "Piji Li"], "abstract": "Associative memory engages in the integration of relevant information for\ncomprehension in the human cognition system. In this work, we seek to improve\nalignment between language models and human brain while processing speech\ninformation by integrating associative memory. After verifying the alignment\nbetween language model and brain by mapping language model activations to brain\nactivity, the original text stimuli expanded with simulated associative memory\nare regarded as input to computational language models. We find the alignment\nbetween language model and brain is improved in brain regions closely related\nto associative memory processing. We also demonstrate large language models\nafter specific supervised fine-tuning better align with brain response, by\nbuilding the \\textit{Association} dataset containing 1000 samples of stories,\nwith instructions encouraging associative memory as input and associated\ncontent as output.", "categories": ["cs.CL"], "published": "2025-05-20 02:39:09", "updated": "2025-05-20 02:39:09", "pdf_url": "http://arxiv.org/pdf/2505.13844v1", "comment": "Accepted by Findings of ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13847v1", "title": "Forensic deepfake audio detection using segmental speech features", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "abstract": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 02:42:46", "updated": "2025-05-20 02:42:46", "pdf_url": "http://arxiv.org/pdf/2505.13847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13855v1", "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "abstract": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 03:02:05", "updated": "2025-05-20 03:02:05", "pdf_url": "http://arxiv.org/pdf/2505.13855v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13862v1", "title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks", "authors": ["Guobin Shen", "Dongcheng Zhao", "Linghao Feng", "Xiang He", "Jihang Wang", "Sicheng Shen", "Haibo Tong", "Yiting Dong", "Jindong Li", "Xiang Zheng", "Yi Zeng"], "abstract": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 03:14:57", "updated": "2025-05-20 03:14:57", "pdf_url": "http://arxiv.org/pdf/2505.13862v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13866v1", "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning", "authors": ["Jiwon Song", "Dongwon Jo", "Yulhwa Kim", "Jae-Joon Kim"], "abstract": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.", "categories": ["cs.CL"], "published": "2025-05-20 03:21:52", "updated": "2025-05-20 03:21:52", "pdf_url": "http://arxiv.org/pdf/2505.13866v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13878v1", "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models", "authors": ["Yanggan Gu", "Zhaoyi Yan", "Yuanyi Wang", "Yiming Zhang", "Qi Zhou", "Fei Wu", "Hongxia Yang"], "abstract": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 03:32:37", "updated": "2025-05-20 03:32:37", "pdf_url": "http://arxiv.org/pdf/2505.13878v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13886v1", "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning", "authors": ["Jingqi Tong", "Jixin Tang", "Hangcheng Li", "Yurong Mou", "Ming Zhang", "Jun Zhao", "Yanbo Wen", "Fan Song", "Jiahao Zhan", "Yuyang Lu", "Chaoran Tao", "Zhiyuan Guo", "Jizhou Yu", "Tianhao Cheng", "Changhao Jiang", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Weifeng Ge", "Guanhua Chen", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "abstract": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable to produce, challenging for state-of-the-art\nmodels, and diverse with 30 games and 158 tasks. Surprisingly, despite training\nsolely on game data, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33\\% across 7 diverse\nvision-language benchmarks. Our code and dataset are available at\nhttps://github.com/tongjingqi/Code2Logic.", "categories": ["cs.CL", "I.2.7; I.2.10"], "published": "2025-05-20 03:47:44", "updated": "2025-05-20 03:47:44", "pdf_url": "http://arxiv.org/pdf/2505.13886v1", "comment": "49 pages, 19 figures, submitted to NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13887v1", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "abstract": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 03:48:19", "updated": "2025-05-20 03:48:19", "pdf_url": "http://arxiv.org/pdf/2505.13887v1", "comment": "17 pages, 7 figures, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:2502.17110", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13890v1", "title": "Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM", "authors": ["Zhen Xiong", "Yujun Cai", "Zhecheng Li", "Yiwei Wang"], "abstract": "Recent advances in test-time scaling have enabled Large Language Models\n(LLMs) to display sophisticated reasoning abilities via extended\nChain-of-Thought (CoT) generation. Despite their potential, these Reasoning\nLLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as\nperformance degradation under few-shot prompting, that challenge our current\nunderstanding of RLMs. In this work, we introduce a unified graph-based\nanalytical framework for better modeling the reasoning processes of RLMs. Our\nmethod first clusters long, verbose CoT outputs into semantically coherent\nreasoning steps, then constructs directed reasoning graphs to capture\ncontextual and logical dependencies among these steps. Through comprehensive\nanalysis across models and prompting regimes, we reveal that structural\nproperties, such as exploration density, branching, and convergence ratios,\nstrongly correlate with reasoning accuracy. Our findings demonstrate how\nprompting strategies substantially reshape the internal reasoning structure of\nRLMs, directly affecting task outcomes. The proposed framework not only enables\nquantitative evaluation of reasoning quality beyond conventional metrics but\nalso provides practical insights for prompt engineering and the cognitive\nanalysis of LLMs. Code and resources will be released to facilitate future\nresearch in this direction.", "categories": ["cs.CL"], "published": "2025-05-20 03:54:57", "updated": "2025-05-20 03:54:57", "pdf_url": "http://arxiv.org/pdf/2505.13890v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13893v1", "title": "InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion", "authors": ["Yuanyi Wang", "Zhaoyi Yan", "Yiming Zhang", "Qi Zhou", "Yanggan Gu", "Fei Wu", "Hongxia Yang"], "abstract": "Recent advances in large language models (LLMs) have intensified efforts to\nfuse heterogeneous open-source models into a unified system that inherits their\ncomplementary strengths. Existing logit-based fusion methods maintain inference\nefficiency but treat vocabulary dimensions independently, overlooking semantic\ndependencies encoded by cross-dimension interactions. These dependencies\nreflect how token types interact under a model's internal reasoning and are\nessential for aligning models with diverse generation behaviors. To explicitly\nmodel these dependencies, we propose \\textbf{InfiGFusion}, the first\nstructure-aware fusion framework with a novel \\textit{Graph-on-Logits\nDistillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output\nand aggregate their outer products across sequence positions to form a global\nco-activation graph, where nodes represent vocabulary channels and edges\nquantify their joint activations. To ensure scalability and efficiency, we\ndesign a sorting-based closed-form approximation that reduces the original\n$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \\log n)$, with provable\napproximation guarantees. Experiments across multiple fusion settings show that\nGLD consistently improves fusion quality and stability. InfiGFusion outperforms\nSOTA models and fusion baselines across 11 benchmarks spanning reasoning,\ncoding, and mathematics. It shows particular strength in complex reasoning\ntasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal\nJudgement over SFT, demonstrating superior multi-step and relational inference.", "categories": ["cs.CL"], "published": "2025-05-20 03:55:35", "updated": "2025-05-20 03:55:35", "pdf_url": "http://arxiv.org/pdf/2505.13893v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13903v1", "title": "Let's Verify Math Questions Step by Step", "authors": ["Chengyu Shen", "Zhen Hao Wong", "Runming He", "Hao Liang", "Meiyi Qiang", "Zimo Meng", "Zhengyang Zhao", "Bohan Zeng", "Zhengzhou Zhu", "Bin Cui", "Wentao Zhang"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress in\nmathematical reasoning. To enable such capabilities, many existing works\ndistill strong reasoning models into long chains of thought or design\nalgorithms to construct high-quality math QA data for training. However, these\nefforts primarily focus on generating correct reasoning paths and answers,\nwhile largely overlooking the validity of the questions themselves. In this\nwork, we propose Math Question Verification (MathQ-Verify), a novel five-stage\npipeline designed to rigorously filter ill-posed or under-specified math\nproblems. MathQ-Verify first performs format-level validation to remove\nredundant instructions and ensure that each question is syntactically\nwell-formed. It then formalizes each question, decomposes it into atomic\nconditions, and verifies them against mathematical definitions. Next, it\ndetects logical contradictions among these conditions, followed by a\ngoal-oriented completeness check to ensure the question provides sufficient\ninformation for solving. To evaluate this task, we use existing benchmarks\nalong with an additional dataset we construct, containing 2,147 math questions\nwith diverse error types, each manually double-validated. Experiments show that\nMathQ-Verify achieves state-of-the-art performance across multiple benchmarks,\nimproving the F1 score by up to 25 percentage points over the direct\nverification baseline. It further attains approximately 90% precision and 63%\nrecall through a lightweight model voting scheme. MathQ-Verify offers a\nscalable and accurate solution for curating reliable mathematical datasets,\nreducing label noise and avoiding unnecessary computation on invalid questions.\nOur code and data are available at https://github.com/scuuy/MathQ-Verify.", "categories": ["cs.CL"], "published": "2025-05-20 04:07:29", "updated": "2025-05-20 04:07:29", "pdf_url": "http://arxiv.org/pdf/2505.13903v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13908v1", "title": "Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology", "authors": ["Ajitesh Bankula", "Praney Bankula"], "abstract": "Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it\nallows for models trained on resource-rich languages to be applied to\nlow-resource languages more effectively. Recently massively multilingual\npre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot\ntransfer capabilities[14] [13]. This paper investigates cross-linguistic\ntransfer through the lens of language families and morphology. Investigating\nhow language family proximity and morphological similarity affect performance\nacross NLP tasks. We further discuss our results and how it relates to findings\nfrom recent literature. Overall, we compare multilingual model performance and\nreview how linguistic distance metrics correlate with transfer outcomes. We\nalso look into emerging approaches that integrate typological and morphological\ninformation into model pre-training to improve transfer to diverse\nlanguages[18] [19].", "categories": ["cs.CL"], "published": "2025-05-20 04:19:34", "updated": "2025-05-20 04:19:34", "pdf_url": "http://arxiv.org/pdf/2505.13908v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13909v1", "title": "Efficient Agent Training for Computer Use", "authors": ["Yanheng He", "Jiahe Jin", "Pengfei Liu"], "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 04:20:18", "updated": "2025-05-20 04:20:18", "pdf_url": "http://arxiv.org/pdf/2505.13909v1", "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13913v1", "title": "Word length predicts word order: \"Min-max\"-ing drives language evolution", "authors": ["Hiram Ring"], "abstract": "Current theories of language propose an innate (Baker 2001; Chomsky 1981) or\na functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface\nstructures (i.e. word order) that we observe in languages of the world, while\nevolutionary modeling (Dunn et al. 2011) suggests that descent is the primary\nfactor influencing such patterns. Although there are hypotheses for word order\nchange from both innate and usage-based perspectives for specific languages and\nfamilies, there are key disagreements between the two major proposals for\nmechanisms that drive the evolution of language more broadly (Wasow 2002; Levy\n2008). This paper proposes a universal underlying mechanism for word order\nchange based on a large tagged parallel dataset of over 1,500 languages\nrepresenting 133 language families and 111 isolates. Results indicate that word\nclass length is significantly correlated with word order crosslinguistically,\nbut not in a straightforward manner, partially supporting opposing theories of\nprocessing, while at the same time predicting historical word order change in\ntwo different phylogenetic lines and explaining more variance than descent or\nlanguage area in regression models. Such findings suggest an integrated\n\"Min-Max\" theory of language evolution driven by competing pressures of\nprocessing and information structure, aligning with recent efficiency-oriented\n(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et\nal. 2025).", "categories": ["cs.CL"], "published": "2025-05-20 04:25:55", "updated": "2025-05-20 04:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13913v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13936v1", "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "abstract": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:04:15", "updated": "2025-05-20 05:04:15", "pdf_url": "http://arxiv.org/pdf/2505.13936v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13941v1", "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "authors": ["Haoyang Fang", "Boran Han", "Nick Erickson", "Xiyuan Zhang", "Su Zhou", "Anirudh Dagar", "Jiani Zhang", "Ali Caner Turkmen", "Cuixiong Hu", "Huzefa Rangwala", "Ying Nian Wu", "Bernie Wang", "George Karypis"], "abstract": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions.", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 05:20:53", "updated": "2025-05-20 05:20:53", "pdf_url": "http://arxiv.org/pdf/2505.13941v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13944v1", "title": "Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Nam Le", "Linh Ngo Van"], "abstract": "Memory-based approaches have shown strong performance in Continual Relation\nExtraction (CRE). However, storing examples from previous tasks increases\nmemory usage and raises privacy concerns. Recently, prompt-based methods have\nemerged as a promising alternative, as they do not rely on storing past\nsamples. Despite this progress, current prompt-based techniques face several\ncore challenges in CRE, particularly in accurately identifying task identities\nand mitigating catastrophic forgetting. Existing prompt selection strategies\noften suffer from inaccuracies, lack robust mechanisms to prevent forgetting in\nshared parameters, and struggle to handle both cross-task and within-task\nvariations. In this paper, we propose WAVE++, a novel approach inspired by the\nconnection between prefix-tuning and mixture of experts. Specifically, we\nintroduce task-specific prompt pools that enhance flexibility and adaptability\nacross diverse tasks while avoiding boundary-spanning risks; this design more\neffectively captures variations within each task and across tasks. To further\nrefine relation classification, we incorporate label descriptions that provide\nricher, more global context, enabling the model to better distinguish among\ndifferent relations. We also propose a training-free mechanism to improve task\nprediction during inference. Moreover, we integrate a generative model to\nconsolidate prior knowledge within the shared parameters, thereby removing the\nneed for explicit data storage. Extensive experiments demonstrate that WAVE++\noutperforms state-of-the-art prompt-based and rehearsal-based methods, offering\na more robust solution for continual relation extraction. Our code is publicly\navailable at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.", "categories": ["cs.CL"], "published": "2025-05-20 05:22:17", "updated": "2025-05-20 05:22:17", "pdf_url": "http://arxiv.org/pdf/2505.13944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13948v1", "title": "Memory-Centric Embodied Question Answer", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "abstract": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.", "categories": ["cs.CL", "cs.AI", "cs.MM"], "published": "2025-05-20 05:27:57", "updated": "2025-05-20 05:27:57", "pdf_url": "http://arxiv.org/pdf/2505.13948v1", "comment": "14pages, 7 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13949v1", "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "abstract": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:28:21", "updated": "2025-05-20 05:28:21", "pdf_url": "http://arxiv.org/pdf/2505.13949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13957v1", "title": "Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation", "authors": ["Jiankun Zhang", "Shenglai Zeng", "Jie Ren", "Tianqi Zheng", "Hui Liu", "Xianfeng Tang", "Hui Liu", "Yi Chang"], "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by\nintegrating external multimodal databases, but introduce unexplored privacy\nvulnerabilities. While text-based RAG privacy risks have been studied,\nmultimodal data presents unique challenges. We provide the first systematic\nanalysis of MRAG privacy vulnerabilities across vision-language and\nspeech-language modalities. Using a novel compositional structured prompt\nattack in a black-box setting, we demonstrate how attackers can extract private\ninformation by manipulating queries. Our experiments reveal that LMMs can both\ndirectly generate outputs resembling retrieved content and produce descriptions\nthat indirectly expose sensitive information, highlighting the urgent need for\nrobust privacy-preserving MRAG techniques.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 05:37:22", "updated": "2025-05-20 05:37:22", "pdf_url": "http://arxiv.org/pdf/2505.13957v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13963v1", "title": "Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability", "authors": ["Qianli Wang", "Mingyang Wang", "Nils Feldhus", "Simon Ostermann", "Yuan Cao", "Hinrich Sch\u00fctze", "Sebastian M\u00f6ller", "Vera Schmitt"], "abstract": "Quantization methods are widely used to accelerate inference and streamline\nthe deployment of large language models (LLMs). While prior research has\nextensively investigated the degradation of various LLM capabilities due to\nquantization, its effects on model explainability and interpretability, which\nare crucial for understanding decision-making processes, remain unexplored. To\naddress this gap, we conduct comprehensive experiments using three common\nquantization techniques at distinct bit widths, in conjunction with two\nexplainability methods, counterfactual examples and natural language\nexplanations, as well as two interpretability approaches, knowledge\nmemorization analysis and latent multi-hop reasoning analysis. We complement\nour analysis with a thorough user study, evaluating selected explainability\nmethods. Our findings reveal that, depending on the configuration, quantization\ncan significantly impact model explainability and interpretability. Notably,\nthe direction of this effect is not consistent, as it strongly depends on (1)\nthe quantization method, (2) the explainability or interpretability approach,\nand (3) the evaluation protocol. In some settings, human evaluation shows that\nquantization degrades explainability, while in others, it even leads to\nimprovements. Our work serves as a cautionary tale, demonstrating that\nquantization can unpredictably affect model transparency. This insight has\nimportant implications for deploying LLMs in applications where transparency is\na critical requirement.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 06:01:09", "updated": "2025-05-20 06:01:09", "pdf_url": "http://arxiv.org/pdf/2505.13963v1", "comment": "In submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13965v1", "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 06:05:56", "updated": "2025-05-20 06:05:56", "pdf_url": "http://arxiv.org/pdf/2505.13965v1", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13972v1", "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals", "authors": ["Qianli Wang", "Van Bach Nguyen", "Nils Feldhus", "Luis Felipe Villa-Arenas", "Christin Seifert", "Sebastian M\u00f6ller", "Vera Schmitt"], "abstract": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models. Through extensive experiments involving two state-of-the-art\nLLM-based methods, three datasets, five generator models, and 15 judge models,\ncomplemented by a user study (n = 90), we demonstrate that judge models with an\nindependent, non-fine-tuned relationship to the generator model provide the\nmost reliable label flipping evaluations. Relationships between the generator\nand judge models, which are closely aligned with the user study for CDA, result\nin better model performance and robustness. Nevertheless, we find that the gap\nbetween the most effective judge models and the results obtained from the user\nstudy remains considerably large. This suggests that a fully automated pipeline\nfor CDA may be inadequate and requires human intervention.", "categories": ["cs.CL"], "published": "2025-05-20 06:12:17", "updated": "2025-05-20 06:12:17", "pdf_url": "http://arxiv.org/pdf/2505.13972v1", "comment": "in submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13973v1", "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "abstract": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 06:12:20", "updated": "2025-05-20 06:12:20", "pdf_url": "http://arxiv.org/pdf/2505.13973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13975v1", "title": "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models", "authors": ["Yuxuan Jiang", "Dawei Li", "Frank Ferraro"], "abstract": "While Large Reasoning Models (LRMs) have demonstrated success in complex\nreasoning tasks through long chain-of-thought (CoT) reasoning, their inference\noften involves excessively verbose reasoning traces, resulting in substantial\ninefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a\nhybrid framework that combines inference-time pruning with tuning-based\ndistillation, two widely used strategies for efficient reasoning. DRP uses a\nteacher model to perform skill-aware step decomposition and content pruning,\nand then distills the pruned reasoning paths into a student model, enabling it\nto reason both efficiently and accurately. Across several challenging\nmathematical reasoning datasets, we find that models trained with DRP achieve\nsubstantial improvements in token efficiency without sacrificing accuracy.\nSpecifically, DRP reduces average token usage on GSM8K from 917 to 328 while\nimproving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on\nAIME with no performance drop. Further analysis shows that aligning the\nreasoning structure of training CoTs with the student's reasoning capacity is\ncritical for effective knowledge transfer and performance gains.", "categories": ["cs.CL"], "published": "2025-05-20 06:15:15", "updated": "2025-05-20 06:15:15", "pdf_url": "http://arxiv.org/pdf/2505.13975v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13979v1", "title": "Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection", "authors": ["Maya Srikanth", "Run Chen", "Julia Hirschberg"], "abstract": "Multimodal models play a key role in empathy detection, but their performance\ncan suffer when modalities provide conflicting cues. To understand these\nfailures, we examine cases where unimodal and multimodal predictions diverge.\nUsing fine-tuned models for text, audio, and video, along with a gated fusion\nmodel, we find that such disagreements often reflect underlying ambiguity, as\nevidenced by annotator uncertainty. Our analysis shows that dominant signals in\none modality can mislead fusion when unsupported by others. We also observe\nthat humans, like models, do not consistently benefit from multimodal input.\nThese insights position disagreement as a useful diagnostic signal for\nidentifying challenging examples and improving empathy system robustness.", "categories": ["cs.CL"], "published": "2025-05-20 06:25:02", "updated": "2025-05-20 06:25:02", "pdf_url": "http://arxiv.org/pdf/2505.13979v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13988v1", "title": "The Hallucination Tax of Reinforcement Finetuning", "authors": ["Linxin Song", "Taiwei Shi", "Jieyu Zhao"], "abstract": "Reinforcement finetuning (RFT) has become a standard approach for enhancing\nthe reasoning capabilities of large language models (LLMs). However, its impact\non model trustworthiness remains underexplored. In this work, we identify and\nsystematically study a critical side effect of RFT, which we term the\nhallucination tax: a degradation in refusal behavior causing models to produce\nhallucinated answers to unanswerable questions confidently. To investigate\nthis, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of\nunanswerable math problems designed to probe models' ability to recognize an\nunanswerable question by reasoning from the insufficient or ambiguous\ninformation. Our results show that standard RFT training could reduce model\nrefusal rates by more than 80%, which significantly increases model's tendency\nto hallucinate. We further demonstrate that incorporating just 10% SUM during\nRFT substantially restores appropriate refusal behavior, with minimal accuracy\ntrade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage\ninference-time compute to reason about their own uncertainty and knowledge\nboundaries, improving generalization not only to out-of-domain math problems\nbut also to factual question answering tasks.", "categories": ["cs.CL"], "published": "2025-05-20 06:36:45", "updated": "2025-05-20 06:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13988v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13990v1", "title": "DecIF: Improving Instruction-Following through Meta-Decomposition", "authors": ["Tingfeng Hui", "Pengyu Zhu", "Bowen Ping", "Ling Tang", "Yaqi Zhang", "Sen Su"], "abstract": "Instruction-following has emerged as a crucial capability for large language\nmodels (LLMs). However, existing approaches often rely on pre-existing\ndocuments or external resources to synthesize instruction-following data, which\nlimits their flexibility and generalizability. In this paper, we introduce\nDecIF, a fully autonomous, meta-decomposition guided framework that generates\ndiverse and high-quality instruction-following data using only LLMs. DecIF is\ngrounded in the principle of decomposition. For instruction generation, we\nguide LLMs to iteratively produce various types of meta-information, which are\nthen combined with response constraints to form well-structured and\nsemantically rich instructions. We further utilize LLMs to detect and resolve\npotential inconsistencies within the generated instructions. Regarding response\ngeneration, we decompose each instruction into atomic-level evaluation\ncriteria, enabling rigorous validation and the elimination of inaccurate\ninstruction-response pairs. Extensive experiments across a wide range of\nscenarios and settings demonstrate DecIF's superior performance on\ninstruction-following tasks. Further analysis highlights its strong\nflexibility, scalability, and generalizability in automatically synthesizing\nhigh-quality instruction data.", "categories": ["cs.CL"], "published": "2025-05-20 06:38:28", "updated": "2025-05-20 06:38:28", "pdf_url": "http://arxiv.org/pdf/2505.13990v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13995v1", "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "abstract": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-20 06:45:17", "updated": "2025-05-20 06:45:17", "pdf_url": "http://arxiv.org/pdf/2505.13995v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14009v1", "title": "Activation-Guided Consensus Merging for Large Language Models", "authors": ["Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Qintong Li", "Mingyang Liu", "Xiongwei Han", "Zhijiang Guo", "Han Wu", "Linqi Song"], "abstract": "Recent research has increasingly focused on reconciling the reasoning\ncapabilities of System 2 with the efficiency of System 1. While existing\ntraining-based and prompt-based approaches face significant challenges in terms\nof efficiency and stability, model merging emerges as a promising strategy to\nintegrate the diverse capabilities of different Large Language Models (LLMs)\ninto a unified model. However, conventional model merging methods often assume\nuniform importance across layers, overlooking the functional heterogeneity\ninherent in neural components. To address this limitation, we propose\n\\textbf{A}ctivation-Guided \\textbf{C}onsensus \\textbf{M}erging (\\textbf{ACM}),\na plug-and-play merging framework that determines layer-specific merging\ncoefficients based on mutual information between activations of pre-trained and\nfine-tuned models. ACM effectively preserves task-specific capabilities without\nrequiring gradient computations or additional training. Extensive experiments\non Long-to-Short (L2S) and general merging tasks demonstrate that ACM\nconsistently outperforms all baseline methods. For instance, in the case of\nQwen-7B models, TIES-Merging equipped with ACM achieves a \\textbf{55.3\\%}\nreduction in response length while simultaneously improving reasoning accuracy\nby \\textbf{1.3} points. We submit the code with the paper for reproducibility,\nand it will be publicly available.", "categories": ["cs.CL"], "published": "2025-05-20 07:04:01", "updated": "2025-05-20 07:04:01", "pdf_url": "http://arxiv.org/pdf/2505.14009v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14015v1", "title": "AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation", "authors": ["Tai D. Nguyen", "Long H. Pham", "Jun Sun"], "abstract": "The rapid advancement of domain-specific large language models (LLMs) in\nfields like law necessitates frameworks that account for nuanced regional legal\ndistinctions, which are critical for ensuring compliance and trustworthiness.\nExisting legal evaluation benchmarks often lack adaptability and fail to\naddress diverse local contexts, limiting their utility in dynamically evolving\nregulatory landscapes. To address these gaps, we propose AutoLaw, a novel\nviolation detection framework that combines adversarial data generation with a\njury-inspired deliberation process to enhance legal compliance of LLMs. Unlike\nstatic approaches, AutoLaw dynamically synthesizes case law to reflect local\nregulations and employs a pool of LLM-based \"jurors\" to simulate judicial\ndecision-making. Jurors are ranked and selected based on synthesized legal\nexpertise, enabling a deliberation process that minimizes bias and improves\ndetection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG\n(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:\nadversarial data generation improves LLM discrimination, while the jury-based\nvoting strategy significantly boosts violation detection rates. Our results\nhighlight the framework's ability to adaptively probe legal misalignments and\ndeliver reliable, context-aware judgments, offering a scalable solution for\nevaluating and enhancing LLMs in legally sensitive applications.", "categories": ["cs.CL"], "published": "2025-05-20 07:09:13", "updated": "2025-05-20 07:09:13", "pdf_url": "http://arxiv.org/pdf/2505.14015v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14038v1", "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data", "authors": ["Xinzhe Zheng", "Sijie Ji", "Jiawei Sun", "Renqi Chen", "Wei Gao", "Mani Srivastava"], "abstract": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 07:36:28", "updated": "2025-05-20 07:36:28", "pdf_url": "http://arxiv.org/pdf/2505.14038v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14045v1", "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 07:43:45", "updated": "2025-05-20 07:43:45", "pdf_url": "http://arxiv.org/pdf/2505.14045v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14052v1", "title": "Improved Methods for Model Pruning and Knowledge Distillation", "authors": ["Wei Jiang", "Anying Fu", "Youling Zhang"], "abstract": "Model pruning is a performance optimization technique for large language\nmodels like R1 or o3-mini. However, existing pruning methods often lead to\nsignificant performance degradation or require extensive retraining and\nfine-tuning. This technique aims to identify and remove neurons, connections\nunlikely leading to the contribution during the human-computer interaction\nphase. Our goal is to obtain a much smaller and faster knowledge distilled\nmodel that can quickly generate content almost as good as those of the unpruned\nones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an\nimproved pruning method that effectively reduces model size and computational\ncomplexity while maintaining performance comparable to the original unpruned\nmodel even at extreme pruned levels. The improved method is based on weights,\nbias fixed in the pre-training phase and GRPO rewards verified during the\npost-training phase as our novel pruning indicators. Preliminary experimental\nresults show that our method outperforms and be comparable to state-of-the-art\nmethods across various pruning levels and different downstream computational\nlinguistics tasks.", "categories": ["cs.CL", "cs.CE"], "published": "2025-05-20 07:53:40", "updated": "2025-05-20 07:53:40", "pdf_url": "http://arxiv.org/pdf/2505.14052v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14070v1", "title": "Enhancing LLMs via High-Knowledge Data Selection", "authors": ["Feiyu Duan", "Xuemiao Zhang", "Sirui Wang", "Haoran Que", "Yuqi Liu", "Wenge Rong", "Xunliang Cai"], "abstract": "The performance of Large Language Models (LLMs) is intrinsically linked to\nthe quality of its training data. Although several studies have proposed\nmethods for high-quality data selection, they do not consider the importance of\nknowledge richness in text corpora. In this paper, we propose a novel and\ngradient-free High-Knowledge Scorer (HKS) to select high-quality data from the\ndimension of knowledge, to alleviate the problem of knowledge scarcity in the\npre-trained corpus. We propose a comprehensive multi-domain knowledge element\npool and introduce knowledge density and coverage as metrics to assess the\nknowledge content of the text. Based on this, we propose a comprehensive\nknowledge scorer to select data with intensive knowledge, which can also be\nutilized for domain-specific high-knowledge data selection by restricting\nknowledge elements to the specific domain. We train models on a high-knowledge\nbilingual dataset, and experimental results demonstrate that our scorer\nimproves the model's performance in knowledge-intensive and general\ncomprehension tasks, and is effective in enhancing both the generic and\ndomain-specific capabilities of the model.", "categories": ["cs.CL"], "published": "2025-05-20 08:21:37", "updated": "2025-05-20 08:21:37", "pdf_url": "http://arxiv.org/pdf/2505.14070v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14071v1", "title": "Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models", "authors": ["Woody Haosheng Gan", "Deqing Fu", "Julian Asilis", "Ollie Liu", "Dani Yogatama", "Vatsal Sharan", "Robin Jia", "Willie Neiswanger"], "abstract": "Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-20 08:23:08", "updated": "2025-05-20 08:23:08", "pdf_url": "http://arxiv.org/pdf/2505.14071v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14079v1", "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "abstract": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules.", "categories": ["cs.CL"], "published": "2025-05-20 08:35:35", "updated": "2025-05-20 08:35:35", "pdf_url": "http://arxiv.org/pdf/2505.14079v1", "comment": null, "doi": null, "journal_ref": "ACL 2025"}
{"arxiv_id": "2505.14080v1", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "abstract": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "published": "2025-05-20 08:36:47", "updated": "2025-05-20 08:36:47", "pdf_url": "http://arxiv.org/pdf/2505.14080v1", "comment": null, "doi": "10.1145/3715275.3732112", "journal_ref": "FAccT '25: Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency"}
{"arxiv_id": "2505.14099v1", "title": "Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering", "authors": ["Yihua Zhu", "Qianying Liu", "Akiko Aizawa", "Hidetoshi Shimodaira"], "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions using structured knowledge from KBs. While LLM-only approaches offer\ngeneralization, they suffer from outdated knowledge, hallucinations, and lack\nof transparency. Chain-based KG-RAG methods address these issues by\nincorporating external KBs, but are limited to simple chain-structured\nquestions due to the absence of planning and logical structuring. Inspired by\nsemantic parsing methods, we propose PDRR: a four-stage framework consisting of\nPredict, Decompose, Retrieve, and Reason. Our method first predicts the\nquestion type and decomposes the question into structured triples. Then\nretrieves relevant information from KBs and guides the LLM as an agent to\nreason over and complete the decomposed triples. Experimental results\ndemonstrate that PDRR consistently outperforms existing methods across various\nLLM backbones and achieves superior performance on both chain-structured and\nnon-chain complex questions.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-20 09:01:52", "updated": "2025-05-20 09:01:52", "pdf_url": "http://arxiv.org/pdf/2505.14099v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14101v1", "title": "MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations", "authors": ["Ernests Lavrinovics", "Russa Biswas", "Katja Hose", "Johannes Bjerva"], "abstract": "Large Language Models (LLMs) have inherent limitations of faithfulness and\nfactuality, commonly referred to as hallucinations. Several benchmarks have\nbeen developed that provide a test bed for factuality evaluation within the\ncontext of English-centric datasets, while relying on supplementary informative\ncontext like web links or text passages but ignoring the available structured\nfactual resources. To this end, Knowledge Graphs (KGs) have been identified as\na useful aid for hallucination mitigation, as they provide a structured way to\nrepresent the facts about entities and their relations with minimal linguistic\noverhead. We bridge the lack of KG paths and multilinguality for factual\nlanguage modeling within the existing hallucination evaluation benchmarks and\npropose a KG-based multilingual, multihop benchmark called \\textbf{MultiHal}\nframed for generative text evaluation. As part of our data collection pipeline,\nwe mined 140k KG-paths from open-domain KGs, from which we pruned noisy\nKG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation\nshows an absolute scale increase by approximately 0.12 to 0.36 points for the\nsemantic similarity score in KG-RAG over vanilla QA across multiple languages\nand multiple models, demonstrating the potential of KG integration. We\nanticipate MultiHal will foster future research towards several graph-based\nhallucination mitigation and fact-checking tasks.", "categories": ["cs.CL"], "published": "2025-05-20 09:03:35", "updated": "2025-05-20 09:03:35", "pdf_url": "http://arxiv.org/pdf/2505.14101v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14104v1", "title": "Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents", "authors": ["Wei Fan", "Tianshi Zheng", "Yiran Hu", "Zheye Deng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Haoran Li", "Weixing Shen", "Yangqiu Song"], "abstract": "Legal rules encompass not only codified statutes but also implicit\nadjudicatory principles derived from precedents that contain discretionary\nnorms, social morality, and policy. While computational legal research has\nadvanced in applying established rules to cases, inducing legal rules from\njudicial decisions remains understudied, constrained by limitations in model\ninference efficacy and symbolic reasoning capability. The advent of Large\nLanguage Models (LLMs) offers unprecedented opportunities for automating the\nextraction of such latent principles, yet progress is stymied by the absence of\nformal task definitions, benchmark datasets, and methodologies. To address this\ngap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,\ngeneralizable doctrinal rules from sets of analogous precedents, distilling\ntheir shared preconditions, normative behaviors, and legal consequences. We\nintroduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese\ncases in total) for model tuning and 216 expert-annotated gold test sets.\nExperimental results reveal that: 1) State-of-the-art LLMs struggle with\nover-generalization and hallucination; 2) Training on our dataset markedly\nenhances LLMs capabilities in capturing nuanced rule patterns across similar\ncases.", "categories": ["cs.CL"], "published": "2025-05-20 09:10:52", "updated": "2025-05-20 09:10:52", "pdf_url": "http://arxiv.org/pdf/2505.14104v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14106v1", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "abstract": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:13:22", "updated": "2025-05-20 09:13:22", "pdf_url": "http://arxiv.org/pdf/2505.14106v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14107v1", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "abstract": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:14:53", "updated": "2025-05-20 09:14:53", "pdf_url": "http://arxiv.org/pdf/2505.14107v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14112v1", "title": "Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking", "authors": ["Tianle Gu", "Zongqi Wang", "Kexin Huang", "Yuanqi Yao", "Xiangliang Zhang", "Yujiu Yang", "Xiuying Chen"], "abstract": "Logit-based LLM watermarking traces and verifies AI-generated content by\nmaintaining green and red token lists and increasing the likelihood of green\ntokens during generation. However, it fails in low-entropy scenarios, where\npredictable outputs make green token selection difficult without disrupting\nnatural text flow. Existing approaches address this by assuming access to the\noriginal LLM to calculate entropy and selectively watermark high-entropy\ntokens. However, these methods face two major challenges: (1) high\ncomputational costs and detection delays due to reliance on the original LLM,\nand (2) potential risks of model leakage. To address these limitations, we\npropose Invisible Entropy (IE), a watermarking paradigm designed to enhance\nboth safety and efficiency. Instead of relying on the original LLM, IE\nintroduces a lightweight feature extractor and an entropy tagger to predict\nwhether the entropy of the next token is high or low. Furthermore, based on\ntheoretical analysis, we develop a threshold navigator that adaptively sets\nentropy thresholds. It identifies a threshold where the watermark ratio\ndecreases as the green token count increases, enhancing the naturalness of the\nwatermarked text and improving detection robustness. Experiments on HumanEval\nand MBPP datasets demonstrate that IE reduces parameter size by 99\\% while\nachieving performance on par with state-of-the-art methods. Our work introduces\na safe and efficient paradigm for low-entropy watermarking.\nhttps://github.com/Carol-gutianle/IE\nhttps://huggingface.co/datasets/Carol0110/IE-Tagger", "categories": ["cs.CL", "cs.CR"], "published": "2025-05-20 09:19:06", "updated": "2025-05-20 09:19:06", "pdf_url": "http://arxiv.org/pdf/2505.14112v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14116v1", "title": "Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst", "authors": ["Hongru Wang", "Deng Cai", "Wanjun Zhong", "Shijue Huang", "Jeff Z. Pan", "Zeming Liu", "Kam-Fai Wong"], "abstract": "Inference-time scaling has attracted much attention which significantly\nenhance the performance of Large Language Models (LLMs) in complex reasoning\ntasks by increasing the length of Chain-of-Thought. These longer intermediate\nreasoning rationales embody various meta-reasoning skills in human cognition,\nsuch as reflection and decomposition, being difficult to create and acquire. In\nthis work, we introduce \\textit{Self-Reasoning Language Model} (SRLM), where\nthe model itself can synthesize longer CoT data and iteratively improve\nperformance through self-training. By incorporating a few demonstration\nexamples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from\nexisting responses, which act as a reasoning catalyst, we demonstrate that SRLM\nnot only enhances the model's initial performance but also ensures more stable\nand consistent improvements in subsequent iterations. Our proposed SRLM\nachieves an average absolute improvement of more than $+2.5$ points across five\nreasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.\nMoreover, it brings more improvements with more times of sampling during\ninference, such as absolute $+7.89$ average improvement with $64$ sampling\ntimes, revealing the in-depth, diverse and creative reasoning paths in SRLM\nagainst the strong baseline.", "categories": ["cs.CL"], "published": "2025-05-20 09:21:26", "updated": "2025-05-20 09:21:26", "pdf_url": "http://arxiv.org/pdf/2505.14116v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14130v1", "title": "Probing BERT for German Compound Semantics", "authors": ["Filip Mileti\u0107", "Aaron Schmid", "Sabine Schulte im Walde"], "abstract": "This paper investigates the extent to which pretrained German BERT encodes\nknowledge of noun compound semantics. We comprehensively vary combinations of\ntarget tokens, layers, and cased vs. uncased models, and evaluate them by\npredicting the compositionality of 868 gold standard compounds. Looking at\nrepresentational patterns within the transformer architecture, we observe\ntrends comparable to equivalent prior work on English, with compositionality\ninformation most easily recoverable in the early layers. However, our strongest\nresults clearly lag behind those reported for English, suggesting an inherently\nmore difficult task in German. This may be due to the higher productivity of\ncompounding in German than in English and the associated increase in\nconstituent-level ambiguity, including in our target compound set.", "categories": ["cs.CL"], "published": "2025-05-20 09:36:17", "updated": "2025-05-20 09:36:17", "pdf_url": "http://arxiv.org/pdf/2505.14130v1", "comment": "Accepted to SwissText 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14131v1", "title": "Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "abstract": "In table question answering (TQA), tables are encoded as either texts or\nimages. Prior work suggests that passing images of tables to multi-modal large\nlanguage models (MLLMs) performs comparably to or even better than using\ntextual input with large language models (LLMs). However, the lack of\ncontrolled setups limits fine-grained distinctions between these approaches. In\nthis paper, we conduct the first controlled study on the effectiveness of\nseveral combinations of table representations and models from two perspectives:\nquestion complexity and table size. We build a new benchmark based on existing\nTQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we\nfind that the best combination of table representation and model varies across\nsetups. We propose FRES, a method selecting table representations dynamically,\nand observe a 10% average performance improvement compared to using both\nrepresentations indiscriminately.", "categories": ["cs.CL"], "published": "2025-05-20 09:36:17", "updated": "2025-05-20 09:36:17", "pdf_url": "http://arxiv.org/pdf/2505.14131v1", "comment": "Accepted at ACL25 (Findings)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14146v1", "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "abstract": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 09:53:56", "updated": "2025-05-20 09:53:56", "pdf_url": "http://arxiv.org/pdf/2505.14146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14149v1", "title": "Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information", "authors": ["Chengzhi Zhang", "Xinyi Yan", "Lei Zhao", "Yingyi Zhang"], "abstract": "The exponential increase in academic papers has significantly increased the\ntime required for researchers to access relevant literature. Keyphrase\nExtraction (KPE) offers a solution to this situation by enabling researchers to\nefficiently retrieve relevant literature. The current study on KPE from\nacademic articles aims to improve the performance of extraction models through\ninnovative approaches using Title and Abstract as input corpora. However, the\nsemantic richness of keywords is significantly constrained by the length of the\nabstract. While full-text-based KPE can address this issue, it simultaneously\nintroduces noise, which significantly diminishes KPE performance. To address\nthis issue, this paper utilized the structural features and section texts\nobtained from the section structure information of academic articles to extract\nkeyphrase from academic papers. The approach consists of two main parts: (1)\nexploring the effect of seven structural features on KPE models, and (2)\nintegrating the extraction results from all section texts used as input corpora\nfor KPE models via a keyphrase integration algorithm to obtain the keyphrase\nintegration result. Furthermore, this paper also examined the effect of the\nclassification quality of section structure on the KPE performance. The results\nshow that incorporating structural features improves KPE performance, though\ndifferent features have varying effects on model efficacy. The keyphrase\nintegration approach yields the best performance, and the classification\nquality of section structure can affect KPE performance. These findings\nindicate that using the section structure information of academic articles\ncontributes to effective KPE from academic articles. The code and dataset\nsupporting this study are available at https://github.com/yan-xinyi/SSB_KPE.", "categories": ["cs.CL", "cs.DL", "cs.IR"], "published": "2025-05-20 09:57:34", "updated": "2025-05-20 09:57:34", "pdf_url": "http://arxiv.org/pdf/2505.14149v1", "comment": null, "doi": "10.1007/s11192-025-05286-2", "journal_ref": "Scientometrics, 2025"}
{"arxiv_id": "2505.14157v1", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "abstract": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:05:11", "updated": "2025-05-20 10:05:11", "pdf_url": "http://arxiv.org/pdf/2505.14157v1", "comment": "25 pages, 42 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14158v1", "title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering", "authors": ["Sanjay Govindan", "Maurice Pagnucco", "Yang Song"], "abstract": "Large Language Models (LLMs) are trained on diverse and often conflicting\nknowledge spanning multiple domains and time periods. Some of this knowledge is\nonly valid within specific temporal contexts, such as answering the question,\n\"Who is the President of the United States in 2022?\" Ensuring LLMs generate\ntime appropriate responses is crucial for maintaining relevance and accuracy.\nIn this work we explore activation engineering as a method for temporally\naligning LLMs to improve factual recall without any training or dataset\ncreation. In this research we explore an activation engineering technique to\nground three versions of LLaMA 2 to specific points in time and examine the\neffects of varying injection layers and prompting strategies. Our experiments\ndemonstrate up to a 44% and 16% improvement in relative and explicit prompting\nrespectively, achieving comparable performance to the fine-tuning method\nproposed by Zhao et al. (2024) . Notably, our approach achieves similar results\nto the fine-tuning baseline while being significantly more computationally\nefficient and requiring no pre-aligned datasets.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:09:40", "updated": "2025-05-20 10:09:40", "pdf_url": "http://arxiv.org/pdf/2505.14158v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14160v1", "title": "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models", "authors": ["Zahraa Al Sahili", "Ioannis Patras", "Matthew Purver"], "abstract": "Multilingual vision-language models promise universal image-text retrieval,\nyet their social biases remain under-explored. We present the first systematic\naudit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and\nCAPIVARA-CLIP -- across ten languages that vary in resource availability and\ngrammatical gender. Using balanced subsets of \\textsc{FairFace} and the\n\\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and\ngender bias and measure stereotype amplification. Contrary to the assumption\nthat multilinguality mitigates bias, every model exhibits stronger gender bias\nthan its English-only baseline. CAPIVARA-CLIP shows its largest biases\nprecisely in the low-resource languages it targets, while the shared\ncross-lingual encoder of NLLB-CLIP transports English gender stereotypes into\ngender-neutral languages; loosely coupled encoders largely avoid this transfer.\nHighly gendered languages consistently magnify all measured bias types, but\neven gender-neutral languages remain vulnerable when cross-lingual weight\nsharing imports foreign stereotypes. Aggregated metrics conceal\nlanguage-specific ``hot spots,'' underscoring the need for fine-grained,\nlanguage-aware bias evaluation in future multilingual vision-language research.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:14:00", "updated": "2025-05-20 10:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14160v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14165v1", "title": "PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore", "authors": ["Zhenkai Qin", "Jiajing He", "Qiao Fang"], "abstract": "Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity\ntoward specific aspects within a text, enabling more precise opinion mining in\ndomains such as product reviews and social media. However, traditional FGSA\napproaches often require task-specific architectures and extensive annotated\ndata, limiting their generalization and scalability. To address these\nchallenges, we propose PL-FGSA, a unified prompt learning-based framework\nimplemented using the MindSpore platform, which integrates prompt design with a\nlightweight TextCNN backbone. Our method reformulates FGSA as a multi-task\nprompt-augmented generation problem, jointly tackling aspect extraction,\nsentiment classification, and causal explanation in a unified paradigm. By\nleveraging prompt-based guidance, PL-FGSA enhances interpretability and\nachieves strong performance under both full-data and low-resource conditions.\nExperiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and\nMAMS-demonstrate that our model consistently outperforms traditional\nfine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,\nrespectively. These results validate the effectiveness of prompt-based\ngeneralization and highlight the practical value of PL-FGSA for real-world\nsentiment analysis tasks.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:18:10", "updated": "2025-05-20 10:18:10", "pdf_url": "http://arxiv.org/pdf/2505.14165v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14172v1", "title": "The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models", "authors": ["Adrian Cosma", "Stefan Ruseti", "Emilian Radoi", "Mihai Dascalu"], "abstract": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge slowly, suddenly, and only late in training. We\nfurther show that percolation-based models of concept emergence explain these\npatterns, suggesting that learning character composition is not fundamentally\ndifferent from learning commonsense knowledge. To address this bottleneck, we\npropose a lightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available.", "categories": ["cs.CL"], "published": "2025-05-20 10:25:17", "updated": "2025-05-20 10:25:17", "pdf_url": "http://arxiv.org/pdf/2505.14172v1", "comment": "1 Table, 8 Figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14173v1", "title": "THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jie Zhou"], "abstract": "The sparse Mixture-of-Experts (MoE) has achieved significant progress for\nneural machine translation (NMT). However, there exist two limitations in\ncurrent MoE solutions which may lead to sub-optimal performance: 1) they\ndirectly use the task knowledge of NMT into MoE (\\emph{e.g.},\ndomain/linguistics-specific knowledge), which are generally unavailable at\npractical application and neglect the naturally grouped domain/linguistic\nproperties; 2) the expert selection only depends on the localized token\nrepresentation without considering the context, which fully grasps the state of\neach token in a global view. To address the above limitations, we propose\nTHOR-MoE via arming the MoE with hierarchical task-guided and\ncontext-responsive routing policies. Specifically, it 1) firstly predicts the\ndomain/language label and then extracts mixed domain/language representation to\nallocate task-level experts in a hierarchical manner; 2) injects the context\ninformation to enhance the token routing from the pre-selected task-level\nexperts set, which can help each token to be accurately routed to more\nspecialized and suitable experts. Extensive experiments on multi-domain\ntranslation and multilingual translation benchmarks with different\narchitectures consistently demonstrate the superior performance of THOR-MoE.\nAdditionally, the THOR-MoE operates as a plug-and-play module compatible with\nexisting Top-$k$~\\cite{shazeer2017} and Top-$p$~\\cite{huang-etal-2024-harder}\nrouting schemes, ensuring broad applicability across diverse MoE architectures.\nFor instance, compared with vanilla Top-$p$~\\cite{huang-etal-2024-harder}\nrouting, the context-aware manner can achieve an average improvement of 0.75\nBLEU with less than 22\\% activated parameters on multi-domain translation\ntasks.", "categories": ["cs.CL"], "published": "2025-05-20 10:27:19", "updated": "2025-05-20 10:27:19", "pdf_url": "http://arxiv.org/pdf/2505.14173v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14174v1", "title": "Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning", "authors": ["Yusuf Denizay D\u00f6nder", "Derek Hommel", "Andrea W Wen-Yi", "David Mimno", "Unso Eun Seo Jo"], "abstract": "LLMs are effective at code generation tasks like text-to-SQL, but is it worth\nthe cost? Many state-of-the-art approaches use non-task-specific LLM techniques\nincluding Chain-of-Thought (CoT), self-consistency, and fine-tuning. These\nmethods can be costly at inference time, sometimes requiring over a hundred LLM\ncalls with reasoning, incurring average costs of up to \\$0.46 per query, while\nfine-tuning models can cost thousands of dollars. We introduce \"N-rep\"\nconsistency, a more cost-efficient text-to-SQL approach that achieves similar\nBIRD benchmark scores as other more expensive methods, at only \\$0.039 per\nquery. N-rep leverages multiple representations of the same schema input to\nmitigate weaknesses in any single representation, making the solution more\nrobust and allowing the use of smaller and cheaper models without any reasoning\nor fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL\napproach in its cost range.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:28:46", "updated": "2025-05-20 10:28:46", "pdf_url": "http://arxiv.org/pdf/2505.14174v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14178v1", "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "abstract": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:32:30", "updated": "2025-05-20 10:32:30", "pdf_url": "http://arxiv.org/pdf/2505.14178v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14179v1", "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "abstract": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-20 10:34:45", "updated": "2025-05-20 10:34:45", "pdf_url": "http://arxiv.org/pdf/2505.14179v1", "comment": null, "doi": "10.1016/j.eswa.2024.125529", "journal_ref": "Expert Systems with Applications, 2025"}
{"arxiv_id": "2505.14181v1", "title": "SlangDIT: Benchmarking LLMs in Interpretative Slang Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jiaan Wang", "Jie Zhou"], "abstract": "The challenge of slang translation lies in capturing context-dependent\nsemantic extensions, as slang terms often convey meanings beyond their literal\ninterpretation. While slang detection, explanation, and translation have been\nstudied as isolated tasks in the era of large language models (LLMs), their\nintrinsic interdependence remains underexplored. The main reason is lacking of\na benchmark where the two tasks can be a prerequisite for the third one, which\ncan facilitate idiomatic translation. In this paper, we introduce the\ninterpretative slang translation task (named SlangDIT) consisting of three\nsub-tasks: slang detection, cross-lingual slang explanation, and slang\ntranslation within the current context, aiming to generate more accurate\ntranslation with the help of slang detection and slang explanation. To this\nend, we construct a SlangDIT dataset, containing over 25k English-Chinese\nsentence pairs. Each source sentence mentions at least one slang term and is\nlabeled with corresponding cross-lingual slang explanation. Based on the\nbenchmark, we propose a deep thinking model, named SlangOWL. It firstly\nidentifies whether the sentence contains a slang, and then judges whether the\nslang is polysemous and analyze its possible meaning. Further, the SlangOWL\nprovides the best explanation of the slang term targeting on the current\ncontext. Finally, according to the whole thought, the SlangOWL offers a\nsuitable translation. Our experiments on LLMs (\\emph{e.g.}, Qwen2.5 and\nLLama-3.1), show that our deep thinking approach indeed enhances the\nperformance of LLMs where the proposed SLangOWL significantly surpasses the\nvanilla models and supervised fine-tuned models without thinking.", "categories": ["cs.CL"], "published": "2025-05-20 10:37:34", "updated": "2025-05-20 10:37:34", "pdf_url": "http://arxiv.org/pdf/2505.14181v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14183v1", "title": "ThinkSwitcher: When to Think Hard, When to Think Fast", "authors": ["Guosheng Liang", "Longguang Zhong", "Ziyi Yang", "Xiaojun Quan"], "abstract": "Large reasoning models (LRMs) excel at solving complex tasks by leveraging\nlong chain-of-thought (CoT) reasoning. However, this often leads to\noverthinking on simple tasks, resulting in unnecessary computational overhead.\nWe observe that LRMs inherently possess the capability for efficient short CoT\nreasoning, which can be reliably elicited through prompt design. To leverage\nthis capability, we propose ThinkSwitcher, a framework that enables a single\nLRM to dynamically switch between short and long CoT modes based on task\ncomplexity. ThinkSwitcher introduces a lightweight switching module trained\nwith supervision signals derived from the relative performance of each\nreasoning mode across tasks. Experiments on multiple reasoning benchmarks show\nthat ThinkSwitcher reduces computational cost by 20-30% while maintaining high\naccuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher\nas a scalable and efficient solution for unified LRM deployment.", "categories": ["cs.CL"], "published": "2025-05-20 10:40:41", "updated": "2025-05-20 10:40:41", "pdf_url": "http://arxiv.org/pdf/2505.14183v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14185v1", "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 10:41:49", "updated": "2025-05-20 10:41:49", "pdf_url": "http://arxiv.org/pdf/2505.14185v1", "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14195v1", "title": "Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification", "authors": ["Tuc Nguyen", "Yifan Hu", "Thai Le"], "abstract": "Recent advancements in large language models (LLMs) have been fueled by large\nscale training corpora drawn from diverse sources such as websites, news\narticles, and books. These datasets often contain explicit user information,\nsuch as person names and addresses, that LLMs may unintentionally reproduce in\ntheir generated outputs. Beyond such explicit content, LLMs can also leak\nidentity revealing cues through implicit signals such as distinctive writing\nstyles, raising significant concerns about authorship privacy. There are three\nmajor automated tasks in authorship privacy, namely authorship obfuscation\n(AO), authorship mimicking (AM), and authorship verification (AV). Prior\nresearch has studied AO, AM, and AV independently. However, their interplays\nremain under explored, which leaves a major research gap, especially in the era\nof LLMs, where they are profoundly shaping how we curate and share user\ngenerated content, and the distinction between machine generated and human\nauthored text is also increasingly blurred. This work then presents the first\nunified framework for analyzing the dynamic relationships among LLM enabled AO,\nAM, and AV in the context of authorship privacy. We quantify how they interact\nwith each other to transform human authored text, examining effects at a single\npoint in time and iteratively over time. We also examine the role of\ndemographic metadata, such as gender, academic background, in modulating their\nperformances, inter-task dynamics, and privacy risks. All source code will be\npublicly available.", "categories": ["cs.CL"], "published": "2025-05-20 10:52:12", "updated": "2025-05-20 10:52:12", "pdf_url": "http://arxiv.org/pdf/2505.14195v1", "comment": "17 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14212v1", "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "abstract": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:16:29", "updated": "2025-05-20 11:16:29", "pdf_url": "http://arxiv.org/pdf/2505.14212v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14216v1", "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "abstract": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 11:22:34", "updated": "2025-05-20 11:22:34", "pdf_url": "http://arxiv.org/pdf/2505.14216v1", "comment": "23 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14226v1", "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "abstract": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:35:25", "updated": "2025-05-20 11:35:25", "pdf_url": "http://arxiv.org/pdf/2505.14226v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14233v1", "title": "Mechanistic Fine-tuning for In-context Learning", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "abstract": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:41:21", "updated": "2025-05-20 11:41:21", "pdf_url": "http://arxiv.org/pdf/2505.14233v1", "comment": "28 pages, 31 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14238v1", "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "abstract": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:43:25", "updated": "2025-05-20 11:43:25", "pdf_url": "http://arxiv.org/pdf/2505.14238v1", "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14242v1", "title": "Technical Report on classification of literature related to children speech disorder", "authors": ["Ziang Wang", "Amir Aryani"], "abstract": "This technical report presents a natural language processing (NLP)-based\napproach for systematically classifying scientific literature on childhood\nspeech disorders. We retrieved and filtered 4,804 relevant articles published\nafter 2015 from the PubMed database using domain-specific keywords. After\ncleaning and pre-processing the abstracts, we applied two topic modeling\ntechniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify\nlatent thematic structures in the corpus. Our models uncovered 14 clinically\nmeaningful clusters, such as infantile hyperactivity and abnormal epileptic\nbehavior. To improve relevance and precision, we incorporated a custom stop\nword list tailored to speech pathology. Evaluation results showed that the LDA\nmodel achieved a coherence score of 0.42 and a perplexity of -7.5, indicating\nstrong topic coherence and predictive performance. The BERTopic model exhibited\na low proportion of outlier topics (less than 20%), demonstrating its capacity\nto classify heterogeneous literature effectively. These results provide a\nfoundation for automating literature reviews in speech-language pathology.", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-20 11:52:17", "updated": "2025-05-20 11:52:17", "pdf_url": "http://arxiv.org/pdf/2505.14242v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14244v1", "title": "TransBench: Benchmarking Machine Translation for Industrial-Scale Applications", "authors": ["Haijun Li", "Tianqi Shi", "Zifu Shang", "Yuxuan Han", "Xueyu Zhao", "Hao Wang", "Yu Qian", "Zhiqiang Qian", "Linlong Xu", "Minghao Wu", "Chenyang Lyu", "Longyue Wang", "Gongbo Tang", "Weihua Luo", "Zhao Xu", "Kaifu Zhang"], "abstract": "Machine translation (MT) has become indispensable for cross-border\ncommunication in globalized industries like e-commerce, finance, and legal\nservices, with recent advancements in large language models (LLMs)\nsignificantly enhancing translation quality. However, applying general-purpose\nMT models to industrial scenarios reveals critical limitations due to\ndomain-specific terminology, cultural nuances, and stylistic conventions absent\nin generic benchmarks. Existing evaluation frameworks inadequately assess\nperformance in specialized contexts, creating a gap between academic benchmarks\nand real-world efficacy. To address this, we propose a three-level translation\ncapability framework: (1) Basic Linguistic Competence, (2) Domain-Specific\nProficiency, and (3) Cultural Adaptation, emphasizing the need for holistic\nevaluation across these dimensions. We introduce TransBench, a benchmark\ntailored for industrial MT, initially targeting international e-commerce with\n17,000 professionally translated sentences spanning 4 main scenarios and 33\nlanguage pairs. TransBench integrates traditional metrics (BLEU, TER) with\nMarco-MOS, a domain-specific evaluation model, and provides guidelines for\nreproducible benchmark construction. Our contributions include: (1) a\nstructured framework for industrial MT evaluation, (2) the first publicly\navailable benchmark for e-commerce translation, (3) novel metrics probing\nmulti-level translation quality, and (4) open-sourced evaluation tools. This\nwork bridges the evaluation gap, enabling researchers and practitioners to\nsystematically assess and enhance MT systems for industry-specific needs.", "categories": ["cs.CL"], "published": "2025-05-20 11:54:58", "updated": "2025-05-20 11:54:58", "pdf_url": "http://arxiv.org/pdf/2505.14244v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14256v1", "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "abstract": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:09:17", "updated": "2025-05-20 12:09:17", "pdf_url": "http://arxiv.org/pdf/2505.14256v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14264v1", "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum", "authors": ["Jian Xiong", "Jingbo Zhou", "Jingyong Ye", "Dejing Dou"], "abstract": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 12:13:44", "updated": "2025-05-20 12:13:44", "pdf_url": "http://arxiv.org/pdf/2505.14264v1", "comment": "14 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14268v1", "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:19:10", "updated": "2025-05-20 12:19:10", "pdf_url": "http://arxiv.org/pdf/2505.14268v1", "comment": "16 pages, 14 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14271v1", "title": "FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning", "authors": ["Minh Ngoc Ta", "Dong Cao Van", "Duc-Anh Hoang", "Minh Le-Anh", "Truong Nguyen", "My Anh Tran Nguyen", "Yuxia Wang", "Preslav Nakov", "Sang Dinh"], "abstract": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nAI-generated, and human-AI collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, meanwhile identifying the underlying AI model family. Unlike\nexisting binary classifiers, FAID is built to capture both authorship and\nmodel-specific characteristics. Our method combines multi-level contrastive\nlearning with multi-task auxiliary classification to learn subtle stylistic\ncues. By modeling AI families as distinct stylistic entities, FAID offers\nimproved interpretability. We incorporate an adaptation to address\ndistributional shifts without retraining for unseen data. Experimental results\ndemonstrate that FAID outperforms several baseline approaches, particularly\nenhancing the generalization accuracy on unseen domains and new AI models. It\nprovide a potential solution for improving transparency and accountability in\nAI-assisted writing.", "categories": ["cs.CL"], "published": "2025-05-20 12:23:31", "updated": "2025-05-20 12:23:31", "pdf_url": "http://arxiv.org/pdf/2505.14271v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14272v1", "title": "Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "abstract": "Considering the importance of detecting hateful language, labeled hate speech\ndata is expensive and time-consuming to collect, particularly for low-resource\nlanguages. Prior work has demonstrated the effectiveness of cross-lingual\ntransfer learning and data augmentation in improving performance on tasks with\nlimited labeled data. To develop an efficient and scalable cross-lingual\ntransfer learning approach, we leverage nearest-neighbor retrieval to augment\nminimal labeled data in the target language, thereby enhancing detection\nperformance. Specifically, we assume access to a small set of labeled training\ninstances in the target language and use these to retrieve the most relevant\nlabeled examples from a large multilingual hate speech detection pool. We\nevaluate our approach on eight languages and demonstrate that it consistently\noutperforms models trained solely on the target language data. Furthermore, in\nmost cases, our method surpasses the current state-of-the-art. Notably, our\napproach is highly data-efficient, retrieving as small as 200 instances in some\ncases while maintaining superior performance. Moreover, it is scalable, as the\nretrieval pool can be easily expanded, and the method can be readily adapted to\nnew languages and tasks. We also apply maximum marginal relevance to mitigate\nredundancy and filter out highly similar retrieved instances, resulting in\nimprovements in some languages.", "categories": ["cs.CL", "cs.CY", "cs.MM"], "published": "2025-05-20 12:25:33", "updated": "2025-05-20 12:25:33", "pdf_url": "http://arxiv.org/pdf/2505.14272v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14279v1", "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin M\u00fcnch"], "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:30:46", "updated": "2025-05-20 12:30:46", "pdf_url": "http://arxiv.org/pdf/2505.14279v1", "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14286v1", "title": "Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs", "authors": ["Rao Ma", "Mengjie Qian", "Vyas Raina", "Mark Gales", "Kate Knill"], "abstract": "The combination of pre-trained speech encoders with large language models has\nenabled the development of speech LLMs that can handle a wide range of spoken\nlanguage processing tasks. While these models are powerful and flexible, this\nvery flexibility may make them more vulnerable to adversarial attacks. To\nexamine the extent of this problem, in this work we investigate universal\nacoustic adversarial attacks on speech LLMs. Here a fixed, universal,\nadversarial audio segment is prepended to the original input audio. We\ninitially investigate attacks that cause the model to either produce no output\nor to perform a modified task overriding the original prompt. We then extend\nthe nature of the attack to be selective so that it activates only when\nspecific input attributes, such as a speaker gender or spoken language, are\npresent. Inputs without the targeted attribute should be unaffected, allowing\nfine-grained control over the model outputs. Our findings reveal critical\nvulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar\nspeech LLMs may be susceptible to universal adversarial attacks. This\nhighlights the need for more robust training strategies and improved resistance\nto adversarial attacks.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-20 12:35:59", "updated": "2025-05-20 12:35:59", "pdf_url": "http://arxiv.org/pdf/2505.14286v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14297v1", "title": "Cross-Lingual Optimization for Language Transfer in Large Language Models", "authors": ["Jungseob Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "abstract": "Adapting large language models to other languages typically employs\nsupervised fine-tuning (SFT) as a standard approach. However, it often suffers\nfrom an overemphasis on English performance, a phenomenon that is especially\npronounced in data-constrained environments. To overcome these challenges, we\npropose \\textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an\nEnglish-centric LLM to a target language while preserving its English\ncapabilities. CLO utilizes publicly available English SFT data and a\ntranslation model to enable cross-lingual transfer. We conduct experiments\nusing five models on six languages, each possessing varying levels of resource.\nOur results show that CLO consistently outperforms SFT in both acquiring target\nlanguage proficiency and maintaining English performance. Remarkably, in\nlow-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400\nsamples, demonstrating that CLO can achieve better performance with less data.\nFurthermore, we find that SFT is particularly sensitive to data quantity in\nmedium and low-resource languages, whereas CLO remains robust. Our\ncomprehensive analysis emphasizes the limitations of SFT and incorporates\nadditional training strategies in CLO to enhance efficiency.", "categories": ["cs.CL"], "published": "2025-05-20 12:45:09", "updated": "2025-05-20 12:45:09", "pdf_url": "http://arxiv.org/pdf/2505.14297v1", "comment": "Accepted for publication at ACL 2025. Jungseob Lee and Seongtae Hong\n  contributed equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14300v1", "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors", "authors": ["Maheep Chaudhary", "Fazl Barez"], "abstract": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 12:49:58", "updated": "2025-05-20 12:49:58", "pdf_url": "http://arxiv.org/pdf/2505.14300v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14302v1", "title": "Scaling Law for Quantization-Aware Training", "authors": ["Mengzhao Chen", "Chaoyi Zhang", "Jing Liu", "Yutao Zeng", "Zeyue Xue", "Zhiheng Liu", "Yunshui Li", "Jin Ma", "Jie Huang", "Xun Zhou", "Ping Luo"], "abstract": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 12:54:43", "updated": "2025-05-20 12:54:43", "pdf_url": "http://arxiv.org/pdf/2505.14302v1", "comment": "A unified scaling law for QAT that models quantization error as a\n  function of model size, training data volume, and quantization group size", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14305v1", "title": "JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling", "authors": ["Jinwang Song", "Hongying Zan", "Kunli Zhang", "Lingling Mu", "Yingjie Han", "Haobo Hua", "Min Peng"], "abstract": "Text-to-SQL, which maps natural language to SQL queries, has benefited\ngreatly from recent advances in Large Language Models (LLMs). While LLMs offer\nvarious paradigms for this task, including prompting and supervised fine-tuning\n(SFT), SFT approaches still face challenges such as complex multi-stage\npipelines and poor robustness to noisy schema information. To address these\nlimitations, we present JOLT-SQL, a streamlined single-stage SFT framework that\njointly optimizes schema linking and SQL generation via a unified loss.\nJOLT-SQL employs discriminative schema linking, enhanced by local bidirectional\nattention, alongside a confusion-aware noisy schema sampling strategy with\nselective attention to improve robustness under noisy schema conditions.\nExperiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL\nachieves state-of-the-art execution accuracy among comparable-size open-source\nmodels, while significantly improving both training and inference efficiency.", "categories": ["cs.CL"], "published": "2025-05-20 12:55:10", "updated": "2025-05-20 12:55:10", "pdf_url": "http://arxiv.org/pdf/2505.14305v1", "comment": "Work in progress. 13 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14309v1", "title": "Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency", "authors": ["Ehsan Doostmohammadi", "Marco Kuhlmann"], "abstract": "Retrieval-augmented language models have demonstrated performance comparable\nto much larger models while requiring fewer computational resources. The\neffectiveness of these models crucially depends on the overlap between query\nand retrieved context, but the optimal degree of this overlap remains\nunexplored. In this paper, we systematically investigate how varying levels of\nquery--context overlap affect model performance during both training and\ninference. Our experiments reveal that increased overlap initially has minimal\neffect, but substantially improves test-time perplexity and accelerates model\nlearning above a critical threshold. Building on these findings, we demonstrate\nthat deliberately increasing overlap through synthetic context can enhance data\nefficiency and reduce training time by approximately 40\\% without compromising\nperformance. We specifically generate synthetic context through paraphrasing\nqueries. We validate our perplexity-based findings on question-answering tasks,\nconfirming that the benefits of retrieval-augmented language modeling extend to\npractical applications. Our results provide empirical evidence of significant\noptimization potential for retrieval mechanisms in language model pretraining.", "categories": ["cs.CL"], "published": "2025-05-20 12:58:07", "updated": "2025-05-20 12:58:07", "pdf_url": "http://arxiv.org/pdf/2505.14309v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14311v1", "title": "HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing", "authors": ["Shamsuddeen Hassan Muhammad", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Falalu Ibrahim Lawan", "Babangida Sani", "Sukairaj Hafiz Imam", "Yusuf Aliyu", "Sani Abdullahi Sani", "Ali Usman Umar", "Kenneth Church", "Vukosi Marivate"], "abstract": "Hausa Natural Language Processing (NLP) has gained increasing attention in\nrecent years, yet remains understudied as a low-resource language despite\nhaving over 120 million first-language (L1) and 80 million second-language (L2)\nspeakers worldwide. While significant advances have been made in high-resource\nlanguages, Hausa NLP faces persistent challenges, including limited open-source\ndatasets and inadequate model representation. This paper presents an overview\nof the current state of Hausa NLP, systematically examining existing resources,\nresearch contributions, and gaps across fundamental NLP tasks: text\nclassification, machine translation, named entity recognition, speech\nrecognition, and question answering. We introduce HausaNLP\n(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\ntools, and research works to enhance accessibility and drive further\ndevelopment. Furthermore, we discuss challenges in integrating Hausa into large\nlanguage models (LLMs), addressing issues of suboptimal tokenization and\ndialectal variation. Finally, we propose strategic research directions\nemphasizing dataset expansion, improved language modeling approaches, and\nstrengthened community collaboration to advance Hausa NLP. Our work provides\nboth a foundation for accelerating Hausa NLP progress and valuable insights for\nbroader multilingual NLP research.", "categories": ["cs.CL"], "published": "2025-05-20 12:59:55", "updated": "2025-05-20 12:59:55", "pdf_url": "http://arxiv.org/pdf/2505.14311v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14313v1", "title": "A MIND for Reasoning: Meta-learning for In-context Deduction", "authors": ["Leonardo Bertolazzi", "Manuel Vargas Guzm\u00e1n", "Raffaella Bernardi", "Maciej Malicki", "Jakub Szymanik"], "abstract": "Large language models (LLMs) are increasingly evaluated on formal tasks,\nwhere strong reasoning abilities define the state of the art. However, their\nability to generalize to out-of-distribution problems remains limited. In this\npaper, we investigate how LLMs can achieve a systematic understanding of\ndeductive rules. Our focus is on the task of identifying the appropriate subset\nof premises within a knowledge base needed to derive a given hypothesis. To\ntackle this challenge, we propose Meta-learning for In-context Deduction\n(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND\nis to enable models to generalize more effectively to unseen knowledge bases\nand to systematically apply inference rules. Our results show that MIND\nsignificantly improves generalization in small LMs ranging from 1.5B to 7B\nparameters. The benefits are especially pronounced in smaller models and\nlow-data settings. Remarkably, small models fine-tuned with MIND outperform\nstate-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.", "categories": ["cs.CL"], "published": "2025-05-20 13:00:48", "updated": "2025-05-20 13:00:48", "pdf_url": "http://arxiv.org/pdf/2505.14313v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14318v1", "title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection", "authors": ["Wenjun Hou", "Yi Cheng", "Kaishuai Xu", "Heng Li", "Yan Hu", "Wenjie Li", "Jiang Liu"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration and inefficient\nutilization of learned representations. To address this limitation, we propose\nRADAR, a framework for enhancing radiology report generation with supplementary\nknowledge injection. RADAR improves report generation by systematically\nleveraging both the internal knowledge of an LLM and externally retrieved\ninformation. Specifically, it first extracts the model's acquired knowledge\nthat aligns with expert image-based classification outputs. It then retrieves\nrelevant supplementary knowledge to further enrich this information. Finally,\nby aggregating both sources, RADAR generates more accurate and informative\nradiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU\nX-ray demonstrate that our model outperforms state-of-the-art LLMs in both\nlanguage quality and clinical accuracy", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-20 13:05:41", "updated": "2025-05-20 13:05:41", "pdf_url": "http://arxiv.org/pdf/2505.14318v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14347v1", "title": "QA-prompting: Improving Summarization with Large Language Models using Question-Answering", "authors": ["Neelabh Sinha"], "abstract": "Language Models (LMs) have revolutionized natural language processing,\nenabling high-quality text generation through prompting and in-context\nlearning. However, models often struggle with long-context summarization due to\npositional biases, leading to suboptimal extraction of critical information.\nThere are techniques to improve this with fine-tuning, pipelining, or using\ncomplex techniques, which have their own challenges. To solve these challenges,\nwe propose QA-prompting - a simple prompting method for summarization that\nutilizes question-answering as an intermediate step prior to summary\ngeneration. Our method extracts key information and enriches the context of\ntext to mitigate positional biases and improve summarization in a single LM\ncall per task without requiring fine-tuning or pipelining. Experiments on\nmultiple datasets belonging to different domains using ten state-of-the-art\npre-trained models demonstrate that QA-prompting outperforms baseline and other\nstate-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This\nprovides an effective and scalable solution for summarization and highlights\nthe importance of domain-specific question selection for optimal performance.", "categories": ["cs.CL"], "published": "2025-05-20 13:29:36", "updated": "2025-05-20 13:29:36", "pdf_url": "http://arxiv.org/pdf/2505.14347v1", "comment": "Submitted to ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14350v1", "title": "OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "abstract": "Fine-tuning Large Language Models (LLMs) has become increasingly challenging\ndue to their massive scale and associated computational costs.\nParameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as\ncomputational alternatives; however, their implementations still require\nsignificant resources. In this paper, we present OSoRA (Output-Dimension and\nSingular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.\nOSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value\nDecomposition (SVD) with learnable scaling vectors in a unified framework. It\nfirst performs an SVD of pre-trained weight matrices, then optimizes an\noutput-dimension vector during training, while keeping the corresponding\nsingular vector matrices frozen. OSoRA substantially reduces computational\nresource requirements by minimizing the number of trainable parameters during\nfine-tuning. Comprehensive evaluations across mathematical reasoning, common\nsense reasoning, and other benchmarks demonstrate that OSoRA achieves\ncomparable or superior performance to state-of-the-art methods like LoRA and\nVeRA, while maintaining a linear parameter scaling even as the rank increases\nto higher dimensions. Our ablation studies further confirm that jointly\ntraining both the singular values and the output-dimension vector is critical\nfor optimal performance.", "categories": ["cs.CL"], "published": "2025-05-20 13:34:06", "updated": "2025-05-20 13:34:06", "pdf_url": "http://arxiv.org/pdf/2505.14350v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14351v1", "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 13:35:55", "updated": "2025-05-20 13:35:55", "pdf_url": "http://arxiv.org/pdf/2505.14351v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14354v1", "title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications", "authors": ["Xin Li", "Mengbing Liu", "Li Wei", "Jiancheng An", "M\u00e9rouane Debbah", "Chau Yuen"], "abstract": "Large Language Models (LLMs) have achieved impressive results across a broad\narray of tasks, yet their capacity for complex, domain-specific mathematical\nreasoning-particularly in wireless communications-remains underexplored. In\nthis work, we introduce WirelessMathBench, a novel benchmark specifically\ndesigned to evaluate LLMs on mathematical modeling challenges to wireless\ncommunications engineering. Our benchmark consists of 587 meticulously curated\nquestions sourced from 40 state-of-the-art research papers, encompassing a\ndiverse spectrum of tasks ranging from basic multiple-choice questions to\ncomplex equation completion tasks, including both partial and full completions,\nall of which rigorously adhere to physical and dimensional constraints. Through\nextensive experimentation with leading LLMs, we observe that while many models\nexcel in basic recall tasks, their performance degrades significantly when\nreconstructing partially or fully obscured equations, exposing fundamental\nlimitations in current LLMs. Even DeepSeek-R1, the best performer on our\nbenchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%\nsuccess rate in full equation completion. By publicly releasing\nWirelessMathBench along with the evaluation toolkit, we aim to advance the\ndevelopment of more robust, domain-aware LLMs for wireless system analysis and\nbroader engineering applications.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 13:38:10", "updated": "2025-05-20 13:38:10", "pdf_url": "http://arxiv.org/pdf/2505.14354v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14356v1", "title": "PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs", "authors": ["Sho Inoue", "Shai Wang", "Haizhou Li"], "abstract": "Despite significant progress in neural spoken dialog systems,\npersonality-aware conversation agents -- capable of adapting behavior based on\npersonalities -- remain underexplored due to the absence of personality\nannotations in speech datasets. We propose a pipeline that preprocesses raw\naudio recordings to create a dialogue dataset annotated with timestamps,\nresponse types, and emotion/sentiment labels. We employ an automatic speech\nrecognition (ASR) system to extract transcripts and timestamps, then generate\nconversation-level annotations. Leveraging these annotations, we design a\nsystem that employs large language models to predict conversational\npersonality. Human evaluators were engaged to identify conversational\ncharacteristics and assign personality labels. Our analysis demonstrates that\nthe proposed system achieves stronger alignment with human judgments compared\nto existing approaches.", "categories": ["cs.SD", "cs.CL", "eess.AS"], "published": "2025-05-20 13:41:32", "updated": "2025-05-20 13:41:32", "pdf_url": "http://arxiv.org/pdf/2505.14356v1", "comment": "This is accepted to Interspeech 2025; Added an extra page for\n  supplementary figures; Project page:\n  https://github.com/shinshoji01/Personality-Prediction-for-Conversation-Agents", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14367v1", "title": "Dual Decomposition of Weights and Singular Value Low Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for\nadapting Large Language Models (LLMs) to downstream tasks, among which Low-rank\nAdaptation (LoRA) represents one of the most widely adopted methodologies.\nHowever, existing LoRA-based approaches exhibit two fundamental limitations:\nunstable training dynamics and inefficient knowledge transfer from pre-trained\nmodels, both stemming from random initialization of adapter parameters. To\novercome these challenges, we propose DuDe, a novel approach that decomposes\nweight matrices into magnitude and direction components, employing Singular\nValue Decomposition (SVD) for principled initialization. Our comprehensive\nevaluation demonstrates DuDe's superior performance and robustness, achieving\nup to 48.35\\% accuracy on MMLU and 62.53\\% ($\\pm$ 1.59) accuracy on GSM8K. Our\ntheoretical analysis and empirical validation collectively demonstrate that\nDuDe's decomposition strategy enhances optimization stability and better\npreserves pre-trained representations, particularly for domain-specific tasks\nrequiring specialized knowledge. The combination of robust empirical\nperformance and rigorous theoretical foundations establishes DuDe as a\nsignificant contribution to PEFT methodologies for LLMs.", "categories": ["cs.CL"], "published": "2025-05-20 13:49:15", "updated": "2025-05-20 13:49:15", "pdf_url": "http://arxiv.org/pdf/2505.14367v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14368v1", "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs", "authors": ["Jiawen Wang", "Pritha Gupta", "Ivan Habernal", "Eyke H\u00fcllermeier"], "abstract": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 13:50:43", "updated": "2025-05-20 13:50:43", "pdf_url": "http://arxiv.org/pdf/2505.14368v1", "comment": "8 pages, 3 figures, EMNLP 2025 under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14376v1", "title": "AutoRev: Automatic Peer Review System for Academic Research Papers", "authors": ["Maitreya Prafulla Chitale", "Ketaki Mangesh Shetye", "Harshit Gupta", "Manav Chaudhary", "Vasudeva Varma"], "abstract": "Generating a review for an academic research paper is a complex task that\nrequires a deep understanding of the document's content and the\ninterdependencies between its sections. It demands not only insight into\ntechnical details but also an appreciation of the paper's overall coherence and\nstructure. Recent methods have predominantly focused on fine-tuning large\nlanguage models (LLMs) to address this challenge. However, they often overlook\nthe computational and performance limitations imposed by long input token\nlengths. To address this, we introduce AutoRev, an Automatic Peer Review System\nfor Academic Research Papers. Our novel framework represents an academic\ndocument as a graph, enabling the extraction of the most critical passages that\ncontribute significantly to the review. This graph-based approach demonstrates\neffectiveness for review generation and is potentially adaptable to various\ndownstream tasks, such as question answering, summarization, and document\nrepresentation. When applied to review generation, our method outperforms SOTA\nbaselines by an average of 58.72% across all evaluation metrics. We hope that\nour work will stimulate further research in applying graph-based extraction\ntechniques to other downstream tasks in NLP. We plan to make our code public\nupon acceptance.", "categories": ["cs.CL"], "published": "2025-05-20 13:59:58", "updated": "2025-05-20 13:59:58", "pdf_url": "http://arxiv.org/pdf/2505.14376v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14393v1", "title": "Editing Across Languages: A Survey of Multilingual Knowledge Editing", "authors": ["Nadir Durrani", "Basel Mousi", "Fahim Dalvi"], "abstract": "While Knowledge Editing has been extensively studied in monolingual settings,\nit remains underexplored in multilingual contexts. This survey systematizes\nrecent research on Multilingual Knowledge Editing (MKE), a growing subdomain of\nmodel editing focused on ensuring factual edits generalize reliably across\nlanguages. We present a comprehensive taxonomy of MKE methods, covering\nparameter-based, memory-based, fine-tuning, and hypernetwork approaches. We\nsurvey available benchmarks,summarize key findings on method effectiveness and\ntransfer patterns, identify challenges in cross-lingual propagation, and\nhighlight open problems related to language anisotropy, evaluation coverage,\nand edit scalability. Our analysis consolidates a rapidly evolving area and\nlays the groundwork for future progress in editable language-aware LLMs.", "categories": ["cs.CL"], "published": "2025-05-20 14:13:04", "updated": "2025-05-20 14:13:04", "pdf_url": "http://arxiv.org/pdf/2505.14393v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14395v1", "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "abstract": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:14:00", "updated": "2025-05-20 14:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14395v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "authors": ["Ga\u00ebl Gendron", "Jo\u017ee M. Ro\u017eanec", "Michael Witbrock", "Gillian Dobbie"], "abstract": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"], "published": "2025-05-20 14:14:05", "updated": "2025-05-20 14:14:05", "pdf_url": "http://arxiv.org/pdf/2505.14396v1", "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14398v1", "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "abstract": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 14:14:38", "updated": "2025-05-20 14:14:38", "pdf_url": "http://arxiv.org/pdf/2505.14398v1", "comment": "Data and code are available at https://peterbaile.github.io/lag/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14402v1", "title": "OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking", "authors": ["Heng Yang", "Jack Cole", "Yuan Li", "Renzhi Chen", "Geyong Min", "Ke Li"], "abstract": "The code of nature, embedded in DNA and RNA genomes since the origin of life,\nholds immense potential to impact both humans and ecosystems through genome\nmodeling. Genomic Foundation Models (GFMs) have emerged as a transformative\napproach to decoding the genome. As GFMs scale up and reshape the landscape of\nAI-driven genomics, the field faces an urgent need for rigorous and\nreproducible evaluation. We present OmniGenBench, a modular benchmarking\nplatform designed to unify the data, model, benchmarking, and interpretability\nlayers across GFMs. OmniGenBench enables standardized, one-command evaluation\nof any GFM across five benchmark suites, with seamless integration of over 31\nopen-source models. Through automated pipelines and community-extensible\nfeatures, the platform addresses critical reproducibility challenges, including\ndata transparency, model interoperability, benchmark fragmentation, and\nblack-box interpretability. OmniGenBench aims to serve as foundational\ninfrastructure for reproducible genomic AI research, accelerating trustworthy\ndiscovery and collaborative innovation in the era of genome-scale modeling.", "categories": ["q-bio.GN", "cs.CL"], "published": "2025-05-20 14:16:25", "updated": "2025-05-20 14:16:25", "pdf_url": "http://arxiv.org/pdf/2505.14402v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14406v1", "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis", "authors": ["Haoming Huang", "Yibo Yan", "Jiahao Huo", "Xin Zou", "Xinfeng Li", "Kun Wang", "Xuming Hu"], "abstract": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the internal workings of attention heads,\ntracing how competing knowledge pathways contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation.", "categories": ["cs.CL"], "published": "2025-05-20 14:20:30", "updated": "2025-05-20 14:20:30", "pdf_url": "http://arxiv.org/pdf/2505.14406v1", "comment": "18 pages, 6 figures, EMNLP under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14410v1", "title": "Pairwise Evaluation of Accent Similarity in Speech Synthesis", "authors": ["Jinzuomu Zhong", "Suyuan Liu", "Dan Wells", "Korin Richmond"], "abstract": "Despite growing interest in generating high-fidelity accents, evaluating\naccent similarity in speech synthesis has been underexplored. We aim to enhance\nboth subjective and objective evaluation methods for accent similarity.\nSubjectively, we refine the XAB listening test by adding components that\nachieve higher statistical significance with fewer listeners and lower costs.\nOur method involves providing listeners with transcriptions, having them\nhighlight perceived accent differences, and implementing meticulous screening\nfor reliability. Objectively, we utilise pronunciation-related metrics, based\non distances between vowel formants and phonetic posteriorgrams, to evaluate\naccent generation. Comparative experiments reveal that these metrics, alongside\naccent similarity, speaker similarity, and Mel Cepstral Distortion, can be\nused. Moreover, our findings underscore significant limitations of common\nmetrics like Word Error Rate in assessing underrepresented accents.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 14:23:50", "updated": "2025-05-20 14:23:50", "pdf_url": "http://arxiv.org/pdf/2505.14410v1", "comment": "Accepted by INTERSPEECH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14412v1", "title": "PRL: Prompts from Reinforcement Learning", "authors": ["Pawe\u0142 Batorski", "Adrian Kosmala", "Paul Swoboda"], "abstract": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 14:26:19", "updated": "2025-05-20 14:26:19", "pdf_url": "http://arxiv.org/pdf/2505.14412v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14418v1", "title": "Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents", "authors": ["Pengzhou Cheng", "Haowen Hu", "Zheng Wu", "Zongru Wu", "Tianjie Ju", "Daizong Ding", "Zhuosheng Zhang", "Gongshen Liu"], "abstract": "Graphical user interface (GUI) agents powered by multimodal large language\nmodels (MLLMs) have shown greater promise for human-interaction. However, due\nto the high fine-tuning cost, users often rely on open-source GUI agents or\nAPIs offered by AI providers, which introduces a critical but underexplored\nsupply chain threat: backdoor attacks. In this work, we first unveil that\nMLLM-powered GUI agents naturally expose multiple interaction-level triggers,\nsuch as historical steps, environment states, and task progress. Based on this\nobservation, we introduce AgentGhost, an effective and stealthy framework for\nred-teaming backdoor attacks. Specifically, we first construct composite\ntriggers by combining goal and interaction levels, allowing GUI agents to\nunintentionally activate backdoors while ensuring task utility. Then, we\nformulate backdoor injection as a Min-Max optimization problem that uses\nsupervised contrastive learning to maximize the feature difference across\nsample classes at the representation space, improving flexibility of the\nbackdoor. Meanwhile, it adopts supervised fine-tuning to minimize the\ndiscrepancy between backdoor and clean behavior generation, enhancing\neffectiveness and utility. Extensive evaluations of various agent models in two\nestablished mobile benchmarks show that AgentGhost is effective and generic,\nwith attack accuracy that reaches 99.7\\% on three attack objectives, and shows\nstealthiness with only 1\\% utility degradation. Furthermore, we tailor a\ndefense method against AgentGhost that reduces the attack accuracy to 22.1\\%.\nOur code is available at \\texttt{anonymous}.", "categories": ["cs.CL"], "published": "2025-05-20 14:29:18", "updated": "2025-05-20 14:29:18", "pdf_url": "http://arxiv.org/pdf/2505.14418v1", "comment": "25 pages, 10 figures, 12 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14420v1", "title": "SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection", "authors": ["Huopu Zhang", "Yanguang Liu", "Mengnan Du"], "abstract": "Predicting earnings surprises through the analysis of earnings conference\ncall transcripts has attracted increasing attention from the financial research\ncommunity. Conference calls serve as critical communication channels between\ncompany executives, analysts, and shareholders, offering valuable\nforward-looking information. However, these transcripts present significant\nanalytical challenges, typically containing over 5,000 words with substantial\nredundancy and industry-specific terminology that creates obstacles for\nlanguage models. In this work, we propose the Sparse Autoencoder for Financial\nRepresentation Enhancement (SAE-FiRE) framework to address these limitations by\nextracting key information while eliminating redundancy. SAE-FiRE employs\nSparse Autoencoders (SAEs) to efficiently identify patterns and filter out\nnoises, and focusing specifically on capturing nuanced financial signals that\nhave predictive power for earnings surprises. Experimental results indicate\nthat the proposed method can significantly outperform comparing baselines.", "categories": ["q-fin.CP", "cs.CL", "cs.LG"], "published": "2025-05-20 14:31:23", "updated": "2025-05-20 14:31:23", "pdf_url": "http://arxiv.org/pdf/2505.14420v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14423v1", "title": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs", "authors": ["Ona de Gibert", "Joseph Attieh", "Teemu Vahtola", "Mikko Aulamo", "Zihao Li", "Ra\u00fal V\u00e1zquez", "Tiancheng Hu", "J\u00f6rg Tiedemann"], "abstract": "We investigate the potential of LLM-generated synthetic data for improving\nlow-resource machine translation (MT). Focusing on seven diverse target\nlanguages, we construct a document-level synthetic corpus from English\nEuroparl, and extend it via pivoting to 147 additional language pairs.\nAutomatic and human evaluation confirm its high overall quality. We study its\npractical application by (i) identifying effective training regimes, (ii)\ncomparing our data with the HPLT dataset, and (iii) testing its utility beyond\nEnglish-centric MT. Finally, we introduce SynOPUS, a public repository for\nsynthetic parallel datasets. Our findings show that LLM-generated synthetic\ndata, even when noisy, can substantially improve MT performance for\nlow-resource languages.", "categories": ["cs.CL"], "published": "2025-05-20 14:31:54", "updated": "2025-05-20 14:31:54", "pdf_url": "http://arxiv.org/pdf/2505.14423v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14425v1", "title": "From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "abstract": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization.", "categories": ["cs.CL"], "published": "2025-05-20 14:33:29", "updated": "2025-05-20 14:33:29", "pdf_url": "http://arxiv.org/pdf/2505.14425v1", "comment": "4 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14432v1", "title": "Rank-K: Test-Time Reasoning for Listwise Reranking", "authors": ["Eugene Yang", "Andrew Yates", "Kathryn Ricci", "Orion Weller", "Vivek Chari", "Benjamin Van Durme", "Dawn Lawrie"], "abstract": "Retrieve-and-rerank is a popular retrieval pipeline because of its ability to\nmake slow but effective rerankers efficient enough at query time by reducing\nthe number of comparisons. Recent works in neural rerankers take advantage of\nlarge language models for their capability in reasoning between queries and\npassages and have achieved state-of-the-art retrieval effectiveness. However,\nsuch rerankers are resource-intensive, even after heavy optimization. In this\nwork, we introduce Rank-K, a listwise passage reranking model that leverages\nthe reasoning capability of the reasoning language model at query time that\nprovides test time scalability to serve hard queries. We show that Rank-K\nimproves retrieval effectiveness by 23\\% over the RankZephyr, the\nstate-of-the-art listwise reranker, when reranking a BM25 initial ranked list\nand 19\\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is\ninherently a multilingual model, we found that it ranks passages based on\nqueries in different languages as effectively as it does in monolingual\nretrieval.", "categories": ["cs.IR", "cs.CL"], "published": "2025-05-20 14:39:34", "updated": "2025-05-20 14:39:34", "pdf_url": "http://arxiv.org/pdf/2505.14432v1", "comment": "15 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14436v1", "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:42:03", "updated": "2025-05-20 14:42:03", "pdf_url": "http://arxiv.org/pdf/2505.14436v1", "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14438v1", "title": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models", "authors": ["Yuanbo Fang", "Haoze Sun", "Jun Liu", "Tao Zhang", "Zenan Zhou", "Weipeng Chen", "Xiaofen Xing", "Xiangmin Xu"], "abstract": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench.", "categories": ["cs.SD", "cs.CL", "eess.AS"], "published": "2025-05-20 14:42:20", "updated": "2025-05-20 14:42:20", "pdf_url": "http://arxiv.org/pdf/2505.14438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14442v1", "title": "Creative Preference Optimization", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:43:41", "updated": "2025-05-20 14:43:41", "pdf_url": "http://arxiv.org/pdf/2505.14442v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14449v1", "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "abstract": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 14:50:44", "updated": "2025-05-20 14:50:44", "pdf_url": "http://arxiv.org/pdf/2505.14449v1", "comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "authors": ["Chihan Huang", "Hao Tang"], "abstract": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:52:41", "updated": "2025-05-20 14:52:41", "pdf_url": "http://arxiv.org/pdf/2505.14455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14462v1", "title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding", "authors": ["Jiaang Li", "Yifei Yuan", "Wenyan Li", "Mohammad Aliannejadi", "Daniel Hershcovich", "Anders S\u00f8gaard", "Ivan Vuli\u0107", "Wenxuan Zhang", "Paul Pu Liang", "Yang Deng", "Serge Belongie"], "abstract": "As vision-language models (VLMs) become increasingly integrated into daily\nlife, the need for accurate visual culture understanding is becoming critical.\nYet, these models frequently fall short in interpreting cultural nuances\neffectively. Prior work has demonstrated the effectiveness of\nretrieval-augmented generation (RAG) in enhancing cultural understanding in\ntext-only settings, while its application in multimodal scenarios remains\nunderexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented\nVisual culturE uNdErstAnding), a new benchmark designed to advance visual\nculture understanding through retrieval, focusing on two tasks: culture-focused\nvisual question answering (cVQA) and culture-informed image captioning (cIC).\nRAVENEA extends existing datasets by integrating over 10,000 Wikipedia\ndocuments curated and ranked by human annotators. With RAVENEA, we train and\nevaluate seven multimodal retrievers for each image query, and measure the\ndownstream impact of retrieval-augmented inputs across fourteen\nstate-of-the-art VLMs. Our results show that lightweight VLMs, when augmented\nwith culture-aware retrieval, outperform their non-augmented counterparts (by\nat least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the\nvalue of retrieval-augmented methods and culturally inclusive benchmarks for\nmultimodal understanding.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-20 14:57:16", "updated": "2025-05-20 14:57:16", "pdf_url": "http://arxiv.org/pdf/2505.14462v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14464v1", "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters", "authors": ["Xiaoyu Tian", "Yunjie Ji", "Haotian Wang", "Shuaiting Chen", "Sitong Zhao", "Yiping Peng", "Han Zhao", "Xiangang Li"], "abstract": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging Face\\footnote{Datasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},\n\\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.", "categories": ["cs.CL"], "published": "2025-05-20 15:00:51", "updated": "2025-05-20 15:00:51", "pdf_url": "http://arxiv.org/pdf/2505.14464v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14467v1", "title": "Void in Language Models", "authors": ["Mani Shemiranifar"], "abstract": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks.", "categories": ["cs.CL"], "published": "2025-05-20 15:01:56", "updated": "2025-05-20 15:01:56", "pdf_url": "http://arxiv.org/pdf/2505.14467v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14469v1", "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "abstract": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:05:03", "updated": "2025-05-20 15:05:03", "pdf_url": "http://arxiv.org/pdf/2505.14469v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14470v1", "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "authors": ["Nadav Har-Tuv", "Or Tal", "Yossi Adi"], "abstract": "We present PAST, a novel end-to-end framework that jointly models phonetic\ninformation alongside signal reconstruction, eliminating the need for external\npretrained models. Unlike previous approaches that rely on pretrained\nself-supervised models, PAST employs supervised phonetic data, directly\nintegrating domain knowledge into the tokenization process via auxiliary tasks.\nAdditionally, we introduce a streamable, causal variant of PAST, enabling\nreal-time speech applications. Results demonstrate that PAST surpasses existing\nevaluated baseline tokenizers across common evaluation metrics, including\nphonetic representation and speech reconstruction. Notably, PAST also achieves\nsuperior performance when serving as a speech representation for speech\nlanguage models, further highlighting its effectiveness as a foundation for\nspoken language generation. To foster further research, we release the full\nimplementation. For code, model checkpoints, and samples see:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/PAST", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "published": "2025-05-20 15:05:14", "updated": "2025-05-20 15:05:14", "pdf_url": "http://arxiv.org/pdf/2505.14470v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14471v1", "title": "Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning", "authors": ["Tong Li", "Jiachuan Wang", "Yongqi Zhang", "Shuangyin Li", "Lei Chen"], "abstract": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss", "categories": ["cs.CL"], "published": "2025-05-20 15:05:27", "updated": "2025-05-20 15:05:27", "pdf_url": "http://arxiv.org/pdf/2505.14471v1", "comment": "Manuscripts, accepted to KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14479v1", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "abstract": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:13:32", "updated": "2025-05-20 15:13:32", "pdf_url": "http://arxiv.org/pdf/2505.14479v1", "comment": "long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14481v1", "title": "PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models", "authors": ["He Zhu", "Junyou Su", "Minxi Chen", "Wen Wang", "Yijie Deng", "Guanhua Chen", "Wenjia Zhang"], "abstract": "In the field of urban planning, existing Vision-Language Models (VLMs)\nfrequently fail to effectively analyze and evaluate planning maps, despite the\ncritical importance of these visual elements for urban planners and related\neducational contexts. Planning maps, which visualize land use, infrastructure\nlayouts, and functional zoning, require specialized understanding of spatial\nconfigurations, regulatory requirements, and multi-scale analysis. To address\nthis challenge, we introduce PlanGPT-VL, the first domain-specific\nVision-Language Model tailored specifically for urban planning maps. PlanGPT-VL\nemploys three innovative approaches: (1) PlanAnno-V framework for high-quality\nVQA data synthesis, (2) Critical Point Thinking to reduce hallucinations\nthrough structured verification, and (3) comprehensive training methodology\ncombining Supervised Fine-Tuning with frozen vision encoder parameters. Through\nsystematic evaluation on our proposed PlanBench-V benchmark, we demonstrate\nthat PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs\nin specialized planning map interpretation tasks, offering urban planning\nprofessionals a reliable tool for map analysis, assessment, and educational\napplications while maintaining high factual accuracy. Our lightweight 7B\nparameter model achieves comparable performance to models exceeding 72B\nparameters, demonstrating efficient domain specialization without sacrificing\nperformance.", "categories": ["cs.CL"], "published": "2025-05-20 15:14:47", "updated": "2025-05-20 15:14:47", "pdf_url": "http://arxiv.org/pdf/2505.14481v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14483v1", "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance", "authors": ["Agam Goyal", "Xianyang Zhan", "Yilun Chen", "Koustuv Saha", "Eshwar Chandrasekharan"], "abstract": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities.", "categories": ["cs.CL"], "published": "2025-05-20 15:16:06", "updated": "2025-05-20 15:16:06", "pdf_url": "http://arxiv.org/pdf/2505.14483v1", "comment": "Preprint: 15 pages, 4 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14489v1", "title": "Reasoning Models Better Express Their Confidence", "authors": ["Dongkeun Yoon", "Seungone Kim", "Sohee Yang", "Sunkyoung Kim", "Soyeon Kim", "Yongil Kim", "Eunbi Choi", "Yireun Kim", "Minjoon Seo"], "abstract": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:19:00", "updated": "2025-05-20 15:19:00", "pdf_url": "http://arxiv.org/pdf/2505.14489v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14499v1", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "abstract": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:28:26", "updated": "2025-05-20 15:28:26", "pdf_url": "http://arxiv.org/pdf/2505.14499v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14505v1", "title": "ModRWKV: Transformer Multimodality in Linear Time", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "abstract": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:34:36", "updated": "2025-05-20 15:34:36", "pdf_url": "http://arxiv.org/pdf/2505.14505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14518v1", "title": "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "abstract": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 15:44:01", "updated": "2025-05-20 15:44:01", "pdf_url": "http://arxiv.org/pdf/2505.14518v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14523v1", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "abstract": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-20 15:46:44", "updated": "2025-05-20 15:46:44", "pdf_url": "http://arxiv.org/pdf/2505.14523v1", "comment": "To be published in ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14530v1", "title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs", "authors": ["Zhipeng Yang", "Junzhuo Li", "Siyu Xia", "Xuming Hu"], "abstract": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 15:49:15", "updated": "2025-05-20 15:49:15", "pdf_url": "http://arxiv.org/pdf/2505.14530v1", "comment": "27 pages, 17 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14536v1", "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders", "authors": ["Agam Goyal", "Vedant Rathi", "William Yeh", "Yian Wang", "Yuen Chen", "Hari Sundaram"], "abstract": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment.", "categories": ["cs.CL"], "published": "2025-05-20 15:55:31", "updated": "2025-05-20 15:55:31", "pdf_url": "http://arxiv.org/pdf/2505.14536v1", "comment": "Preprint: 19 pages, 7 figures, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14552v1", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "abstract": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:06:32", "updated": "2025-05-20 16:06:32", "pdf_url": "http://arxiv.org/pdf/2505.14552v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14553v1", "title": "Pivot Language for Low-Resource Machine Translation", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "abstract": "Certain pairs of languages suffer from lack of a parallel corpus which is\nlarge in size and diverse in domain. One of the ways this is overcome is via\nuse of a pivot language. In this paper we use Hindi as a pivot language to\ntranslate Nepali into English. We describe what makes Hindi a good candidate\nfor the pivot. We discuss ways in which a pivot language can be used, and use\ntwo such approaches - the Transfer Method (fully supervised) and\nBacktranslation (semi-supervised) - to translate Nepali into English. Using the\nformer, we are able to achieve a devtest Set SacreBLEU score of 14.2, which\nimproves the baseline fully supervised score reported by (Guzman et al., 2019)\nby 6.6 points. While we are slightly below the semi-supervised baseline score\nof 15.1, we discuss what may have caused this under-performance, and suggest\nscope for future work.", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "published": "2025-05-20 16:10:10", "updated": "2025-05-20 16:10:10", "pdf_url": "http://arxiv.org/pdf/2505.14553v1", "comment": "7 pages, 3 figures, paper dated May 13, 2019", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "abstract": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 16:28:08", "updated": "2025-05-20 16:28:08", "pdf_url": "http://arxiv.org/pdf/2505.14569v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14577v1", "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring", "authors": ["Sohaila Eltanbouly", "Salam Albatarni", "Tamer Elsayed"], "abstract": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant.", "categories": ["cs.CL"], "published": "2025-05-20 16:34:37", "updated": "2025-05-20 16:34:37", "pdf_url": "http://arxiv.org/pdf/2505.14577v1", "comment": "Accepted at ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14582v1", "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning", "authors": ["Shangziqi Zhao", "Jiahao Yuan", "Guisong Yang", "Usman Naseem"], "abstract": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity.", "categories": ["cs.CL"], "published": "2025-05-20 16:38:32", "updated": "2025-05-20 16:38:32", "pdf_url": "http://arxiv.org/pdf/2505.14582v1", "comment": "17 pages,4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14585v1", "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "abstract": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "categories": ["cs.CL"], "published": "2025-05-20 16:40:09", "updated": "2025-05-20 16:40:09", "pdf_url": "http://arxiv.org/pdf/2505.14585v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14590v1", "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "abstract": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.", "categories": ["cs.CL"], "published": "2025-05-20 16:41:45", "updated": "2025-05-20 16:41:45", "pdf_url": "http://arxiv.org/pdf/2505.14590v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14597v1", "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals", "authors": ["Xianzhen Luo", "Qingfu Zhu", "Zhiming Zhang", "Mingzheng Xu", "Tianhao Cheng", "Yixuan Wang", "Zheng Chu", "Shijie Xuyang", "Zhiyuan Ma", "YuanTao Fan", "Wanxiang Che"], "abstract": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance.", "categories": ["cs.CL"], "published": "2025-05-20 16:48:57", "updated": "2025-05-20 16:48:57", "pdf_url": "http://arxiv.org/pdf/2505.14597v1", "comment": "Code & Model is https://github.com/Luowaterbi/CTF-Instruct", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14599v1", "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "abstract": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 16:49:40", "updated": "2025-05-20 16:49:40", "pdf_url": "http://arxiv.org/pdf/2505.14599v1", "comment": "Accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14607v1", "title": "sudoLLM : On Multi-role Alignment of Language Models", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Joy Mahapatra", "Utpal Garain"], "abstract": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.", "categories": ["cs.CL", "cs.CR", "I.2.7"], "published": "2025-05-20 16:54:34", "updated": "2025-05-20 16:54:34", "pdf_url": "http://arxiv.org/pdf/2505.14607v1", "comment": "Under review. Code and data to be released later", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14608v1", "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "abstract": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:55:44", "updated": "2025-05-20 16:55:44", "pdf_url": "http://arxiv.org/pdf/2505.14608v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14615v1", "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", "authors": ["Anjiang Wei", "Yuheng Wu", "Yingjia Wan", "Tarun Suresh", "Huanmi Tan", "Zhanke Zhou", "Sanmi Koyejo", "Ke Wang", "Alex Aiken"], "abstract": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "published": "2025-05-20 17:00:22", "updated": "2025-05-20 17:00:22", "pdf_url": "http://arxiv.org/pdf/2505.14615v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14617v1", "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models", "authors": ["Sahar Abdelnabi", "Ahmed Salem"], "abstract": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation.", "categories": ["cs.CL", "cs.CY"], "published": "2025-05-20 17:03:12", "updated": "2025-05-20 17:03:12", "pdf_url": "http://arxiv.org/pdf/2505.14617v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14620v1", "title": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs", "authors": ["Morgan Lindsay Heisler", "Linzi Xing", "Ge Shi", "Hanieh Sadri", "Gursimran Singh", "Weiwei Zhang", "Tao Ye", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "abstract": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 17:11:18", "updated": "2025-05-20 17:11:18", "pdf_url": "http://arxiv.org/pdf/2505.14620v1", "comment": "Accepted at ACM KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14625v1", "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 17:16:44", "updated": "2025-05-20 17:16:44", "pdf_url": "http://arxiv.org/pdf/2505.14625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14627v1", "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "authors": ["Ashutosh Adhikari", "Mirella Lapata"], "abstract": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:18:17", "updated": "2025-05-20 17:18:17", "pdf_url": "http://arxiv.org/pdf/2505.14627v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14629v1", "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "abstract": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-20 17:19:57", "updated": "2025-05-20 17:19:57", "pdf_url": "http://arxiv.org/pdf/2505.14629v1", "comment": "Accepted at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14631v1", "title": "Think Only When You Need with Large Hybrid-Reasoning Models", "authors": ["Lingjie Jiang", "Xun Wu", "Shaohan Huang", "Qingxiu Dong", "Zewen Chi", "Li Dong", "Xingxing Zhang", "Tengchao Lv", "Lei Cui", "Furu Wei"], "abstract": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.", "categories": ["cs.CL"], "published": "2025-05-20 17:23:25", "updated": "2025-05-20 17:23:25", "pdf_url": "http://arxiv.org/pdf/2505.14631v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "published": "2025-05-20 17:24:09", "updated": "2025-05-20 17:24:09", "pdf_url": "http://arxiv.org/pdf/2505.14633v1", "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14638v1", "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference", "authors": ["Tomer Gafni", "Asaf Karnieli", "Yair Hanani"], "abstract": "Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model.", "categories": ["cs.CV", "cs.CL", "cs.LG"], "published": "2025-05-20 17:26:12", "updated": "2025-05-20 17:26:12", "pdf_url": "http://arxiv.org/pdf/2505.14638v1", "comment": "Accepted at eLVM Workshop, CVPR, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14652v1", "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "abstract": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.", "categories": ["cs.CL"], "published": "2025-05-20 17:41:33", "updated": "2025-05-20 17:41:33", "pdf_url": "http://arxiv.org/pdf/2505.14652v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "abstract": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-20 17:42:34", "updated": "2025-05-20 17:42:34", "pdf_url": "http://arxiv.org/pdf/2505.14654v1", "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14660v1", "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "authors": ["Ronald Seoh", "Dan Goldwasser"], "abstract": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 17:47:04", "updated": "2025-05-20 17:47:04", "pdf_url": "http://arxiv.org/pdf/2505.14660v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:54:54", "updated": "2025-05-20 17:54:54", "pdf_url": "http://arxiv.org/pdf/2505.14667v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "categories": ["cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:55:25", "updated": "2025-05-20 17:55:25", "pdf_url": "http://arxiv.org/pdf/2505.14668v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14674v1", "title": "Reward Reasoning Model", "authors": ["Jiaxin Guo", "Zewen Chi", "Li Dong", "Qingxiu Dong", "Xun Wu", "Shaohan Huang", "Furu Wei"], "abstract": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.", "categories": ["cs.CL"], "published": "2025-05-20 17:58:03", "updated": "2025-05-20 17:58:03", "pdf_url": "http://arxiv.org/pdf/2505.14674v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14679v1", "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "abstract": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.", "categories": ["cs.CL"], "published": "2025-05-20 17:59:04", "updated": "2025-05-20 17:59:04", "pdf_url": "http://arxiv.org/pdf/2505.14679v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "abstract": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:59:13", "updated": "2025-05-20 17:59:13", "pdf_url": "http://arxiv.org/pdf/2505.14680v1", "comment": "SIGIR 2025 Perspective Paper", "doi": "10.1145/3726302.3730353", "journal_ref": null}
{"arxiv_id": "2505.14681v1", "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "published": "2025-05-20 17:59:16", "updated": "2025-05-20 17:59:16", "pdf_url": "http://arxiv.org/pdf/2505.14681v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14684v1", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 17:59:31", "updated": "2025-05-20 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.14684v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14685v1", "title": "Language Models use Lookbacks to Track Beliefs", "authors": ["Nikhil Prakash", "Natalie Shapira", "Arnab Sen Sharma", "Christoph Riedl", "Yonatan Belinkov", "Tamar Rott Shaham", "David Bau", "Atticus Geiger"], "abstract": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs.", "categories": ["cs.CL"], "published": "2025-05-20 17:59:45", "updated": "2025-05-20 17:59:45", "pdf_url": "http://arxiv.org/pdf/2505.14685v1", "comment": "32 pages, 32 figures. Code and data at https://belief.baulab.info/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16086v1", "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "abstract": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 00:00:27", "updated": "2025-05-22 00:00:27", "pdf_url": "http://arxiv.org/pdf/2505.16086v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16088v1", "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "abstract": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday).", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 00:06:29", "updated": "2025-05-22 00:06:29", "pdf_url": "http://arxiv.org/pdf/2505.16088v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16090v1", "title": "Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance", "authors": ["Dominick Kubica", "Dylan T. Gordon", "Nanami Emura", "Derleen Saini", "Charlie Goldenberg"], "abstract": "As of 2025, Generative Artificial Intelligence (GenAI) has become a central\ntool for productivity across industries. Beyond text generation, GenAI now\nplays a critical role in coding, data analysis, and research workflows. As\nlarge language models (LLMs) continue to evolve, it is essential to assess the\nreliability and accuracy of their outputs, especially in specialized,\nhigh-stakes domains like finance. Most modern LLMs transform text into\nnumerical vectors, which are used in operations such as cosine similarity\nsearches to generate responses. However, this abstraction process can lead to\nmisinterpretation of emotional tone, particularly in nuanced financial\ncontexts. While LLMs generally excel at identifying sentiment in everyday\nlanguage, these models often struggle with the nuanced, strategically ambiguous\nlanguage found in earnings call transcripts. Financial disclosures frequently\nembed sentiment in hedged statements, forward-looking language, and\nindustry-specific jargon, making it difficult even for human analysts to\ninterpret consistently, let alone AI models. This paper presents findings from\nthe Santa Clara Microsoft Practicum Project, led by Professor Charlie\nGoldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's\nChatGPT, Google's Gemini, and traditional machine learning models for sentiment\nanalysis of financial text. Using Microsoft earnings call transcripts, the\nanalysis assesses how well LLM-derived sentiment correlates with market\nsentiment and stock movements and evaluates the accuracy of model outputs.\nPrompt engineering techniques are also examined to improve sentiment analysis\nresults. Visualizations of sentiment consistency are developed to evaluate\nalignment between tone and stock performance, with sentiment trends analyzed\nacross Microsoft's lines of business to determine which segments exert the\ngreatest influence.", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.7"], "published": "2025-05-22 00:09:11", "updated": "2025-05-22 00:09:11", "pdf_url": "http://arxiv.org/pdf/2505.16090v1", "comment": "6 pages, 4 figures. Research conducted as part of a\n  Microsoft-sponsored Capstone Project at Santa Clara University", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16097v1", "title": "TrialPanorama: Database and Benchmark for Systematic Review and Design of Clinical Trials", "authors": ["Zifeng Wang", "Qiao Jin", "Jiacheng Lin", "Junyi Gao", "Jathurshan Pradeepkumar", "Pengcheng Jiang", "Benjamin Danek", "Zhiyong Lu", "Jimeng Sun"], "abstract": "Developing artificial intelligence (AI) for vertical domains requires a solid\ndata foundation for both training and evaluation. In this work, we introduce\nTrialPanorama, a large-scale, structured database comprising 1,657,476 clinical\ntrial records aggregated from 15 global sources. The database captures key\naspects of trial design and execution, including trial setups, interventions,\nconditions, biomarkers, and outcomes, and links them to standard biomedical\nontologies such as DrugBank and MedDRA. This structured and ontology-grounded\ndesign enables TrialPanorama to serve as a unified, extensible resource for a\nwide range of clinical trial tasks, including trial planning, design, and\nsummarization. To demonstrate its utility, we derive a suite of benchmark tasks\ndirectly from the TrialPanorama database. The benchmark spans eight tasks\nacross two categories: three for systematic review (study search, study\nscreening, and evidence summarization) and five for trial design (arm design,\neligibility criteria, endpoint selection, sample size estimation, and trial\ncompletion assessment). The experiments using five state-of-the-art large\nlanguage models (LLMs) show that while general-purpose LLMs exhibit some\nzero-shot capability, their performance is still inadequate for high-stakes\nclinical trial workflows. We release TrialPanorama database and the benchmark\nto facilitate further research on AI for clinical trials.", "categories": ["cs.AI"], "published": "2025-05-22 00:58:43", "updated": "2025-05-22 00:58:43", "pdf_url": "http://arxiv.org/pdf/2505.16097v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16100v1", "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "abstract": "Validating scientific hypotheses is a central challenge in biomedical\nresearch, and remains difficult for artificial intelligence (AI) agents due to\nthe complexity of real-world data analysis and evidence interpretation. In this\nwork, we present BioDSA-1K, a benchmark designed to evaluate AI agents on\nrealistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K\nconsists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,\ncurated from over 300 published biomedical studies to reflect the structure and\nreasoning found in authentic research workflows. Each task includes a\nstructured hypothesis derived from the original study's conclusions, expressed\nin the affirmative to reflect the language of scientific reporting, and one or\nmore pieces of supporting evidence grounded in empirical data tables. While\nthese hypotheses mirror published claims, they remain testable using standard\nstatistical or machine learning methods. The benchmark enables evaluation along\nfour axes: (1) hypothesis decision accuracy, (2) alignment between evidence and\nconclusion, (3) correctness of the reasoning process, and (4) executability of\nthe AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable\nhypotheses: cases where the available data are insufficient to support or\nrefute a claim, reflecting a common yet underexplored scenario in real-world\nscience. We propose BioDSA-1K as a foundation for building and evaluating\ngeneralizable, trustworthy AI agents for biomedical discovery.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 01:02:21", "updated": "2025-05-22 01:02:21", "pdf_url": "http://arxiv.org/pdf/2505.16100v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16103v1", "title": "Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI", "authors": ["Monirul Islam Mahmud"], "abstract": "Keylogger detection involves monitoring for unusual system behaviors such as\ndelays between typing and character display, analyzing network traffic patterns\nfor data exfiltration. In this study, we provide a comprehensive analysis for\nkeylogger detection with traditional machine learning models - SVC, Random\nForest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes\nand advanced ensemble methods including Stacking, Blending and Voting.\nMoreover, feature selection approaches such as Information gain, Lasso L1 and\nFisher Score are thoroughly assessed to improve predictive performance and\nlower computational complexity. The Keylogger Detection dataset from publicly\navailable Kaggle website is used in this project. In addition to accuracy-based\nclassification, this study implements the approach for model interpretation\nusing Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to\ndeliver finer explanations for how much each feature contributes in assisting\nor hindering the detection process. To evaluate the models result, we have used\nAUC score, sensitivity, Specificity, Accuracy and F1 score. The best\nperformance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,\n100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is\nnear-perfect classification with Fisher Score.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 01:04:13", "updated": "2025-05-22 01:04:13", "pdf_url": "http://arxiv.org/pdf/2505.16103v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16114v1", "title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language", "authors": ["Naiqi Li", "Peiyuan Liu", "Zheng Liu", "Tao Dai", "Yong Jiang", "Shu-Tao Xia"], "abstract": "Solving puzzles in natural language poses a long-standing challenge in AI.\nWhile large language models (LLMs) have recently shown impressive capabilities\nin a variety of tasks, they continue to struggle with complex puzzles that\ndemand precise reasoning and exhaustive search. In this paper, we propose\nLogic-of-Thought (Logot), a novel framework that bridges LLMs with logic\nprogramming to address this problem. Our method leverages LLMs to translate\npuzzle rules and states into answer set programs (ASPs), the solution of which\nare then accurately and efficiently inferred by an ASP interpreter. This hybrid\napproach combines the natural language understanding of LLMs with the precise\nreasoning capabilities of logic programs. We evaluate our method on various\ngrid puzzles and dynamic puzzles involving actions, demonstrating near-perfect\naccuracy across all tasks. Our code and data are available at:\nhttps://github.com/naiqili/Logic-of-Thought.", "categories": ["cs.AI"], "published": "2025-05-22 01:37:40", "updated": "2025-05-22 01:37:40", "pdf_url": "http://arxiv.org/pdf/2505.16114v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16120v1", "title": "LLM-Powered AI Agent Systems and Their Applications in Industry", "authors": ["Guannan Liang", "Qianqian Tong"], "abstract": "The emergence of Large Language Models (LLMs) has reshaped agent systems.\nUnlike traditional rule-based agents with limited task scope, LLM-powered\nagents offer greater flexibility, cross-domain reasoning, and natural language\ninteraction. Moreover, with the integration of multi-modal LLMs, current agent\nsystems are highly capable of processing diverse data modalities, including\ntext, images, audio, and structured tabular data, enabling richer and more\nadaptive real-world behavior. This paper comprehensively examines the evolution\nof agent systems from the pre-LLM era to current LLM-powered architectures. We\ncategorize agent systems into software-based, physical, and adaptive hybrid\nsystems, highlighting applications across customer service, software\ndevelopment, manufacturing automation, personalized education, financial\ntrading, and healthcare. We further discuss the primary challenges posed by\nLLM-powered agents, including high inference latency, output uncertainty, lack\nof evaluation metrics, and security vulnerabilities, and propose potential\nsolutions to mitigate these concerns.", "categories": ["cs.AI"], "published": "2025-05-22 01:52:15", "updated": "2025-05-22 01:52:15", "pdf_url": "http://arxiv.org/pdf/2505.16120v1", "comment": "This is the author's accepted version of the paper accepted to appear\n  at IEEE AIIoT 2025. The final version will be available via IEEE Xplore.\n  \\c{opyright}2025 IEEE. Personal use of this material is permitted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16130v1", "title": "Scalable Graph Generative Modeling via Substructure Sequences", "authors": ["Zehong Wang", "Zheyuan Zhang", "Tianyi Ma", "Chuxu Zhang", "Yanfang Ye"], "abstract": "Graph neural networks (GNNs) has been predominantly driven by\nmessage-passing, where node representations are iteratively updated via local\nneighborhood aggregation. Despite their success, message-passing suffers from\nfundamental limitations -- including constrained expressiveness,\nover-smoothing, over-squashing, and limited capacity to model long-range\ndependencies. These issues hinder scalability: increasing data size or model\nsize often fails to yield improved performance, limiting the viability of GNNs\nas backbones for graph foundation models. In this work, we explore pathways\nbeyond message-passing and introduce Generative Graph Pattern Machine\n(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM\nrepresents graph instances (nodes, edges, or entire graphs) as sequences of\nsubstructures, and employs generative pre-training over the sequences to learn\ngeneralizable, transferable representations. Empirically, G$^2$PM demonstrates\nstrong scalability: on the ogbn-arxiv benchmark, it continues to improve with\nmodel sizes up to 60M parameters, outperforming prior generative approaches\nthat plateau at significantly smaller scales (e.g., 3M). In addition, we\nsystematically analyze the model design space, highlighting key architectural\nchoices that contribute to its scalability and generalization. Across diverse\ntasks -- including node classification, graph classification, and transfer\nlearning -- G$^2$PM consistently outperforms strong baselines, establishing a\ncompelling foundation for scalable graph learning. The code and dataset are\navailable at https://github.com/Zehong-Wang/G2PM.", "categories": ["cs.LG", "cs.AI", "cs.SI"], "published": "2025-05-22 02:16:34", "updated": "2025-05-22 02:16:34", "pdf_url": "http://arxiv.org/pdf/2505.16130v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16135v1", "title": "Sudoku-Bench: Evaluating creative reasoning with Sudoku variants", "authors": ["Jeffrey Seely", "Yuki Imajuku", "Tianyu Zhao", "Edoardo Cetin", "Llion Jones"], "abstract": "Existing reasoning benchmarks for large language models (LLMs) frequently\nfail to capture authentic creativity, often rewarding memorization of\npreviously observed patterns. We address this shortcoming with Sudoku-Bench, a\ncurated benchmark of challenging and unconventional Sudoku variants\nspecifically selected to evaluate creative, multi-step logical reasoning.\nSudoku variants form an unusually effective domain for reasoning research: each\npuzzle introduces unique or subtly interacting constraints, making memorization\ninfeasible and requiring solvers to identify novel logical breakthroughs\n(``break-ins''). Despite their diversity, Sudoku variants maintain a common and\ncompact structure, enabling clear and consistent evaluation. Sudoku-Bench\nincludes a carefully chosen puzzle set, a standardized text-based puzzle\nrepresentation, and flexible tools compatible with thousands of publicly\navailable puzzles -- making it easy to extend into a general research\nenvironment. Baseline experiments show that state-of-the-art LLMs solve fewer\nthan 15\\% of puzzles unaided, highlighting significant opportunities to advance\nlong-horizon, strategic reasoning capabilities.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 02:24:35", "updated": "2025-05-22 02:24:35", "pdf_url": "http://arxiv.org/pdf/2505.16135v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16136v1", "title": "Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study", "authors": ["Yuke Zhang"], "abstract": "This study introduces an interpretable machine learning (ML) framework to\nextract macroeconomic alpha from global news sentiment. We process the Global\nDatabase of Events, Language, and Tone (GDELT) Project's worldwide news feed\nusing FinBERT -- a Bidirectional Encoder Representations from Transformers\n(BERT) based model pretrained on finance-specific language -- to construct\ndaily sentiment indices incorporating mean tone, dispersion, and event impact.\nThese indices drive an XGBoost classifier, benchmarked against logistic\nregression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S.\nTreasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold\nexpanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates\nexceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios\nachieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective\ncompound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and\n22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment\ndispersion and article impact are key predictive features. Our findings\nestablish that integrating domain-specific Natural Language Processing (NLP)\nwith interpretable ML offers a potent and explainable source of macro alpha.", "categories": ["q-fin.CP", "cs.AI", "cs.LG", "q-fin.TR"], "published": "2025-05-22 02:24:45", "updated": "2025-05-22 02:24:45", "pdf_url": "http://arxiv.org/pdf/2505.16136v1", "comment": "18 pages (including references), 1 figure, 1 table. Code available at\n  \\url{https://github.com/yukepenn/macro-news-sentiment-trading}. Keywords:\n  Macro Sentiment, News Sentiment, Algorithmic Trading, GDELT, FinBERT, NLP,\n  Alternative Data, Foreign Exchange, Treasury Futures, Quantitative Finance,\n  Machine Learning, SHAP, Interpretability", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16146v1", "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "abstract": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 02:45:45", "updated": "2025-05-22 02:45:45", "pdf_url": "http://arxiv.org/pdf/2505.16146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16147v1", "title": "Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value", "authors": ["Le Ma", "Shirao Yang", "Zihao Wang", "Yinggui Wang", "Lei Wang", "Tao Wei", "Kejun Zhang"], "abstract": "The proliferation of large models has intensified the need for efficient data\nvaluation methods to quantify the contribution of individual data providers.\nTraditional approaches, such as game-theory-based Shapley value and\ninfluence-function-based techniques, face prohibitive computational costs or\nrequire access to full data and model training details, making them hardly\nachieve partial data valuation. To address this, we propose Unlearning Shapley,\na novel framework that leverages machine unlearning to estimate data values\nefficiently. By unlearning target data from a pretrained model and measuring\nperformance shifts on a reachable test set, our method computes Shapley values\nvia Monte Carlo sampling, avoiding retraining and eliminating dependence on\nfull data. Crucially, Unlearning Shapley supports both full and partial data\nvaluation, making it scalable for large models (e.g., LLMs) and practical for\ndata markets. Experiments on benchmark datasets and large-scale text corpora\ndemonstrate that our approach matches the accuracy of state-of-the-art methods\nwhile reducing computational overhead by orders of magnitude. Further analysis\nconfirms a strong correlation between estimated values and the true impact of\ndata subsets, validating its reliability in real-world scenarios. This work\nbridges the gap between data valuation theory and practical deployment,\noffering a scalable, privacy-compliant solution for modern AI ecosystems.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-22 02:46:03", "updated": "2025-05-22 02:46:03", "pdf_url": "http://arxiv.org/pdf/2505.16147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16149v1", "title": "When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification", "authors": ["Zirui Pang", "Haosheng Tan", "Yuhan Pu", "Zhijie Deng", "Zhouan Shen", "Keyu Hu", "Jiaheng Wei"], "abstract": "Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet\nserve as critical tools for model evaluation. However, despite the cleaning\nefforts, these datasets still suffer from pervasive noisy labels and often\ncontain missing labels due to the co-existing image pattern where multiple\nclasses appear in an image sample. This results in misleading model comparisons\nand unfair evaluations. Existing label cleaning methods focus primarily on\nnoisy labels, but the issue of missing labels remains largely overlooked.\nMotivated by these challenges, we present a comprehensive framework named\nREVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,\nLLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods\n(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and\nmissing label detection in widely-used image classification test sets. REVEAL\ndetects potential noisy labels and omissions, aggregates predictions from\nvarious methods, and refines label accuracy through confidence-informed\npredictions and consensus-based filtering. Additionally, we provide a thorough\nanalysis of state-of-the-art vision-language models and pre-trained image\nclassifiers, highlighting their strengths and limitations within the context of\ndataset renovation by revealing 10 observations. Our method effectively reveals\nmissing labels from public datasets and provides soft-labeled results with\nlikelihoods. Through human verifications, REVEAL significantly improves the\nquality of 6 benchmark test sets, highly aligning to human judgments and\nenabling more accurate and meaningful comparisons in image classification.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-22 02:47:36", "updated": "2025-05-22 02:47:36", "pdf_url": "http://arxiv.org/pdf/2505.16149v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16172v1", "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "abstract": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 03:19:49", "updated": "2025-05-22 03:19:49", "pdf_url": "http://arxiv.org/pdf/2505.16172v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16175v1", "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design", "authors": ["Benjamin Schneider", "Dongfu Jiang", "Chao Du", "Tianyu Pang", "Wenhu Chen"], "abstract": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 03:26:50", "updated": "2025-05-22 03:26:50", "pdf_url": "http://arxiv.org/pdf/2505.16175v1", "comment": "19 pages, 6 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16176v1", "title": "Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning", "authors": ["Jun Rao", "Xuebo Liu", "Hexuan Deng", "Zepeng Lin", "Zixiong Yu", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "abstract": "In the realm of data selection for reasoning tasks, existing approaches\npredominantly rely on externally predefined static metrics such as difficulty\nand diversity, which are often designed for supervised fine-tuning (SFT) and\nlack adaptability to continuous training processes. A critical limitation of\nthese methods is their inability to dynamically align with the evolving\ncapabilities of models during online training, a gap that becomes increasingly\npronounced with the rise of dynamic training paradigms and online reinforcement\nlearning (RL) frameworks (e.g., R1 models). To address this, we introduce\nSAI-DPO, an algorithm that dynamically selects training data by continuously\nassessing a model's stage-specific reasoning abilities across different\ntraining phases. By integrating real-time model performance feedback, SAI-DPO\nadaptively adapts data selection to the evolving strengths and weaknesses of\nthe model, thus enhancing both data utilization efficiency and final task\nperformance. Extensive experiments on three state-of-the-art models and eight\nmathematical reasoning benchmarks, including challenging competition-level\ndatasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average\nperformance boost of up to 21.3 percentage points, with particularly notable\nimprovements of 10 and 15 points on AIME24 and AMC23, respectively. These\nresults highlight the superiority of dynamic, model-adaptive data selection\nover static, externally defined strategies in advancing reasoning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 03:27:05", "updated": "2025-05-22 03:27:05", "pdf_url": "http://arxiv.org/pdf/2505.16176v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16181v1", "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks", "authors": ["Mohammad Reza Taesiri", "Brandon Collins", "Logan Bolton", "Viet Dac Lai", "Franck Dernoncourt", "Trung Bui", "Anh Totti Nguyen"], "abstract": "Generative AI (GenAI) holds significant promise for automating everyday image\nediting tasks, especially following the recent release of GPT-4o on March 25,\n2025. However, what subjects do people most often want edited? What kinds of\nediting actions do they want to perform (e.g., removing or stylizing the\nsubject)? Do people prefer precise edits with predictable outcomes or highly\ncreative ones? By understanding the characteristics of real-world requests and\nthe corresponding edits made by freelance photo-editing wizards, can we draw\nlessons for improving AI-based editors and determine which types of requests\ncan currently be handled successfully by AI editors? In this paper, we present\na unique study addressing these questions by analyzing 83k requests from the\npast 12 years (2013-2025) on the Reddit community, which collected 305k\nPSR-wizard edits. According to human ratings, approximately only 33% of\nrequests can be fulfilled by the best AI editors (including GPT-4o,\nGemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on\nlow-creativity requests that require precise editing than on more open-ended\ntasks. They often struggle to preserve the identity of people and animals, and\nfrequently make non-requested touch-ups. On the other side of the table, VLM\njudges (e.g., o1) perform differently from human judges and may prefer AI edits\nmore than human edits. Code and qualitative examples are available at:\nhttps://psrdataset.github.io", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 03:35:15", "updated": "2025-05-22 03:35:15", "pdf_url": "http://arxiv.org/pdf/2505.16181v1", "comment": "Code and qualitative examples are available at:\n  https://psrdataset.github.io", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16186v1", "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning", "authors": ["Kaiwen Zhou", "Xuandong Zhao", "Gaowen Liu", "Jayanth Srinivasa", "Aosong Feng", "Dawn Song", "Xin Eric Wang"], "abstract": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.", "categories": ["cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-22 03:46:03", "updated": "2025-05-22 03:46:03", "pdf_url": "http://arxiv.org/pdf/2505.16186v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16187v1", "title": "EasyInsert: A Data-Efficient and Generalizable Insertion Policy", "authors": ["Guanghe Li", "Junming Zhao", "Shengjie Wang", "Yang Gao"], "abstract": "Insertion task is highly challenging that requires robots to operate with\nexceptional precision in cluttered environments. Existing methods often have\npoor generalization capabilities. They typically function in restricted and\nstructured environments, and frequently fail when the plug and socket are far\napart, when the scene is densely cluttered, or when handling novel objects.\nThey also rely on strong assumptions such as access to CAD models or a digital\ntwin in simulation. To address this, we propose EasyInsert, a framework which\nleverages the human intuition that relative pose (delta pose) between plug and\nsocket is sufficient for successful insertion, and employs efficient and\nautomated real-world data collection with minimal human labor to train a\ngeneralizable model for relative pose prediction. During execution, EasyInsert\nfollows a coarse-to-fine execution procedure based on predicted delta pose, and\nsuccessfully performs various insertion tasks. EasyInsert demonstrates strong\nzero-shot generalization capability for unseen objects in cluttered\nenvironments, handling cases with significant initial pose deviations while\nmaintaining high sample efficiency and requiring little human effort. In\nreal-world experiments, with just 5 hours of training data, EasyInsert achieves\nover 90% success in zero-shot insertion for 13 out of 15 unseen novel objects,\nincluding challenging objects like Type-C cables, HDMI cables, and Ethernet\ncables. Furthermore, with only one human demonstration and 4 minutes of\nautomatically collected data for fine-tuning, it reaches over 90% success rate\nfor all 15 objects.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 03:46:05", "updated": "2025-05-22 03:46:05", "pdf_url": "http://arxiv.org/pdf/2505.16187v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16192v1", "title": "VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "authors": ["Chaoya Jiang", "Yongrui Heng", "Wei Ye", "Han Yang", "Haiyang Xu", "Ming Yan", "Ji Zhang", "Fei Huang", "Shikun Zhang"], "abstract": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce \\textbf{VLM-R$^3$} (\\textbf{V}isual\n\\textbf{L}anguage \\textbf{M}odel with \\textbf{R}egion \\textbf{R}ecognition and\n\\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)\ndecide \\emph{when} additional visual evidence is needed, (ii) determine\n\\emph{where} to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is \\textbf{Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO)}, a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 03:50:13", "updated": "2025-05-22 03:50:13", "pdf_url": "http://arxiv.org/pdf/2505.16192v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16195v1", "title": "SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet", "authors": ["Zhi Zhong", "Akira Takahashi", "Shuyang Cui", "Keisuke Toyama", "Shusuke Takahashi", "Yuki Mitsufuji"], "abstract": "Foley synthesis aims to synthesize high-quality audio that is both\nsemantically and temporally aligned with video frames. Given its broad\napplication in creative industries, the task has gained increasing attention in\nthe research community. To avoid the non-trivial task of training audio\ngenerative models from scratch, adapting pretrained audio generative models for\nvideo-synchronized foley synthesis presents an attractive direction.\nControlNet, a method for adding fine-grained controls to pretrained generative\nmodels, has been applied to foley synthesis, but its use has been limited to\nhandcrafted human-readable temporal conditions. In contrast, from-scratch\nmodels achieved success by leveraging high-dimensional deep features extracted\nusing pretrained video encoders. We have observed a performance gap between\nControlNet-based and from-scratch foley models. To narrow this gap, we propose\nSpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward\nvideo-synchronized foley synthesis via ControlNet. To unlock the potential of a\nsingle ControlNet branch, we resolve the discrepancy between the temporal video\nfeatures and the time-frequency nature of the pretrained SpecMaskGIT via a\nfrequency-aware temporal feature aligner, eliminating the need for complicated\nconditioning mechanisms widely used in prior arts. Evaluations on a common\nfoley synthesis benchmark demonstrate that SpecMaskFoley could even outperform\nstrong from-scratch baselines, substantially advancing the development of\nControlNet-based foley synthesis models. Demo page:\nhttps://zzaudio.github.io/SpecMaskFoley_Demo/", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.IV"], "published": "2025-05-22 03:58:16", "updated": "2025-05-22 03:58:16", "pdf_url": "http://arxiv.org/pdf/2505.16195v1", "comment": "4 pages, 2 figures, 2 tables. Demo page:\n  https://zzaudio.github.io/SpecMaskFoley_Demo/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16196v1", "title": "SEM: Enhancing Spatial Understanding for Robust Robot Manipulation", "authors": ["Xuewu Lin", "Tianwei Lin", "Lichao Huang", "Hongyu Xie", "Yiwei Jin", "Keyu Li", "Zhizhong Su"], "abstract": "A key challenge in robot manipulation lies in developing policy models with\nstrong spatial understanding, the ability to reason about 3D geometry, object\nrelations, and robot embodiment. Existing methods often fall short: 3D point\ncloud models lack semantic abstraction, while 2D image encoders struggle with\nspatial reasoning. To address this, we propose SEM (Spatial Enhanced\nManipulation model), a novel diffusion-based policy framework that explicitly\nenhances spatial understanding from two complementary perspectives. A spatial\nenhancer augments visual representations with 3D geometric context, while a\nrobot state encoder captures embodiment-aware structure through graphbased\nmodeling of joint dependencies. By integrating these modules, SEM significantly\nimproves spatial understanding, leading to robust and generalizable\nmanipulation across diverse tasks that outperform existing baselines.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-05-22 04:00:12", "updated": "2025-05-22 04:00:12", "pdf_url": "http://arxiv.org/pdf/2505.16196v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16199v1", "title": "Velocity Completion Task and Method for Event-based Player Positional Data in Soccer", "authors": ["Rikuhei Umemoto", "Keisuke Fujii"], "abstract": "In many real-world complex systems, the behavior can be observed as a\ncollection of discrete events generated by multiple interacting agents.\nAnalyzing the dynamics of these multi-agent systems, especially team sports,\noften relies on understanding the movement and interactions of individual\nagents. However, while providing valuable snapshots, event-based positional\ndata typically lacks the continuous temporal information needed to directly\ncalculate crucial properties such as velocity. This absence severely limits the\ndepth of dynamic analysis, preventing a comprehensive understanding of\nindividual agent behaviors and emergent team strategies. To address this\nchallenge, we propose a new method to simultaneously complete the velocity of\nall agents using only the event-based positional data from team sports. Based\non this completed velocity information, we investigate the applicability of\nexisting team sports analysis and evaluation methods. Experiments using soccer\nevent data demonstrate that neural network-based approaches outperformed\nrule-based methods regarding velocity completion error, considering the\nunderlying temporal dependencies and graph structure of player-to-player or\nplayer-to-ball interaction. Moreover, the space evaluation results obtained\nusing the completed velocity are closer to those derived from complete tracking\ndata, highlighting our method's potential for enhanced team sports system\nanalysis.", "categories": ["cs.AI"], "published": "2025-05-22 04:01:49", "updated": "2025-05-22 04:01:49", "pdf_url": "http://arxiv.org/pdf/2505.16199v1", "comment": "24 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16208v1", "title": "Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems", "authors": ["Anton Erofeev", "Balasubramanya T. Nadiga", "Ilya Timofeyev"], "abstract": "We apply the Echo-State Networks to predict the time series and statistical\nproperties of the competitive Lotka-Volterra model in the chaotic regime. In\nparticular, we demonstrate that Echo-State Networks successfully learn the\nchaotic attractor of the competitive Lotka-Volterra model and reproduce\nhistograms of dependent variables, including tails and rare events. We use the\nGeneralized Extreme Value distribution to quantify the tail behavior.", "categories": ["nlin.CD", "cs.AI", "cs.LG", "math.DS", "37N99, 68T30"], "published": "2025-05-22 04:21:05", "updated": "2025-05-22 04:21:05", "pdf_url": "http://arxiv.org/pdf/2505.16208v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16210v1", "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 04:23:19", "updated": "2025-05-22 04:23:19", "pdf_url": "http://arxiv.org/pdf/2505.16210v1", "comment": "11 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16211v1", "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhuo Chen", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "abstract": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-22 04:27:46", "updated": "2025-05-22 04:27:46", "pdf_url": "http://arxiv.org/pdf/2505.16211v1", "comment": "Technical Report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16221v1", "title": "LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead", "authors": ["Yifan Zhang", "Xinkui Zhao", "Zuxin Wang", "Guanjie Cheng", "Yueshen Xu", "Shuiguang Deng", "Jianwei Yin"], "abstract": "The rapid advancement of large language models has unlocked remarkable\ncapabilities across a diverse array of natural language processing tasks.\nHowever, the considerable differences among available LLMs-in terms of cost,\nperformance, and computational demands-pose significant challenges for users\naiming to identify the most suitable model for specific tasks. In this work, we\npresent LightRouter, a novel framework designed to systematically select and\nintegrate a small subset of LLMs from a larger pool, with the objective of\njointly optimizing both task performance and cost efficiency. LightRouter\nleverages an adaptive selection mechanism to identify models that require only\na minimal number of boot tokens, thereby reducing costs, and further employs an\neffective integration strategy to combine their outputs. Extensive experiments\nacross multiple benchmarks demonstrate that LightRouter matches or outperforms\nwidely-used ensemble baselines, achieving up to a 25% improvement in accuracy.\nCompared with leading high-performing models, LightRouter achieves comparable\nperformance while reducing inference costs by up to 27%. Importantly, our\nframework operates without any prior knowledge of individual models and relies\nexclusively on inexpensive, lightweight models. This work introduces a\npractical approach for efficient LLM selection and provides valuable insights\ninto optimal strategies for model combination.", "categories": ["cs.AI"], "published": "2025-05-22 04:46:04", "updated": "2025-05-22 04:46:04", "pdf_url": "http://arxiv.org/pdf/2505.16221v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16223v1", "title": "MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network", "authors": ["Sangyong Lee", "Subo Hwang", "Dohoon Kim"], "abstract": "In this paper, we propose MADCluster, a novel model-agnostic anomaly\ndetection framework utilizing self-supervised clustering. MADCluster is\napplicable to various deep learning architectures and addresses the\n'hypersphere collapse' problem inherent in existing deep learning-based anomaly\ndetection methods. The core idea is to cluster normal pattern data into a\n'single cluster' while simultaneously learning the cluster center and mapping\ndata close to this center. Also, to improve expressiveness and enable effective\nsingle clustering, we propose a new 'One-directed Adaptive loss'. The\noptimization of this loss is mathematically proven. MADCluster consists of\nthree main components: Base Embedder capturing high-dimensional temporal\ndynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous\ncenter updates. Its model-agnostic characteristics are achieved by applying\nvarious architectures to the Base Embedder. Experiments on four time series\nbenchmark datasets demonstrate that applying MADCluster improves the overall\nperformance of comparative models. In conclusion, the compatibility of\nMADCluster shows potential for enhancing model performance across various\narchitectures.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-22 04:50:44", "updated": "2025-05-22 04:50:44", "pdf_url": "http://arxiv.org/pdf/2505.16223v1", "comment": "24 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16225v1", "title": "MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning", "authors": ["Zihan Chen", "Song Wang", "Zhen Tan", "Jundong Li", "Cong Shen"], "abstract": "In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle\ndiverse tasks by incorporating multiple input-output examples, known as\ndemonstrations, into the input of LLMs. More recently, advancements in the\nexpanded context windows of LLMs have led to many-shot ICL, which uses hundreds\nof demonstrations and outperforms few-shot ICL, which relies on fewer examples.\nHowever, this approach is often hindered by the high cost of obtaining large\namounts of labeled data. To address this challenge, we propose Many-Shot\nAdaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL\nframework that utilizes pseudo-labeled samples to compensate for the lack of\nlabel information. We first identify a subset of impactful unlabeled samples\nand perform pseudo-labeling on them by querying LLMs. These pseudo-labeled\nsamples are then adaptively selected and tailored to each test query as input\nto improve the performance of many-shot ICL, without significant labeling\ncosts. Extensive experiments on real-world datasets demonstrate the\neffectiveness of our framework, showcasing its ability to enhance LLM\nadaptability and performance with limited labeled data.", "categories": ["cs.AI"], "published": "2025-05-22 04:54:27", "updated": "2025-05-22 04:54:27", "pdf_url": "http://arxiv.org/pdf/2505.16225v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16227v1", "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "abstract": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 04:55:41", "updated": "2025-05-22 04:55:41", "pdf_url": "http://arxiv.org/pdf/2505.16227v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16234v1", "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "abstract": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 05:08:27", "updated": "2025-05-22 05:08:27", "pdf_url": "http://arxiv.org/pdf/2505.16234v1", "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16249v1", "title": "Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control", "authors": ["Zhen Zhang", "Xiangyu Chu", "Yunxi Tang", "Lulu Zhao", "Jing Huang", "Zhongliang Jiang", "K. W. Samuel Au"], "abstract": "Manipulating elasto-plastic objects remains a significant challenge due to\nsevere self-occlusion, difficulties of representation, and complicated\ndynamics. This work proposes a novel framework for elasto-plastic object\nmanipulation with a quasi-static assumption for motions, leveraging 3D\noccupancy to represent such objects, a learned dynamics model trained with 3D\noccupancy, and a learning-based predictive control algorithm to address these\nchallenges effectively. We build a novel data collection platform to collect\nfull spatial information and propose a pipeline for generating a 3D occupancy\ndataset. To infer the 3D occupancy during manipulation, an occupancy prediction\nnetwork is trained with multiple RGB images supervised by the generated\ndataset. We design a deep neural network empowered by a 3D convolution neural\nnetwork (CNN) and a graph neural network (GNN) to predict the complex\ndeformation with the inferred 3D occupancy results. A learning-based predictive\ncontrol algorithm is introduced to plan the robot actions, incorporating a\nnovel shape-based action initialization module specifically designed to improve\nthe planner efficiency. The proposed framework in this paper can successfully\nshape the elasto-plastic objects into a given goal shape and has been verified\nin various experiments both in simulation and the real world.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 05:36:00", "updated": "2025-05-22 05:36:00", "pdf_url": "http://arxiv.org/pdf/2505.16249v1", "comment": "8 Pages, 5 figures, accepted for publication in IEEE Robotics and\n  Automation Letters (RA-L)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16256v1", "title": "DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor", "authors": ["Yan Zhao", "Zhengxue Cheng", "Junxuan Zhang", "Qunshan Gu", "Qi Wang", "Li Song"], "abstract": "Most learning-based lossless compressors are designed for a single modality,\nrequiring separate models for multi-modal data and lacking flexibility.\nHowever, different modalities vary significantly in format and statistical\nproperties, making it ineffective to use compressors that lack\nmodality-specific adaptations. While multi-modal large language models (MLLMs)\noffer a potential solution for modality-unified compression, their excessive\ncomplexity hinders practical deployment. To address these challenges, we focus\non the two most common modalities, image and text, and propose DualComp, the\nfirst unified and lightweight learning-based dual-modality lossless compressor.\nBuilt on a lightweight backbone, DualComp incorporates three key structural\nenhancements to handle modality heterogeneity: modality-unified tokenization,\nmodality-switching contextual learning, and modality-routing\nmixture-of-experts. A reparameterization training strategy is also used to\nboost compression performance. DualComp integrates both modality-specific and\nshared parameters for efficient parameter utilization, enabling near real-time\ninference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp\nachieves compression performance on par with the SOTA LLM-based methods for\nboth text and image datasets. Its simplified single-modality variant surpasses\nthe previous best image compressor on the Kodak dataset by about 9% using just\n1.2% of the model size.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "published": "2025-05-22 05:46:14", "updated": "2025-05-22 05:46:14", "pdf_url": "http://arxiv.org/pdf/2505.16256v1", "comment": "18 pages, 11 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16258v1", "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "abstract": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "published": "2025-05-22 05:49:01", "updated": "2025-05-22 05:49:01", "pdf_url": "http://arxiv.org/pdf/2505.16258v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16259v1", "title": "Dialogue in Resonance: An Interactive Music Piece for Piano and Real-Time Automatic Transcription System", "authors": ["Hayeon Bang", "Taegyun Kwon", "Juhan Nam"], "abstract": "This paper presents <Dialogue in Resonance>, an interactive music piece for a\nhuman pianist and a computer-controlled piano that integrates real-time\nautomatic music transcription into a score-driven framework. Unlike previous\napproaches that primarily focus on improvisation-based interactions, our work\nestablishes a balanced framework that combines composed structure with dynamic\ninteraction. Through real-time automatic transcription as its core mechanism,\nthe computer interprets and responds to the human performer's input in real\ntime, creating a musical dialogue that balances compositional intent with live\ninteraction while incorporating elements of unpredictability. In this paper, we\npresent the development process from composition to premiere performance,\nincluding technical implementation, rehearsal process, and performance\nconsiderations.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-22 05:50:13", "updated": "2025-05-22 05:50:13", "pdf_url": "http://arxiv.org/pdf/2505.16259v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16270v1", "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "abstract": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 06:00:45", "updated": "2025-05-22 06:00:45", "pdf_url": "http://arxiv.org/pdf/2505.16270v1", "comment": "33 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16276v1", "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance", "authors": ["Desiree Heim", "Lars-Peter Meyer", "Markus Schr\u00f6der", "Johannes Frey", "Andreas Dengel"], "abstract": "When using Large Language Models (LLMs) to support Knowledge Graph\nEngineering (KGE), one of the first indications when searching for an\nappropriate model is its size. According to the scaling laws, larger models\ntypically show higher capabilities. However, in practice, resource costs are\nalso an important factor and thus it makes sense to consider the ratio between\nmodel performance and costs. The LLM-KG-Bench framework enables the comparison\nof LLMs in the context of KGE tasks and assesses their capabilities of\nunderstanding and producing KGs and KG queries. Based on a dataset created in\nan LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the\nmodel size scaling laws specific to KGE tasks. In our analyses, we assess how\nbenchmark scores evolve between different model size categories. Additionally,\nwe inspect how the general score development of single models and families of\nmodels correlates to their size. Our analyses revealed that, with a few\nexceptions, the model size scaling laws generally also apply to the selected\nKGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,\nthe task performance did not change much between a model and the next larger\nmodel. In these cases, smaller models could be considered to achieve high\ncost-effectiveness. Regarding models of the same family, sometimes larger\nmodels performed worse than smaller models of the same family. These effects\noccurred only locally. Hence it is advisable to additionally test the next\nsmallest and largest model of the same family.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 06:21:40", "updated": "2025-05-22 06:21:40", "pdf_url": "http://arxiv.org/pdf/2505.16276v1", "comment": "Peer reviewed and to appear in the ESWC 2025 Workshops and Tutorials\n  Joint Proceedings (Workshop on Evaluation of Language Models in Knowledge\n  Engineering [ELMKE])", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16278v1", "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "authors": ["Zhenjie Yang", "Yilin Chai", "Xiaosong Jia", "Qifeng Li", "Yuqian Shao", "Xuekai Zhu", "Haisheng Su", "Junchi Yan"], "abstract": "End-to-end autonomous driving (E2E-AD) demands effective processing of\nmulti-view sensory data and robust handling of diverse and complex driving\nscenarios, particularly rare maneuvers such as aggressive turns. Recent success\nof Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)\ndemonstrates that specialization of parameters enables strong scalability. In\nthis work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a\nScene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is\nbuilt upon our $\\pi_0$ Vision-Language-Action (VLA) baseline (originally from\nthe embodied AI field), called Drive-$\\pi_0$. Specifically, we add Vision MoE\nto Drive-$\\pi_0$ by training a router to select relevant cameras according to\nthe driving context dynamically. This design mirrors human driving cognition,\nwhere drivers selectively attend to crucial visual cues rather than\nexhaustively processing all visual information. In addition, we add Action MoE\nby training another router to activate specialized expert modules for different\ndriving behaviors. Through explicit behavioral specialization, DriveMoE is able\nto handle diverse scenarios without suffering from modes averaging like\nexisting models. In Bench2Drive closed-loop evaluation experiments, DriveMoE\nachieves state-of-the-art (SOTA) performance, demonstrating the effectiveness\nof combining vision and action MoE in autonomous driving tasks. We will release\nour code and models of DriveMoE and Drive-$\\pi_0$.", "categories": ["cs.CV", "cs.AI", "cs.RO"], "published": "2025-05-22 06:23:04", "updated": "2025-05-22 06:23:04", "pdf_url": "http://arxiv.org/pdf/2505.16278v1", "comment": "Project Page: https://thinklab-sjtu.github.io/DriveMoE/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16288v1", "title": "No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery", "authors": ["Xiaoxue Han", "Pengfei Hu", "Jun-En Ding", "Chang Lu", "Feng Liu", "Yue Ning"], "abstract": "Deep learning models trained on extensive Electronic Health Records (EHR)\ndata have achieved high accuracy in diagnosis prediction, offering the\npotential to assist clinicians in decision-making and treatment planning.\nHowever, these models lack two crucial features that clinicians highly value:\ninterpretability and interactivity. The ``black-box'' nature of these models\nmakes it difficult for clinicians to understand the reasoning behind\npredictions, limiting their ability to make informed decisions. Additionally,\nthe absence of interactive mechanisms prevents clinicians from incorporating\ntheir own knowledge and experience into the decision-making process. To address\nthese limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal\ndiscovery framework that integrates personalized knowledge databases and\nagentic LLMs. II-KEA enhances interpretability through explicit reasoning and\ncausal analysis, while also improving interactivity by allowing clinicians to\ninject their knowledge and experience through customized knowledge bases and\nprompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating\nsuperior performance along with enhanced interpretability and interactivity, as\nevidenced by its strong results from extensive case studies.", "categories": ["cs.AI"], "published": "2025-05-22 06:36:30", "updated": "2025-05-22 06:36:30", "pdf_url": "http://arxiv.org/pdf/2505.16288v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16290v1", "title": "Multimodal Generative AI for Story Point Estimation in Software Development", "authors": ["Mohammad Rubyet Islam", "Peter Sandborn"], "abstract": "This research explores the application of Multimodal Generative AI to enhance\nstory point estimation in Agile software development. By integrating text,\nimage, and categorical data using advanced models like BERT, CNN, and XGBoost,\nour approach surpasses the limitations of traditional single-modal estimation\nmethods. The results demonstrate strong accuracy for simpler story points,\nwhile also highlighting challenges in more complex categories due to data\nimbalance. This study further explores the impact of categorical data,\nparticularly severity, on the estimation process, emphasizing its influence on\nmodel performance. Our findings emphasize the transformative potential of\nmultimodal data integration in refining AI-driven project management, paving\nthe way for more precise, adaptable, and domain-specific AI capabilities.\nAdditionally, this work outlines future directions for addressing data\nvariability and enhancing the robustness of AI in Agile methodologies.", "categories": ["cs.SE", "cs.AI", "68T07, 68T45", "I.2.6; I.2.10; D.2.9; H.2.8"], "published": "2025-05-22 06:40:41", "updated": "2025-05-22 06:40:41", "pdf_url": "http://arxiv.org/pdf/2505.16290v1", "comment": null, "doi": null, "journal_ref": "A revised version of this work is published in the proceedings of\n  the IEEE Conference on Artificial Intelligence 2025"}
{"arxiv_id": "2505.16301v1", "title": "Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space", "authors": ["Fuchun Ge", "Pavlo O. Dral"], "abstract": "Molecular dynamics (MD) is a powerful tool for exploring the behavior of\natomistic systems, but its reliance on sequential numerical integration limits\nsimulation efficiency. We present MDtrajNet-1, a foundational AI model that\ndirectly generates MD trajectories across chemical space, bypassing force\ncalculations and integration. This approach accelerates simulations by up to\ntwo orders of magnitude compared to traditional MD, even those enhanced by\nmachine-learning interatomic potentials. MDtrajNet-1 combines equivariant\nneural networks with a Transformer-based architecture to achieve strong\naccuracy and transferability in predicting long-time trajectories for both\nknown and unseen systems. Remarkably, the errors of the trajectories generated\nby MDtrajNet-1 for various molecular systems are close to those of the\nconventional ab initio MD. The model's flexible design supports diverse\napplication scenarios, including different statistical ensembles, boundary\nconditions, and interaction types. By overcoming the intrinsic speed barrier of\nconventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable\natomistic simulations.", "categories": ["physics.chem-ph", "cs.AI", "cs.LG"], "published": "2025-05-22 06:56:19", "updated": "2025-05-22 06:56:19", "pdf_url": "http://arxiv.org/pdf/2505.16301v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16306v1", "title": "Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models", "authors": ["Yizhi Zhou", "Haina Zhu", "Hangting Chen"], "abstract": "Recently, pre-trained models for music information retrieval based on\nself-supervised learning (SSL) are becoming popular, showing success in various\ndownstream tasks. However, there is limited research on the specific meanings\nof the encoded information and their applicability. Exploring these aspects can\nhelp us better understand their capabilities and limitations, leading to more\neffective use in downstream tasks.\n  In this study, we analyze the advanced music representation model MusicFM and\nthe newly emerged SSL model MuQ. We focus on three main aspects: (i) validating\nthe advantages of SSL models across multiple downstream tasks, (ii) exploring\nthe specialization of layer-wise information for different tasks, and (iii)\ncomparing performance differences when selecting specific layers. Through this\nanalysis, we reveal insights into the structure and potential applications of\nSSL models in music information retrieval.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-22 06:58:24", "updated": "2025-05-22 06:58:24", "pdf_url": "http://arxiv.org/pdf/2505.16306v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16307v1", "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "abstract": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 06:59:10", "updated": "2025-05-22 06:59:10", "pdf_url": "http://arxiv.org/pdf/2505.16307v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16312v1", "title": "EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning", "authors": ["Jiawei Liu", "Qisi Chen", "Jianshu Zhang", "Quan Liu", "Defu Lian"], "abstract": "Large Language Models (LLMs) excel at complex reasoning through search\nalgorithms, yet current strategies often suffer from massive token consumption\ndue to redundant exploration of semantically equivalent steps. Existing\nsemantic similarity methods struggle to accurately identify such equivalence in\ndomain-specific contexts like mathematical reasoning. To address this, we\npropose EquivPruner, a simple yet effective approach that identifies and prunes\nsemantically equivalent actions during LLM reasoning search. We also introduce\nMathEquiv, the first dataset we created for mathematical statement equivalence,\nwhich enables the training of a lightweight equivalence detector. Extensive\nexperiments across various models and tasks demonstrate that EquivPruner\nsignificantly reduces token consumption, improving searching efficiency and\noften bolstering reasoning accuracy. For instance, when applied to\nQwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by\n48.1\\% while also improving accuracy. Our code is available at\nhttps://github.com/Lolo1222/EquivPruner.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 07:07:43", "updated": "2025-05-22 07:07:43", "pdf_url": "http://arxiv.org/pdf/2505.16312v1", "comment": "11 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16314v1", "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment", "authors": ["Shuhao Han", "Haotian Fan", "Fangyuan Kong", "Wenjie Liao", "Chunle Guo", "Chongyi Li", "Radu Timofte", "Liang Li", "Tao Li", "Junhui Cui", "Yunqiu Wang", "Yang Tai", "Jingwei Sun", "Jianhui Sun", "Xinli Yue", "Tianyi Wang", "Huan Hou", "Junda Lu", "Xinyang Huang", "Zitang Zhou", "Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao", "Trong-Hieu Nguyen-Mau", "Minh-Hoang Le", "Minh-Khoa Le-Phan", "Duy-Nam Ly", "Hai-Dang Nguyen", "Minh-Triet Tran", "Yukang Lin", "Yan Hong", "Chuanbiao Song", "Siyuan Li", "Jun Lan", "Zhichao Zhang", "Xinyue Li", "Wei Sun", "Zicheng Zhang", "Yunhao Li", "Xiaohong Liu", "Guangtao Zhai", "Zitong Xu", "Huiyu Duan", "Jiarui Wang", "Guangji Ma", "Liu Yang", "Lu Liu", "Qiang Hu", "Xiongkuo Min", "Zichuan Wang", "Zhenchen Tang", "Bo Peng", "Jing Dong", "Fengbin Guan", "Zihao Yu", "Yiting Lu", "Wei Luo", "Xin Li", "Minhao Lin", "Haofeng Chen", "Xuanxuan He", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Bo-Cheng Qiu", "Chih-Chung Hsu", "Chia-ming Lee", "Yu-Fan Lin", "Bo Yu", "Zehao Wang", "Da Mu", "Mingxiu Chen", "Junkang Fang", "Huamei Sun", "Wending Zhao", "Zhiyu Wang", "Wang Liu", "Weikang Yu", "Puhong Duan", "Bin Sun", "Xudong Kang", "Shutao Li", "Shuai He", "Lingzhi Fu", "Heng Cong", "Rongyu Zhang", "Jiarong He", "Zhishan Qiao", "Yongqing Huang", "Zewen Chen", "Zhe Pang", "Juan Wang", "Jian Guo", "Zhizhuo Shao", "Ziyu Feng", "Bing Li", "Weiming Hu", "Hesong Li", "Dehua Liu", "Zeming Liu", "Qingsong Xie", "Ruichen Wang", "Zhihao Li", "Yuqi Liang", "Jianqi Bi", "Jun Luo", "Junfeng Yang", "Can Li", "Jing Fu", "Hongwei Xu", "Mingrui Long", "Lulin Tang"], "abstract": "This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)\ngeneration model quality assessment, which will be held in conjunction with the\nNew Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.\nThe aim of this challenge is to address the fine-grained quality assessment of\ntext-to-image generation models. This challenge evaluates text-to-image models\nfrom two aspects: image-text alignment and image structural distortion\ndetection, and is divided into the alignment track and the structural track.\nThe alignment track uses the EvalMuse-40K, which contains around 40K\nAI-Generated Images (AIGIs) generated by 20 popular generative models. The\nalignment track has a total of 371 registered participants. A total of 1,883\nsubmissions are received in the development phase, and 507 submissions are\nreceived in the test phase. Finally, 12 participating teams submitted their\nmodels and fact sheets. The structure track uses the EvalMuse-Structure, which\ncontains 10,000 AI-Generated Images (AIGIs) with corresponding structural\ndistortion mask. A total of 211 participants have registered in the structure\ntrack. A total of 1155 submissions are received in the development phase, and\n487 submissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Almost all methods have achieved better\nresults than baseline methods, and the winning methods in both tracks have\ndemonstrated superior prediction performance on T2I model quality assessment.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 07:12:36", "updated": "2025-05-22 07:12:36", "pdf_url": "http://arxiv.org/pdf/2505.16314v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16315v1", "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning", "authors": ["Xiaoxue Cheng", "Junyi Li", "Zhenduo Zhang", "Xinyu Tang", "Wayne Xin Zhao", "Xinyu Kong", "Zhiqiang Zhang"], "abstract": "Large reasoning models (LRMs) have demonstrated strong performance on complex\nreasoning tasks, but often suffer from overthinking, generating redundant\ncontent regardless of task difficulty. Inspired by the dual process theory in\ncognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a\nreinforcement learning framework that enables LRMs to achieve efficient\nreasoning through adaptive cognitive allocation and dynamic system switch. ACPO\nincorporates two key components: (1) introducing system-aware reasoning tokens\nto explicitly represent the thinking modes thereby making the model's cognitive\nprocess transparent, and (2) integrating online difficulty estimation and token\nlength budget to guide adaptive system switch and reasoning during\nreinforcement learning. To this end, we propose a two-stage training strategy.\nThe first stage begins with supervised fine-tuning to cold start the model,\nenabling it to generate reasoning paths with explicit thinking modes. In the\nsecond stage, we apply ACPO to further enhance adaptive system switch for\ndifficulty-aware reasoning. Experimental results demonstrate that ACPO\neffectively reduces redundant reasoning while adaptively adjusting cognitive\nallocation based on task complexity, achieving efficient hybrid reasoning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 07:15:08", "updated": "2025-05-22 07:15:08", "pdf_url": "http://arxiv.org/pdf/2505.16315v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16322v1", "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "abstract": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 07:24:11", "updated": "2025-05-22 07:24:11", "pdf_url": "http://arxiv.org/pdf/2505.16322v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16325v1", "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "abstract": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-22 07:32:12", "updated": "2025-05-22 07:32:12", "pdf_url": "http://arxiv.org/pdf/2505.16325v1", "comment": "18 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16330v1", "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "abstract": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM.", "categories": ["cs.CL", "cs.AI", "cs.DL"], "published": "2025-05-22 07:34:59", "updated": "2025-05-22 07:34:59", "pdf_url": "http://arxiv.org/pdf/2505.16330v1", "comment": null, "doi": "10.1016/j.eswa.2025.126778", "journal_ref": "Expert Systems With Applications, 2025"}
{"arxiv_id": "2505.16332v1", "title": "Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing", "authors": ["Zhehui Wanga", "Benjamin Chen Ming Choonga", "Tian Huang", "Daniel Gerlinghoffa", "Rick Siow Mong Goh", "Cheng Liu", "Tao Luo"], "abstract": "Quantum optimization is the most mature quantum computing technology to date,\nproviding a promising approach towards efficiently solving complex\ncombinatorial problems. Methods such as adiabatic quantum computing (AQC) have\nbeen employed in recent years on important optimization problems across various\ndomains. In deep learning, deep neural networks (DNN) have reached immense\nsizes to support new predictive capabilities. Optimization of large-scale\nmodels is critical for sustainable deployment, but becomes increasingly\nchallenging with ever-growing model sizes and complexity. While quantum\noptimization is suitable for solving complex problems, its application to DNN\noptimization is not straightforward, requiring thorough reformulation for\ncompatibility with commercially available quantum devices. In this work, we\nexplore the potential of adopting AQC for fine-grained pruning-quantization of\nconvolutional neural networks. We rework established heuristics to formulate\nmodel compression as a quadratic unconstrained binary optimization (QUBO)\nproblem, and assess the solution space offered by commercial quantum annealing\ndevices. Through our exploratory efforts of reformulation, we demonstrate that\nAQC can achieve effective compression of practical DNN models. Experiments\ndemonstrate that adiabatic quantum computing (AQC) not only outperforms\nclassical algorithms like genetic algorithms and reinforcement learning in\nterms of time efficiency but also excels at identifying global optima.", "categories": ["quant-ph", "cs.AI"], "published": "2025-05-22 07:40:23", "updated": "2025-05-22 07:40:23", "pdf_url": "http://arxiv.org/pdf/2505.16332v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16335v1", "title": "FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design", "authors": ["Renjie Wei", "Songqiang Xu", "Qingyu Guo", "Meng Li"], "abstract": "Visual autoregressive (VAR) modeling has marked a paradigm shift in image\ngeneration from next-token prediction to next-scale prediction. VAR predicts a\nset of tokens at each step from coarse to fine scale, leading to better image\nquality and faster inference speed compared to existing diffusion models.\nHowever, the large parameter size and computation cost hinder its deployment on\nedge devices. To reduce the memory and computation cost, we propose FPQVAR, an\nefficient post-training floating-point (FP) quantization framework for VAR\nfeaturing algorithm and hardware co-design. At the algorithm level, we first\nidentify the challenges of quantizing VAR. To address them, we propose Dual\nFormat Quantization for the highly imbalanced input activation. We further\npropose Group-wise Hadamard Transformation and GHT-Aware Learnable\nTransformation to address the time-varying outlier channels. At the hardware\nlevel, we design the first low-bit FP quantizer and multiplier with lookup\ntables on FPGA and propose the first FPGA-based VAR accelerator featuring\nlow-bit FP computation and an elaborate two-level pipeline. Extensive\nexperiments show that compared to the state-of-the-art quantization method, our\nproposed FPQVAR significantly improves Fr\\'echet Inception Distance (FID) from\n10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit\nquantization. FPQVAR also significantly improves the performance of 6-bit\nquantized VAR, bringing it on par with the FP16 model. Our accelerator on\nAMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x\nhigher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x\nhigher energy efficiency compared to the integer-based accelerator and GPU\nbaseline, respectively.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 07:47:51", "updated": "2025-05-22 07:47:51", "pdf_url": "http://arxiv.org/pdf/2505.16335v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16351v1", "title": "Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection", "authors": ["Chenxu Guo", "Jiachen Lian", "Xuanru Zhou", "Jinming Zhang", "Shuhe Li", "Zongli Ye", "Hwi Joo Park", "Anaisha Das", "Zoe Ezzes", "Jet Vonk", "Brittany Morin", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "abstract": "Automatic detection of speech dysfluency aids speech-language pathologists in\nefficient transcription of disordered speech, enhancing diagnostics and\ntreatment planning. Traditional methods, often limited to classification,\nprovide insufficient clinical insight, and text-independent models misclassify\ndysfluency, especially in context-dependent cases. This work introduces\nDysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes\nand detects dysfluency. Unlike previous models, Dysfluent-WFST operates with\nupstream encoders like WavLM and requires no additional training. It achieves\nstate-of-the-art performance in both phonetic error rate and dysfluency\ndetection on simulated and real speech data. Our approach is lightweight,\ninterpretable, and effective, demonstrating that explicit modeling of\npronunciation behavior in decoding, rather than complex architectures, is key\nto improving dysfluency processing systems.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-22 08:02:50", "updated": "2025-05-22 08:02:50", "pdf_url": "http://arxiv.org/pdf/2505.16351v1", "comment": null, "doi": null, "journal_ref": "Interspeech 2025"}
{"arxiv_id": "2505.16362v1", "title": "Neuromorphic-based metaheuristics: A new generation of low power, low latency and small footprint optimization algorithms", "authors": ["El-ghazali Talbi"], "abstract": "Neuromorphic computing (NC) introduces a novel algorithmic paradigm\nrepresenting a major shift from traditional digital computing of Von Neumann\narchitectures. NC emulates or simulates the neural dynamics of brains in the\nform of Spiking Neural Networks (SNNs). Much of the research in NC has\nconcentrated on machine learning applications and neuroscience simulations.\nThis paper investigates the modelling and implementation of optimization\nalgorithms and particularly metaheuristics using the NC paradigm as an\nalternative to Von Neumann architectures, leading to breakthroughs in solving\noptimization problems.\n  Neuromorphic-based metaheuristics (Nheuristics) are supposed to be\ncharacterized by low power, low latency and small footprint. Since NC systems\nare fundamentally different from conventional Von Neumann computers, several\nchallenges are posed to the design and implementation of Nheuristics. A\nguideline based on a classification and critical analysis is conducted on the\ndifferent families of metaheuristics and optimization problems they address. We\nalso discuss future directions that need to be addressed to expand both the\ndevelopment and application of Nheuristics.", "categories": ["cs.NE", "cs.AI"], "published": "2025-05-22 08:14:07", "updated": "2025-05-22 08:14:07", "pdf_url": "http://arxiv.org/pdf/2505.16362v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16363v1", "title": "AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training", "authors": ["Huishuai Zhang", "Bohan Wang", "Luoxin Chen"], "abstract": "We introduce AdamS, a simple yet effective alternative to Adam for large\nlanguage model (LLM) pretraining and post-training. By leveraging a novel\ndenominator, i.e., the root of weighted sum of squares of the momentum and the\ncurrent gradient, AdamS eliminates the need for second-moment estimates. Hence,\nAdamS is efficient, matching the memory and compute footprint of SGD with\nmomentum while delivering superior optimization performance. Moreover, AdamS is\neasy to adopt: it can directly inherit hyperparameters of AdamW, and is\nentirely model-agnostic, integrating seamlessly into existing pipelines without\nmodifications to optimizer APIs or architectures. The motivation behind AdamS\nstems from the observed $(L_0, L_1)$ smoothness properties in transformer\nobjectives, where local smoothness is governed by gradient magnitudes that can\nbe further approximated by momentum magnitudes. We establish rigorous\ntheoretical convergence guarantees and provide practical guidelines for\nhyperparameter selection. Empirically, AdamS demonstrates strong performance in\nvarious tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B\nparameters) and reinforcement learning in post-training regimes. With its\nefficiency, simplicity, and theoretical grounding, AdamS stands as a compelling\nalternative to existing optimizers.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-22 08:16:48", "updated": "2025-05-22 08:16:48", "pdf_url": "http://arxiv.org/pdf/2505.16363v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16365v1", "title": "A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules", "authors": ["Manuel Ruiz-Botella", "Marta Sales-Pardo", "Roger Guimer\u00e0"], "abstract": "Developing new molecular compounds is crucial to address pressing challenges,\nfrom health to environmental sustainability. However, exploring the molecular\nspace to discover new molecules is difficult due to the vastness of the space.\nHere we introduce CoCoGraph, a collaborative and constrained graph diffusion\nmodel capable of generating molecules that are guaranteed to be chemically\nvalid. Thanks to the constraints built into the model and to the collaborative\nmechanism, CoCoGraph outperforms state-of-the-art approaches on standard\nbenchmarks while requiring up to an order of magnitude fewer parameters.\nAnalysis of 36 chemical properties also demonstrates that CoCoGraph generates\nmolecules with distributions more closely matching real molecules than current\nmodels. Leveraging the model's efficiency, we created a database of 8.2M\nmillion synthetically generated molecules and conducted a Turing-like test with\norganic chemistry experts to further assess the plausibility of the generated\nmolecules, and potential biases and limitations of CoCoGraph.", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "q-bio.QM"], "published": "2025-05-22 08:21:27", "updated": "2025-05-22 08:21:27", "pdf_url": "http://arxiv.org/pdf/2505.16365v1", "comment": "28 pages, 10 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16368v1", "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning", "authors": ["Huanyu Liu", "Jia Li", "Hao Zhu", "Kechi Zhang", "Yihong Dong", "Ge Li"], "abstract": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLM\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 08:23:10", "updated": "2025-05-22 08:23:10", "pdf_url": "http://arxiv.org/pdf/2505.16368v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16372v1", "title": "Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition", "authors": ["Feng Liu", "Bingyu Nan", "Xuezhong Qian", "Xiaolan Fu"], "abstract": "When emotions are repressed, an individual's true feelings may be revealed\nthrough micro-expressions. Consequently, micro-expressions are regarded as a\ngenuine source of insight into an individual's authentic emotions. However, the\ntransient and highly localised nature of micro-expressions poses a significant\nchallenge to their accurate recognition, with the accuracy rate of\nmicro-expression recognition being as low as 50%, even for professionals. In\norder to address these challenges, it is necessary to explore the field of\ndynamic micro expression recognition (DMER) using multimodal fusion techniques,\nwith special attention to the diverse fusion of temporal and spatial modal\nfeatures. In this paper, we propose a novel Temporal and Spatial feature Fusion\nframework for DMER (TSFmicro). This framework integrates a Retention Network\n(RetNet) and a transformer-based DMER network, with the objective of efficient\nmicro-expression recognition through the capture and fusion of temporal and\nspatial relations. Meanwhile, we propose a novel parallel time-space fusion\nmethod from the perspective of modal fusion, which fuses spatio-temporal\ninformation in high-dimensional feature space, resulting in complementary\n\"where-how\" relationships at the semantic level and providing richer semantic\ninformation for the model. The experimental results demonstrate the superior\nperformance of the TSFmicro method in comparison to other contemporary\nstate-of-the-art methods. This is evidenced by its effectiveness on three\nwell-recognised micro-expression datasets.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 08:26:19", "updated": "2025-05-22 08:26:19", "pdf_url": "http://arxiv.org/pdf/2505.16372v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16376v1", "title": "DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos", "authors": ["Zijia Lu", "A S M Iftekhar", "Gaurav Mittal", "Tianjian Meng", "Xiawei Wang", "Cheng Zhao", "Rohith Kukkala", "Ehsan Elhamifar", "Mei Chen"], "abstract": "Long Video Temporal Grounding (LVTG) aims at identifying specific moments\nwithin lengthy videos based on user-provided text queries for effective content\nretrieval. The approach taken by existing methods of dividing video into clips\nand processing each clip via a full-scale expert encoder is challenging to\nscale due to prohibitive computational costs of processing a large number of\nclips in long videos. To address this issue, we introduce DeCafNet, an approach\nemploying ``delegate-and-conquer'' strategy to achieve computation efficiency\nwithout sacrificing grounding performance. DeCafNet introduces a sidekick\nencoder that performs dense feature extraction over all video clips in a\nresource-efficient manner, while generating a saliency map to identify the most\nrelevant clips for full processing by the expert encoder. To effectively\nleverage features from sidekick and expert encoders that exist at different\ntemporal resolutions, we introduce DeCaf-Grounder, which unifies and refines\nthem via query-aware temporal aggregation and multi-scale temporal refinement\nfor accurate grounding. Experiments on two LTVG benchmark datasets demonstrate\nthat DeCafNet reduces computation by up to 47\\% while still outperforming\nexisting methods, establishing a new state-of-the-art for LTVG in terms of both\nefficiency and performance. Our code is available at\nhttps://github.com/ZijiaLewisLu/CVPR2025-DeCafNet.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 08:29:57", "updated": "2025-05-22 08:29:57", "pdf_url": "http://arxiv.org/pdf/2505.16376v1", "comment": "Accepted by CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16377v1", "title": "VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving", "authors": ["Yansong Qu", "Zilin Huang", "Zihao Sheng", "Jiancong Chen", "Sikai Chen", "Samuel Labi"], "abstract": "Reinforcement learning (RL)-based autonomous driving policy learning faces\ncritical limitations such as low sample efficiency and poor generalization; its\nreliance on online interactions and trial-and-error learning is especially\nunacceptable in safety-critical scenarios. Existing methods including safe RL\noften fail to capture the true semantic meaning of \"safety\" in complex driving\ncontexts, leading to either overly conservative driving behavior or constraint\nviolations. To address these challenges, we propose VL-SAFE, a world\nmodel-based safe RL framework with Vision-Language model\n(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.\nSpecifically, we construct offline datasets containing data collected by expert\nagents and labeled with safety scores derived from VLMs. A world model is\ntrained to generate imagined rollouts together with safety estimations,\nallowing the agent to perform safe planning without interacting with the real\nenvironment. Based on these imagined trajectories and safety evaluations,\nactor-critic learning is conducted under VLM-based safety guidance to optimize\nthe driving policy more safely and efficiently. Extensive evaluations\ndemonstrate that VL-SAFE achieves superior sample efficiency, generalization,\nsafety, and overall performance compared to existing baselines. To the best of\nour knowledge, this is the first work that introduces a VLM-guided world\nmodel-based approach for safe autonomous driving. The demo video and code can\nbe accessed at: https://ys-qu.github.io/vlsafe-website/", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 08:29:59", "updated": "2025-05-22 08:29:59", "pdf_url": "http://arxiv.org/pdf/2505.16377v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16379v1", "title": "Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey", "authors": ["Zhixun Li", "Bin Cao", "Rui Jiao", "Liang Wang", "Ding Wang", "Yang Liu", "Dingshuo Chen", "Jia Li", "Qiang Liu", "Yu Rong", "Liang Wang", "Tong-yi Zhang", "Jeffrey Xu Yu"], "abstract": "Materials are the foundation of modern society, underpinning advancements in\nenergy, electronics, healthcare, transportation, and infrastructure. The\nability to discover and design new materials with tailored properties is\ncritical to solving some of the most pressing global challenges. In recent\nyears, the growing availability of high-quality materials data combined with\nrapid advances in Artificial Intelligence (AI) has opened new opportunities for\naccelerating materials discovery. Data-driven generative models provide a\npowerful tool for materials design by directly create novel materials that\nsatisfy predefined property requirements. Despite the proliferation of related\nwork, there remains a notable lack of up-to-date and systematic surveys in this\narea. To fill this gap, this paper provides a comprehensive overview of recent\nprogress in AI-driven materials generation. We first organize various types of\nmaterials and illustrate multiple representations of crystalline materials. We\nthen provide a detailed summary and taxonomy of current AI-driven materials\ngeneration approaches. Furthermore, we discuss the common evaluation metrics\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future directions and challenges in this fast-growing field. The\nrelated sources can be found at\nhttps://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation.", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "published": "2025-05-22 08:33:21", "updated": "2025-05-22 08:33:21", "pdf_url": "http://arxiv.org/pdf/2505.16379v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16388v1", "title": "Serious Games: Human-AI Interaction, Evolution, and Coevolution", "authors": ["Nandini Doreswamy", "Louise Horstmanshof"], "abstract": "The serious games between humans and AI have only just begun. Evolutionary\nGame Theory (EGT) models the competitive and cooperative strategies of\nbiological entities. EGT could help predict the potential evolutionary\nequilibrium of humans and AI. The objective of this work was to examine some of\nthe EGT models relevant to human-AI interaction, evolution, and coevolution. Of\nthirteen EGT models considered, three were examined: the Hawk-Dove Game,\nIterated Prisoner's Dilemma, and the War of Attrition. This selection was based\non the widespread acceptance and clear relevance of these models to potential\nhuman-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove\nGame predicts balanced mixed-strategy equilibria based on the costs of\nconflict. It also shows the potential for balanced coevolution rather than\ndominance. Iterated Prisoner's Dilemma suggests that repeated interaction may\nlead to cognitive coevolution. It demonstrates how memory and reciprocity can\nlead to cooperation. The War of Attrition suggests that competition for\nresources may result in strategic coevolution, asymmetric equilibria, and\nconventions on sharing resources. Therefore, EGT may provide a suitable\nframework to understand and predict the human-AI evolutionary dynamic. However,\nfuture research could extend beyond EGT and explore additional frameworks,\nempirical validation methods, and interdisciplinary perspectives. AI is being\nshaped by human input and is evolving in response to it. So too,\nneuroplasticity allows the human brain to grow and evolve in response to\nstimuli. If humans and AI converge in future, what might be the result of human\nneuroplasticity combined with an ever-evolving AI? Future research should be\nmindful of the ethical and cognitive implications of human-AI interaction,\nevolution, and coevolution.", "categories": ["cs.AI", "cs.GT", "91A22 (Primary), 68T99 (Secondary)", "J.4; I.2.0; K.4.1; J.3; K.4.0"], "published": "2025-05-22 08:41:37", "updated": "2025-05-22 08:41:37", "pdf_url": "http://arxiv.org/pdf/2505.16388v1", "comment": "8 pages, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16392v1", "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "abstract": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts.", "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "published": "2025-05-22 08:45:14", "updated": "2025-05-22 08:45:14", "pdf_url": "http://arxiv.org/pdf/2505.16392v1", "comment": "Accepted at SIGIR 2025", "doi": "10.1145/3726302.3730304", "journal_ref": null}
{"arxiv_id": "2505.16394v1", "title": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "authors": ["Zhenjie Yang", "Xiaosong Jia", "Qifeng Li", "Xue Yang", "Maoqing Yao", "Junchi Yan"], "abstract": "Reinforcement Learning (RL) can mitigate the causal confusion and\ndistribution shift inherent to imitation learning (IL). However, applying RL to\nend-to-end autonomous driving (E2E-AD) remains an open problem for its training\ndifficulty, and IL is still the mainstream paradigm in both academia and\nindustry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated\npromising results in neural planning; however, these methods typically require\nprivileged information as input rather than raw sensor data. We fill this gap\nby designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently\ntrain an auxiliary privileged world model paired with a neural planner that\nuses privileged information as input. Subsequently, we introduce a raw sensor\nworld model trained via our proposed Guidance Mechanism, which ensures\nconsistency between the raw sensor world model and the privileged world model\nduring rollouts. Finally, the raw sensor world model combines the prior\nknowledge embedded in the heads of the privileged world model to effectively\nguide the training of the raw sensor policy. Raw2Drive is so far the only RL\nbased end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it\nachieves state-of-the-art performance.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-05-22 08:46:53", "updated": "2025-05-22 08:46:53", "pdf_url": "http://arxiv.org/pdf/2505.16394v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16400v1", "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "abstract": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 08:50:47", "updated": "2025-05-22 08:50:47", "pdf_url": "http://arxiv.org/pdf/2505.16400v1", "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16409v1", "title": "FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS", "authors": ["Chaeeun Kim", "Seungone Kim"], "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\nmulti-step reasoning and calling search engines at appropriate steps. However,\nexisting retrieval-augmented reasoning approaches rely on separate retrieval\nmodels, limiting the LRM's role in retrieval to deciding when to retrieve and\nhow to query. This separation not only increases hardware and operational costs\nbut also leads to errors in the retrieval process due to the representation\nbottleneck, a phenomenon where the retriever's embedding space is not\nexpressive enough to meet the generator's requirements. To address this, we\nshift our perspective from sequence-to-sequence matching to locating the\nanswer-containing paths within the corpus, and propose a novel framework called\nFREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables\nLRMs to retrieve relevant knowledge on their own by acting as both a generator\nand retriever. To achieve this, we introduce a variant of the MCTS algorithm\nspecialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing\nMonte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus\ntoward answer-containing regions. Our results on five open-domain QA\nbenchmarks, including single-hop and multi-hop questions, show that FREESON\nachieves an average improvement of 14.4% in EM and F1 over four multi-step\nreasoning models with a separate retriever, and it also performs comparably to\nthe strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.", "categories": ["cs.AI"], "published": "2025-05-22 09:00:08", "updated": "2025-05-22 09:00:08", "pdf_url": "http://arxiv.org/pdf/2505.16409v1", "comment": "Work In Progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16410v1", "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 09:00:19", "updated": "2025-05-22 09:00:19", "pdf_url": "http://arxiv.org/pdf/2505.16410v1", "comment": "Working in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16415v1", "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "abstract": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 09:04:03", "updated": "2025-05-22 09:04:03", "pdf_url": "http://arxiv.org/pdf/2505.16415v1", "comment": "Work in process", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16416v1", "title": "Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models", "authors": ["Chengcheng Wang", "Jianyuan Guo", "Hongguang Li", "Yuchuan Tian", "Ying Nie", "Chang Xu", "Kai Han"], "abstract": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding\nrelative positional information in large language models (LLMs). However, when\nextended to large vision-language models (LVLMs), its variants introduce\nunintended cross-modal positional biases. Specifically, they enforce relative\npositional dependencies between text token indices and image tokens, causing\nspurious alignments. This issue arises because image tokens representing the\nsame content but located at different spatial positions are assigned distinct\npositional biases, leading to inconsistent cross-modal associations. To address\nthis, we propose Per-Token Distance (PTD) - a simple yet effective metric for\nquantifying the independence of positional encodings across modalities.\nInformed by this analysis, we introduce Circle-RoPE, a novel encoding scheme\nthat maps image token indices onto a circular trajectory orthogonal to the\nlinear path of text token indices, forming a cone-like structure. This\nconfiguration ensures that each text token maintains an equal distance to all\nimage tokens, reducing artificial cross-modal biases while preserving\nintra-image spatial information. To further enhance performance, we propose a\nstaggered layer strategy that applies different RoPE variants across layers.\nThis design leverages the complementary strengths of each RoPE variant, thereby\nenhancing the model's overall performance. Our experimental results demonstrate\nthat our method effectively preserves spatial information from images while\nreducing relative positional bias, offering a more robust and flexible\npositional encoding framework for LVLMs. The code is available at\n[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 09:05:01", "updated": "2025-05-22 09:05:01", "pdf_url": "http://arxiv.org/pdf/2505.16416v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16419v1", "title": "Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment", "authors": ["Soh Takahashi", "Masaru Sasaki", "Ken Takeda", "Masafumi Oizumi"], "abstract": "The learning mechanisms by which humans acquire internal representations of\nobjects are not fully understood. Deep neural networks (DNNs) have emerged as a\nuseful tool for investigating this question, as they have internal\nrepresentations similar to those of humans as a byproduct of optimizing their\nobjective functions. While previous studies have shown that models trained with\nvarious learning paradigms - such as supervised, self-supervised, and CLIP -\nacquire human-like representations, it remains unclear whether their similarity\nto human representations is primarily at a coarse category level or extends to\nfiner details. Here, we employ an unsupervised alignment method based on\nGromov-Wasserstein Optimal Transport to compare human and model object\nrepresentations at both fine-grained and coarse-grained levels. The unique\nfeature of this method compared to conventional representational similarity\nanalysis is that it estimates optimal fine-grained mappings between the\nrepresentation of each object in human and model representations. We used this\nunsupervised alignment method to assess the extent to which the representation\nof each object in humans is correctly mapped to the corresponding\nrepresentation of the same object in models. Using human similarity judgments\nof 1,854 objects from the THINGS dataset, we find that models trained with CLIP\nconsistently achieve strong fine- and coarse-grained matching with human object\nrepresentations. In contrast, self-supervised models showed limited matching at\nboth fine- and coarse-grained levels, but still formed object clusters that\nreflected human coarse category structure. Our results offer new insights into\nthe role of linguistic information in acquiring precise object representations\nand the potential of self-supervised learning to capture coarse categorical\nstructures.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 09:06:06", "updated": "2025-05-22 09:06:06", "pdf_url": "http://arxiv.org/pdf/2505.16419v1", "comment": "34 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16425v1", "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "abstract": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 09:10:09", "updated": "2025-05-22 09:10:09", "pdf_url": "http://arxiv.org/pdf/2505.16425v1", "comment": "13 pages, 5 figures, under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16429v1", "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "abstract": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 09:14:23", "updated": "2025-05-22 09:14:23", "pdf_url": "http://arxiv.org/pdf/2505.16429v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16430v1", "title": "AutoMCQ -- Automatically Generate Code Comprehension Questions using GenAI", "authors": ["Martin Goodfellow", "Robbie Booth", "Andrew Fagan", "Alasdair Lambert"], "abstract": "Students often do not fully understand the code they have written. This\nsometimes does not become evident until later in their education, which can\nmean it is harder to fix their incorrect knowledge or misunderstandings. In\naddition, being able to fully understand code is increasingly important in a\nworld where students have access to generative artificial intelligence (GenAI)\ntools, such as GitHub Copilot. One effective solution is to utilise code\ncomprehension questions, where a marker asks questions about a submission to\ngauge understanding, this can also have the side effect of helping to detect\nplagiarism. However, this approach is time consuming and can be difficult\nand/or expensive to scale. This paper introduces AutoMCQ, which uses GenAI for\nthe automatic generation of multiple-choice code comprehension questions. This\nis integrated with the CodeRunner automated assessment platform.", "categories": ["cs.SE", "cs.AI", "cs.PL"], "published": "2025-05-22 09:14:41", "updated": "2025-05-22 09:14:41", "pdf_url": "http://arxiv.org/pdf/2505.16430v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16448v1", "title": "Internal Bias in Reasoning Models leads to Overthinking", "authors": ["Renfei Dang", "Shujian Huang", "Jiajun Chen"], "abstract": "While current reasoning models possess strong exploratory capabilities, they\nare often criticized for overthinking due to redundant and unnecessary\nreflections. In this work, we reveal for the first time that overthinking in\nreasoning models may stem from their internal bias towards input texts. Upon\nencountering a reasoning problem, the model immediately forms a preliminary\nguess about the answer, which we term as an internal bias since it is not\nderived through actual reasoning. When this guess conflicts with its reasoning\nresult, the model tends to engage in reflection, leading to the waste of\ncomputational resources. Through further interpretability experiments, we find\nthat this behavior is largely driven by the model's excessive attention to the\ninput section, which amplifies the influence of internal bias on its\ndecision-making process. Additionally, by masking out the original input\nsection, the affect of internal bias can be effectively alleviated and the\nreasoning length could be reduced by 31%-53% across different complex reasoning\ntasks. Notably, in most cases, this approach also leads to improvements in\naccuracy. These findings demonstrate a causal relationship between internal\nbias and overthinking.", "categories": ["cs.AI"], "published": "2025-05-22 09:35:52", "updated": "2025-05-22 09:35:52", "pdf_url": "http://arxiv.org/pdf/2505.16448v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16452v1", "title": "CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI", "authors": ["Mohamed S. Elmahdy", "Marius Staring", "Patrick J. H. de Koning", "Samer Alabed", "Mahan Salehi", "Faisal Alandejani", "Michael Sharkey", "Ziad Aldabbagh", "Andrew J. Swift", "Rob J. van der Geest"], "abstract": "Accurate and efficient quantification of cardiac function is essential for\nthe estimation of prognosis of cardiovascular diseases (CVDs). One of the most\ncommonly used metrics for evaluating cardiac pumping performance is left\nventricular ejection fraction (LVEF). However, LVEF can be affected by factors\nsuch as inter-observer variability and varying pre-load and after-load\nconditions, which can reduce its reproducibility. Additionally, cardiac\ndysfunction may not always manifest as alterations in LVEF, such as in heart\nfailure and cardiotoxicity diseases. An alternative measure that can provide a\nrelatively load-independent quantitative assessment of myocardial contractility\nis myocardial strain and strain rate. By using LVEF in combination with\nmyocardial strain, it is possible to obtain a thorough description of cardiac\nfunction. Automated estimation of LVEF and other volumetric measures from\ncine-MRI sequences can be achieved through segmentation models, while strain\ncalculation requires the estimation of tissue displacement between sequential\nframes, which can be accomplished using registration models. These tasks are\noften performed separately, potentially limiting the assessment of cardiac\nfunction. To address this issue, in this study we propose an end-to-end deep\nlearning (DL) model that jointly estimates groupwise (GW) registration and\nsegmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep\nGW network was trained and validated on a large dataset of 4-chamber view\ncine-MRI image series of 374 subjects. A quantitative comparison with\nconventional GW registration using elastix and two DL-based methods showed that\nthe proposed model improved performance and substantially reduced computation\ntime.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-22 09:36:42", "updated": "2025-05-22 09:36:42", "pdf_url": "http://arxiv.org/pdf/2505.16452v1", "comment": "15 pages, 7 figures, 1 appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16455v1", "title": "Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events", "authors": ["Mengzhu Liu", "Zhengqiu Zhu", "Chuan Ai", "Chen Gao", "Xinghong Li", "Lingnan He", "Kaisheng Lai", "Yingfeng Chen", "Xin Lu", "Yong Li", "Quanjun Yin"], "abstract": "During sudden disaster events, accurately predicting public panic sentiment\non social media is crucial for proactive governance and crisis management.\nCurrent efforts on this problem face three main challenges: lack of finely\nannotated data hinders emotion prediction studies, unmodeled risk perception\ncauses prediction inaccuracies, and insufficient interpretability of panic\nformation mechanisms. We address these issues by proposing a Psychology-driven\ngenerative Agent framework (PsychoAgent) for explainable panic prediction based\non emotion arousal theory. Specifically, we first construct a fine-grained open\npanic emotion dataset (namely COPE) via human-large language models (LLMs)\ncollaboration to mitigate semantic bias. Then, we develop a framework\nintegrating cross-domain heterogeneous data grounded in psychological\nmechanisms to model risk perception and cognitive differences in emotion\ngeneration. To enhance interpretability, we design an LLM-based role-playing\nagent that simulates individual psychological chains through dedicatedly\ndesigned prompts. Experimental results on our annotated dataset show that\nPsychoAgent improves panic emotion prediction performance by 12.6% to 21.7%\ncompared to baseline models. Furthermore, the explainability and generalization\nof our approach is validated. Crucially, this represents a paradigm shift from\nopaque \"data-driven fitting\" to transparent \"role-based simulation with\nmechanistic interpretation\" for panic emotion prediction during emergencies.\nOur implementation is publicly available at:\nhttps://anonymous.4open.science/r/PsychoAgent-19DD.", "categories": ["cs.AI", "cs.CY"], "published": "2025-05-22 09:39:39", "updated": "2025-05-22 09:39:39", "pdf_url": "http://arxiv.org/pdf/2505.16455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16459v1", "title": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks", "authors": ["Guiyao Tie", "Xueyang Zhou", "Tianhe Gu", "Ruihang Zhang", "Chaoran Hu", "Sizhe Zhang", "Mengqu Sun", "Yan Zhang", "Pan Zhou", "Lichao Sun"], "abstract": "Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled\nunified processing of language, vision, and structured inputs, opening the door\nto complex tasks such as logical deduction, spatial reasoning, and scientific\nanalysis. Despite their promise, the reasoning capabilities of MLLMs,\nparticularly those augmented with intermediate thinking traces (MLLMs-T),\nremain poorly understood and lack standardized evaluation benchmarks. Existing\nwork focuses primarily on perception or final answer correctness, offering\nlimited insight into how models reason or fail across modalities. To address\nthis gap, we introduce the MMMR, a new benchmark designed to rigorously\nevaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a\nhigh-difficulty dataset of 1,083 questions spanning six diverse reasoning types\nwith symbolic depth and multi-hop demands and 2) a modular Reasoning Trace\nEvaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy\nthrough metrics like relevance, consistency, and structured error annotations.\nEmpirical results show that MLLMs-T overall outperform non-thinking\ncounterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro\nsuffer from reasoning pathologies such as inconsistency and overthinking. This\nbenchmark reveals persistent gaps between accuracy and reasoning quality and\nprovides an actionable evaluation pipeline for future model development.\nOverall, the MMMR offers a scalable foundation for evaluating, comparing, and\nimproving the next generation of multi-modal reasoning systems.", "categories": ["cs.AI"], "published": "2025-05-22 09:41:55", "updated": "2025-05-22 09:41:55", "pdf_url": "http://arxiv.org/pdf/2505.16459v1", "comment": "39 pages, 28 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16460v1", "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "abstract": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-22 09:42:11", "updated": "2025-05-22 09:42:11", "pdf_url": "http://arxiv.org/pdf/2505.16460v1", "comment": "16 pages, 13 tables, 1 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16466v1", "title": "Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for GNN-based Recommendation Methods", "authors": ["Meng Yan", "Cai Xu", "Xujing Wang", "Ziyu Guan", "Wei Zhao", "Yuhang Zhou"], "abstract": "Recommender systems based on graph neural networks perform well in tasks such\nas rating and ranking. However, in real-world recommendation scenarios, noise\nsuch as user misuse and malicious advertisement gradually accumulates through\nthe message propagation mechanism. Even if existing studies mitigate their\neffects by reducing the noise propagation weights, the severe sparsity of the\nrecommender system still leads to the low-weighted noisy neighbors being\nmistaken as meaningful information, and the prediction result obtained based on\nthe polluted nodes is not entirely trustworthy. Therefore, it is crucial to\nmeasure the confidence of the prediction results in this highly noisy\nframework. Furthermore, our evaluation of the existing representative GNN-based\nrecommendation shows that it suffers from overconfidence. Based on the above\nconsiderations, we propose a new method to quantify and calibrate the\nprediction confidence of GNN-based recommendations (Conf-GNNRec). Specifically,\nwe propose a rating calibration method that dynamically adjusts excessive\nratings to mitigate overconfidence based on user personalization. We also\ndesign a confidence loss function to reduce the overconfidence of negative\nsamples and effectively improve recommendation performance. Experiments on\npublic datasets demonstrate the validity of Conf-GNNRec in prediction\nconfidence and recommendation performance.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-22 09:48:17", "updated": "2025-05-22 09:48:17", "pdf_url": "http://arxiv.org/pdf/2505.16466v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16475v1", "title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection", "authors": ["Jiaqi Li", "Xinyi Dong", "Yang Liu", "Zhizhuo Yang", "Quansen Wang", "Xiaobo Wang", "SongChun Zhu", "Zixia Jia", "Zilong Zheng"], "abstract": "We present a novel pipeline, ReflectEvo, to demonstrate that small language\nmodels (SLMs) can enhance meta introspection through reflection learning. This\nprocess iteratively generates self-reflection for self-training, fostering a\ncontinuous and self-evolving process. Leveraging this pipeline, we construct\nReflectEvo-460k, a large-scale, comprehensive, self-generated reflection\ndataset with broadened instructions and diverse multi-domain tasks. Building\nupon this dataset, we demonstrate the effectiveness of reflection learning to\nimprove SLMs' reasoning abilities using SFT and DPO with remarkable\nperformance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral\nfrom 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the\nreasoning capability of the three prominent open-sourced models on BIG-bench\nwithout distillation from superior models or fine-grained human annotation. We\nfurther conduct a deeper analysis of the high quality of self-generated\nreflections and their impact on error localization and correction. Our work\nhighlights the potential of continuously enhancing the reasoning performance of\nSLMs through iterative reflection learning in the long run.", "categories": ["cs.AI"], "published": "2025-05-22 10:03:05", "updated": "2025-05-22 10:03:05", "pdf_url": "http://arxiv.org/pdf/2505.16475v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16477v1", "title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery", "authors": ["Yanbo Zhang", "Sumeer A. Khan", "Adnan Mahmud", "Huck Yang", "Alexander Lavin", "Michael Levin", "Jeremy Frey", "Jared Dunnmon", "James Evans", "Alan Bundy", "Saso Dzeroski", "Jesper Tegner", "Hector Zenil"], "abstract": "With recent Nobel Prizes recognising AI contributions to science, Large\nLanguage Models (LLMs) are transforming scientific research by enhancing\nproductivity and reshaping the scientific method. LLMs are now involved in\nexperimental design, data analysis, and workflows, particularly in chemistry\nand biology. However, challenges such as hallucinations and reliability\npersist. In this contribution, we review how Large Language Models (LLMs) are\nredefining the scientific method and explore their potential applications\nacross different stages of the scientific cycle, from hypothesis testing to\ndiscovery. We conclude that, for LLMs to serve as relevant and effective\ncreative engines and productivity enhancers, their deep integration into all\nsteps of the scientific process should be pursued in collaboration and\nalignment with human scientific goals, with clear evaluation metrics. The\ntransition to AI-driven science raises ethical questions about creativity,\noversight, and responsibility. With careful guidance, LLMs could evolve into\ncreative engines, driving transformative breakthroughs across scientific\ndisciplines responsibly and effectively. However, the scientific community must\nalso decide how much it leaves to LLMs to drive science, even when associations\nwith 'reasoning', mostly currently undeserved, are made in exchange for the\npotential to explore hypothesis and solution regions that might otherwise\nremain unexplored by human exploration alone.", "categories": ["cs.AI"], "published": "2025-05-22 10:05:48", "updated": "2025-05-22 10:05:48", "pdf_url": "http://arxiv.org/pdf/2505.16477v1", "comment": "45 pages", "doi": null, "journal_ref": "npj Artificial Intelligence, 2025"}
{"arxiv_id": "2505.16482v1", "title": "Minimizing the energy depletion in wireless rechargeable sensor networks using bi-level metaheuristic charging schemes", "authors": ["Huynh Thi Thanh Binh", "Le Van Cuong", "Dang Hai Dang", "Le Trong Vinh"], "abstract": "Recently, Wireless Rechargeable Sensor Networks (WRSNs) that leveraged the\nadvantage of wireless energy transfer technology have opened a promising\nopportunity in solving the limited energy issue. However, an ineffective\ncharging strategy may reduce the charging performance. Although many practical\ncharging algorithms have been introduced, these studies mainly focus on\noptimizing the charging path with a fully charging approach. This approach may\nlead to the death of a series of sensors due to their extended charging\nlatency. This paper introduces a novel partial charging approach that follows a\nbi-level optimized scheme to minimize energy depletion in WRSNs. We aim at\noptimizing simultaneously two factors: the charging path and time. To\naccomplish this, we first formulate a mathematical model of the investigated\nproblem. We then propose two approximate algorithms in which the optimization\nof the charging path and the charging time are considered as the upper and\nlower level, respectively. The first algorithm combines a Multi-start Local\nSearch method and a Genetic Algorithm to find a solution. The second algorithm\nadopts a nested approach that utilizes the advantages of the Multitasking and\nCovariance Matrix Adaptation Evolutionary Strategies. Experimental validations\non various network scenarios demonstrate that our proposed algorithms\noutperform the existing works.", "categories": ["cs.AI", "cs.NE"], "published": "2025-05-22 10:09:21", "updated": "2025-05-22 10:09:21", "pdf_url": "http://arxiv.org/pdf/2505.16482v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16483v1", "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "abstract": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:10:07", "updated": "2025-05-22 10:10:07", "pdf_url": "http://arxiv.org/pdf/2505.16483v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16491v1", "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "abstract": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:22:39", "updated": "2025-05-22 10:22:39", "pdf_url": "http://arxiv.org/pdf/2505.16491v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16498v1", "title": "Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models", "authors": ["Augusto Luis Ballardini", "Miguel \u00c1ngel Sotelo"], "abstract": "Achieving full automation in self-driving vehicles remains a challenge,\nespecially in dynamic urban environments where navigation requires real-time\nadaptability. Existing systems struggle to handle navigation plans when faced\nwith unpredictable changes in road layouts, spontaneous detours, or missing map\ndata, due to their heavy reliance on predefined cartographic information. In\nthis work, we explore the use of Large Language Models to generate Answer Set\nProgramming rules by translating informal navigation instructions into\nstructured, logic-based reasoning. ASP provides non-monotonic reasoning,\nallowing autonomous vehicles to adapt to evolving scenarios without relying on\npredefined maps. We present an experimental evaluation in which LLMs generate\nASP constraints that encode real-world urban driving logic into a formal\nknowledge representation. By automating the translation of informal navigation\ninstructions into logical rules, our method improves adaptability and\nexplainability in autonomous navigation. Results show that LLM-driven ASP rule\ngeneration supports semantic-based decision-making, offering an explainable\nframework for dynamic navigation planning that aligns closely with how humans\ncommunicate navigational intent.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 10:32:43", "updated": "2025-05-22 10:32:43", "pdf_url": "http://arxiv.org/pdf/2505.16498v1", "comment": "7 pages, 5 figures, submitted for IEEE conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16499v1", "title": "Smaller, Smarter, Closer: The Edge of Collaborative Generative AI", "authors": ["Roberto Morabito", "SiYoung Jang"], "abstract": "The rapid adoption of generative AI (GenAI), particularly Large Language\nModels (LLMs), has exposed critical limitations of cloud-centric deployments,\nincluding latency, cost, and privacy concerns. Meanwhile, Small Language Models\n(SLMs) are emerging as viable alternatives for resource-constrained edge\nenvironments, though they often lack the capabilities of their larger\ncounterparts. This article explores the potential of collaborative inference\nsystems that leverage both edge and cloud resources to address these\nchallenges. By presenting distinct cooperation strategies alongside practical\ndesign principles and experimental insights, we offer actionable guidance for\ndeploying GenAI across the computing continuum.", "categories": ["cs.DC", "cs.AI", "cs.NI"], "published": "2025-05-22 10:34:48", "updated": "2025-05-22 10:34:48", "pdf_url": "http://arxiv.org/pdf/2505.16499v1", "comment": "This paper is currently under review for publication in an IEEE\n  magazine. If accepted, the copyright will be transferred to IEEE", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16505v1", "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "abstract": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-22 10:41:35", "updated": "2025-05-22 10:41:35", "pdf_url": "http://arxiv.org/pdf/2505.16505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16507v1", "title": "Relevance for Stability of Verification Status of a Set of Arguments in Incomplete Argumentation Frameworks (with Proofs)", "authors": ["Anshu Xiong", "Songmao Zhang"], "abstract": "The notion of relevance was proposed for stability of justification status of\na single argument in incomplete argumentation frameworks (IAFs) in 2024 by\nOdekerken et al. To extend the notion, we study the relevance for stability of\nverification status of a set of arguments in this paper, i.e., the\nuncertainties in an IAF that have to be resolved in some situations so that\nanswering whether a given set of arguments is an extension obtains the same\nresult in every completion of the IAF. Further we propose the notion of strong\nrelevance for describing the necessity of resolution in all situations reaching\nstability. An analysis of complexity reveals that detecting the (strong)\nrelevance for stability of sets of arguments can be accomplished in P time\nunder the most semantics discussed in the paper. We also discuss the difficulty\nin finding tractable methods for relevance detection under grounded semantics.", "categories": ["cs.AI"], "published": "2025-05-22 10:42:16", "updated": "2025-05-22 10:42:16", "pdf_url": "http://arxiv.org/pdf/2505.16507v1", "comment": "This is a version of paper 'Relevance for Stability of Verification\n  Status of a Set of Arguments in Incomplete Argumentation Frameworks' extented\n  with proofs of the results in the paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16508v1", "title": "Edge-First Language Model Inference: Models, Metrics, and Tradeoffs", "authors": ["SiYoung Jang", "Roberto Morabito"], "abstract": "The widespread adoption of Language Models (LMs) across industries is driving\ninterest in deploying these services across the computing continuum, from the\ncloud to the network edge. This shift aims to reduce costs, lower latency, and\nimprove reliability and privacy. Small Language Models (SLMs), enabled by\nadvances in model compression, are central to this shift, offering a path to\non-device inference on resource-constrained edge platforms. This work examines\nthe interplay between edge and cloud deployments, starting from detailed\nbenchmarking of SLM capabilities on single edge devices, and extending to\ndistributed edge clusters. We identify scenarios where edge inference offers\ncomparable performance with lower costs, and others where cloud fallback\nbecomes essential due to limits in scalability or model capacity. Rather than\nproposing a one-size-fits-all solution, we present platform-level comparisons\nand design insights for building efficient, adaptive LM inference systems\nacross heterogeneous environments.", "categories": ["cs.DC", "cs.AI", "cs.NI", "cs.PF"], "published": "2025-05-22 10:43:00", "updated": "2025-05-22 10:43:00", "pdf_url": "http://arxiv.org/pdf/2505.16508v1", "comment": "This paper has been accepted for publication and presentation at the\n  45th IEEE International Conference on Distributed Computing Systems (IEEE\n  ICDCS 2025). The copyright will be transferred to IEEE upon publication in\n  the conference proceedings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16512v1", "title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection", "authors": ["Jiaxin Liu", "Jia Wang", "Saihui Hou", "Min Ren", "Huijia Wu", "Zhaofeng He"], "abstract": "In recent years, the rapid development of deepfake technology has given rise\nto an emerging and serious threat to public security: diffusion model-based\ndigital human generation. Unlike traditional face manipulation methods, such\nmodels can generate highly realistic videos with consistency through multimodal\ncontrol signals. Their flexibility and covertness pose severe challenges to\nexisting detection strategies. To bridge this gap, we introduce DigiFakeAV, the\nfirst large-scale multimodal digital human forgery dataset based on diffusion\nmodels. Employing five latest digital human generation methods (Sonic, Hallo,\netc.) and voice cloning method, we systematically produce a dataset comprising\n60,000 videos (8.4 million frames), covering multiple nationalities, skin\ntones, genders, and real-world scenarios, significantly enhancing data\ndiversity and realism. User studies show that the confusion rate between forged\nand real videos reaches 68%, and existing state-of-the-art (SOTA) detection\nmodels exhibit large drops in AUC values on DigiFakeAV, highlighting the\nchallenge of the dataset. To address this problem, we further propose\nDigiShield, a detection baseline based on spatiotemporal and cross-modal\nfusion. By jointly modeling the 3D spatiotemporal features of videos and the\nsemantic-acoustic features of audio, DigiShield achieves SOTA performance on\nboth the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method\neffectively identifies covert artifacts through fine-grained analysis of the\ntemporal evolution of facial features in synthetic videos.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 10:46:37", "updated": "2025-05-22 10:46:37", "pdf_url": "http://arxiv.org/pdf/2505.16512v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16516v1", "title": "Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods", "authors": ["Majid Mohammadi", "Siu Lun Chau", "Krikamol Muandet"], "abstract": "Kernel methods are widely used in machine learning due to their flexibility\nand expressive power. However, their black-box nature poses significant\nchallenges to interpretability, limiting their adoption in high-stakes\napplications. Shapley value-based feature attribution techniques, such as SHAP\nand kernel-specific variants like RKHS-SHAP, offer a promising path toward\nexplainability. Yet, computing exact Shapley values remains computationally\nintractable in general, motivating the development of various approximation\nschemes. In this work, we introduce PKeX-Shapley, a novel algorithm that\nutilizes the multiplicative structure of product kernels to enable the exact\ncomputation of Shapley values in polynomial time. We show that product-kernel\nmodels admit a functional decomposition that allows for a recursive formulation\nof Shapley values. This decomposition not only yields computational efficiency\nbut also enhances interpretability in kernel-based learning. We also\ndemonstrate how our framework can be generalized to explain kernel-based\nstatistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the\nHilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for\ninterpretable statistical inference.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 10:53:04", "updated": "2025-05-22 10:53:04", "pdf_url": "http://arxiv.org/pdf/2505.16516v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16518v1", "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "authors": ["Lovisa Hagstr\u00f6m", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "abstract": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:57:08", "updated": "2025-05-22 10:57:08", "pdf_url": "http://arxiv.org/pdf/2505.16518v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16520v1", "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "abstract": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 11:00:53", "updated": "2025-05-22 11:00:53", "pdf_url": "http://arxiv.org/pdf/2505.16520v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16522v1", "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "abstract": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 11:04:09", "updated": "2025-05-22 11:04:09", "pdf_url": "http://arxiv.org/pdf/2505.16522v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16530v1", "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection", "authors": ["Yuliang Yan", "Haochun Tang", "Shuo Yan", "Enyan Dai"], "abstract": "Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-22 11:16:46", "updated": "2025-05-22 11:16:46", "pdf_url": "http://arxiv.org/pdf/2505.16530v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16540v1", "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation", "authors": ["Inbal Cohen", "Boaz Meivar", "Peihan Tu", "Shai Avidan", "Gal Oren"], "abstract": "Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 11:31:56", "updated": "2025-05-22 11:31:56", "pdf_url": "http://arxiv.org/pdf/2505.16540v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16547v1", "title": "Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation", "authors": ["Nitesh Subedi", "Hsin-Jung Yang", "Devesh K. Jha", "Soumik Sarkar"], "abstract": "This paper presents an end-to-end deep reinforcement learning (RL) framework\nfor occlusion-aware robotic manipulation in cluttered plant environments. Our\napproach enables a robot to interact with a deformable plant to reveal hidden\nobjects of interest, such as fruits, using multimodal observations. We decouple\nthe kinematic planning problem from robot control to simplify zero-shot\nsim2real transfer for the trained policy. Our results demonstrate that the\ntrained policy, deployed using our framework, achieves up to 86.7% success in\nreal-world trials across diverse initial conditions. Our findings pave the way\ntoward autonomous, perception-driven agricultural robots that intelligently\ninteract with complex foliage plants to \"find the fruit\" in challenging\noccluded scenarios, without the need for explicitly designed geometric and\ndynamic models of every plant scenario.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 11:37:39", "updated": "2025-05-22 11:37:39", "pdf_url": "http://arxiv.org/pdf/2505.16547v1", "comment": "18 Pages, 15 Figures, 5 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16561v1", "title": "Auto-nnU-Net: Towards Automated Medical Image Segmentation", "authors": ["Jannis Becktepe", "Leona Hennig", "Steffen Oeltze-Jafra", "Marius Lindauer"], "abstract": "Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ\nsegmentation, each with its own challenges in finding the best segmentation\nmodel. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many\naspects of model configuration but remains constrained by fixed hyperparameters\nand heuristic design choices. As a full-AutoML framework for MIS, we propose\nAuto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization\n(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).\nAdditionally, we propose Regularized PriorBand to balance model accuracy with\nthe computational resources required for training, addressing the resource\nconstraints often faced in real-world medical settings that limit the\nfeasibility of extensive training procedures. We evaluate our approach across\ndiverse MIS datasets from the well-established Medical Segmentation Decathlon,\nanalyzing the impact of AutoML techniques on segmentation performance,\ncomputational efficiency, and model design choices. The results demonstrate\nthat our AutoML approach substantially improves the segmentation performance of\nnnU-Net on 6 out of 10 datasets and is on par on the other datasets while\nmaintaining practical resource requirements. Our code is available at\nhttps://github.com/LUH-AI/AutonnUNet.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 11:52:16", "updated": "2025-05-22 11:52:16", "pdf_url": "http://arxiv.org/pdf/2505.16561v1", "comment": "31 pages, 19 figures. Accepted for publication at AutoML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16567v1", "title": "Finetuning-Activated Backdoors in LLMs", "authors": ["Thibaud Gloaguen", "Mark Vero", "Robin Staab", "Martin Vechev"], "abstract": "Finetuning openly accessible Large Language Models (LLMs) has become standard\npractice for achieving task-specific performance improvements. Until now,\nfinetuning has been regarded as a controlled and secure process in which\ntraining on benign datasets led to predictable behaviors. In this paper, we\ndemonstrate for the first time that an adversary can create poisoned LLMs that\ninitially appear benign but exhibit malicious behaviors once finetuned by\ndownstream users. To this end, our proposed attack, FAB (Finetuning-Activated\nBackdoor), poisons an LLM via meta-learning techniques to simulate downstream\nfinetuning, explicitly optimizing for the emergence of malicious behaviors in\nthe finetuned models. At the same time, the poisoned LLM is regularized to\nretain general capabilities and to exhibit no malicious behaviors prior to\nfinetuning. As a result, when users finetune the seemingly benign model on\ntheir own datasets, they unknowingly trigger its hidden backdoor behavior. We\ndemonstrate the effectiveness of FAB across multiple LLMs and three target\nbehaviors: unsolicited advertising, refusal, and jailbreakability.\nAdditionally, we show that FAB-backdoors are robust to various finetuning\nchoices made by the user (e.g., dataset, number of steps, scheduler). Our\nfindings challenge prevailing assumptions about the security of finetuning,\nrevealing yet another critical attack vector exploiting the complexities of\nLLMs.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "published": "2025-05-22 11:59:44", "updated": "2025-05-22 11:59:44", "pdf_url": "http://arxiv.org/pdf/2505.16567v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16573v1", "title": "From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling", "authors": ["Yi Hu", "Hanchi Ren", "Jingjing Deng", "Xianghua Xie"], "abstract": "Stock price prediction is a critical area of financial forecasting,\ntraditionally approached by training models using the historical price data of\nindividual stocks. While these models effectively capture single-stock\npatterns, they fail to leverage potential correlations among stock trends,\nwhich could improve predictive performance. Current single-stock learning\nmethods are thus limited in their ability to provide a broader understanding of\nprice dynamics across multiple stocks. To address this, we propose a novel\nmethod that merges local patterns into a global understanding through\ncross-stock pattern integration. Our strategy is inspired by Federated Learning\n(FL), a paradigm designed for decentralized model training. FL enables\ncollaborative learning across distributed datasets without sharing raw data,\nfacilitating the aggregation of global insights while preserving data privacy.\nIn our adaptation, we train models on individual stock data and iteratively\nmerge them to create a unified global model. This global model is subsequently\nfine-tuned on specific stock data to retain local relevance. The proposed\nstrategy enables parallel training of individual stock models, facilitating\nefficient utilization of computational resources and reducing overall training\ntime. We conducted extensive experiments to evaluate the proposed method,\ndemonstrating that it outperforms benchmark models and enhances the predictive\ncapabilities of state-of-the-art approaches. Our results highlight the efficacy\nof Cross-Stock Trend Integration (CSTI) in advancing stock price prediction,\noffering a robust alternative to traditional single-stock learning\nmethodologies.", "categories": ["cs.CE", "cs.AI"], "published": "2025-05-22 12:04:10", "updated": "2025-05-22 12:04:10", "pdf_url": "http://arxiv.org/pdf/2505.16573v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16579v1", "title": "Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning", "authors": ["Siqu Ou", "Hongcheng Liu", "Pingjie Wang", "Yusheng Liao", "Chuan Xuan", "Yanfeng Wang", "Yu Wang"], "abstract": "While chains-of-thought (CoT) have advanced complex reasoning in multimodal\nlarge language models (MLLMs), existing methods remain confined to text or\nstatic visual domains, often faltering in dynamic spatial reasoning tasks. To\nbridge this gap, we present GRASSLAND, a novel maze navigation benchmark\ndesigned to evaluate dynamic spatial reasoning. Our experiments show that\naugmenting textual reasoning chains with dynamic visual drafts, overlaid on\ninput images, significantly outperforms conventional approaches, offering new\ninsights into spatial reasoning in evolving environments. To generalize this\ncapability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free\nframework that seamlessly integrates textual CoT with corresponding visual\ndrafts into MLLMs. Extensive evaluations demonstrate that D2R consistently\nenhances performance across diverse tasks, establishing a robust baseline for\ndynamic spatial reasoning without requiring model fine-tuning. Project is open\nat https://github.com/Cratileo/D2R.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-22 12:14:23", "updated": "2025-05-22 12:14:23", "pdf_url": "http://arxiv.org/pdf/2505.16579v1", "comment": "19 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16581v1", "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning", "authors": ["Max Weltevrede", "Moritz A. Zanger", "Matthijs T. J. Spaan", "Wendelin B\u00f6hmer"], "abstract": "In the zero-shot policy transfer setting in reinforcement learning, the goal\nis to train an agent on a fixed set of training environments so that it can\ngeneralise to similar, but unseen, testing environments. Previous work has\nshown that policy distillation after training can sometimes produce a policy\nthat outperforms the original in the testing environments. However, it is not\nyet entirely clear why that is, or what data should be used to distil the\npolicy. In this paper, we prove, under certain assumptions, a generalisation\nbound for policy distillation after training. The theory provides two practical\ninsights: for improved generalisation, you should 1) train an ensemble of\ndistilled policies, and 2) distil it on as much data from the training\nenvironments as possible. We empirically verify that these insights hold in\nmore general settings, when the assumptions required for the theory no longer\nhold. Finally, we demonstrate that an ensemble of policies distilled on a\ndiverse dataset can generalise significantly better than the original agent.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 12:15:52", "updated": "2025-05-22 12:15:52", "pdf_url": "http://arxiv.org/pdf/2505.16581v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16582v1", "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "abstract": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 12:17:13", "updated": "2025-05-22 12:17:13", "pdf_url": "http://arxiv.org/pdf/2505.16582v1", "comment": "25 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16596v1", "title": "Safe Uncertainty-Aware Learning of Robotic Suturing", "authors": ["Wilbert Peter Empleo", "Yitaek Kim", "Hansoul Kim", "Thiusius Rajeeth Savarimuthu", "I\u00f1igo Iturrate"], "abstract": "Robot-Assisted Minimally Invasive Surgery is currently fully manually\ncontrolled by a trained surgeon. Automating this has great potential for\nalleviating issues, e.g., physical strain, highly repetitive tasks, and\nshortages of trained surgeons. For these reasons, recent works have utilized\nArtificial Intelligence methods, which show promising adaptability. Despite\nthese advances, there is skepticism of these methods because they lack\nexplainability and robust safety guarantees. This paper presents a framework\nfor a safe, uncertainty-aware learning method. We train an Ensemble Model of\nDiffusion Policies using expert demonstrations of needle insertion. Using an\nEnsemble model, we can quantify the policy's epistemic uncertainty, which is\nused to determine Out-Of-Distribution scenarios. This allows the system to\nrelease control back to the surgeon in the event of an unsafe scenario.\nAdditionally, we implement a model-free Control Barrier Function to place\nformal safety guarantees on the predicted action. We experimentally evaluate\nour proposed framework using a state-of-the-art robotic suturing simulator. We\nevaluate multiple scenarios, such as dropping the needle, moving the camera,\nand moving the phantom. The learned policy is robust to these perturbations,\nshowing corrective behaviors and generalization, and it is possible to detect\nOut-Of-Distribution scenarios. We further demonstrate that the Control Barrier\nFunction successfully limits the action to remain within our specified safety\nset in the case of unsafe predictions.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 12:31:18", "updated": "2025-05-22 12:31:18", "pdf_url": "http://arxiv.org/pdf/2505.16596v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16612v1", "title": "Steering Large Language Models for Machine Translation Personalization", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "abstract": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 12:47:16", "updated": "2025-05-22 12:47:16", "pdf_url": "http://arxiv.org/pdf/2505.16612v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "abstract": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "categories": ["cs.AI", "q-bio.OT", "92", "J.3"], "published": "2025-05-22 12:52:34", "updated": "2025-05-22 12:52:34", "pdf_url": "http://arxiv.org/pdf/2505.16619v1", "comment": "1 PDF, 24 Pages, 2 figures within. Co-corresponding authors:\n  Institute of Applied Biosciences, Centre for Research and Technology Hellas,\n  Thessaloniki, Greece and Department of Biomedical Sciences, University of\n  Padova, Padova, Italy. E-mails: fpsom@certh.gr, silvio.tosatto@unipd.it", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16630v1", "title": "SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding", "authors": ["Sushant Gautam", "Cise Midoglu", "Vajira Thambawita", "Michael A. Riegler", "P\u00e5l Halvorsen", "Mubarak Shah"], "abstract": "The integration of artificial intelligence in sports analytics has\ntransformed soccer video understanding, enabling real-time, automated insights\ninto complex game dynamics. Traditional approaches rely on isolated data\nstreams, limiting their effectiveness in capturing the full context of a match.\nTo address this, we introduce SoccerChat, a multimodal conversational AI\nframework that integrates visual and textual data for enhanced soccer video\ncomprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey\ncolor annotations and automatic speech recognition (ASR) transcripts,\nSoccerChat is fine-tuned on a structured video instruction dataset to\nfacilitate accurate game understanding, event classification, and referee\ndecision making. We benchmark SoccerChat on action classification and referee\ndecision-making tasks, demonstrating its performance in general soccer event\ncomprehension while maintaining competitive accuracy in referee decision\nmaking. Our findings highlight the importance of multimodal integration in\nadvancing soccer analytics, paving the way for more interactive and explainable\nAI-driven sports analysis. https://github.com/simula/SoccerChat", "categories": ["cs.CV", "cs.AI", "68T45, 68T50", "I.2.10; I.2.7; H.5.2"], "published": "2025-05-22 13:01:51", "updated": "2025-05-22 13:01:51", "pdf_url": "http://arxiv.org/pdf/2505.16630v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16637v1", "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "abstract": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 13:08:25", "updated": "2025-05-22 13:08:25", "pdf_url": "http://arxiv.org/pdf/2505.16637v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16640v1", "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization", "authors": ["Xueyang Zhou", "Guiyao Tie", "Guowen Zhang", "Hechang Wang", "Pan Zhou", "Lichao Sun"], "abstract": "Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/.", "categories": ["cs.CR", "cs.AI", "68T07", "I.2.6; I.2.9"], "published": "2025-05-22 13:12:46", "updated": "2025-05-22 13:12:46", "pdf_url": "http://arxiv.org/pdf/2505.16640v1", "comment": "19 pages, 12 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16643v1", "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models", "authors": ["Yiwei Sun", "Peiqi Jiang", "Chuanbin Liu", "Luohao Lin", "Zhiying Lu", "Hongtao Xie"], "abstract": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 13:16:53", "updated": "2025-05-22 13:16:53", "pdf_url": "http://arxiv.org/pdf/2505.16643v1", "comment": "49 pages, 12 figures, 17 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16646v1", "title": "SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving", "authors": ["Yujie Hou", "Ting Zhang", "Mei Wang", "Xuetao Ma", "Hu Huang"], "abstract": "Large Language Models have achieved remarkable results on a variety of\nmathematical benchmarks. However, concerns remain as to whether these successes\nreflect genuine mathematical reasoning or superficial pattern recognition.\nCommon evaluation metrics, such as final answer accuracy, fail to disentangle\nthe underlying competencies involved, offering limited diagnostic value. To\naddress these limitations, we introduce SMART: a Self-Generating and\nSelf-Validating Multi-Dimensional Assessment Framework. SMART decomposes\nmathematical problem solving into four distinct dimensions: understanding,\nreasoning, arithmetic, and reflection \\& refinement. Each dimension is\nevaluated independently through tailored tasks, enabling interpretable and\nfine-grained analysis of LLM behavior. Crucially, SMART integrates an automated\nself-generating and self-validating mechanism to produce and verify benchmark\ndata, ensuring both scalability and reliability. We apply SMART to 21\nstate-of-the-art open- and closed-source LLMs, uncovering significant\ndiscrepancies in their abilities across different dimensions. Our findings\ndemonstrate the inadequacy of final answer accuracy as a sole metric and\nmotivate a new holistic metric to better capture true problem-solving\ncapabilities. Code and benchmarks will be released upon acceptance.", "categories": ["cs.AI"], "published": "2025-05-22 13:18:24", "updated": "2025-05-22 13:18:24", "pdf_url": "http://arxiv.org/pdf/2505.16646v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16647v1", "title": "Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models", "authors": ["Sushant Gautam", "Michael A. Riegler", "P\u00e5l Halvorsen"], "abstract": "We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount.", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.4.8"], "published": "2025-05-22 13:18:44", "updated": "2025-05-22 13:18:44", "pdf_url": "http://arxiv.org/pdf/2505.16647v1", "comment": "Accepted as a full paper at the 38th IEEE International Symposium on\n  Computer-Based Medical Systems (CBMS) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16648v1", "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "abstract": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:18:45", "updated": "2025-05-22 13:18:45", "pdf_url": "http://arxiv.org/pdf/2505.16648v1", "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16660v1", "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "abstract": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:24:52", "updated": "2025-05-22 13:24:52", "pdf_url": "http://arxiv.org/pdf/2505.16660v1", "comment": "29pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16664v1", "title": "End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries", "authors": ["Khoa Tran", "Tri Le", "Bao Huynh", "Hung-Cuong Trinh", "Vy-Rin Nguyen"], "abstract": "Accurate prediction of the Remaining Useful Life (RUL) is essential for\nenabling timely maintenance of lithium-ion batteries, impacting the operational\nefficiency of electric applications that rely on them. This paper proposes a\nRUL prediction approach that leverages data from recent charge-discharge cycles\nto estimate the number of remaining usable cycles. The approach introduces both\na novel signal processing pipeline and a deep learning prediction model. In the\nsignal preprocessing pipeline, a derived capacity feature is computed based on\ncurrent and capacity signals. Alongside original capacity, voltage and current,\nthese features are denoised and enhanced using statistical metrics and a\ndelta-based method to capture differences between the current and previous\ncycles. In the prediction model, the processed features are then fed into a\nhybrid deep learning architecture composed of 1D Convolutional Neural Networks\n(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential\nEquation-based LSTM (ODE-LSTM) modules. This architecture is designed to\ncapture both local signal characteristics and long-range temporal dependencies\nwhile modeling the continuous-time dynamics of battery degradation. The model\nis further evaluated using transfer learning across different learning\nstrategies and target data partitioning scenarios. Results indicate that the\nmodel maintains robust performance, even when fine-tuned on limited target\ndata. Experimental results on two publicly available large-scale datasets\ndemonstrate that the proposed method outperforms a baseline deep learning\napproach and machine learning techniques, achieving an RMSE of 101.59,\nhighlighting its strong potential for real-world RUL prediction applications.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 13:28:18", "updated": "2025-05-22 13:28:18", "pdf_url": "http://arxiv.org/pdf/2505.16664v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16667v1", "title": "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming", "authors": ["Xinwei Yang", "Zhaofeng Liu", "Chen Huang", "Jiashuai Zhang", "Tong Zhang", "Yifan Zhang", "Wenqiang Lei"], "abstract": "While recent research increasingly emphasizes the value of human-LLM\ncollaboration in competitive programming and proposes numerous empirical\nmethods, a comprehensive understanding remains elusive due to the fragmented\nnature of existing studies and their use of diverse, application-specific human\nfeedback. Thus, our work serves a three-fold purpose: First, we present the\nfirst taxonomy of human feedback consolidating the entire programming process,\nwhich promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a\nnovel programming dataset specifically designed for human-LLM collaboration,\nmeticulously annotated to enable large-scale simulated human feedback and\nfacilitate costeffective real human interaction studies. Third, we introduce\nELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM\ncompetitive programming. With ELABORATION, we pinpoint strengthes and\nweaknesses of existing methods, thereby setting the foundation for future\nimprovement. Our code and dataset are available at\nhttps://github.com/SCUNLP/ELABORATION", "categories": ["cs.AI"], "published": "2025-05-22 13:32:39", "updated": "2025-05-22 13:32:39", "pdf_url": "http://arxiv.org/pdf/2505.16667v1", "comment": "ACL 2025 Main. Our code and dataset are available at\n  https://github.com/SCUNLP/ELABORATION", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16670v1", "title": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models", "authors": ["Xiaobei Yan", "Yiming Li", "Zhaoxin Fan", "Han Qiu", "Tianwei Zhang"], "abstract": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-22 13:36:00", "updated": "2025-05-22 13:36:00", "pdf_url": "http://arxiv.org/pdf/2505.16670v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16673v1", "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "authors": ["Huanjin Yao", "Qixiang Yin", "Jingyi Zhang", "Min Yang", "Yibo Wang", "Wenhao Wu", "Fei Su", "Li Shen", "Minghui Qiu", "Dacheng Tao", "Jiaxing Huang"], "abstract": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-22 13:39:32", "updated": "2025-05-22 13:39:32", "pdf_url": "http://arxiv.org/pdf/2505.16673v1", "comment": "Technical report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16679v1", "title": "Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds", "authors": ["Jordan Dotzel", "Tony Montes", "Mohamed S. Abdelfattah", "Zhiru Zhang"], "abstract": "Traditional methods for 3D object compression operate only on structural\ninformation within the object vertices, polygons, and textures. These methods\nare effective at compression rates up to 10x for standard object sizes but\nquickly deteriorate at higher compression rates with texture artifacts,\nlow-polygon counts, and mesh gaps. In contrast, semantic compression ignores\nstructural information and operates directly on the core concepts to push to\nextreme levels of compression. In addition, it uses natural language as its\nstorage format, which makes it natively human-readable and a natural fit for\nemerging applications built around large-scale, collaborative projects within\naugmented and virtual reality. It deprioritizes structural information like\nlocation, size, and orientation and predicts the missing information with\nstate-of-the-art deep generative models. In this work, we construct a pipeline\nfor 3D semantic compression from public generative models and explore the\nquality-compression frontier for 3D object compression. We apply this pipeline\nto achieve rates as high as 105x for 3D objects taken from the Objaverse\ndataset and show that semantic compression can outperform traditional methods\nin the important quality-preserving region around 100x compression.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 13:45:35", "updated": "2025-05-22 13:45:35", "pdf_url": "http://arxiv.org/pdf/2505.16679v1", "comment": "First two authors have equal contribution", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16686v1", "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge", "authors": ["Lars Benedikt Kaesberg", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "abstract": "Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 13:53:50", "updated": "2025-05-22 13:53:50", "pdf_url": "http://arxiv.org/pdf/2505.16686v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16690v1", "title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator", "authors": ["Beier Luo", "Shuoyuan Wang", "Yixuan Li", "Hongxin Wei"], "abstract": "Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 13:55:39", "updated": "2025-05-22 13:55:39", "pdf_url": "http://arxiv.org/pdf/2505.16690v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16691v1", "title": "EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion", "authors": ["Advait Joglekar", "Divyanshu Singh", "Rooshil Rohit Bhatia", "S. Umesh"], "abstract": "Voice Conversion research in recent times has increasingly focused on\nimproving the zero-shot capabilities of existing methods. Despite remarkable\nadvancements, current architectures still tend to struggle in zero-shot\ncross-lingual settings. They are also often unable to generalize for speakers\nof unseen languages and accents. In this paper, we adopt a simple yet effective\napproach that combines discrete speech representations from self-supervised\nmodels with a non-autoregressive Diffusion-Transformer based conditional flow\nmatching speech decoder. We show that this architecture allows us to train a\nvoice-conversion model in a purely textless, self-supervised fashion. Our\ntechnique works without requiring multiple encoders to disentangle speech\nfeatures. Our model also manages to excel in zero-shot cross-lingual settings\neven for unseen languages.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-22 13:57:02", "updated": "2025-05-22 13:57:02", "pdf_url": "http://arxiv.org/pdf/2505.16691v1", "comment": "Submitted to EMNLP 2025, 7 pages, 2 figures, 5 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16694v1", "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:59:30", "updated": "2025-05-22 13:59:30", "pdf_url": "http://arxiv.org/pdf/2505.16694v1", "comment": "Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16700v1", "title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "authors": ["Xuanqi Gao", "Siyi Xie", "Juan Zhai", "Shqing Ma", "Chao Shen"], "abstract": "As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143.", "categories": ["cs.AI"], "published": "2025-05-22 14:02:37", "updated": "2025-05-22 14:02:37", "pdf_url": "http://arxiv.org/pdf/2505.16700v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16705v1", "title": "An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations", "authors": ["Seonghwan Park", "Jueun Mun", "Donghyun Oh", "Namhoon Lee"], "abstract": "Concept bottleneck models (CBMs) ensure interpretability by decomposing\npredictions into human interpretable concepts. Yet the annotations used for\ntraining CBMs that enable this transparency are often noisy, and the impact of\nsuch corruption is not well understood. In this study, we present the first\nsystematic study of noise in CBMs and show that even moderate corruption\nsimultaneously impairs prediction performance, interpretability, and the\nintervention effectiveness. Our analysis identifies a susceptible subset of\nconcepts whose accuracy declines far more than the average gap between noisy\nand clean supervision and whose corruption accounts for most performance loss.\nTo mitigate this vulnerability we propose a two-stage framework. During\ntraining, sharpness-aware minimization stabilizes the learning of\nnoise-sensitive concepts. During inference, where clean labels are unavailable,\nwe rank concepts by predictive entropy and correct only the most uncertain\nones, using uncertainty as a proxy for susceptibility. Theoretical analysis and\nextensive ablations elucidate why sharpness-aware training confers robustness\nand why uncertainty reliably identifies susceptible concepts, providing a\nprincipled basis that preserves both interpretability and resilience in the\npresence of noise.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 14:06:55", "updated": "2025-05-22 14:06:55", "pdf_url": "http://arxiv.org/pdf/2505.16705v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16710v1", "title": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Daohai Yu", "Rongrong Ji"], "abstract": "While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 14:11:34", "updated": "2025-05-22 14:11:34", "pdf_url": "http://arxiv.org/pdf/2505.16710v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16722v1", "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "abstract": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 14:30:14", "updated": "2025-05-22 14:30:14", "pdf_url": "http://arxiv.org/pdf/2505.16722v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16724v1", "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model", "authors": ["Konstantinos Barmpas", "Na Lee", "Yannis Panagakis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "abstract": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs.", "categories": ["cs.LG", "cs.AI", "cs.HC"], "published": "2025-05-22 14:32:56", "updated": "2025-05-22 14:32:56", "pdf_url": "http://arxiv.org/pdf/2505.16724v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16732v1", "title": "Sequential Monte Carlo for Policy Optimization in Continuous POMDPs", "authors": ["Hany Abdulsamad", "Sahel Iqbal", "Simo S\u00e4rkk\u00e4"], "abstract": "Optimal decision-making under partial observability requires agents to\nbalance reducing uncertainty (exploration) against pursuing immediate\nobjectives (exploitation). In this paper, we introduce a novel policy\noptimization framework for continuous partially observable Markov decision\nprocesses (POMDPs) that explicitly addresses this challenge. Our method casts\npolicy learning as probabilistic inference in a non-Markovian Feynman--Kac\nmodel that inherently captures the value of information gathering by\nanticipating future observations, without requiring extrinsic exploration\nbonuses or handcrafted heuristics. To optimize policies under this model, we\ndevelop a nested sequential Monte Carlo~(SMC) algorithm that efficiently\nestimates a history-dependent policy gradient under samples from the optimal\ntrajectory distribution induced by the POMDP. We demonstrate the effectiveness\nof our algorithm across standard continuous POMDP benchmarks, where existing\nmethods struggle to act under uncertainty.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-22 14:45:46", "updated": "2025-05-22 14:45:46", "pdf_url": "http://arxiv.org/pdf/2505.16732v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16735v1", "title": "Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting", "authors": ["Youngmoon Jung", "Yong-Hyeok Lee", "Myunghun Jung", "Jaeyoung Roh", "Chang Woo Han", "Hoon-Young Cho"], "abstract": "For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic\nand text embeddings are typically compared at either the phoneme or utterance\nlevel. To facilitate this, we optimize acoustic and text encoders using deep\nmetric learning (DML), enabling direct comparison of multi-modal embeddings in\na shared embedding space. However, the inherent heterogeneity between audio and\ntext modalities presents a significant challenge. To address this, we propose\nModality Adversarial Learning (MAL), which reduces the domain gap in\nheterogeneous modality representations. Specifically, we train a modality\nclassifier adversarially to encourage both encoders to generate\nmodality-invariant embeddings. Additionally, we apply DML to achieve\nphoneme-level alignment between audio and text, and conduct comprehensive\ncomparisons across various DML objectives. Experiments on the Wall Street\nJournal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the\nproposed approach.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-22 14:49:46", "updated": "2025-05-22 14:49:46", "pdf_url": "http://arxiv.org/pdf/2505.16735v1", "comment": "5 pages, 1 figures, Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16737v1", "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "abstract": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "published": "2025-05-22 14:52:10", "updated": "2025-05-22 14:52:10", "pdf_url": "http://arxiv.org/pdf/2505.16737v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16740v1", "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP", "authors": ["Alya Zouzou", "L\u00e9o and\u00e9ol", "M\u00e9lanie Ducoffe", "Ryma Boumazouza"], "abstract": "We explore the use of conformal prediction to provide statistical uncertainty\nguarantees for runway detection in vision-based landing systems (VLS). Using\nfine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal\nprediction to quantify localization reliability under user-defined risk levels.\nWe also introduce Conformal mean Average Precision (C-mAP), a novel metric\naligning object detection performance with conformal guarantees. Our results\nshow that conformal prediction can improve the reliability of runway detection\nby quantifying uncertainty in a statistically sound way, increasing safety\non-board and paving the way for certification of ML system in the aerospace\ndomain.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 14:52:59", "updated": "2025-05-22 14:52:59", "pdf_url": "http://arxiv.org/pdf/2505.16740v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16743v1", "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "abstract": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "published": "2025-05-22 14:53:53", "updated": "2025-05-22 14:53:53", "pdf_url": "http://arxiv.org/pdf/2505.16743v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16752v1", "title": "Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation", "authors": ["Hao Guo", "Erpeng Xue", "Lei Huang", "Shichao Wang", "Xiaolei Wang", "Lei Wang", "Jinpeng Wang", "Sheng Chen"], "abstract": "We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream\narchitecture designed for recommendation systems. DFGR integrates innovative\ninteraction patterns between real and fake flows within the QKV modules of the\nself-attention mechanism, enhancing both training and inference efficiency.\nThis approach effectively addresses a key limitation observed in Meta's\nproposed HSTU generative recommendation approach, where heterogeneous\ninformation volumes are mapped into identical vector spaces, leading to\ntraining instability. Unlike traditional recommendation models, DFGR only\nrelies on user history behavior sequences and minimal attribute information,\neliminating the need for extensive manual feature engineering. Comprehensive\nevaluations on open-source and industrial datasets reveal DFGR's superior\nperformance compared to established baselines such as DIN, DCN, DIEN, and\nDeepFM. We also investigate optimal parameter allocation strategies under\ncomputational constraints, establishing DFGR as an efficient and effective\nnext-generation generate ranking paradigm.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-22 14:58:53", "updated": "2025-05-22 14:58:53", "pdf_url": "http://arxiv.org/pdf/2505.16752v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16765v1", "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques", "authors": ["Jianing Geng", "Biao Yi", "Zekun Fei", "Tongxi Wu", "Lihai Nie", "Zheli Liu"], "abstract": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-22 15:07:34", "updated": "2025-05-22 15:07:34", "pdf_url": "http://arxiv.org/pdf/2505.16765v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16771v1", "title": "Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review", "authors": ["Beyazit Bestami Yuksel", "Ayse Yilmazer Metin"], "abstract": "This paper presents a comprehensive synthesis of major breakthroughs in\nartificial intelligence (AI) over the past fifteen years, integrating\nhistorical, theoretical, and technological perspectives. It identifies key\ninflection points in AI' s evolution by tracing the convergence of\ncomputational resources, data access, and algorithmic innovation. The analysis\nhighlights how researchers enabled GPU based model training, triggered a data\ncentric shift with ImageNet, simplified architectures through the Transformer,\nand expanded modeling capabilities with the GPT series. Rather than treating\nthese advances as isolated milestones, the paper frames them as indicators of\ndeeper paradigm shifts. By applying concepts from statistical learning theory\nsuch as sample complexity and data efficiency, the paper explains how\nresearchers translated breakthroughs into scalable solutions and why the field\nmust now embrace data centric approaches. In response to rising privacy\nconcerns and tightening regulations, the paper evaluates emerging solutions\nlike federated learning, privacy enhancing technologies (PETs), and the data\nsite paradigm, which reframe data access and security. In cases where real\nworld data remains inaccessible, the paper also assesses the utility and\nconstraints of mock and synthetic data generation. By aligning technical\ninsights with evolving data infrastructure, this study offers strategic\nguidance for future AI research and policy development.", "categories": ["cs.AI"], "published": "2025-05-22 15:12:48", "updated": "2025-05-22 15:12:48", "pdf_url": "http://arxiv.org/pdf/2505.16771v1", "comment": "10 pages, 6 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16773v1", "title": "Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis", "authors": ["Iv\u00e1n Matas", "Carmen Serrano", "Miguel Nogales", "David Moreno", "Lara Ferr\u00e1ndiz", "Teresa Ojeda", "Bego\u00f1a Acha"], "abstract": "Deep learning has transformed computer vision but relies heavily on large\nlabeled datasets and computational resources. Transfer learning, particularly\nfine-tuning pretrained models, offers a practical alternative; however, models\npretrained on natural image datasets such as ImageNet may fail to capture\ndomain-specific characteristics in medical imaging. This study introduces an\nunsupervised learning framework that extracts high-value dermatological\nfeatures instead of relying solely on ImageNet-based pretraining. We employ a\nVariational Autoencoder (VAE) trained from scratch on a proprietary\ndermatological dataset, allowing the model to learn a structured and clinically\nrelevant latent space. This self-supervised feature extractor is then compared\nto an ImageNet-pretrained backbone under identical classification conditions,\nhighlighting the trade-offs between general-purpose and domain-specific\npretraining. Our results reveal distinct learning patterns. The self-supervised\nmodel achieves a final validation loss of 0.110 (-33.33%), while the\nImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.\nAccuracy trends confirm this: the self-supervised model improves from 45% to\n65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained\nmodel reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting\ngap increasing to +0.060. These findings suggest that while ImageNet\npretraining accelerates convergence, it also amplifies overfitting on\nnon-clinically relevant features. In contrast, self-supervised learning\nachieves steady improvements, stronger generalization, and superior\nadaptability, underscoring the importance of domain-specific feature extraction\nin medical imaging.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 15:15:17", "updated": "2025-05-22 15:15:17", "pdf_url": "http://arxiv.org/pdf/2505.16773v1", "comment": "6 pages, 2 tables, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16781v1", "title": "Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making", "authors": ["Qianlei Jia", "Xinliang Zhou", "Ondrej Krejcar", "Enrique Herrera-Viedma"], "abstract": "In group decision-making (GDM) scenarios, uncertainty, dynamic social\nstructures, and vague information present major challenges for traditional\nopinion dynamics models. To address these issues, this study proposes a novel\nsocial network group decision-making (SNGDM) framework that integrates\nthree-way decision (3WD) theory, dynamic network reconstruction, and linguistic\nopinion representation. First, the 3WD mechanism is introduced to explicitly\nmodel hesitation and ambiguity in agent judgments, thereby preventing\nirrational decisions. Second, a connection adjustment rule based on opinion\nsimilarity is developed, enabling agents to adaptively update their\ncommunication links and better reflect the evolving nature of social\nrelationships. Third, linguistic terms are used to describe agent opinions,\nallowing the model to handle subjective, vague, or incomplete information more\neffectively. Finally, an integrated multi-agent decision-making framework is\nconstructed, which simultaneously considers individual uncertainty, opinion\nevolution, and network dynamics. The proposed model is applied to a multi-UAV\ncooperative decision-making scenario, where simulation results and consensus\nanalysis demonstrate its effectiveness. Experimental comparisons further verify\nthe advantages of the algorithm in enhancing system stability and representing\nrealistic decision-making behaviors.", "categories": ["cs.AI"], "published": "2025-05-22 15:26:48", "updated": "2025-05-22 15:26:48", "pdf_url": "http://arxiv.org/pdf/2505.16781v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16785v1", "title": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models", "authors": ["Zhenzhen Ren", "GuoBiao Li", "Sheng Li", "Zhenxing Qian", "Xinpeng Zhang"], "abstract": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-22 15:28:25", "updated": "2025-05-22 15:28:25", "pdf_url": "http://arxiv.org/pdf/2505.16785v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16787v1", "title": "Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce", "authors": ["Ashish Sundar", "Chunbo Luo", "Xiaoyang Wang"], "abstract": "Model-based reinforcement learning (MBRL) offers an intuitive way to increase\nthe sample efficiency of model-free RL methods by simultaneously training a\nworld model that learns to predict the future. MBRL methods have progressed by\nlargely prioritising the actor; optimising the world model learning has been\nneglected meanwhile. Improving the fidelity of the world model and reducing its\ntime to convergence can yield significant downstream benefits, one of which is\nimproving the ensuing performance of any actor it may train. We propose a novel\napproach that anticipates and actively seeks out high-entropy states using\nshort-horizon latent predictions generated by the world model, offering a\nprincipled alternative to traditional curiosity-driven methods that chase\nonce-novel states well after they were stumbled into. While many model\npredictive control (MPC) based methods offer similar alternatives, they\ntypically lack commitment, synthesising multi step plans after every step. To\nmitigate this, we present a hierarchical planner that dynamically decides when\nto replan, planning horizon length, and the weighting between reward and\nentropy. While our method can theoretically be applied to any model that trains\nits own actors with solely model generated data, we have applied it to just\nDreamer as a proof of concept. Our method finishes the Miniworld procedurally\ngenerated mazes 50% faster than base Dreamer at convergence and the policy\ntrained in imagination converges in only 60% of the environment steps that base\nDreamer needs.", "categories": ["cs.AI"], "published": "2025-05-22 15:28:50", "updated": "2025-05-22 15:28:50", "pdf_url": "http://arxiv.org/pdf/2505.16787v1", "comment": "9 pages without appendix, 15 Figures, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16789v1", "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "abstract": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 15:30:00", "updated": "2025-05-22 15:30:00", "pdf_url": "http://arxiv.org/pdf/2505.16789v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16790v1", "title": "Learning Flexible Forward Trajectories for Masked Molecular Diffusion", "authors": ["Hyunjin Seo", "Taewon Kim", "Sihyun Yu", "SungSoo Ahn"], "abstract": "Masked diffusion models (MDMs) have achieved notable progress in modeling\ndiscrete data, while their potential in molecular generation remains\nunderexplored. In this work, we explore their potential and introduce the\nsurprising result that naively applying standards MDMs severely degrades the\nperformance. We identify the critical cause of this issue as a state-clashing\nproblem-where the forward diffusion of distinct molecules collapse into a\ncommon state, resulting in a mixture of reconstruction targets that cannot be\nlearned using typical reverse diffusion process with unimodal predictions. To\nmitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that\norchestrates per-element corruption trajectories to avoid collision between\ndistinct molecular graphs. This is achieved through a parameterized noise\nscheduling network that assigns distinct corruption rates to individual graph\nelements, i.e., atoms and bonds. Extensive experiments on diverse molecular\nbenchmarks reveal that MELD markedly enhances overall generation quality\ncompared to element-agnostic noise scheduling, increasing the chemical validity\nof vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves\nstate-of-the-art property alignment in conditional generation tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 15:30:17", "updated": "2025-05-22 15:30:17", "pdf_url": "http://arxiv.org/pdf/2505.16790v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16791v1", "title": "Cohort-Based Active Modality Acquisition", "authors": ["Tillmann Rheude", "Roland Eils", "Benjamin Wild"], "abstract": "Real-world machine learning applications often involve data from multiple\nmodalities that must be integrated effectively to make robust predictions.\nHowever, in many practical settings, not all modalities are available for every\nsample, and acquiring additional modalities can be costly. This raises the\nquestion: which samples should be prioritized for additional modality\nacquisition when resources are limited? While prior work has explored\nindividual-level acquisition strategies and training-time active learning\nparadigms, test-time and cohort-based acquisition remain underexplored despite\ntheir importance in many real-world settings. We introduce Cohort-based Active\nModality Acquisition (CAMA), a novel test-time setting to formalize the\nchallenge of selecting which samples should receive additional modalities. We\nderive acquisition strategies that leverage a combination of generative\nimputation and discriminative modeling to estimate the expected benefit of\nacquiring missing modalities based on common evaluation metrics. We also\nintroduce upper-bound heuristics that provide performance ceilings to benchmark\nacquisition strategies. Experiments on common multimodal datasets demonstrate\nthat our proposed imputation-based strategies can more effectively guide the\nacquisition of new samples in comparison to those relying solely on unimodal\ninformation, entropy guidance, and random selections. Our work provides an\neffective solution for optimizing modality acquisition at the cohort level,\nenabling better utilization of resources in constrained settings.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 15:32:50", "updated": "2025-05-22 15:32:50", "pdf_url": "http://arxiv.org/pdf/2505.16791v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16792v1", "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training", "authors": ["Ziqiao Wang", "Wangbo Zhao", "Yuhao Zhou", "Zekai Li", "Zhiyuan Liang", "Mingjia Shi", "Xuanlei Zhao", "Pengfei Zhou", "Kaipeng Zhang", "Zhangyang Wang", "Kai Wang", "Yang You"], "abstract": "Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet\ntheir training remains notoriously slow. A recent remedy -- representation\nalignment (REPA) that matches DiT hidden features to those of a non-generative\nteacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus\nor even degrades performance later. We trace this failure to a capacity\nmismatch: once the generative student begins modelling the joint data\ndistribution, the teacher's lower-dimensional embeddings and attention patterns\nbecome a straitjacket rather than a guide. We then introduce HASTE (Holistic\nAlignment with Stage-wise Termination for Efficient training), a two-phase\nschedule that keeps the help and drops the hindrance. Phase I applies a\nholistic alignment loss that simultaneously distills attention maps (relational\npriors) and feature projections (semantic anchors) from the teacher into\nmid-level layers of the DiT, yielding rapid convergence. Phase II then performs\none-shot termination that deactivates the alignment loss, once a simple trigger\nsuch as a fixed iteration is hit, freeing the DiT to focus on denoising and\nexploit its generative capacity. HASTE speeds up training of diverse DiTs\nwithout architecture changes. On ImageNet 256X256, it reaches the vanilla\nSiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,\namounting to a 28X reduction in optimization steps. HASTE also improves\ntext-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled\nrecipe for efficient diffusion training across various tasks. Our code is\navailable at https://github.com/NUS-HPC-AI-Lab/HASTE .", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 15:34:33", "updated": "2025-05-22 15:34:33", "pdf_url": "http://arxiv.org/pdf/2505.16792v1", "comment": "24 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16798v1", "title": "SEED: Speaker Embedding Enhancement Diffusion Model", "authors": ["KiHyun Nam", "Jungwoo Heo", "Jee-weon Jung", "Gangin Park", "Chaeyoung Jung", "Ha-Jin Yu", "Joon Son Chung"], "abstract": "A primary challenge when deploying speaker recognition systems in real-world\napplications is performance degradation caused by environmental mismatch. We\npropose a diffusion-based method that takes speaker embeddings extracted from a\npre-trained speaker recognition model and generates refined embeddings. For\ntraining, our approach progressively adds Gaussian noise to both clean and\nnoisy speaker embeddings extracted from clean and noisy speech, respectively,\nvia forward process of a diffusion model, and then reconstructs them to clean\nembeddings in the reverse process. While inferencing, all embeddings are\nregenerated via diffusion process. Our method needs neither speaker label nor\nany modification to the existing speaker recognition pipeline. Experiments on\nevaluation sets simulating environment mismatch scenarios show that our method\ncan improve recognition accuracy by up to 19.6% over baseline models while\nretaining performance on conventional scenarios. We publish our code here\nhttps://github.com/kaistmm/seed-pytorch", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-22 15:38:37", "updated": "2025-05-22 15:38:37", "pdf_url": "http://arxiv.org/pdf/2505.16798v1", "comment": "Accepted to Interspeech 2025. The official code can be found at\n  https://github.com/kaistmm/seed-pytorch", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16801v1", "title": "A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents", "authors": ["Eleftherios Kalafatis", "Konstantinos Mitsis", "Konstantia Zarkogianni", "Maria Athanasiou", "Konstantina Nikita"], "abstract": "Serious Games (SGs) are nowadays shifting focus to include procedural content\ngeneration (PCG) in the development process as a means of offering personalized\nand enhanced player experience. However, the development of a framework to\nassess the impact of PCG techniques when integrated into SGs remains\nparticularly challenging. This study proposes a methodology for automated\nevaluation of PCG integration in SGs, incorporating deep reinforcement learning\n(DRL) game testing agents. To validate the proposed framework, a previously\nintroduced SG featuring card game mechanics and incorporating three different\nversions of PCG for nonplayer character (NPC) creation has been deployed.\nVersion 1 features random NPC creation, while versions 2 and 3 utilize a\ngenetic algorithm approach. These versions are used to test the impact of\ndifferent dynamic SG environments on the proposed framework's agents. The\nobtained results highlight the superiority of the DRL game testing agents\ntrained on Versions 2 and 3 over those trained on Version 1 in terms of win\nrate (i.e. number of wins per played games) and training time. More\nspecifically, within the execution of a test emulating regular gameplay, both\nVersions 2 and 3 peaked at a 97% win rate and achieved statistically\nsignificant higher (p=0009) win rates compared to those achieved in Version 1\nthat peaked at 94%. Overall, results advocate towards the proposed framework's\ncapability to produce meaningful data for the evaluation of procedurally\ngenerated content in SGs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 15:40:56", "updated": "2025-05-22 15:40:56", "pdf_url": "http://arxiv.org/pdf/2505.16801v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16813v1", "title": "Dynamic Reservoir Computing with Physical Neuromorphic Networks", "authors": ["Yinhao Xu", "Georg A. Gottwald", "Zdenka Kuncic"], "abstract": "Reservoir Computing (RC) with physical systems requires an understanding of\nthe underlying structure and internal dynamics of the specific physical\nreservoir. In this study, physical nano-electronic networks with neuromorphic\ndynamics are investigated for their use as physical reservoirs in an RC\nframework. These neuromorphic networks operate as dynamic reservoirs, with node\nactivities in general coupled to the edge dynamics through nonlinear\nnano-electronic circuit elements, and the reservoir outputs influenced by the\nunderlying network connectivity structure. This study finds that networks with\nvarying degrees of sparsity generate more useful nonlinear temporal outputs for\ndynamic RC compared to dense networks. Dynamic RC is also tested on an\nautonomous multivariate chaotic time series prediction task with networks of\nvarying densities, which revealed the importance of network sparsity in\nmaintaining network activity and overall dynamics, that in turn enabled the\nlearning of the chaotic Lorenz63 system's attractor behavior.", "categories": ["cs.ET", "cond-mat.dis-nn", "cs.AI"], "published": "2025-05-22 15:50:45", "updated": "2025-05-22 15:50:45", "pdf_url": "http://arxiv.org/pdf/2505.16813v1", "comment": "8 pages, 8 figures, IJCNN 2025, accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16826v1", "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning", "authors": ["Wei Sun", "Wen Yang", "Pu Jian", "Qianlong Du", "Fuwei Cui", "Shuo Ren", "Jiajun Zhang"], "abstract": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 16:00:33", "updated": "2025-05-22 16:00:33", "pdf_url": "http://arxiv.org/pdf/2505.16826v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16827v1", "title": "GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent", "authors": ["Bin Xie", "Rui Shao", "Gongwei Chen", "Kaiwen Zhou", "Yinchuan Li", "Jie Liu", "Min Zhang", "Liqiang Nie"], "abstract": "GUI automation faces critical challenges in dynamic environments. MLLMs\nsuffer from two key issues: misinterpreting UI components and outdated\nknowledge. Traditional fine-tuning methods are costly for app-specific\nknowledge updates. We propose GUI-explorer, a training-free GUI agent that\nincorporates two fundamental mechanisms: (1) Autonomous Exploration of\nFunction-aware Trajectory. To comprehensively cover all application\nfunctionalities, we design a Function-aware Task Goal Generator that\nautomatically constructs exploration goals by analyzing GUI structural\ninformation (e.g., screenshots and activity hierarchies). This enables\nsystematic exploration to collect diverse trajectories. (2) Unsupervised Mining\nof Transition-aware Knowledge. To establish precise screen-operation logic, we\ndevelop a Transition-aware Knowledge Extractor that extracts effective\nscreen-operation logic through unsupervised analysis the state transition of\nstructured interaction triples (observation, action, outcome). This eliminates\nthe need for human involvement in knowledge extraction. With a task success\nrate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows\nsignificant improvements over SOTA agents. It requires no parameter updates for\nnew apps. GUI-explorer is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/GUI-explorer.", "categories": ["cs.AI"], "published": "2025-05-22 16:01:06", "updated": "2025-05-22 16:01:06", "pdf_url": "http://arxiv.org/pdf/2505.16827v1", "comment": "ACL 2025. Github: https://github.com/JiuTian-VL/GUI-explorer", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16831v1", "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "abstract": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "published": "2025-05-22 16:02:10", "updated": "2025-05-22 16:02:10", "pdf_url": "http://arxiv.org/pdf/2505.16831v1", "comment": "44 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16832v1", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "abstract": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "published": "2025-05-22 16:02:18", "updated": "2025-05-22 16:02:18", "pdf_url": "http://arxiv.org/pdf/2505.16832v1", "comment": "16 pages; 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16834v1", "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "abstract": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-22 16:05:02", "updated": "2025-05-22 16:05:02", "pdf_url": "http://arxiv.org/pdf/2505.16834v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16836v1", "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning", "authors": ["Fanrui Zhang", "Dian Li", "Qiang Zhang", "Chenjun", "sinbadliu", "Junxiong Lin", "Jiahong Yan", "Jiawei Liu", "Zheng-Jun Zha"], "abstract": "The rapid spread of multimodal misinformation on social media has raised\ngrowing concerns, while research on video misinformation detection remains\nlimited due to the lack of large-scale, diverse datasets. Existing methods\noften overfit to rigid templates and lack deep reasoning over deceptive\ncontent. To address these challenges, we introduce FakeVV, a large-scale\nbenchmark comprising over 100,000 video-text pairs with fine-grained,\ninterpretable annotations. In addition, we further propose Fact-R1, a novel\nframework that integrates deep reasoning with collaborative rule-based\nreinforcement learning. Fact-R1 is trained through a three-stage process: (1)\nmisinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference\nalignment via Direct Preference Optimization (DPO), and (3) Group Relative\nPolicy Optimization (GRPO) using a novel verifiable reward function. This\nenables Fact-R1 to exhibit emergent reasoning behaviors comparable to those\nobserved in advanced text-based reinforcement learning systems, but in the more\ncomplex multimodal misinformation setting. Our work establishes a new paradigm\nfor misinformation detection, bridging large-scale video understanding,\nreasoning-guided alignment, and interpretable verification.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 16:05:06", "updated": "2025-05-22 16:05:06", "pdf_url": "http://arxiv.org/pdf/2505.16836v1", "comment": "28 pages, 27 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16845v1", "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate", "authors": ["Hanglei Zhang", "Yiwei Guo", "Zhihan Li", "Xiang Hao", "Xie Chen", "Kai Yu"], "abstract": "Most neural speech codecs achieve bitrate adjustment through intra-frame\nmechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However,\nspeech segments inherently have time-varying information density (e.g., silent\nintervals versus voiced regions). This property makes CFR not optimal in terms\nof bitrate and token sequence length, hindering efficiency in real-time\napplications. In this work, we propose a Temporally Flexible Coding (TFC)\ntechnique, introducing variable frame rate (VFR) into neural speech codecs for\nthe first time. TFC enables seamlessly tunable average frame rates and\ndynamically allocates frame rates based on temporal entropy. Experimental\nresults show that a codec with TFC achieves optimal reconstruction quality with\nhigh flexibility, and maintains competitive performance even at lower frame\nrates. Our approach is promising for the integration with other efforts to\ndevelop low-frame-rate neural speech codecs for more efficient downstream\ntasks.", "categories": ["eess.AS", "cs.AI", "cs.SD"], "published": "2025-05-22 16:10:01", "updated": "2025-05-22 16:10:01", "pdf_url": "http://arxiv.org/pdf/2505.16845v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16854v1", "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "authors": ["Jiaqi Wang", "Kevin Qinghong Lin", "James Cheng", "Mike Zheng Shou"], "abstract": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-22 16:13:29", "updated": "2025-05-22 16:13:29", "pdf_url": "http://arxiv.org/pdf/2505.16854v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16856v1", "title": "Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only", "authors": ["Wei Xiao", "Jiacheng Liu", "Zifeng Zhuang", "Runze Suo", "Shangke Lyu", "Donglin Wang"], "abstract": "Improving the performance of pre-trained policies through online\nreinforcement learning (RL) is a critical yet challenging topic. Existing\nonline RL fine-tuning methods require continued training with offline\npretrained Q-functions for stability and performance. However, these offline\npretrained Q-functions commonly underestimate state-action pairs beyond the\noffline dataset due to the conservatism in most offline RL methods, which\nhinders further exploration when transitioning from the offline to the online\nsetting. Additionally, this requirement limits their applicability in scenarios\nwhere only pre-trained policies are available but pre-trained Q-functions are\nabsent, such as in imitation learning (IL) pre-training. To address these\nchallenges, we propose a method for efficient online RL fine-tuning using\nsolely the offline pre-trained policy, eliminating reliance on pre-trained\nQ-functions. We introduce PORL (Policy-Only Reinforcement Learning\nFine-Tuning), which rapidly initializes the Q-function from scratch during the\nonline phase to avoid detrimental pessimism. Our method not only achieves\ncompetitive performance with advanced offline-to-online RL algorithms and\nonline RL approaches that leverage data or policies prior, but also pioneers a\nnew path for directly fine-tuning behavior cloning (BC) policies.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-22 16:14:08", "updated": "2025-05-22 16:14:08", "pdf_url": "http://arxiv.org/pdf/2505.16856v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16860v1", "title": "GCAL: Adapting Graph Models to Evolving Domain Shifts", "authors": ["Ziyue Qiao", "Qianyi Cai", "Hao Dong", "Jiawei Gu", "Pengyang Wang", "Meng Xiao", "Xiao Luo", "Hui Xiong"], "abstract": "This paper addresses the challenge of graph domain adaptation on evolving,\nmultiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation\nmethods are confined to single-step adaptation, making them ineffective in\nhandling continuous domain shifts and prone to catastrophic forgetting. This\npaper introduces the Graph Continual Adaptive Learning (GCAL) method, designed\nto enhance model sustainability and adaptability across various graph domains.\nGCAL employs a bilevel optimization strategy. The \"adapt\" phase uses an\ninformation maximization approach to fine-tune the model with new graph domains\nwhile re-adapting past memories to mitigate forgetting. Concurrently, the\n\"generate memory\" phase, guided by a theoretical lower bound derived from\ninformation bottleneck theory, involves a variational memory graph generation\nmodule to condense original graphs into memories. Extensive experimental\nevaluations demonstrate that GCAL substantially outperforms existing methods in\nterms of adaptability and knowledge retention.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 16:19:19", "updated": "2025-05-22 16:19:19", "pdf_url": "http://arxiv.org/pdf/2505.16860v1", "comment": "Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16875v1", "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training", "authors": ["Zhehao Huang", "Yuhang Liu", "Yixin Lou", "Zhengbao He", "Mingzhen He", "Wenxing Zhou", "Tao Li", "Kehan Li", "Zeyi Huang", "Xiaolin Huang"], "abstract": "Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-22 16:31:43", "updated": "2025-05-22 16:31:43", "pdf_url": "http://arxiv.org/pdf/2505.16875v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16877v1", "title": "Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings", "authors": ["Yuqicheng Zhu", "Daniel Hern\u00e1ndez", "Yuan He", "Zifeng Ding", "Bo Xiong", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is\ncrucial for ensuring the reliability of downstream applications. A recent work\napplies conformal prediction to KGE methods, providing uncertainty estimates by\ngenerating a set of answers that is guaranteed to include the true answer with\na predefined confidence level. However, existing methods provide probabilistic\nguarantees averaged over a reference set of queries and answers (marginal\ncoverage guarantee). In high-stakes applications such as medical diagnosis, a\nstronger guarantee is often required: the predicted sets must provide\nconsistent coverage per query (conditional coverage guarantee). We propose\nCondKGCP, a novel method that approximates predicate-conditional coverage\nguarantees while maintaining compact prediction sets. CondKGCP merges\npredicates with similar vector representations and augments calibration with\nrank information. We prove the theoretical guarantees and demonstrate empirical\neffectiveness of CondKGCP by comprehensive evaluations.", "categories": ["cs.AI"], "published": "2025-05-22 16:33:20", "updated": "2025-05-22 16:33:20", "pdf_url": "http://arxiv.org/pdf/2505.16877v1", "comment": "Accepted to the Finding of ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16881v1", "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "abstract": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 16:35:33", "updated": "2025-05-22 16:35:33", "pdf_url": "http://arxiv.org/pdf/2505.16881v1", "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16886v1", "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "abstract": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 16:41:37", "updated": "2025-05-22 16:41:37", "pdf_url": "http://arxiv.org/pdf/2505.16886v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16888v1", "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "authors": ["Viet Pham", "Thai Le"], "abstract": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-22 16:47:15", "updated": "2025-05-22 16:47:15", "pdf_url": "http://arxiv.org/pdf/2505.16888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16896v1", "title": "Structure-Aligned Protein Language Model", "authors": ["Can Chen", "David Heurtel-Depeiges", "Robert M. Vernon", "Christopher James Langmead", "Yoshua Bengio", "Quentin Fournier"], "abstract": "Protein language models (pLMs) pre-trained on vast protein sequence databases\nexcel at various downstream tasks but lack the structural knowledge essential\nfor many biological applications. To address this, we integrate structural\ninsights from pre-trained protein graph neural networks (pGNNs) into pLMs\nthrough a latent-level contrastive learning task. This task aligns residue\nrepresentations from pLMs with those from pGNNs across multiple proteins,\nenriching pLMs with inter-protein structural knowledge. Additionally, we\nincorporate a physical-level task that infuses intra-protein structural\nknowledge by optimizing pLMs to predict structural tokens. The proposed\ndual-task framework effectively incorporates both inter-protein and\nintra-protein structural knowledge into pLMs. Given the variability in the\nquality of protein structures in PDB, we further introduce a residue loss\nselection module, which uses a small model trained on high-quality structures\nto select reliable yet challenging residue losses for the pLM to learn.\nApplying our structure alignment method to the state-of-the-art ESM2 and\nAMPLIFY results in notable performance gains across a wide range of tasks,\nincluding a 12.7% increase in ESM2 contact prediction. The data, code, and\nresulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 16:56:12", "updated": "2025-05-22 16:56:12", "pdf_url": "http://arxiv.org/pdf/2505.16896v1", "comment": "16 pages, 8 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16899v1", "title": "Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships", "authors": ["Kerem Oktar", "Katherine M. Collins", "Jose Hernandez-Orallo", "Diane Coyle", "Stephen Cave", "Adrian Weller", "Ilia Sucholutsky"], "abstract": "Artificial Intelligence (AI) systems have historically been used as tools\nthat execute narrowly defined tasks. Yet recent advances in AI have unlocked\npossibilities for a new class of models that genuinely collaborate with humans\nin complex reasoning, from conceptualizing problems to brainstorming solutions.\nSuch AI thought partners enable novel forms of collaboration and extended\ncognition, yet they also pose major risks-including and beyond risks of typical\nAI tools and agents. In this commentary, we systematically identify risks of AI\nthought partners through a novel framework that identifies risks at multiple\nlevels of analysis, including Real-time, Individual, and Societal risks arising\nfrom collaborative cognition (RISc). We leverage this framework to propose\nconcrete metrics for risk evaluation, and finally suggest specific mitigation\nstrategies for developers and policymakers. As AI thought partners continue to\nproliferate, these strategies can help prevent major harms and ensure that\nhumans actively benefit from productive thought partnerships.", "categories": ["cs.AI"], "published": "2025-05-22 16:58:48", "updated": "2025-05-22 16:58:48", "pdf_url": "http://arxiv.org/pdf/2505.16899v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16911v1", "title": "Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation", "authors": ["Ofir Yaish", "Yehuda Mishaly", "Eliya Nachmani"], "abstract": "We introduce a new paradigm for active sound modification: Active Speech\nEnhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on\nsuppressing external interference, ASE goes further by actively shaping the\nspeech signal -- both attenuating unwanted noise components and amplifying\nspeech-relevant frequencies -- to improve intelligibility and perceptual\nquality. To enable this, we propose a novel Transformer-Mamba-based\narchitecture, along with a task-specific loss function designed to jointly\noptimize interference suppression and signal enrichment. Our method outperforms\nexisting baselines across multiple speech processing tasks -- including\ndenoising, dereverberation, and declipping -- demonstrating the effectiveness\nof active, targeted modulation in challenging acoustic environments.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-22 17:10:18", "updated": "2025-05-22 17:10:18", "pdf_url": "http://arxiv.org/pdf/2505.16911v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16915v1", "title": "DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?", "authors": ["Qirui Jiao", "Daoyuan Chen", "Yilun Huang", "Xika Lin", "Ying Shen", "Yaliang Li"], "abstract": "While recent text-to-image (T2I) models show impressive capabilities in\nsynthesizing images from brief descriptions, their performance significantly\ndegrades when confronted with long, detail-intensive prompts required in\nprofessional applications. We present DetailMaster, the first comprehensive\nbenchmark specifically designed to evaluate T2I models' systematical abilities\nto handle extended textual inputs that contain complex compositional\nrequirements. Our benchmark introduces four critical evaluation dimensions:\nCharacter Attributes, Structured Character Locations, Multi-Dimensional Scene\nAttributes, and Explicit Spatial/Interactive Relationships. The benchmark\ncomprises long and detail-rich prompts averaging 284.89 tokens, with high\nquality validated by expert annotators. Evaluation on 7 general-purpose and 5\nlong-prompt-optimized T2I models reveals critical performance limitations:\nstate-of-the-art models achieve merely ~50% accuracy in key dimensions like\nattribute binding and spatial reasoning, while all models showing progressive\nperformance degradation as prompt length increases. Our analysis highlights\nsystemic failures in structural comprehension and detail overload handling,\nmotivating future research into architectures with enhanced compositional\nreasoning. We open-source the dataset, data curation code, and evaluation tools\nto advance detail-rich T2I generation and enable broad applications that would\notherwise be infeasible due to the lack of a dedicated benchmark.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 17:11:27", "updated": "2025-05-22 17:11:27", "pdf_url": "http://arxiv.org/pdf/2505.16915v1", "comment": "22 pages, 8 figures, 10 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16927v1", "title": "Latent Principle Discovery for Language Model Self-Improvement", "authors": ["Keshav Ramji", "Tahira Naseem", "Ram\u00f3n Fernandez Astudillo"], "abstract": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 17:20:18", "updated": "2025-05-22 17:20:18", "pdf_url": "http://arxiv.org/pdf/2505.16927v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16928v1", "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning", "authors": ["Bosung Kim", "Prithviraj Ammanabrolu"], "abstract": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.", "categories": ["cs.AI", "cs.LG", "cs.RO"], "published": "2025-05-22 17:20:38", "updated": "2025-05-22 17:20:38", "pdf_url": "http://arxiv.org/pdf/2505.16928v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16932v1", "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "abstract": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "published": "2025-05-22 17:23:14", "updated": "2025-05-22 17:23:14", "pdf_url": "http://arxiv.org/pdf/2505.16932v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16938v1", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.", "categories": ["cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-22 17:27:43", "updated": "2025-05-22 17:27:43", "pdf_url": "http://arxiv.org/pdf/2505.16938v1", "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16941v1", "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "abstract": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 17:29:52", "updated": "2025-05-22 17:29:52", "pdf_url": "http://arxiv.org/pdf/2505.16941v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16944v1", "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 17:31:10", "updated": "2025-05-22 17:31:10", "pdf_url": "http://arxiv.org/pdf/2505.16944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16947v1", "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs", "authors": ["Csaba D\u00e9k\u00e1ny", "Stefan Balauca", "Robin Staab", "Dimitar I. Dimitrov", "Martin Vechev"], "abstract": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT.", "categories": ["cs.LG", "cs.AI", "I.2.7; K.4.1"], "published": "2025-05-22 17:32:50", "updated": "2025-05-22 17:32:50", "pdf_url": "http://arxiv.org/pdf/2505.16947v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16950v1", "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning", "authors": ["Adnan Oomerjee", "Zafeirios Fountas", "Zhongwei Yu", "Haitham Bou-Ammar", "Jun Wang"], "abstract": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "published": "2025-05-22 17:33:49", "updated": "2025-05-22 17:33:49", "pdf_url": "http://arxiv.org/pdf/2505.16950v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16957v1", "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models", "authors": ["Junjie Xiong", "Changjia Zhu", "Shuhang Lin", "Chong Zhang", "Yongfeng Zhang", "Yao Liu", "Lingyao Li"], "abstract": "Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-22 17:36:33", "updated": "2025-05-22 17:36:33", "pdf_url": "http://arxiv.org/pdf/2505.16957v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16965v1", "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "abstract": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 17:46:23", "updated": "2025-05-22 17:46:23", "pdf_url": "http://arxiv.org/pdf/2505.16965v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16967v1", "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval", "authors": ["Nandan Thakur", "Crystina Zhang", "Xueguang Ma", "Jimmy Lin"], "abstract": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "published": "2025-05-22 17:47:57", "updated": "2025-05-22 17:47:57", "pdf_url": "http://arxiv.org/pdf/2505.16967v1", "comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16968v1", "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "abstract": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "published": "2025-05-22 17:48:53", "updated": "2025-05-22 17:48:53", "pdf_url": "http://arxiv.org/pdf/2505.16968v1", "comment": "20 pages, 11 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16978v1", "title": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation", "authors": ["Weizhi Tang", "Yixuan Li", "Chris Sypherd", "Elizabeth Polgreen", "Vaishak Belle"], "abstract": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs.", "categories": ["cs.AI", "cs.PL"], "published": "2025-05-22 17:52:31", "updated": "2025-05-22 17:52:31", "pdf_url": "http://arxiv.org/pdf/2505.16978v1", "comment": "Accepted to ACL 2025 Findings. Code available at\n  https://github.com/RutaTang/HyGenar", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16979v1", "title": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design", "authors": ["Zhenkun Li", "Lingyao Li", "Shuhang Lin", "Yongfeng Zhang"], "abstract": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-22 17:52:33", "updated": "2025-05-22 17:52:33", "pdf_url": "http://arxiv.org/pdf/2505.16979v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "abstract": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "categories": ["cs.AI", "physics.med-ph"], "published": "2025-05-22 17:52:59", "updated": "2025-05-22 17:52:59", "pdf_url": "http://arxiv.org/pdf/2505.16982v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16985v1", "title": "Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation", "authors": ["Moru Liu", "Hao Dong", "Jessica Kelly", "Olga Fink", "Mario Trapp"], "abstract": "Out-of-distribution (OOD) detection and segmentation are crucial for\ndeploying machine learning models in safety-critical applications such as\nautonomous driving and robot-assisted surgery. While prior research has\nprimarily focused on unimodal image data, real-world applications are\ninherently multimodal, requiring the integration of multiple modalities for\nimproved OOD detection. A key challenge is the lack of supervision signals from\nunknown data, leading to overconfident predictions on OOD samples. To address\nthis challenge, we propose Feature Mixing, an extremely simple and fast method\nfor multimodal outlier synthesis with theoretical support, which can be further\noptimized to help the model better distinguish between in-distribution (ID) and\nOOD data. Feature Mixing is modality-agnostic and applicable to various\nmodality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal\ndataset for OOD segmentation, featuring synthetic OOD objects across diverse\nscenes and weather conditions. Extensive experiments on SemanticKITTI,\nnuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that\nFeature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370\n\\times$ speedup. Our source code and dataset will be available at\nhttps://github.com/mona4399/FeatureMixing.", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "published": "2025-05-22 17:54:30", "updated": "2025-05-22 17:54:30", "pdf_url": "http://arxiv.org/pdf/2505.16985v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16986v1", "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 17:54:32", "updated": "2025-05-22 17:54:32", "pdf_url": "http://arxiv.org/pdf/2505.16986v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16988v1", "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-22 17:54:38", "updated": "2025-05-22 17:54:38", "pdf_url": "http://arxiv.org/pdf/2505.16988v1", "comment": "18 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16994v1", "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning", "authors": ["Runyang You", "Yongqi Li", "Xinyu Lin", "Xin Zhang", "Wenjie Wang", "Wenjie Li", "Liqiang Nie"], "abstract": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "published": "2025-05-22 17:55:43", "updated": "2025-05-22 17:55:43", "pdf_url": "http://arxiv.org/pdf/2505.16994v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16997v1", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.", "categories": ["cs.AI", "cs.CL", "cs.MA"], "published": "2025-05-22 17:56:39", "updated": "2025-05-22 17:56:39", "pdf_url": "http://arxiv.org/pdf/2505.16997v1", "comment": "19 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16998v1", "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "abstract": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 17:57:23", "updated": "2025-05-22 17:57:23", "pdf_url": "http://arxiv.org/pdf/2505.16998v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17002v1", "title": "PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association", "authors": ["Abdul Hannan", "Muhammad Arslan Manzoor", "Shah Nawaz", "Muhammad Irzam Liaqat", "Markus Schedl", "Mubashir Noman"], "abstract": "We study the task of learning association between faces and voices, which is\ngaining interest in the multimodal community lately. These methods suffer from\nthe deliberate crafting of negative mining procedures as well as the reliance\non the distant margin parameter. These issues are addressed by learning a joint\nembedding space in which orthogonality constraints are applied to the fused\nembeddings of faces and voices. However, embedding spaces of faces and voices\npossess different characteristics and require spaces to be aligned before\nfusing them. To this end, we propose a method that accurately aligns the\nembedding spaces and fuses them with an enhanced gated fusion thereby improving\nthe performance of face-voice association. Extensive experiments on the\nVoxCeleb dataset reveals the merits of the proposed approach.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 17:57:55", "updated": "2025-05-22 17:57:55", "pdf_url": "http://arxiv.org/pdf/2505.17002v1", "comment": "Accepted at InterSpeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17004v1", "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs", "authors": ["Jiachen Yao", "Abbas Mammadov", "Julius Berner", "Gavin Kerrigan", "Jong Chul Ye", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "abstract": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "stat.ML"], "published": "2025-05-22 17:58:12", "updated": "2025-05-22 17:58:12", "pdf_url": "http://arxiv.org/pdf/2505.17004v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17005v1", "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "abstract": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-22 17:58:26", "updated": "2025-05-22 17:58:26", "pdf_url": "http://arxiv.org/pdf/2505.17005v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17010v1", "title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "authors": ["Tim Genewein", "Kevin Wenliang Li", "Jordi Grau-Moya", "Anian Ruoss", "Laurent Orseau", "Marcus Hutter"], "abstract": "Prompting is one of the main ways to adapt a pretrained model to target\ntasks. Besides manually constructing prompts, many prompt optimization methods\nhave been proposed in the literature. Method development is mainly empirically\ndriven, with less emphasis on a conceptual understanding of prompting. In this\npaper we discuss how optimal prompting can be understood through a Bayesian\nview, which also implies some fundamental limitations of prompting that can\nonly be overcome by tuning weights. The paper explains in detail how\nmeta-trained neural networks behave as Bayesian predictors over the pretraining\ndistribution, whose hallmark feature is rapid in-context adaptation. Optimal\nprompting can be studied formally as conditioning these Bayesian predictors,\nyielding criteria for target tasks where optimal prompting is and is not\npossible. We support the theory with educational experiments on LSTMs and\nTransformers, where we compare different versions of prefix-tuning and\ndifferent weight-tuning methods. We also confirm that soft prefixes, which are\nsequences of real-valued vectors outside the token alphabet, can lead to very\neffective prompts for trained and even untrained networks by manipulating\nactivations in ways that are not achievable by hard tokens. This adds an\nimportant mechanistic aspect beyond the conceptual Bayesian theory.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-22 17:58:53", "updated": "2025-05-22 17:58:53", "pdf_url": "http://arxiv.org/pdf/2505.17010v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17012v1", "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding", "authors": ["Haoning Wu", "Xiao Huang", "Yaohui Chen", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "abstract": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 17:59:03", "updated": "2025-05-22 17:59:03", "pdf_url": "http://arxiv.org/pdf/2505.17012v1", "comment": "Technical Report; Project Page:\n  https://haoningwu3639.github.io/SpatialScore", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17016v1", "title": "Interactive Post-Training for Vision-Language-Action Models", "authors": ["Shuhan Tan", "Kairan Dou", "Yue Zhao", "Philipp Kr\u00e4henb\u00fchl"], "abstract": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "published": "2025-05-22 17:59:45", "updated": "2025-05-22 17:59:45", "pdf_url": "http://arxiv.org/pdf/2505.17016v1", "comment": "Project page: https://ariostgx.github.io/ript_vla/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17017v1", "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "abstract": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 17:59:49", "updated": "2025-05-22 17:59:49", "pdf_url": "http://arxiv.org/pdf/2505.17017v1", "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17019v1", "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework", "authors": ["Chenhao Zhang", "Yazhe Niu"], "abstract": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.", "categories": ["cs.CV", "cs.AI", "cs.CY"], "published": "2025-05-22 17:59:53", "updated": "2025-05-22 17:59:53", "pdf_url": "http://arxiv.org/pdf/2505.17019v1", "comment": "16 pages, 9 figures. Code & Dataset:\n  https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17022v1", "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "abstract": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "published": "2025-05-22 17:59:58", "updated": "2025-05-22 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.17022v1", "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16086v1", "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "abstract": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 00:00:27", "updated": "2025-05-22 00:00:27", "pdf_url": "http://arxiv.org/pdf/2505.16086v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16088v1", "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "abstract": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday).", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 00:06:29", "updated": "2025-05-22 00:06:29", "pdf_url": "http://arxiv.org/pdf/2505.16088v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16090v1", "title": "Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance", "authors": ["Dominick Kubica", "Dylan T. Gordon", "Nanami Emura", "Derleen Saini", "Charlie Goldenberg"], "abstract": "As of 2025, Generative Artificial Intelligence (GenAI) has become a central\ntool for productivity across industries. Beyond text generation, GenAI now\nplays a critical role in coding, data analysis, and research workflows. As\nlarge language models (LLMs) continue to evolve, it is essential to assess the\nreliability and accuracy of their outputs, especially in specialized,\nhigh-stakes domains like finance. Most modern LLMs transform text into\nnumerical vectors, which are used in operations such as cosine similarity\nsearches to generate responses. However, this abstraction process can lead to\nmisinterpretation of emotional tone, particularly in nuanced financial\ncontexts. While LLMs generally excel at identifying sentiment in everyday\nlanguage, these models often struggle with the nuanced, strategically ambiguous\nlanguage found in earnings call transcripts. Financial disclosures frequently\nembed sentiment in hedged statements, forward-looking language, and\nindustry-specific jargon, making it difficult even for human analysts to\ninterpret consistently, let alone AI models. This paper presents findings from\nthe Santa Clara Microsoft Practicum Project, led by Professor Charlie\nGoldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's\nChatGPT, Google's Gemini, and traditional machine learning models for sentiment\nanalysis of financial text. Using Microsoft earnings call transcripts, the\nanalysis assesses how well LLM-derived sentiment correlates with market\nsentiment and stock movements and evaluates the accuracy of model outputs.\nPrompt engineering techniques are also examined to improve sentiment analysis\nresults. Visualizations of sentiment consistency are developed to evaluate\nalignment between tone and stock performance, with sentiment trends analyzed\nacross Microsoft's lines of business to determine which segments exert the\ngreatest influence.", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.7"], "published": "2025-05-22 00:09:11", "updated": "2025-05-22 00:09:11", "pdf_url": "http://arxiv.org/pdf/2505.16090v1", "comment": "6 pages, 4 figures. Research conducted as part of a\n  Microsoft-sponsored Capstone Project at Santa Clara University", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16094v1", "title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization", "authors": ["Ziqing Wang", "Kexin Zhang", "Zihan Zhao", "Yibo Wen", "Abhishek Pandey", "Han Liu", "Kaize Ding"], "abstract": "Large language models (LLMs) are introducing a paradigm shift in molecular\ndiscovery by enabling text-guided interaction with chemical spaces through\nnatural language, symbolic notations, with emerging extensions to incorporate\nmulti-modal inputs. To advance the new field of LLM for molecular discovery,\nthis survey provides an up-to-date and forward-looking review of the emerging\nuse of LLMs for two central tasks: molecule generation and molecule\noptimization. Based on our proposed taxonomy for both problems, we analyze\nrepresentative techniques in each category, highlighting how LLM capabilities\nare leveraged across different learning settings. In addition, we include the\ncommonly used datasets and evaluation protocols. We conclude by discussing key\nchallenges and future directions, positioning this survey as a resource for\nresearchers working at the intersection of LLMs and molecular science. A\ncontinuously updated reading list is available at\nhttps://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-22 00:26:27", "updated": "2025-05-22 00:26:27", "pdf_url": "http://arxiv.org/pdf/2505.16094v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16100v1", "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "abstract": "Validating scientific hypotheses is a central challenge in biomedical\nresearch, and remains difficult for artificial intelligence (AI) agents due to\nthe complexity of real-world data analysis and evidence interpretation. In this\nwork, we present BioDSA-1K, a benchmark designed to evaluate AI agents on\nrealistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K\nconsists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,\ncurated from over 300 published biomedical studies to reflect the structure and\nreasoning found in authentic research workflows. Each task includes a\nstructured hypothesis derived from the original study's conclusions, expressed\nin the affirmative to reflect the language of scientific reporting, and one or\nmore pieces of supporting evidence grounded in empirical data tables. While\nthese hypotheses mirror published claims, they remain testable using standard\nstatistical or machine learning methods. The benchmark enables evaluation along\nfour axes: (1) hypothesis decision accuracy, (2) alignment between evidence and\nconclusion, (3) correctness of the reasoning process, and (4) executability of\nthe AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable\nhypotheses: cases where the available data are insufficient to support or\nrefute a claim, reflecting a common yet underexplored scenario in real-world\nscience. We propose BioDSA-1K as a foundation for building and evaluating\ngeneralizable, trustworthy AI agents for biomedical discovery.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 01:02:21", "updated": "2025-05-22 01:02:21", "pdf_url": "http://arxiv.org/pdf/2505.16100v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16102v1", "title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "authors": ["Yash Kumar Atri", "Thomas H Shin", "Thomas Hartvigsen"], "abstract": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses.", "categories": ["cs.CL"], "published": "2025-05-22 01:02:51", "updated": "2025-05-22 01:02:51", "pdf_url": "http://arxiv.org/pdf/2505.16102v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16104v1", "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "abstract": "With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning.", "categories": ["cs.CL", "cs.CV", "cs.LG"], "published": "2025-05-22 01:06:28", "updated": "2025-05-22 01:06:28", "pdf_url": "http://arxiv.org/pdf/2505.16104v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16107v1", "title": "MPL: Multiple Programming Languages with Large Language Models for Information Extraction", "authors": ["Bo Li", "Gexiang Fang", "Wei Ye", "Zhenghua Xu", "Jinglei Zhang", "Hao Cheng", "Shikun Zhang"], "abstract": "Recent research in information extraction (IE) focuses on utilizing\ncode-style inputs to enhance structured output generation. The intuition behind\nthis is that the programming languages (PLs) inherently exhibit greater\nstructural organization than natural languages (NLs). This structural advantage\nmakes PLs particularly suited for IE tasks. Nevertheless, existing research\nprimarily focuses on Python for code-style simulation, overlooking the\npotential of other widely-used PLs (e.g., C++ and Java) during the supervised\nfine-tuning (SFT) phase. In this research, we propose \\textbf{M}ultiple\n\\textbf{P}rogramming \\textbf{L}anguages with large language models for\ninformation extraction (abbreviated as \\textbf{MPL}), a novel framework that\nexplores the potential of incorporating different PLs in the SFT phase.\nAdditionally, we introduce \\texttt{function-prompt} with virtual running to\nsimulate code-style inputs more effectively and efficiently. Experimental\nresults on a wide range of datasets demonstrate the effectiveness of MPL.\nFurthermore, we conduct extensive experiments to provide a comprehensive\nanalysis. We have released our code for future research.", "categories": ["cs.CL"], "published": "2025-05-22 01:28:23", "updated": "2025-05-22 01:28:23", "pdf_url": "http://arxiv.org/pdf/2505.16107v1", "comment": "Findings of ACL2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16118v1", "title": "Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics", "authors": ["Haotian Lan", "Yao Gao", "Yujun Cheng", "Wei Yuan", "Kun Wang"], "abstract": "Social media's rise establishes user-generated content (UGC) as pivotal for\ntravel decisions, yet analytical methods lack scalability. This study\nintroduces a dual-method LLM framework: unsupervised expectation extraction\nfrom UGC paired with survey-informed supervised fine-tuning. Findings reveal\nleisure/social expectations drive engagement more than foundational\nnatural/emotional factors. By establishing LLMs as precision tools for\nexpectation quantification, we advance tourism analytics methodology and\npropose targeted strategies for experience personalization and social travel\npromotion. The framework's adaptability extends to consumer behavior research,\ndemonstrating computational social science's transformative potential in\nmarketing optimization.", "categories": ["cs.CL", "stat.AP"], "published": "2025-05-22 01:52:01", "updated": "2025-05-22 01:52:01", "pdf_url": "http://arxiv.org/pdf/2505.16118v1", "comment": "33 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16125v1", "title": "KoBALT: Korean Benchmark For Advanced Linguistic Tasks", "authors": ["Hyopil Shin", "Sangah Lee", "Dongjun Jang", "Wooseok Song", "Jaeyoon Kim", "Chaeyoung Oh", "Hyemi Jo", "Youngchae Ahn", "Sihyun Oh", "Hyohyeong Chang", "Sunkyoung Kim", "Jinsik Lee"], "abstract": "We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a\ncomprehensive linguistically-motivated benchmark comprising 700 multiple-choice\nquestions spanning 24 phenomena across five linguistic domains: syntax,\nsemantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed\nto advance the evaluation of large language models (LLMs) in Korean, a\nmorphologically rich language, by addressing the limitations of conventional\nbenchmarks that often lack linguistic depth and typological grounding. It\nintroduces a suite of expert-curated, linguistically motivated questions with\nminimal n-gram overlap with standard Korean corpora, substantially mitigating\nthe risk of data contamination and allowing a more robust assessment of true\nlanguage understanding. Our evaluation of 20 contemporary LLMs reveals\nsignificant performance disparities, with the highest-performing model\nachieving 61\\% general accuracy but showing substantial variation across\nlinguistic domains - from stronger performance in semantics (66\\%) to\nconsiderable weaknesses in phonology (31\\%) and morphology (36\\%). Through\nhuman preference evaluation with 95 annotators, we demonstrate a strong\ncorrelation between KoBALT scores and human judgments, validating our\nbenchmark's effectiveness as a discriminative measure of Korean language\nunderstanding. KoBALT addresses critical gaps in linguistic evaluation for\ntypologically diverse languages and provides a robust framework for assessing\ngenuine linguistic competence in Korean language models.", "categories": ["cs.CL"], "published": "2025-05-22 02:03:07", "updated": "2025-05-22 02:03:07", "pdf_url": "http://arxiv.org/pdf/2505.16125v1", "comment": "Under Reveiw", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16128v1", "title": "Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning", "authors": ["Yue Zhou", "Barbara Di Eugenio"], "abstract": "Despite LLMs' explicit alignment against demographic stereotypes, they have\nbeen shown to exhibit biases under various social contexts. In this work, we\nfind that LLMs exhibit concerning biases in how they associate solution\nveracity with demographics. Through experiments across five human value-aligned\nLLMs on mathematics, coding, commonsense, and writing problems, we reveal two\nforms of such veracity biases: Attribution Bias, where models\ndisproportionately attribute correct solutions to certain demographic groups,\nand Evaluation Bias, where models' assessment of identical solutions varies\nbased on perceived demographic authorship. Our results show pervasive biases:\nLLMs consistently attribute fewer correct solutions and more incorrect ones to\nAfrican-American groups in math and coding, while Asian authorships are least\npreferred in writing evaluation. In additional studies, we show LLMs\nautomatically assign racially stereotypical colors to demographic groups in\nvisualization code, suggesting these biases are deeply embedded in models'\nreasoning processes. Our findings indicate that demographic bias extends beyond\nsurface-level stereotypes and social context provocations, raising concerns\nabout LLMs' deployment in educational and evaluation settings.", "categories": ["cs.CL"], "published": "2025-05-22 02:13:48", "updated": "2025-05-22 02:13:48", "pdf_url": "http://arxiv.org/pdf/2505.16128v1", "comment": "Accepted to ACL 2025 (Main)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16129v1", "title": "LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods", "authors": ["Hyang Cui"], "abstract": "Recent studies have applied large language models (LLMs) to machine\ntranslation quality estimation (MTQE) by prompting models to assign numeric\nscores. Nonetheless, these direct scoring methods tend to show low\nsegment-level correlation with human judgments. In this paper, we propose a\ngeneration-based evaluation paradigm that leverages decoder-only LLMs to\nproduce high-quality references, followed by semantic similarity scoring using\nsentence embeddings. We conduct the most extensive evaluation to date in MTQE,\ncovering 8 LLMs and 8 language pairs. Empirical results show that our method\noutperforms both intra-LLM direct scoring baselines and external non-LLM\nreference-free metrics from MTME. These findings demonstrate the strength of\ngeneration-based evaluation and support a shift toward hybrid approaches that\ncombine fluent generation with accurate semantic assessment.", "categories": ["cs.CL", "I.2.7"], "published": "2025-05-22 02:14:38", "updated": "2025-05-22 02:14:38", "pdf_url": "http://arxiv.org/pdf/2505.16129v1", "comment": "5 pages, 2 figures, 2 tables. Conforms to the ACL Rolling Review\n  (ARR) short paper track. Code and data available at:\n  https://github.com/CuiNiki/LLMs-Are-Not-Scorers", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16134v1", "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models", "authors": ["Menschikov Mikhail", "Alexander Kharitonov", "Maiia Kotyga", "Vadim Porvatov", "Anna Zhukovskaya", "David Kagramanyan", "Egor Shvetsov", "Evgeny Burnaev"], "abstract": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-22 02:23:00", "updated": "2025-05-22 02:23:00", "pdf_url": "http://arxiv.org/pdf/2505.16134v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16142v1", "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Jia Gu", "Zihao Wei", "Jingcheng Deng", "Feiyang Pan", "Huawei Shen", "Xueqi Cheng"], "abstract": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation.", "categories": ["cs.CL"], "published": "2025-05-22 02:36:36", "updated": "2025-05-22 02:36:36", "pdf_url": "http://arxiv.org/pdf/2505.16142v1", "comment": "15 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16146v1", "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "abstract": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 02:45:45", "updated": "2025-05-22 02:45:45", "pdf_url": "http://arxiv.org/pdf/2505.16146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16148v1", "title": "NAN: A Training-Free Solution to Coefficient Estimation in Model Merging", "authors": ["Chongjie Si", "Kangtao Lv", "Jingjing Jiang", "Yadao Wang", "Yongwei Wang", "Xiaokang Yang", "Wenbo Su", "Bo Zheng", "Wei Shen"], "abstract": "Model merging offers a training-free alternative to multi-task learning by\ncombining independently fine-tuned models into a unified one without access to\nraw data. However, existing approaches often rely on heuristics to determine\nthe merging coefficients, limiting their scalability and generality. In this\nwork, we revisit model merging through the lens of least-squares optimization\nand show that the optimal merging weights should scale with the amount of\ntask-specific information encoded in each model. Based on this insight, we\npropose NAN, a simple yet effective method that estimates model merging\ncoefficients via the inverse of parameter norm. NAN is training-free,\nplug-and-play, and applicable to a wide range of merging strategies. Extensive\nexperiments on show that NAN consistently improves performance of baseline\nmethods.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-22 02:46:08", "updated": "2025-05-22 02:46:08", "pdf_url": "http://arxiv.org/pdf/2505.16148v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16149v1", "title": "When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification", "authors": ["Zirui Pang", "Haosheng Tan", "Yuhan Pu", "Zhijie Deng", "Zhouan Shen", "Keyu Hu", "Jiaheng Wei"], "abstract": "Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet\nserve as critical tools for model evaluation. However, despite the cleaning\nefforts, these datasets still suffer from pervasive noisy labels and often\ncontain missing labels due to the co-existing image pattern where multiple\nclasses appear in an image sample. This results in misleading model comparisons\nand unfair evaluations. Existing label cleaning methods focus primarily on\nnoisy labels, but the issue of missing labels remains largely overlooked.\nMotivated by these challenges, we present a comprehensive framework named\nREVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,\nLLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods\n(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and\nmissing label detection in widely-used image classification test sets. REVEAL\ndetects potential noisy labels and omissions, aggregates predictions from\nvarious methods, and refines label accuracy through confidence-informed\npredictions and consensus-based filtering. Additionally, we provide a thorough\nanalysis of state-of-the-art vision-language models and pre-trained image\nclassifiers, highlighting their strengths and limitations within the context of\ndataset renovation by revealing 10 observations. Our method effectively reveals\nmissing labels from public datasets and provides soft-labeled results with\nlikelihoods. Through human verifications, REVEAL significantly improves the\nquality of 6 benchmark test sets, highly aligning to human judgments and\nenabling more accurate and meaningful comparisons in image classification.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-22 02:47:36", "updated": "2025-05-22 02:47:36", "pdf_url": "http://arxiv.org/pdf/2505.16149v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16160v1", "title": "EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios", "authors": ["Bin Xu", "Yu Bai", "Huashan Sun", "Yiguan Lin", "Siming Liu", "Xinyue Liang", "Yaolin Li", "Yang Gao", "Heyan Huang"], "abstract": "As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench.", "categories": ["cs.CL"], "published": "2025-05-22 03:01:28", "updated": "2025-05-22 03:01:28", "pdf_url": "http://arxiv.org/pdf/2505.16160v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16162v1", "title": "KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization", "authors": ["Mingbo Song", "Heming Xia", "Jun Zhang", "Chak Tou Leong", "Qiancheng Xu", "Wenjie Li", "Sujian Li"], "abstract": "Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by efficiently drafting multiple tokens using a compact model\nand then verifying them in parallel using the target LLM. Notably,\nSelf-Speculative Decoding proposes skipping certain layers to construct the\ndraft model, which eliminates the need for additional parameters or training.\nDespite its strengths, we observe in this work that drafting with layer\nskipping exhibits significant sensitivity to domain shifts, leading to a\nsubstantial drop in acceleration performance. To enhance the domain\ngeneralizability of this paradigm, we introduce KNN-SSD, an algorithm that\nleverages K-Nearest Neighbor (KNN) search to match different skipped layers\nwith various domain inputs. We evaluated our algorithm in various models and\nmultiple tasks, observing that its application leads to 1.3x-1.6x speedup in\nLLM inference.", "categories": ["cs.CL"], "published": "2025-05-22 03:04:47", "updated": "2025-05-22 03:04:47", "pdf_url": "http://arxiv.org/pdf/2505.16162v1", "comment": "8 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16164v1", "title": "Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task", "authors": ["Mengyang Qiu", "Zoe Brisebois", "Siena Sun"], "abstract": "Large language models (LLMs) are increasingly explored as substitutes for\nhuman participants in cognitive tasks, but their ability to simulate human\nbehavioral variability remains unclear. This study examines whether LLMs can\napproximate individual differences in the phonemic fluency task, where\nparticipants generate words beginning with a target letter. We evaluated 34\nmodel configurations, varying prompt specificity, sampling temperature, and\nmodel type, and compared outputs to responses from 106 human participants.\nWhile some configurations, especially Claude 3.7 Sonnet, matched human averages\nand lexical preferences, none reproduced the scope of human variability. LLM\noutputs were consistently less diverse and structurally rigid, and LLM\nensembles failed to increase diversity. Network analyses further revealed\nfundamental differences in retrieval structure between humans and models. These\nresults highlight key limitations in using LLMs to simulate human cognition and\nbehavior.", "categories": ["cs.CL"], "published": "2025-05-22 03:08:27", "updated": "2025-05-22 03:08:27", "pdf_url": "http://arxiv.org/pdf/2505.16164v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16170v1", "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction", "authors": ["Yuqing Yang", "Robin Jia"], "abstract": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction.", "categories": ["cs.CL"], "published": "2025-05-22 03:16:00", "updated": "2025-05-22 03:16:00", "pdf_url": "http://arxiv.org/pdf/2505.16170v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16172v1", "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "abstract": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 03:19:49", "updated": "2025-05-22 03:19:49", "pdf_url": "http://arxiv.org/pdf/2505.16172v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16176v1", "title": "Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning", "authors": ["Jun Rao", "Xuebo Liu", "Hexuan Deng", "Zepeng Lin", "Zixiong Yu", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "abstract": "In the realm of data selection for reasoning tasks, existing approaches\npredominantly rely on externally predefined static metrics such as difficulty\nand diversity, which are often designed for supervised fine-tuning (SFT) and\nlack adaptability to continuous training processes. A critical limitation of\nthese methods is their inability to dynamically align with the evolving\ncapabilities of models during online training, a gap that becomes increasingly\npronounced with the rise of dynamic training paradigms and online reinforcement\nlearning (RL) frameworks (e.g., R1 models). To address this, we introduce\nSAI-DPO, an algorithm that dynamically selects training data by continuously\nassessing a model's stage-specific reasoning abilities across different\ntraining phases. By integrating real-time model performance feedback, SAI-DPO\nadaptively adapts data selection to the evolving strengths and weaknesses of\nthe model, thus enhancing both data utilization efficiency and final task\nperformance. Extensive experiments on three state-of-the-art models and eight\nmathematical reasoning benchmarks, including challenging competition-level\ndatasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average\nperformance boost of up to 21.3 percentage points, with particularly notable\nimprovements of 10 and 15 points on AIME24 and AMC23, respectively. These\nresults highlight the superiority of dynamic, model-adaptive data selection\nover static, externally defined strategies in advancing reasoning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 03:27:05", "updated": "2025-05-22 03:27:05", "pdf_url": "http://arxiv.org/pdf/2505.16176v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16178v1", "title": "Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge", "authors": ["Ying Zhang", "Benjamin Heinzerling", "Dongyuan Li", "Ryoma Ishigaki", "Yuta Hitomi", "Kentaro Inui"], "abstract": "Fact recall, the ability of language models (LMs) to retrieve specific\nfactual knowledge, remains a challenging task despite their impressive general\ncapabilities. Common training strategies often struggle to promote robust\nrecall behavior with two-stage training, which first trains a model with\nfact-storing examples (e.g., factual statements) and then with fact-recalling\nexamples (question-answer pairs), tending to encourage rote memorization rather\nthan generalizable fact retrieval. In contrast, mixed training, which jointly\nuses both types of examples, has been empirically shown to improve the ability\nto recall facts, but the underlying mechanisms are still poorly understood. In\nthis work, we investigate how these training strategies affect how model\nparameters are shaped during training and how these differences relate to their\nability to recall facts. We introduce cross-task gradient trace to identify\nshared parameters, those strongly influenced by both fact-storing and\nfact-recalling examples. Our analysis on synthetic fact recall datasets with\nthe Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a\nlarger and more centralized set of shared parameters. These findings suggest\nthat the emergence of parameters may play a key role in enabling LMs to\ngeneralize factual knowledge across task formulations.", "categories": ["cs.CL"], "published": "2025-05-22 03:34:29", "updated": "2025-05-22 03:34:29", "pdf_url": "http://arxiv.org/pdf/2505.16178v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16180v1", "title": "Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics", "authors": ["Ashim Dahal", "Ankit Ghimire", "Saydul Akbar Murad", "Nick Rahimi"], "abstract": "Evaluating image captions requires cohesive assessment of both visual\nsemantics and language pragmatics, which is often not entirely captured by most\nmetrics. We introduce Redemption Score, a novel hybrid framework that ranks\nimage captions by triangulating three complementary signals: (1) Mutual\nInformation Divergence (MID) for global image-text distributional alignment,\n(2) DINO-based perceptual similarity of cycle-generated images for visual\ngrounding, and (3) BERTScore for contextual text similarity against human\nreferences. A calibrated fusion of these signals allows Redemption Score to\noffer a more holistic assessment. On the Flickr8k benchmark, Redemption Score\nachieves a Kendall-$\\tau$ of 56.43, outperforming twelve prior methods and\ndemonstrating superior correlation with human judgments without requiring\ntask-specific training. Our framework provides a more robust and nuanced\nevaluation by effectively redeeming image semantics and linguistic\ninterpretability indicated by strong transfer of knowledge in the Conceptual\nCaptions and MS COCO datasets.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-22 03:35:12", "updated": "2025-05-22 03:35:12", "pdf_url": "http://arxiv.org/pdf/2505.16180v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16186v1", "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning", "authors": ["Kaiwen Zhou", "Xuandong Zhao", "Gaowen Liu", "Jayanth Srinivasa", "Aosong Feng", "Dawn Song", "Xin Eric Wang"], "abstract": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.", "categories": ["cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-22 03:46:03", "updated": "2025-05-22 03:46:03", "pdf_url": "http://arxiv.org/pdf/2505.16186v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16188v1", "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models", "authors": ["Zirui He", "Mingyu Jin", "Bo Shen", "Ali Payani", "Yongfeng Zhang", "Mengnan Du"], "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but controlling their behavior\nreliably remains challenging, especially in open-ended generation settings.\nThis paper introduces a novel supervised steering approach that operates in\nsparse, interpretable representation spaces. We employ sparse autoencoders\n(SAEs)to obtain sparse latent representations that aim to disentangle semantic\nattributes from model activations. Then we train linear classifiers to identify\na small subspace of task-relevant dimensions in latent representations.\nFinally, we learn supervised steering vectors constrained to this subspace,\noptimized to align with target behaviors. Experiments across sentiment,\ntruthfulness, and politics polarity steering tasks with multiple LLMs\ndemonstrate that our supervised steering vectors achieve higher success rates\nwith minimal degradation in generation quality compared to existing methods.\nFurther analysis reveals that a notably small subspace is sufficient for\neffective steering, enabling more targeted and interpretable interventions.", "categories": ["cs.CL"], "published": "2025-05-22 03:46:57", "updated": "2025-05-22 03:46:57", "pdf_url": "http://arxiv.org/pdf/2505.16188v1", "comment": "30 pages, 24 figures, 12 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16189v1", "title": "The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions", "authors": ["Sophie Wu", "Jan Philip Wahle", "Saif M. Mohammad"], "abstract": "This paper is the first investigation of the connection between emotion,\nembodiment, and everyday language in a large sample of natural language data.\nWe created corpora of body part mentions (BPMs) in online English text (blog\nposts and tweets). This includes a subset featuring human annotations for the\nemotions of the person whose body part is mentioned in the text. We show that\nBPMs are common in personal narratives and tweets (~5% to 10% of posts include\nBPMs) and that their usage patterns vary markedly by time and %geographic\nlocation. Using word-emotion association lexicons and our annotated data, we\nshow that text containing BPMs tends to be more emotionally charged, even when\nthe BPM is not explicitly used to describe a physical reaction to the emotion\nin the text. Finally, we discover a strong and statistically significant\ncorrelation between body-related language and a variety of poorer health\noutcomes. In sum, we argue that investigating the role of body-part related\nwords in language can open up valuable avenues of future research at the\nintersection of NLP, the affective sciences, and the study of human wellbeing.", "categories": ["cs.CL"], "published": "2025-05-22 03:47:12", "updated": "2025-05-22 03:47:12", "pdf_url": "http://arxiv.org/pdf/2505.16189v1", "comment": "8 pages, 26 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16193v1", "title": "An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability", "authors": ["Daiqing Wu", "Dongbao Yang", "Sicheng Zhao", "Can Ma", "Yu Zhou"], "abstract": "The advancements in Multimodal Large Language Models (MLLMs) have enabled\nvarious multimodal tasks to be addressed under a zero-shot paradigm. This\nparadigm sidesteps the cost of model fine-tuning, emerging as a dominant trend\nin practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a\npivotal challenge in the quest for general artificial intelligence, fails to\naccommodate this convenience. The zero-shot paradigm exhibits undesirable\nperformance on MSA, casting doubt on whether MLLMs can perceive sentiments as\ncompetent as supervised models. By extending the zero-shot paradigm to\nIn-Context Learning (ICL) and conducting an in-depth study on configuring\ndemonstrations, we validate that MLLMs indeed possess such capability.\nSpecifically, three key factors that cover demonstrations' retrieval,\npresentation, and distribution are comprehensively investigated and optimized.\nA sentimental predictive bias inherent in MLLMs is also discovered and later\neffectively counteracted. By complementing each other, the devised strategies\nfor three factors result in average accuracy improvements of 15.9% on six MSA\ndatasets against the zero-shot paradigm and 11.2% against the random ICL\nbaseline.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-22 03:51:41", "updated": "2025-05-22 03:51:41", "pdf_url": "http://arxiv.org/pdf/2505.16193v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16210v1", "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 04:23:19", "updated": "2025-05-22 04:23:19", "pdf_url": "http://arxiv.org/pdf/2505.16210v1", "comment": "11 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16211v1", "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhuo Chen", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "abstract": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-22 04:27:46", "updated": "2025-05-22 04:27:46", "pdf_url": "http://arxiv.org/pdf/2505.16211v1", "comment": "Technical Report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16212v1", "title": "Large Language Models based ASR Error Correction for Child Conversations", "authors": ["Anfeng Xu", "Tiantian Feng", "So Hyun Kim", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "abstract": "Automatic Speech Recognition (ASR) has recently shown remarkable progress,\nbut accurately transcribing children's speech remains a significant challenge.\nRecent developments in Large Language Models (LLMs) have shown promise in\nimproving ASR transcriptions. However, their applications in child speech\nincluding conversational scenarios are underexplored. In this study, we explore\nthe use of LLMs in correcting ASR errors for conversational child speech. We\ndemonstrate the promises and challenges of LLMs through experiments on two\nchildren's conversational speech datasets with both zero-shot and fine-tuned\nASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR\noutputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs\nto improve ASR performance when incorporating contextual information or when\nusing fine-tuned autoregressive ASR (e.g., Whisper) outputs.", "categories": ["cs.CL", "eess.AS"], "published": "2025-05-22 04:28:02", "updated": "2025-05-22 04:28:02", "pdf_url": "http://arxiv.org/pdf/2505.16212v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16216v1", "title": "Memorization or Reasoning? Exploring the Idiom Understanding of LLMs", "authors": ["Jisu Kim", "Youngwoo Shin", "Uiji Hwang", "Jihun Choi", "Richeng Xuan", "Taeuk Kim"], "abstract": "Idioms have long posed a challenge due to their unique linguistic properties,\nwhich set them apart from other common expressions. While recent studies have\nleveraged large language models (LLMs) to handle idioms across various tasks,\ne.g., idiom-containing sentence generation and idiomatic machine translation,\nlittle is known about the underlying mechanisms of idiom processing in LLMs,\nparticularly in multilingual settings. To this end, we introduce MIDAS, a new\nlarge-scale dataset of idioms in six languages, each paired with its\ncorresponding meaning. Leveraging this resource, we conduct a comprehensive\nevaluation of LLMs' idiom processing ability, identifying key factors that\ninfluence their performance. Our findings suggest that LLMs rely not only on\nmemorization, but also adopt a hybrid approach that integrates contextual cues\nand reasoning, especially when processing compositional idioms. This implies\nthat idiom understanding in LLMs emerges from an interplay between internal\nknowledge retrieval and reasoning-based inference.", "categories": ["cs.CL"], "published": "2025-05-22 04:31:25", "updated": "2025-05-22 04:31:25", "pdf_url": "http://arxiv.org/pdf/2505.16216v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16220v1", "title": "Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning", "authors": ["Liang-Yeh Shen", "Shi-Xin Fang", "Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "abstract": "This paper introduces Meta-PerSER, a novel meta-learning framework that\npersonalizes Speech Emotion Recognition (SER) by adapting to each listener's\nunique way of interpreting emotion. Conventional SER systems rely on aggregated\nannotations, which often overlook individual subtleties and lead to\ninconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic\nMeta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,\nDerivative Annealing, and per-layer per-step learning rates, enabling rapid\nadaptation with only a few labeled examples. By integrating robust\nrepresentations from pre-trained self-supervised models, our framework first\ncaptures general emotional cues and then fine-tunes itself to personal\nannotation styles. Experiments on the IEMOCAP corpus demonstrate that\nMeta-PerSER significantly outperforms baseline methods in both seen and unseen\ndata scenarios, highlighting its promise for personalized emotion recognition.", "categories": ["eess.AS", "cs.CL"], "published": "2025-05-22 04:44:20", "updated": "2025-05-22 04:44:20", "pdf_url": "http://arxiv.org/pdf/2505.16220v1", "comment": "Accepted by INTERSPEECH 2025. 7 pages, including 2 pages of appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16222v1", "title": "Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation", "authors": ["Jiwon Moon", "Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "abstract": "With the growing use of large language models(LLMs) as evaluators, their\napplication has expanded to code evaluation tasks, where they assess the\ncorrectness of generated code without relying on reference implementations.\nWhile this offers scalability and flexibility, it also raises a critical,\nunresolved question: Can LLM judges fairly and robustly evaluate semantically\nequivalent code with superficial variations? Functionally correct code often\nexhibits variations-such as differences in variable names, comments, or\nformatting-that should not influence its correctness. Yet, whether LLM judges\ncan reliably handle these variations remains unclear. We present the first\ncomprehensive study of this issue, defining six types of potential bias in code\nevaluation and revealing their systematic impact on LLM judges. Across five\nprogramming languages and multiple LLMs, we empirically demonstrate that all\ntested LLM judges are susceptible to both positive and negative biases,\nresulting in inflated or unfairly low scores. Moreover, we observe that LLM\njudges remain vulnerable to these biases even when prompted to generate test\ncases before scoring, highlighting the need for more robust code evaluation\nmethods.", "categories": ["cs.CL", "cs.SE"], "published": "2025-05-22 04:49:33", "updated": "2025-05-22 04:49:33", "pdf_url": "http://arxiv.org/pdf/2505.16222v1", "comment": "26 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16227v1", "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "abstract": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 04:55:41", "updated": "2025-05-22 04:55:41", "pdf_url": "http://arxiv.org/pdf/2505.16227v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16232v1", "title": "MuseRAG: Idea Originality Scoring At Scale", "authors": ["Ali Sarosh Bangash", "Krish Veera", "Ishfat Abrar Islam", "Raiyan Abdul Baten"], "abstract": "An objective, face-valid way to assess the originality of creative ideas is\nto measure how rare each idea is within a population -- an approach long used\nin creativity research but difficult to automate at scale. Tabulating response\nfrequencies via manual bucketing of idea rephrasings is labor-intensive,\nerror-prone, and brittle under large corpora. We introduce a fully automated,\npsychometrically validated pipeline for frequency-based originality scoring.\nOur method, MuseRAG, combines large language models (LLMs) with an externally\norchestrated retrieval-augmented generation (RAG) framework. Given a new idea,\nthe system retrieves semantically similar prior idea buckets and zero-shot\nprompts the LLM to judge whether the new idea belongs to an existing bucket or\nforms a new one. The resulting buckets enable computation of frequency-based\noriginality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG\nmatches human annotators in idea clustering structure and resolution (AMI =\n0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong\nconvergent and external validity. Our work enables intent-sensitive,\nhuman-aligned originality scoring at scale to aid creativity research.", "categories": ["cs.CL"], "published": "2025-05-22 05:05:25", "updated": "2025-05-22 05:05:25", "pdf_url": "http://arxiv.org/pdf/2505.16232v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16234v1", "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "abstract": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 05:08:27", "updated": "2025-05-22 05:08:27", "pdf_url": "http://arxiv.org/pdf/2505.16234v1", "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16237v1", "title": "Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation", "authors": ["Derong Xu", "Pengyue Jia", "Xiaopeng Li", "Yingyi Zhang", "Maolin Wang", "Qidong Liu", "Xiangyu Zhao", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Enhong Chen", "Tong Xu"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nstill struggle with issues like hallucinations and outdated information.\nRetrieval-augmented generation (RAG) addresses these issues by grounding LLM\noutputs in external knowledge with an Information Retrieval (IR) system.\nBuilding on this foundation, graph-based RAG systems go a step further by\nretrieving subgraphs, which preserve the relationships between knowledge\nentities and provide more comprehensive context. However, graph RAG faces two\nchallenges: (1) Retrieving relevant information introduces irrelevant nodes\n(especially in dense graph databases, where retrieval usually extends to\nadjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)\nThe representation gap between graph and language during generation with LLMs\nlimits the ability to fully leverage graph structures for enhanced\nunderstanding. To address these limitations, we propose Align-GRAG, a novel\nreasoning-guided dual alignment framework in post-retrieval phrase. It first\nformulates a subgraph by retrieving nodes and edges. Then an Aligner is\nproposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It\nachieves dual alignment of graph node and representation by leveraging KL\ndivergence loss and contrastive loss, facilitating efficient pruning of\nirrelevant knowledge and establishing a unified semantic space. The Generator\nintegrates the aligned graph data with LLM to produce coherent and accurate\nanswers. Experiments on GraphQA benchmark across three tasks (including common\nsense reasoning, scene graph understanding, and knowledge graph reasoning)\nvalidate the effectiveness of our method. The code will be available upon\naccepted.", "categories": ["cs.CL"], "published": "2025-05-22 05:15:27", "updated": "2025-05-22 05:15:27", "pdf_url": "http://arxiv.org/pdf/2505.16237v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16241v1", "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers", "authors": ["Viet-Anh Nguyen", "Shiqian Zhao", "Gia Dao", "Runyi Hu", "Yi Xie", "Luu Anh Tuan"], "abstract": "Recently, Large Reasoning Models (LRMs) have demonstrated superior logical\ncapabilities compared to traditional Large Language Models (LLMs), gaining\nsignificant attention. Despite their impressive performance, the potential for\nstronger reasoning abilities to introduce more severe security vulnerabilities\nremains largely underexplored. Existing jailbreak methods often struggle to\nbalance effectiveness with robustness against adaptive safety mechanisms. In\nthis work, we propose SEAL, a novel jailbreak attack that targets LRMs through\nan adaptive encryption pipeline designed to override their reasoning processes\nand evade potential adaptive alignment. Specifically, SEAL introduces a stacked\nencryption approach that combines multiple ciphers to overwhelm the models\nreasoning capabilities, effectively bypassing built-in safety mechanisms. To\nfurther prevent LRMs from developing countermeasures, we incorporate two\ndynamic strategies - random and adaptive - that adjust the cipher length,\norder, and combination. Extensive experiments on real-world reasoning models,\nincluding DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the\neffectiveness of our approach. Notably, SEAL achieves an attack success rate of\n80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant\nmargin of 27.2%. Warning: This paper contains examples of inappropriate,\noffensive, and harmful content.", "categories": ["cs.CL"], "published": "2025-05-22 05:19:42", "updated": "2025-05-22 05:19:42", "pdf_url": "http://arxiv.org/pdf/2505.16241v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16245v1", "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models", "authors": ["Vijeta Deshpande", "Debasmita Ghose", "John D. Patterson", "Roger Beaty", "Anna Rumshisky"], "abstract": "Diverse language model responses are crucial for creative generation,\nopen-ended tasks, and self-improvement training. We show that common diversity\nmetrics, and even reward models used for preference optimization,\nsystematically bias models toward shorter outputs, limiting expressiveness. To\naddress this, we introduce Diverse, not Short (Diverse-NS), a length-controlled\nself-learning framework that improves response diversity while maintaining\nlength parity. By generating and filtering preference data that balances\ndiversity, quality, and length, Diverse-NS enables effective training using\nonly 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,\nDiverse-NS substantially enhances lexical and semantic diversity. We show\nconsistent improvement in diversity with minor reduction or gains in response\nquality on four creative generation tasks: Divergent Associations, Persona\nGeneration, Alternate Uses, and Creative Writing. Surprisingly, experiments\nwith the Olmo-2 model family (7B, and 13B) show that smaller models like\nOlmo-2-7B can serve as effective \"diversity teachers\" for larger models. By\nexplicitly addressing length bias, our method efficiently pushes models toward\nmore diverse and expressive outputs.", "categories": ["cs.CL"], "published": "2025-05-22 05:29:47", "updated": "2025-05-22 05:29:47", "pdf_url": "http://arxiv.org/pdf/2505.16245v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16252v1", "title": "Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models", "authors": ["Hwiyeong Lee", "Uiji Hwang", "Hyelim Lim", "Taeuk Kim"], "abstract": "Large language models often retain unintended content, prompting growing\ninterest in knowledge unlearning. Recent approaches emphasize localized\nunlearning, which restricts parameter updates to specific regions in an effort\nto remove target knowledge while preserving unrelated general knowledge.\nHowever, their effectiveness remains uncertain due to the lack of robust and\nthorough evaluation of the trade-off between the competing goals of unlearning.\nIn this paper, we begin by revisiting existing localized unlearning approaches.\nWe then conduct controlled experiments to rigorously evaluate whether local\nparameter updates causally contribute to unlearning. Our findings reveal that\nthe set of parameters that must be modified for effective unlearning is not\nstrictly determined, challenging the core assumption of localized unlearning\nthat parameter locality is inherently indicative of effective knowledge\nremoval.", "categories": ["cs.CL", "I.2.7"], "published": "2025-05-22 05:41:53", "updated": "2025-05-22 05:41:53", "pdf_url": "http://arxiv.org/pdf/2505.16252v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16258v1", "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "abstract": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "published": "2025-05-22 05:49:01", "updated": "2025-05-22 05:49:01", "pdf_url": "http://arxiv.org/pdf/2505.16258v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16263v1", "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI", "authors": ["Sampanna Yashwant Kahu", "Naman Ahuja"], "abstract": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text.", "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5"], "published": "2025-05-22 05:55:26", "updated": "2025-05-22 05:55:26", "pdf_url": "http://arxiv.org/pdf/2505.16263v1", "comment": "10 pages, 22 figures, The source code and data used in this work is\n  available at: https://github.com/SampannaKahu/all_you_need_is_leet", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16270v1", "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "abstract": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 06:00:45", "updated": "2025-05-22 06:00:45", "pdf_url": "http://arxiv.org/pdf/2505.16270v1", "comment": "33 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16276v1", "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance", "authors": ["Desiree Heim", "Lars-Peter Meyer", "Markus Schr\u00f6der", "Johannes Frey", "Andreas Dengel"], "abstract": "When using Large Language Models (LLMs) to support Knowledge Graph\nEngineering (KGE), one of the first indications when searching for an\nappropriate model is its size. According to the scaling laws, larger models\ntypically show higher capabilities. However, in practice, resource costs are\nalso an important factor and thus it makes sense to consider the ratio between\nmodel performance and costs. The LLM-KG-Bench framework enables the comparison\nof LLMs in the context of KGE tasks and assesses their capabilities of\nunderstanding and producing KGs and KG queries. Based on a dataset created in\nan LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the\nmodel size scaling laws specific to KGE tasks. In our analyses, we assess how\nbenchmark scores evolve between different model size categories. Additionally,\nwe inspect how the general score development of single models and families of\nmodels correlates to their size. Our analyses revealed that, with a few\nexceptions, the model size scaling laws generally also apply to the selected\nKGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,\nthe task performance did not change much between a model and the next larger\nmodel. In these cases, smaller models could be considered to achieve high\ncost-effectiveness. Regarding models of the same family, sometimes larger\nmodels performed worse than smaller models of the same family. These effects\noccurred only locally. Hence it is advisable to additionally test the next\nsmallest and largest model of the same family.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 06:21:40", "updated": "2025-05-22 06:21:40", "pdf_url": "http://arxiv.org/pdf/2505.16276v1", "comment": "Peer reviewed and to appear in the ESWC 2025 Workshops and Tutorials\n  Joint Proceedings (Workshop on Evaluation of Language Models in Knowledge\n  Engineering [ELMKE])", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16277v1", "title": "Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility", "authors": ["Sheng-Fu Wang", "Laurent Prevot", "Jou-an Chi", "Ri-Sheng Huang", "Shu-Kai Hsieh"], "abstract": "The achievements of Large Language Models in Natural Language Processing,\nespecially for high-resource languages, call for a better understanding of\ntheir characteristics from a cognitive perspective. Researchers have attempted\nto evaluate artificial models by testing their ability to predict behavioral\n(e.g., eye-tracking fixations) and physiological (e.g., brain responses)\nvariables during language processing (e.g., reading/listening). In this paper,\nwe propose using spontaneous speech corpora to derive production variables\n(speech reductions, prosodic prominences) and applying them in a similar\nfashion. More precisely, we extract. We then test models trained with a\nstandard procedure on different pretraining datasets (written, spoken, and\nmixed genres) for their ability to predict these two variables. Our results\nshow that, after some fine-tuning, the models can predict these production\nvariables well above baselines. We also observe that spoken genre training data\nprovides more accurate predictions than written genres. These results\ncontribute to the broader effort of using high-quality speech corpora as\nbenchmarks for LLMs.", "categories": ["cs.CL"], "published": "2025-05-22 06:23:02", "updated": "2025-05-22 06:23:02", "pdf_url": "http://arxiv.org/pdf/2505.16277v1", "comment": "The 14th Workshop on Cognitive Modeling and Computational Linguistics\n  (CMCL). May 3, 2025. Collocated with NAACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16281v1", "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "abstract": "The advancement of Large Language Models (LLMs) enables flexible and\ninterpretable automatic evaluations. In the field of machine translation\nevaluation, utilizing LLMs with translation error annotations based on\nMultidimensional Quality Metrics (MQM) yields more human-aligned judgments.\nHowever, current LLM-based evaluation methods still face challenges in\naccurately identifying error spans and assessing their severity. In this paper,\nwe propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation\nEvaluation. We argue that existing approaches inadequately exploit the\nfine-grained structural and semantic information within the MQM hierarchy. To\naddress this, we develop a hierarchical multi-agent system grounded in the MQM\nerror typology, enabling granular evaluation of subtype errors. Two key\nstrategies are incorporated to further mitigate systemic hallucinations within\nthe framework: the utilization of the model's self-reflection capability and\nthe facilitation of agent discussion involving asymmetric information.\nEmpirically, HiMATE outperforms competitive baselines across different datasets\nin conducting human-aligned evaluations. Further analyses underscore its\nsignificant advantage in error span detection and severity assessment,\nachieving an average F1-score improvement of 89% over the best-performing\nbaseline. We make our code and data publicly available at\nhttps://anonymous.4open.science/r/HiMATE-Anony.", "categories": ["cs.CL"], "published": "2025-05-22 06:24:08", "updated": "2025-05-22 06:24:08", "pdf_url": "http://arxiv.org/pdf/2505.16281v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16293v1", "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA", "authors": ["Rishabh Maheshwary", "Masoud Hashemi", "Khyati Mahajan", "Shiva Krishna Reddy Malay", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Spandana Gella", "Vikas Yadav"], "abstract": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.", "categories": ["cs.CL"], "published": "2025-05-22 06:45:05", "updated": "2025-05-22 06:45:05", "pdf_url": "http://arxiv.org/pdf/2505.16293v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16297v1", "title": "ToDi: Token-wise Distillation via Fine-Grained Divergence Control", "authors": ["Seongryong Jung", "Suwan Yoon", "DongGeon Kim", "Hwanhee Lee"], "abstract": "Large language models (LLMs) offer impressive performance but are impractical\nfor resource-constrained deployment due to high latency and energy consumption.\nKnowledge distillation (KD) addresses this by transferring knowledge from a\nlarge teacher to a smaller student model. However, conventional KD, notably\napproaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence\nloss across the entire vocabulary, neglecting token-level prediction\ndiscrepancies. By investigating these representative divergences via gradient\nanalysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses\noverestimated ones, showing their complementary roles. Based on this\nobservation, we propose Token-wise Distillation (ToDi), a novel method that\nadaptively combines FKL and RKL per token using a sigmoid-based weighting\nfunction derived from the teacher-student probability log-ratio. ToDi\ndynamically emphasizes the appropriate divergence for each token, enabling\nprecise distribution alignment. We demonstrate that ToDi consistently\noutperforms recent distillation baselines using uniform or less granular\nstrategies across instruction-following benchmarks. Extensive ablation studies\nand efficiency analysis further validate ToDi's effectiveness and practicality.", "categories": ["cs.CL"], "published": "2025-05-22 06:51:16", "updated": "2025-05-22 06:51:16", "pdf_url": "http://arxiv.org/pdf/2505.16297v1", "comment": "13 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16303v1", "title": "INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling", "authors": ["Haochen Shi", "Tianshi Zheng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Chunkit Chan", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "abstract": "Large Language Model (LLM) routing is a pivotal technique for navigating a\ndiverse landscape of LLMs, aiming to select the best-performing LLMs tailored\nto the domains of user queries, while managing computational resources.\nHowever, current routing approaches often face limitations in scalability when\ndealing with a large pool of specialized LLMs, or in their adaptability to\nextending model scope and evolving capability domains. To overcome those\nchallenges, we propose InferenceDynamics, a flexible and scalable\nmulti-dimensional routing framework by modeling the capability and knowledge of\nmodels. We operate it on our comprehensive dataset RouteMix, and demonstrate\nits effectiveness and generalizability in group-level routing using modern\nbenchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its\nability to identify and leverage top-performing models for given tasks, leading\nto superior outcomes with efficient resource utilization. The broader adoption\nof Inference Dynamics can empower users to harness the full specialized\npotential of the LLM ecosystem, and our code will be made publicly available to\nencourage further research.", "categories": ["cs.CL"], "published": "2025-05-22 06:56:51", "updated": "2025-05-22 06:56:51", "pdf_url": "http://arxiv.org/pdf/2505.16303v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16307v1", "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "abstract": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 06:59:10", "updated": "2025-05-22 06:59:10", "pdf_url": "http://arxiv.org/pdf/2505.16307v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16315v1", "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning", "authors": ["Xiaoxue Cheng", "Junyi Li", "Zhenduo Zhang", "Xinyu Tang", "Wayne Xin Zhao", "Xinyu Kong", "Zhiqiang Zhang"], "abstract": "Large reasoning models (LRMs) have demonstrated strong performance on complex\nreasoning tasks, but often suffer from overthinking, generating redundant\ncontent regardless of task difficulty. Inspired by the dual process theory in\ncognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a\nreinforcement learning framework that enables LRMs to achieve efficient\nreasoning through adaptive cognitive allocation and dynamic system switch. ACPO\nincorporates two key components: (1) introducing system-aware reasoning tokens\nto explicitly represent the thinking modes thereby making the model's cognitive\nprocess transparent, and (2) integrating online difficulty estimation and token\nlength budget to guide adaptive system switch and reasoning during\nreinforcement learning. To this end, we propose a two-stage training strategy.\nThe first stage begins with supervised fine-tuning to cold start the model,\nenabling it to generate reasoning paths with explicit thinking modes. In the\nsecond stage, we apply ACPO to further enhance adaptive system switch for\ndifficulty-aware reasoning. Experimental results demonstrate that ACPO\neffectively reduces redundant reasoning while adaptively adjusting cognitive\nallocation based on task complexity, achieving efficient hybrid reasoning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 07:15:08", "updated": "2025-05-22 07:15:08", "pdf_url": "http://arxiv.org/pdf/2505.16315v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16322v1", "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "abstract": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 07:24:11", "updated": "2025-05-22 07:24:11", "pdf_url": "http://arxiv.org/pdf/2505.16322v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16325v1", "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "abstract": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-22 07:32:12", "updated": "2025-05-22 07:32:12", "pdf_url": "http://arxiv.org/pdf/2505.16325v1", "comment": "18 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16330v1", "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "abstract": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM.", "categories": ["cs.CL", "cs.AI", "cs.DL"], "published": "2025-05-22 07:34:59", "updated": "2025-05-22 07:34:59", "pdf_url": "http://arxiv.org/pdf/2505.16330v1", "comment": null, "doi": "10.1016/j.eswa.2025.126778", "journal_ref": "Expert Systems With Applications, 2025"}
{"arxiv_id": "2505.16348v1", "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance", "authors": ["Taeyoon Kwon", "Dongwook Choi", "Sunghwan Kim", "Hyojun Kim", "Seungjun Moon", "Beong-woo Kwak", "Kuan-Hao Huang", "Jinyoung Yeo"], "abstract": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO", "categories": ["cs.CL"], "published": "2025-05-22 08:00:10", "updated": "2025-05-22 08:00:10", "pdf_url": "http://arxiv.org/pdf/2505.16348v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16349v1", "title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization", "authors": ["Pierre Achkar", "Tim Gollub", "Martin Potthast"], "abstract": "The exponential growth of scientific publications has made it increasingly\ndifficult for researchers to stay updated and synthesize knowledge effectively.\nThis paper presents XSum, a modular pipeline for multi-document summarization\n(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The\npipeline includes two core components: a question-generation module and an\neditor module. The question-generation module dynamically generates questions\nadapted to the input papers, ensuring the retrieval of relevant and accurate\ninformation. The editor module synthesizes the retrieved content into coherent\nand well-structured summaries that adhere to academic standards for proper\ncitation. Evaluated on the SurveySum dataset, XSum demonstrates strong\nperformance, achieving considerable improvements in metrics such as CheckEval,\nG-Eval and Ref-F1 compared to existing approaches. This work provides a\ntransparent, adaptable framework for scientific summarization with potential\napplications in a wide range of domains. Code available at\nhttps://github.com/webis-de/scolia25-xsum", "categories": ["cs.CL"], "published": "2025-05-22 08:00:59", "updated": "2025-05-22 08:00:59", "pdf_url": "http://arxiv.org/pdf/2505.16349v1", "comment": "Accepted at SCOLIA@ECIR 2025 Workshop", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16381v1", "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "abstract": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-22 08:36:09", "updated": "2025-05-22 08:36:09", "pdf_url": "http://arxiv.org/pdf/2505.16381v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16385v1", "title": "Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models", "authors": ["Kaiyu He", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "abstract": "Large language models (LLMs) demonstrate remarkable ability in cross-lingual\ntasks. Understanding how LLMs acquire this ability is crucial for their\ninterpretability. To quantify the cross-lingual ability of LLMs accurately, we\npropose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn\ncross-lingual ability, we trace the outputs of LLMs' intermediate layers in the\nword translation task. We identify and distinguish two distinct behaviors in\nthe forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.\nWe attribute LLMs' two distinct behaviors to the co-occurrence frequency of\nwords and find the semantic pivot from the pre-training dataset. Finally, to\napply our findings to improve the cross-lingual ability of LLMs, we reconstruct\na semantic pivot-aware pre-training dataset using documents with a high\nproportion of semantic pivots. Our experiments validate the effectiveness of\nour approach in enhancing cross-lingual ability. Our research contributes\ninsights into the interpretability of LLMs and offers a method for improving\nLLMs' cross-lingual ability.", "categories": ["cs.CL"], "published": "2025-05-22 08:37:04", "updated": "2025-05-22 08:37:04", "pdf_url": "http://arxiv.org/pdf/2505.16385v1", "comment": "14 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16392v1", "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "abstract": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts.", "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "published": "2025-05-22 08:45:14", "updated": "2025-05-22 08:45:14", "pdf_url": "http://arxiv.org/pdf/2505.16392v1", "comment": "Accepted at SIGIR 2025", "doi": "10.1145/3726302.3730304", "journal_ref": null}
{"arxiv_id": "2505.16400v1", "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "abstract": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 08:50:47", "updated": "2025-05-22 08:50:47", "pdf_url": "http://arxiv.org/pdf/2505.16400v1", "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16406v1", "title": "On the reliability of feature attribution methods for speech classification", "authors": ["Gaofei Shen", "Hosein Mohebbi", "Arianna Bisazza", "Afra Alishahi", "Grzegorz Chrupa\u0142a"], "abstract": "As the capabilities of large-scale pre-trained models evolve, understanding\nthe determinants of their outputs becomes more important. Feature attribution\naims to reveal which parts of the input elements contribute the most to model\noutputs. In speech processing, the unique characteristics of the input signal\nmake the application of feature attribution methods challenging. We study how\nfactors such as input type and aggregation and perturbation timespan impact the\nreliability of standard feature attribution methods, and how these factors\ninteract with characteristics of each classification task. We find that\nstandard approaches to feature attribution are generally unreliable when\napplied to the speech domain, with the exception of word-aligned perturbation\nmethods when applied to word-based classification tasks.", "categories": ["cs.CL", "eess.AS"], "published": "2025-05-22 08:59:25", "updated": "2025-05-22 08:59:25", "pdf_url": "http://arxiv.org/pdf/2505.16406v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16408v1", "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs", "authors": ["Muhammad Farid Adilazuarda", "Chen Cecilia Liu", "Iryna Gurevych", "Alham Fikri Aji"], "abstract": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior.", "categories": ["cs.CL"], "published": "2025-05-22 09:00:01", "updated": "2025-05-22 09:00:01", "pdf_url": "http://arxiv.org/pdf/2505.16408v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16410v1", "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 09:00:19", "updated": "2025-05-22 09:00:19", "pdf_url": "http://arxiv.org/pdf/2505.16410v1", "comment": "Working in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16415v1", "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "abstract": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 09:04:03", "updated": "2025-05-22 09:04:03", "pdf_url": "http://arxiv.org/pdf/2505.16415v1", "comment": "Work in process", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16418v1", "title": "Exploring the Relationship Between Diversity and Quality in Ad Text Generation", "authors": ["Yoichi Aoki", "Soichiro Murakami", "Ukyo Honda", "Akihiko Kato"], "abstract": "In natural language generation for advertising, creating diverse and engaging\nad texts is crucial for capturing a broad audience and avoiding advertising\nfatigue. Regardless of the importance of diversity, the impact of the\ndiversity-enhancing methods in ad text generation -- mainly tested on tasks\nsuch as summarization and machine translation -- has not been thoroughly\nexplored. Ad text generation significantly differs from these tasks owing to\nthe text style and requirements. This research explores the relationship\nbetween diversity and ad quality in ad text generation by considering multiple\nfactors, such as diversity-enhancing methods, their hyperparameters,\ninput-output formats, and the models.", "categories": ["cs.CL"], "published": "2025-05-22 09:05:44", "updated": "2025-05-22 09:05:44", "pdf_url": "http://arxiv.org/pdf/2505.16418v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16421v1", "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "abstract": "While reinforcement learning (RL) has demonstrated remarkable success in\nenhancing large language models (LLMs), it has primarily focused on single-turn\ntasks such as solving math problems. Training effective web agents for\nmulti-turn interactions remains challenging due to the complexity of\nlong-horizon decision-making across dynamic web interfaces. In this work, we\npresent WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework\nfor training web agents. It learns directly from online interactions with web\nenvironments by asynchronously generating diverse trajectories, entirely guided\nby binary rewards depending on task success. Experiments on the WebArena-Lite\nbenchmark demonstrate the effectiveness of WebAgent-R1, boosting the task\nsuccess rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to\n44.8%, significantly outperforming existing state-of-the-art methods and strong\nproprietary models such as OpenAI o3. In-depth analyses reveal the\neffectiveness of the thinking-based prompting strategy and test-time scaling\nthrough increased interactions for web tasks. We further investigate different\nRL initialization policies by introducing two variants, namely WebAgent-R1-Zero\nand WebAgent-R1-CoT, which highlight the importance of the warm-up training\nstage (i.e., behavior cloning) and provide insights on incorporating long\nchain-of-thought (CoT) reasoning in web agents.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-22 09:07:43", "updated": "2025-05-22 09:07:43", "pdf_url": "http://arxiv.org/pdf/2505.16421v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16425v1", "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "abstract": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 09:10:09", "updated": "2025-05-22 09:10:09", "pdf_url": "http://arxiv.org/pdf/2505.16425v1", "comment": "13 pages, 5 figures, under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16429v1", "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "abstract": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 09:14:23", "updated": "2025-05-22 09:14:23", "pdf_url": "http://arxiv.org/pdf/2505.16429v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16460v1", "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "abstract": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-22 09:42:11", "updated": "2025-05-22 09:42:11", "pdf_url": "http://arxiv.org/pdf/2505.16460v1", "comment": "16 pages, 13 tables, 1 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16467v1", "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fern\u00e1ndez"], "abstract": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity.", "categories": ["cs.CL"], "published": "2025-05-22 09:48:51", "updated": "2025-05-22 09:48:51", "pdf_url": "http://arxiv.org/pdf/2505.16467v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16470v1", "title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "authors": ["Kuicai Dong", "Yujing Chang", "Shijie Huang", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "abstract": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/.", "categories": ["cs.IR", "cs.CL", "cs.CV"], "published": "2025-05-22 09:52:57", "updated": "2025-05-22 09:52:57", "pdf_url": "http://arxiv.org/pdf/2505.16470v1", "comment": "preprint. code available at\n  \\url{https://mmdocrag.github.io/MMDocRAG/}", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16483v1", "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "abstract": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:10:07", "updated": "2025-05-22 10:10:07", "pdf_url": "http://arxiv.org/pdf/2505.16483v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16491v1", "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "abstract": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:22:39", "updated": "2025-05-22 10:22:39", "pdf_url": "http://arxiv.org/pdf/2505.16491v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16505v1", "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "abstract": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-22 10:41:35", "updated": "2025-05-22 10:41:35", "pdf_url": "http://arxiv.org/pdf/2505.16505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16514v1", "title": "AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios", "authors": ["Yuting Huang", "Meitong Guo", "Yiquan Wu", "Ang Li", "Xiaozhong Liu", "Keting Yin", "Changlong Sun", "Fei Wu", "Kun Kuang"], "abstract": "Recent advances in LegalAI have primarily focused on individual case judgment\nanalysis, often overlooking the critical appellate process within the judicial\nsystem. Appeals serve as a core mechanism for error correction and ensuring\nfair trials, making them highly significant both in practice and in research.\nTo address this gap, we present the AppealCase dataset, consisting of 10,000\npairs of real-world, matched first-instance and second-instance documents\nacross 91 categories of civil cases. The dataset also includes detailed\nannotations along five dimensions central to appellate review: judgment\nreversals, reversal reasons, cited legal provisions, claim-level decisions, and\nwhether there is new information in the second instance. Based on these\nannotations, we propose five novel LegalAI tasks and conduct a comprehensive\nevaluation across 20 mainstream models. Experimental results reveal that all\ncurrent models achieve less than 50% F1 scores on the judgment reversal\nprediction task, highlighting the complexity and challenge of the appeal\nscenario. We hope that the AppealCase dataset will spur further research in\nLegalAI for appellate case analysis and contribute to improving consistency in\njudicial decision-making.", "categories": ["cs.CL"], "published": "2025-05-22 10:50:33", "updated": "2025-05-22 10:50:33", "pdf_url": "http://arxiv.org/pdf/2505.16514v1", "comment": "15 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16518v1", "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "authors": ["Lovisa Hagstr\u00f6m", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "abstract": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:57:08", "updated": "2025-05-22 10:57:08", "pdf_url": "http://arxiv.org/pdf/2505.16518v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16520v1", "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "abstract": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 11:00:53", "updated": "2025-05-22 11:00:53", "pdf_url": "http://arxiv.org/pdf/2505.16520v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16522v1", "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "abstract": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 11:04:09", "updated": "2025-05-22 11:04:09", "pdf_url": "http://arxiv.org/pdf/2505.16522v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16526v1", "title": "EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance", "authors": ["Heejae Suh", "Yejin Jeon", "Deokhyung Kang", "Taehee Park", "Yejin Min", "Gary Geunbae Lee"], "abstract": "Small large language models (sLLMs) offer the advantage of being lightweight\nand efficient, which makes them suitable for resource-constrained environments.\nHowever, sLLMs often struggle to maintain topic consistency in task-oriented\ndialogue systems, which is critical for scenarios such as service chatbots.\nSpecifically, it is important to ensure that the model denies off-topic or\nmalicious inputs and adheres to its intended functionality so as to prevent\npotential misuse and uphold reliability. Towards this, existing activation\nengineering approaches have been proposed to manipulate internal activations\nduring inference. While these methods are effective in certain scenarios, our\npreliminary experiments reveal their limitations in ensuring topic adherence.\nTherefore, to address this, we propose a novel approach termed Entropy-scaled\nSteering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the\nsteering intensity based on input uncertainty, which allows the model to handle\noff-topic distractors effectively while preserving on-topic accuracy. Our\nexperiments demonstrate that EnSToM achieves significant performance gain with\na relatively small data size compared to fine-tuning approaches. By improving\ntopic adherence without compromising efficiency, our approach provides a robust\nsolution for enhancing sLLM-based dialogue systems.", "categories": ["cs.CL"], "published": "2025-05-22 11:12:27", "updated": "2025-05-22 11:12:27", "pdf_url": "http://arxiv.org/pdf/2505.16526v1", "comment": "Accepted at ACL 2025 (Findings, long paper)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16530v1", "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection", "authors": ["Yuliang Yan", "Haochun Tang", "Shuo Yan", "Enyan Dai"], "abstract": "Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-22 11:16:46", "updated": "2025-05-22 11:16:46", "pdf_url": "http://arxiv.org/pdf/2505.16530v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16538v1", "title": "Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models", "authors": ["Ercong Nie", "Helmut Schmid", "Hinrich Sch\u00fctze"], "abstract": "Language confusion -- where large language models (LLMs) generate unintended\nlanguages against the user's need -- remains a critical challenge, especially\nfor English-centric models. We present the first mechanistic interpretability\n(MI) study of language confusion, combining behavioral benchmarking with\nneuron-level analysis. Using the Language Confusion Benchmark (LCB), we show\nthat confusion points (CPs) -- specific positions where language switches occur\n-- are central to this phenomenon. Through layer-wise analysis with TunedLens\nand targeted neuron attribution, we reveal that transition failures in the\nfinal layers drive confusion. We further demonstrate that editing a small set\nof critical neurons, identified via comparative analysis with\nmultilingual-tuned models, substantially mitigates confusion without harming\ngeneral competence or fluency. Our approach matches multilingual alignment in\nconfusion reduction for most languages and yields cleaner, higher-quality\noutputs. These findings provide new insights into the internal dynamics of LLMs\nand highlight neuron-level interventions as a promising direction for robust,\ninterpretable multilingual language modeling.", "categories": ["cs.CL"], "published": "2025-05-22 11:29:17", "updated": "2025-05-22 11:29:17", "pdf_url": "http://arxiv.org/pdf/2505.16538v1", "comment": "16 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16552v1", "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "authors": ["Wenhui Tan", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Ruihua Song"], "abstract": "Large Language Models (LLMs) achieve superior performance through\nChain-of-Thought (CoT) reasoning, but these token-level reasoning chains are\ncomputationally expensive and inefficient. In this paper, we introduce\nCompressed Latent Reasoning (CoLaR), a novel framework that dynamically\ncompresses reasoning processes in latent space through a two-stage training\napproach. First, during supervised fine-tuning, CoLaR extends beyond next-token\nprediction by incorporating an auxiliary next compressed embedding prediction\nobjective. This process merges embeddings of consecutive tokens using a\ncompression factor randomly sampled from a predefined range, and trains a\nspecialized latent head to predict distributions of subsequent compressed\nembeddings. Second, we enhance CoLaR through reinforcement learning (RL) that\nleverages the latent head's non-deterministic nature to explore diverse\nreasoning paths and exploit more compact ones. This approach enables CoLaR to:\ni) perform reasoning at a dense latent level (i.e., silently), substantially\nreducing reasoning chain length, and ii) dynamically adjust reasoning speed at\ninference time by simply prompting the desired compression factor. Extensive\nexperiments across four mathematical reasoning datasets demonstrate that CoLaR\nachieves 14.1% higher accuracy than latent-based baseline methods at comparable\ncompression ratios, and reduces reasoning chain length by 53.3% with only 4.8%\nperformance degradation compared to explicit CoT method. Moreover, when applied\nto more challenging mathematical reasoning tasks, our RL-enhanced CoLaR\ndemonstrates performance gains of up to 5.4% while dramatically reducing latent\nreasoning chain length by 82.8%. The code and models will be released upon\nacceptance.", "categories": ["cs.CL"], "published": "2025-05-22 11:40:26", "updated": "2025-05-22 11:40:26", "pdf_url": "http://arxiv.org/pdf/2505.16552v1", "comment": "15 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16559v1", "title": "CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning", "authors": ["Biao Yi", "Tiansheng Huang", "Baolei Zhang", "Tong Li", "Lihai Nie", "Zheli Liu", "Li Shen"], "abstract": "Fine-tuning-as-a-service, while commercially successful for Large Language\nModel (LLM) providers, exposes models to harmful fine-tuning attacks. As a\nwidely explored defense paradigm against such attacks, unlearning attempts to\nremove malicious knowledge from LLMs, thereby essentially preventing them from\nbeing used to perform malicious tasks. However, we highlight a critical flaw:\nthe powerful general adaptability of LLMs allows them to easily bypass\nselective unlearning by rapidly relearning or repurposing their capabilities\nfor harmful tasks. To address this fundamental limitation, we propose a\nparadigm shift: instead of selective removal, we advocate for inducing model\ncollapse--effectively forcing the model to \"unlearn everything\"--specifically\nin response to updates characteristic of malicious adaptation. This collapse\ndirectly neutralizes the very general capabilities that attackers exploit,\ntackling the core issue unaddressed by selective unlearning. We introduce the\nCollapse Trap (CTRAP) as a practical mechanism to implement this concept\nconditionally. Embedded during alignment, CTRAP pre-configures the model's\nreaction to subsequent fine-tuning dynamics. If updates during fine-tuning\nconstitute a persistent attempt to reverse safety alignment, the pre-configured\ntrap triggers a progressive degradation of the model's core language modeling\nabilities, ultimately rendering it inert and useless for the attacker.\nCrucially, this collapse mechanism remains dormant during benign fine-tuning,\nensuring the model's utility and general capabilities are preserved for\nlegitimate users. Extensive empirical results demonstrate that CTRAP\neffectively counters harmful fine-tuning risks across various LLMs and attack\nsettings, while maintaining high performance in benign scenarios. Our code is\navailable at https://anonymous.4open.science/r/CTRAP.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-22 11:47:08", "updated": "2025-05-22 11:47:08", "pdf_url": "http://arxiv.org/pdf/2505.16559v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16566v1", "title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts", "authors": ["Dongwon Noh", "Donghyeok Koh", "Junghun Yuk", "Gyuwan Kim", "Jaeyong Lee", "Kyungtae Lim", "Cheoneum Park"], "abstract": "Prior benchmarks for evaluating the domain-specific knowledge of large\nlanguage models (LLMs) lack the scalability to handle complex academic tasks.\nTo address this, we introduce \\texttt{ScholarBench}, a benchmark centered on\ndeep expert knowledge and complex academic problem-solving, which evaluates the\nacademic reasoning ability of LLMs and is constructed through a three-step\nprocess. \\texttt{ScholarBench} targets more specialized and logically complex\ncontexts derived from academic literature, encompassing five distinct problem\ntypes. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the\nabstraction, comprehension, and reasoning capabilities of LLMs across eight\ndistinct research domains. To ensure high-quality evaluation data, we define\ncategory-specific example attributes and design questions that are aligned with\nthe characteristic research methodologies and discourse structures of each\ndomain. Additionally, this benchmark operates as an English-Korean bilingual\ndataset, facilitating simultaneous evaluation for linguistic capabilities of\nLLMs in both languages. The benchmark comprises 5,031 examples in Korean and\n5,309 in English, with even state-of-the-art models like o3-mini achieving an\naverage evaluation score of only 0.543, demonstrating the challenging nature of\nthis benchmark.", "categories": ["cs.CL"], "published": "2025-05-22 11:59:06", "updated": "2025-05-22 11:59:06", "pdf_url": "http://arxiv.org/pdf/2505.16566v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16570v1", "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training", "authors": ["Dongyang Fan", "Vinko Sabol\u010dec", "Martin Jaggi"], "abstract": "Large Language Models (LLMs) are commonly pretrained on vast corpora of text\nwithout utilizing contextual metadata such as source, quality, or topic,\nleading to a context-free learning paradigm. While recent studies suggest that\nadding metadata like URL information as context (i.e., auxiliary inputs not\nused in the loss calculation) can improve training efficiency and downstream\nperformance, they offer limited understanding of which types of metadata are\ntruly effective and under what conditions. In this work, we conduct a\nsystematic evaluation and find that not all metadata types contribute equally.\nOnly URL context speeds up training, whereas quality scores and topic/format\ndomain information offer no clear benefit. Furthermore, the improved downstream\nperformances of URL conditioning emerge only when longer prompts are used at\ninference time. In addition, we demonstrate that context-aware pretraining\nenables more controllable generation than context-free pretraining, in a\nclassifier-free guidance fashion. Although topic and format metadata do not\naccelerate training, they are effective for steering outputs, offering\nhuman-interpretable control over generation.", "categories": ["cs.CL"], "published": "2025-05-22 12:01:20", "updated": "2025-05-22 12:01:20", "pdf_url": "http://arxiv.org/pdf/2505.16570v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16576v1", "title": "EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions", "authors": ["Spencer Hong", "Meng Luo", "Xinyi Wan"], "abstract": "Determining the veracity of atomic claims is an imperative component of many\nrecently proposed fact-checking systems. Many approaches tackle this problem by\nfirst retrieving evidence by querying a search engine and then performing\nclassification by providing the evidence set and atomic claim to a large\nlanguage model, but this process deviates from what a human would do in order\nto perform the task. Recent work attempted to address this issue by proposing\niterative evidence retrieval, allowing for evidence to be collected several\ntimes and only when necessary. Continuing along this line of research, we\npropose a novel claim verification system, called EMULATE, which is designed to\nbetter emulate human actions through the use of a multi-agent framework where\neach agent performs a small part of the larger task, such as ranking search\nresults according to predefined criteria or evaluating webpage content.\nExtensive experiments on several benchmarks show clear improvements over prior\nwork, demonstrating the efficacy of our new multi-agent framework.", "categories": ["cs.CL"], "published": "2025-05-22 12:08:08", "updated": "2025-05-22 12:08:08", "pdf_url": "http://arxiv.org/pdf/2505.16576v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16582v1", "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "abstract": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 12:17:13", "updated": "2025-05-22 12:17:13", "pdf_url": "http://arxiv.org/pdf/2505.16582v1", "comment": "25 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16591v1", "title": "Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering", "authors": ["Bowen Jiang", "Runchuan Zhu", "Jiang Wu", "Zinco Jiang", "Yifan He", "Junyuan Gao", "Jia Yu", "Rui Min", "Yinfan Wang", "Haote Yang", "Songyang Zhang", "Dahua Lin", "Lijun Wu", "Conghui He"], "abstract": "We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual\nfactual ability of Large Language Models (LLMs). Inspired by existing research,\nwe created the question set with features such as single knowledge point\ncoverage, absolute objectivity, unique answers, and temporal stability. These\nquestions enable efficient evaluation using the LLM-as-judge paradigm, testing\nboth the LLMs' factual memory and self-awareness (\"know what they don't know\").\nKoLasSimpleQA expands existing research in two key dimensions: (1) Breadth\n(Multilingual Coverage): It includes 9 languages, supporting global\napplicability evaluation. (2) Depth (Dual Domain Design): It covers both the\ngeneral domain (global facts) and the language-specific domain (such as\nhistory, culture, and regional traditions) for a comprehensive assessment of\nmultilingual capabilities. We evaluated mainstream LLMs, including traditional\nLLM and emerging Large Reasoning Models. Results show significant performance\ndifferences between the two domains, particularly in performance metrics,\nranking, calibration, and robustness. This highlights the need for targeted\nevaluation and optimization in multilingual contexts. We hope KoLasSimpleQA\nwill help the research community better identify LLM capability boundaries in\nmultilingual contexts and provide guidance for model optimization. We will\nrelease KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .", "categories": ["cs.CL"], "published": "2025-05-22 12:27:02", "updated": "2025-05-22 12:27:02", "pdf_url": "http://arxiv.org/pdf/2505.16591v1", "comment": "Equal contribution: Bowen Jiang, Runchuan Zhu, Jiang Wu;\n  Corresponding author: Conghui He", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16592v1", "title": "What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse", "authors": ["Shijia Zhou", "Siyao Peng", "Simon Luebke", "J\u00f6rg Ha\u00dfler", "Mario Haim", "Saif M. Mohammad", "Barbara Plank"], "abstract": "Media framing refers to the emphasis on specific aspects of perceived reality\nto shape how an issue is defined and understood. Its primary purpose is to\nshape public perceptions often in alignment with the authors' opinions and\nstances. However, the interaction between stance and media frame remains\nlargely unexplored. In this work, we apply an interdisciplinary approach to\nconceptualize and computationally explore this interaction with internet memes\non climate change. We curate CLIMATEMEMES, the first dataset of climate-change\nmemes annotated with both stance and media frames, inspired by research in\ncommunication science. CLIMATEMEMES includes 1,184 memes sourced from 47\nsubreddits, enabling analysis of frame prominence over time and communities,\nand sheds light on the framing preferences of different stance holders. We\npropose two meme understanding tasks: stance detection and media frame\ndetection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the\ncorresponding results on their LLM backbone. Human captions consistently\nenhance performance. Synthetic captions and human-corrected OCR also help\noccasionally. Our findings highlight that VLMs perform well on stance, but\nstruggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'\nlimitations in handling nuanced frames and stance expressions on climate change\ninternet memes.", "categories": ["cs.CL", "cs.MM"], "published": "2025-05-22 12:27:12", "updated": "2025-05-22 12:27:12", "pdf_url": "http://arxiv.org/pdf/2505.16592v1", "comment": "19 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16610v1", "title": "From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "abstract": "Effective emotional support hinges on understanding users' emotions and needs\nto provide meaningful comfort during multi-turn interactions. Large Language\nModels (LLMs) show great potential for expressing empathy; however, they often\ndeliver generic and one-size-fits-all responses that fail to address users'\nspecific needs. To tackle this issue, we propose a self-evolution framework\ndesigned to help LLMs improve their responses to better align with users'\nimplicit preferences concerning user profiles (personalities), emotional\nstates, and specific situations. Our framework consists of two distinct phases:\n\\textit{(1)} \\textit{Emotional Support Experience Acquisition}, where LLMs are\nfine-tuned on limited emotional support conversation data to provide basic\nsupport, and \\textit{(2)} \\textit{Self-Improvement for Personalized Emotional\nSupport}, where LLMs leverage self-reflection and self-refinement to generate\npersonalized responses. Through iterative direct preference optimization\nbetween the pre- and post-refined responses, our model generates responses that\nreflect a better understanding of the user's implicit preferences. Extensive\nexperiments and evaluations demonstrate that our method significantly enhances\nthe model's performance in emotional support, reducing unhelpful responses and\nminimizing discrepancies between user preferences and model outputs.", "categories": ["cs.CL"], "published": "2025-05-22 12:45:12", "updated": "2025-05-22 12:45:12", "pdf_url": "http://arxiv.org/pdf/2505.16610v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16612v1", "title": "Steering Large Language Models for Machine Translation Personalization", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "abstract": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 12:47:16", "updated": "2025-05-22 12:47:16", "pdf_url": "http://arxiv.org/pdf/2505.16612v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16624v1", "title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports", "authors": ["Francesco Dalla Serra", "Patrick Schrempf", "Chaoyang Wang", "Zaiqiao Meng", "Fani Deligianni", "Alison Q. O'Neil"], "abstract": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-22 12:57:35", "updated": "2025-05-22 12:57:35", "pdf_url": "http://arxiv.org/pdf/2505.16624v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16631v1", "title": "MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries", "authors": ["Jonghwi Kim", "Deokhyung Kang", "Seonjeong Hwang", "Yunsu Kim", "Jungseul Ok", "Gary Lee"], "abstract": "Despite bilingual speakers frequently using mixed-language queries in web\nsearches, Information Retrieval (IR) research on them remains scarce. To\naddress this, we introduce MiLQ,Mixed-Language Query test set, the first public\nbenchmark of mixed-language queries, confirmed as realistic and highly\npreferred. Experiments show that multilingual IR models perform moderately on\nMiLQ and inconsistently across native, English, and mixed-language queries,\nalso suggesting code-switched training data's potential for robust IR models\nhandling such queries. Meanwhile, intentional English mixing in queries proves\nan effective strategy for bilinguals searching English documents, which our\nanalysis attributes to enhanced token matching compared to native queries.", "categories": ["cs.IR", "cs.CL"], "published": "2025-05-22 13:03:15", "updated": "2025-05-22 13:03:15", "pdf_url": "http://arxiv.org/pdf/2505.16631v1", "comment": "16 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16637v1", "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "abstract": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 13:08:25", "updated": "2025-05-22 13:08:25", "pdf_url": "http://arxiv.org/pdf/2505.16637v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16648v1", "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "abstract": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:18:45", "updated": "2025-05-22 13:18:45", "pdf_url": "http://arxiv.org/pdf/2505.16648v1", "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16660v1", "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "abstract": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:24:52", "updated": "2025-05-22 13:24:52", "pdf_url": "http://arxiv.org/pdf/2505.16660v1", "comment": "29pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16661v1", "title": "A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP", "authors": ["Issey Sukeda", "Takuro Fujii", "Kosei Buma", "Shunsuke Sasaki", "Shinnosuke Ono"], "abstract": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.", "categories": ["cs.CL"], "published": "2025-05-22 13:27:37", "updated": "2025-05-22 13:27:37", "pdf_url": "http://arxiv.org/pdf/2505.16661v1", "comment": "15 pages, 9 tables, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16673v1", "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "authors": ["Huanjin Yao", "Qixiang Yin", "Jingyi Zhang", "Min Yang", "Yibo Wang", "Wenhao Wu", "Fei Su", "Li Shen", "Minghui Qiu", "Dacheng Tao", "Jiaxing Huang"], "abstract": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-22 13:39:32", "updated": "2025-05-22 13:39:32", "pdf_url": "http://arxiv.org/pdf/2505.16673v1", "comment": "Technical report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16686v1", "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge", "authors": ["Lars Benedikt Kaesberg", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "abstract": "Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 13:53:50", "updated": "2025-05-22 13:53:50", "pdf_url": "http://arxiv.org/pdf/2505.16686v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16694v1", "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:59:30", "updated": "2025-05-22 13:59:30", "pdf_url": "http://arxiv.org/pdf/2505.16694v1", "comment": "Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16703v1", "title": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs", "authors": ["Zeping Yu", "Sophia Ananiadou"], "abstract": "Although multimodal large language models (MLLMs) have achieved impressive\nperformance, the multimodal instruction tuning stage often causes catastrophic\nforgetting of the base LLM's language ability, even in strong models like\nLlama3. To address this, we propose Locate-then-Merge, a training-free\nparameter fusion framework that first locates important parameters and then\nselectively merges them. We further introduce Neuron-Fusion, a neuron-level\nstrategy that preserves the influence of neurons with large parameter\nshifts--neurons likely responsible for newly acquired visual\ncapabilities--while attenuating the influence of neurons with smaller changes\nthat likely encode general-purpose language skills. This design enables better\nretention of visual adaptation while mitigating language degradation.\nExperiments on 13 benchmarks across both language and visual tasks show that\nNeuron-Fusion consistently outperforms existing model merging methods. Further\nanalysis reveals that our method effectively reduces context hallucination in\ngeneration.", "categories": ["cs.CL"], "published": "2025-05-22 14:04:43", "updated": "2025-05-22 14:04:43", "pdf_url": "http://arxiv.org/pdf/2505.16703v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16722v1", "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "abstract": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 14:30:14", "updated": "2025-05-22 14:30:14", "pdf_url": "http://arxiv.org/pdf/2505.16722v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16737v1", "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "abstract": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "published": "2025-05-22 14:52:10", "updated": "2025-05-22 14:52:10", "pdf_url": "http://arxiv.org/pdf/2505.16737v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16743v1", "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "abstract": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "published": "2025-05-22 14:53:53", "updated": "2025-05-22 14:53:53", "pdf_url": "http://arxiv.org/pdf/2505.16743v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16774v1", "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models", "authors": ["Yiming Gao", "Bin Wang", "Chengwei Wei", "Shuo Sun", "AiTi Aw"], "abstract": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area.", "categories": ["cs.CL"], "published": "2025-05-22 15:15:29", "updated": "2025-05-22 15:15:29", "pdf_url": "http://arxiv.org/pdf/2505.16774v1", "comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16782v1", "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning", "authors": ["Xinghao Chen", "Anhao Zhao", "Heming Xia", "Xuan Lu", "Hanlin Wang", "Yanjun Chen", "Wei Zhang", "Jian Wang", "Wenjie Li", "Xiaoyu Shen"], "abstract": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT.", "categories": ["cs.CL"], "published": "2025-05-22 15:26:51", "updated": "2025-05-22 15:26:51", "pdf_url": "http://arxiv.org/pdf/2505.16782v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16789v1", "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "abstract": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 15:30:00", "updated": "2025-05-22 15:30:00", "pdf_url": "http://arxiv.org/pdf/2505.16789v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16800v1", "title": "Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation", "authors": ["Changbing Yang", "Garrett Nicolai"], "abstract": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages.", "categories": ["cs.CL"], "published": "2025-05-22 15:40:09", "updated": "2025-05-22 15:40:09", "pdf_url": "http://arxiv.org/pdf/2505.16800v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16806v1", "title": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement", "authors": ["Kexin Zhang", "Junlan Chen", "Daifeng Li", "Yuxuan Zhang", "Yangyang Feng", "Bowen Deng", "Weixu Chen"], "abstract": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-22 15:45:29", "updated": "2025-05-22 15:45:29", "pdf_url": "http://arxiv.org/pdf/2505.16806v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16814v1", "title": "Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?", "authors": ["Gaurav Kamath", "Sowmya Vajjala"], "abstract": "Named Entity Recognition(NER) for low-resource languages aims to produce\nrobust systems for languages where there is limited labeled training data\navailable, and has been an area of increasing interest within NLP. Data\naugmentation for increasing the amount of low-resource labeled data is a common\npractice. In this paper, we explore the role of synthetic data in the context\nof multilingual, low-resource NER, considering 11 languages from diverse\nlanguage families. Our results suggest that synthetic data does in fact hold\npromise for low-resource language NER, though we see significant variation\nbetween languages.", "categories": ["cs.CL"], "published": "2025-05-22 15:50:47", "updated": "2025-05-22 15:50:47", "pdf_url": "http://arxiv.org/pdf/2505.16814v1", "comment": "pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16826v1", "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning", "authors": ["Wei Sun", "Wen Yang", "Pu Jian", "Qianlong Du", "Fuwei Cui", "Shuo Ren", "Jiajun Zhang"], "abstract": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 16:00:33", "updated": "2025-05-22 16:00:33", "pdf_url": "http://arxiv.org/pdf/2505.16826v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16831v1", "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "abstract": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "published": "2025-05-22 16:02:10", "updated": "2025-05-22 16:02:10", "pdf_url": "http://arxiv.org/pdf/2505.16831v1", "comment": "44 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16832v1", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "abstract": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "published": "2025-05-22 16:02:18", "updated": "2025-05-22 16:02:18", "pdf_url": "http://arxiv.org/pdf/2505.16832v1", "comment": "16 pages; 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16834v1", "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "abstract": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-22 16:05:02", "updated": "2025-05-22 16:05:02", "pdf_url": "http://arxiv.org/pdf/2505.16834v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16838v1", "title": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search", "authors": ["Yibo Wang", "Li Shen", "Huanjin Yao", "Tiansheng Huang", "Rui Liu", "Naiqiang Tan", "Jiaxing Huang", "Kai Zhang", "Dacheng Tao"], "abstract": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress", "categories": ["cs.CL"], "published": "2025-05-22 16:06:59", "updated": "2025-05-22 16:06:59", "pdf_url": "http://arxiv.org/pdf/2505.16838v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16847v1", "title": "Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study", "authors": ["Baran Barbarestani", "Isa Maks", "Piek Vossen"], "abstract": "This paper introduces a method for detecting inappropriately targeting\nlanguage in online conversations by integrating crowd and expert annotations\nwith ChatGPT. We focus on English conversation threads from Reddit, examining\ncomments that target individuals or groups. Our approach involves a\ncomprehensive annotation framework that labels a diverse data set for various\ntarget categories and specific target words within the conversational context.\nWe perform a comparative analysis of annotations from human experts, crowd\nannotators, and ChatGPT, revealing strengths and limitations of each method in\nrecognizing both explicit hate speech and subtler discriminatory language. Our\nfindings highlight the significant role of contextual factors in identifying\nhate speech and uncover new categories of targeting, such as social belief and\nbody image. We also address the challenges and subjective judgments involved in\nannotation and the limitations of ChatGPT in grasping nuanced language. This\nstudy provides insights for improving automated content moderation strategies\nto enhance online safety and inclusivity.", "categories": ["cs.CL"], "published": "2025-05-22 16:10:43", "updated": "2025-05-22 16:10:43", "pdf_url": "http://arxiv.org/pdf/2505.16847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16850v1", "title": "ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning", "authors": ["Tajamul Ashraf", "Mohammed Mohsen Peerzada", "Moloud Abdar", "Yutong Xie", "Yuyin Zhou", "Xiaofeng Liu", "Iqra Altaf Gillani", "Janibul Bashir"], "abstract": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy across decentralized participants.\nAs FL adoption grows, numerous techniques have been proposed to tackle its\npractical challenges. However, the lack of standardized evaluation across key\ndimensions hampers systematic progress and fair comparison of FL methods. In\nthis work, we introduce ATR-Bench, a unified framework for analyzing federated\nlearning through three foundational dimensions: Adaptation, Trust, and\nReasoning. We provide an in-depth examination of the conceptual foundations,\ntask formulations, and open research challenges associated with each theme. We\nhave extensively benchmarked representative methods and datasets for adaptation\nto heterogeneous clients and trustworthiness in adversarial or unreliable\nenvironments. Due to the lack of reliable metrics and models for reasoning in\nFL, we only provide literature-driven insights for this dimension. ATR-Bench\nlays the groundwork for a systematic and holistic evaluation of federated\nlearning with real-world relevance. We will make our complete codebase publicly\naccessible and a curated repository that continuously tracks new developments\nand research in the FL literature.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-22 16:11:38", "updated": "2025-05-22 16:11:38", "pdf_url": "http://arxiv.org/pdf/2505.16850v1", "comment": "Federated Learning Benchmark for Domain Adaptation, Trustworthiness,\n  and Reasoning", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16855v1", "title": "Nested Named Entity Recognition as Single-Pass Sequence Labeling", "authors": ["Alberto Mu\u00f1oz-Ortiz", "David Vilares", "Caio COrro", "Carlos G\u00f3mez-Rodr\u00edguez"], "abstract": "We cast nested named entity recognition (NNER) as a sequence labeling task by\nleveraging prior work that linearizes constituency structures, effectively\nreducing the complexity of this structured prediction problem to\nstraightforward token classification. By combining these constituency\nlinearizations with pretrained encoders, our method captures nested entities\nwhile performing exactly $n$ tagging actions. Our approach achieves competitive\nperformance compared to less efficient systems, and it can be trained using any\noff-the-shelf sequence labeling library.", "categories": ["cs.CL", "68T50", "I.2.7"], "published": "2025-05-22 16:13:39", "updated": "2025-05-22 16:13:39", "pdf_url": "http://arxiv.org/pdf/2505.16855v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16868v1", "title": "Comparative analysis of subword tokenization approaches for Indian languages", "authors": ["Sudhansu Bala Das", "Samujjal Choudhury", "Tapas Kumar Mishra", "Bidyut Kr. Patra"], "abstract": "Tokenization is the act of breaking down text into smaller parts, or tokens,\nthat are easier for machines to process. This is a key phase in machine\ntranslation (MT) models. Subword tokenization enhances this process by breaking\ndown words into smaller subword units, which is especially beneficial in\nlanguages with complicated morphology or a vast vocabulary. It is useful in\ncapturing the intricate structure of words in Indian languages (ILs), such as\nprefixes, suffixes, and other morphological variations. These languages\nfrequently use agglutinative structures, in which words are formed by the\ncombination of multiple morphemes such as suffixes, prefixes, and stems. As a\nresult, a suitable tokenization strategy must be chosen to address these\nscenarios. This paper examines how different subword tokenization techniques,\nsuch as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,\naffect ILs. The effectiveness of these subword tokenization techniques is\ninvestigated in statistical, neural, and multilingual neural machine\ntranslation models. All models are examined using standard evaluation metrics,\nsuch as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,\nRIBES, and COMET. Based on the results, it appears that for the majority of\nlanguage pairs for the Statistical and Neural MT models, the SentencePiece\ntokenizer continuously performed better than other tokenizers in terms of BLEU\nscore. However, BPE tokenization outperformed other tokenization techniques in\nthe context of Multilingual Neural Machine Translation model. The results show\nthat, despite using the same tokenizer and dataset for each model, translations\nfrom ILs to English surpassed translations from English to ILs.", "categories": ["cs.CL"], "published": "2025-05-22 16:24:37", "updated": "2025-05-22 16:24:37", "pdf_url": "http://arxiv.org/pdf/2505.16868v1", "comment": "24 pages, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16869v1", "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Jiahe Guo", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "abstract": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility.", "categories": ["cs.CL"], "published": "2025-05-22 16:24:51", "updated": "2025-05-22 16:24:51", "pdf_url": "http://arxiv.org/pdf/2505.16869v1", "comment": "To Appear at ACL 2025 (Main)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16881v1", "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "abstract": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 16:35:33", "updated": "2025-05-22 16:35:33", "pdf_url": "http://arxiv.org/pdf/2505.16881v1", "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16886v1", "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "abstract": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 16:41:37", "updated": "2025-05-22 16:41:37", "pdf_url": "http://arxiv.org/pdf/2505.16886v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16888v1", "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "authors": ["Viet Pham", "Thai Le"], "abstract": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-22 16:47:15", "updated": "2025-05-22 16:47:15", "pdf_url": "http://arxiv.org/pdf/2505.16888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16894v1", "title": "Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs", "authors": ["Zeyu Wei", "Shuo Wang", "Xiaohui Rong", "Xuemin Liu", "He Li"], "abstract": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms.", "categories": ["cs.CL"], "published": "2025-05-22 16:50:58", "updated": "2025-05-22 16:50:58", "pdf_url": "http://arxiv.org/pdf/2505.16894v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16900v1", "title": "Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "abstract": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-22 16:59:26", "updated": "2025-05-22 16:59:26", "pdf_url": "http://arxiv.org/pdf/2505.16900v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16922v1", "title": "UNCLE: Uncertainty Expressions in Long-Form Generation", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Dong Yu", "Nigel Collier", "Deqing Yang"], "abstract": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE.", "categories": ["cs.CL"], "published": "2025-05-22 17:16:08", "updated": "2025-05-22 17:16:08", "pdf_url": "http://arxiv.org/pdf/2505.16922v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16927v1", "title": "Latent Principle Discovery for Language Model Self-Improvement", "authors": ["Keshav Ramji", "Tahira Naseem", "Ram\u00f3n Fernandez Astudillo"], "abstract": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 17:20:18", "updated": "2025-05-22 17:20:18", "pdf_url": "http://arxiv.org/pdf/2505.16927v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16931v1", "title": "PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues", "authors": ["Matthew Zent", "Digory Smith", "Simon Woodhead"], "abstract": "Personally identifiable information (PII) anonymization is a high-stakes task\nthat poses a barrier to many open-science data sharing initiatives. While PII\nidentification has made large strides in recent years, in practice, error\nthresholds and the recall/precision trade-off still limit the uptake of these\nanonymization pipelines. We present PIIvot, a lighter-weight framework for PII\nanonymization that leverages knowledge of the data context to simplify the PII\ndetection problem. To demonstrate its effectiveness, we also contribute\nQATD-2k, the largest open-source real-world tutoring dataset of its kind, to\nsupport the demand for quality educational dialogue data.", "categories": ["cs.CL"], "published": "2025-05-22 17:22:28", "updated": "2025-05-22 17:22:28", "pdf_url": "http://arxiv.org/pdf/2505.16931v1", "comment": "6 pages, 2 figures, submitted to EMNLP 2025, for associated dataset,\n  see\n  https://huggingface.co/datasets/Eedi/Question-Anchored-Tutoring-Dialogues-2k", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16932v1", "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "abstract": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "published": "2025-05-22 17:23:14", "updated": "2025-05-22 17:23:14", "pdf_url": "http://arxiv.org/pdf/2505.16932v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16933v1", "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "abstract": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-22 17:23:26", "updated": "2025-05-22 17:23:26", "pdf_url": "http://arxiv.org/pdf/2505.16933v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16934v1", "title": "In-Context Watermarks for Large Language Models", "authors": ["Yepeng Liu", "Xuandong Zhao", "Christopher Kruegel", "Dawn Song", "Yuheng Bu"], "abstract": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution.", "categories": ["cs.CL"], "published": "2025-05-22 17:24:51", "updated": "2025-05-22 17:24:51", "pdf_url": "http://arxiv.org/pdf/2505.16934v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16938v1", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.", "categories": ["cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-22 17:27:43", "updated": "2025-05-22 17:27:43", "pdf_url": "http://arxiv.org/pdf/2505.16938v1", "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16944v1", "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 17:31:10", "updated": "2025-05-22 17:31:10", "pdf_url": "http://arxiv.org/pdf/2505.16944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16956v1", "title": "On Multilingual Encoder Language Model Compression for Low-Resource Languages", "authors": ["Daniil Gurgurov", "Michal Gregor", "Josef van Genabith", "Simon Ostermann"], "abstract": "In this paper, we combine two-step knowledge distillation, structured\npruning, truncation, and vocabulary trimming for extremely compressing\nmultilingual encoder-only language models for low-resource languages. Our novel\napproach systematically combines existing techniques and takes them to the\nextreme, reducing layer depth, feed-forward hidden size, and intermediate layer\nembedding size to create significantly smaller monolingual models while\nretaining essential language-specific knowledge. We achieve compression rates\nof up to 92% with only a marginal performance drop of 2-10% in four downstream\ntasks, including sentiment analysis, topic classification, named entity\nrecognition, and part-of-speech tagging, across three low-resource languages.\nNotably, the performance degradation correlates with the amount of\nlanguage-specific data in the teacher model, with larger datasets resulting in\nsmaller performance losses. Additionally, we conduct extensive ablation studies\nto identify best practices for multilingual model compression using these\ntechniques.", "categories": ["cs.CL"], "published": "2025-05-22 17:35:39", "updated": "2025-05-22 17:35:39", "pdf_url": "http://arxiv.org/pdf/2505.16956v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16964v1", "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning", "authors": ["Suhao Yu", "Haojin Wang", "Juncheng Wu", "Cihang Xie", "Yuyin Zhou"], "abstract": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-22 17:46:11", "updated": "2025-05-22 17:46:11", "pdf_url": "http://arxiv.org/pdf/2505.16964v1", "comment": "9 pages, 4 Figures Benchmark data:\n  https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16965v1", "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "abstract": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 17:46:23", "updated": "2025-05-22 17:46:23", "pdf_url": "http://arxiv.org/pdf/2505.16965v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16967v1", "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval", "authors": ["Nandan Thakur", "Crystina Zhang", "Xueguang Ma", "Jimmy Lin"], "abstract": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "published": "2025-05-22 17:47:57", "updated": "2025-05-22 17:47:57", "pdf_url": "http://arxiv.org/pdf/2505.16967v1", "comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16968v1", "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "abstract": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "published": "2025-05-22 17:48:53", "updated": "2025-05-22 17:48:53", "pdf_url": "http://arxiv.org/pdf/2505.16968v1", "comment": "20 pages, 11 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16972v1", "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition", "authors": ["Tianduo Wang", "Lu Xu", "Wei Lu", "Shanbo Cheng"], "abstract": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-22 17:51:05", "updated": "2025-05-22 17:51:05", "pdf_url": "http://arxiv.org/pdf/2505.16972v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16973v1", "title": "VeriFastScore: Speeding up long-form factuality evaluation", "authors": ["Rishanth Rajendhran", "Amir Zadeh", "Matthew Sarte", "Chuan Li", "Mohit Iyyer"], "abstract": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets.", "categories": ["cs.CL"], "published": "2025-05-22 17:51:25", "updated": "2025-05-22 17:51:25", "pdf_url": "http://arxiv.org/pdf/2505.16973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16975v1", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "abstract": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.", "categories": ["cs.SE", "cs.CL"], "published": "2025-05-22 17:51:49", "updated": "2025-05-22 17:51:49", "pdf_url": "http://arxiv.org/pdf/2505.16975v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16983v1", "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding", "authors": ["Junlong Tong", "Jinlan Fu", "Zixuan Lin", "Yingqi Fan", "Anhao Zhao", "Hui Su", "Xiaoyu Shen"], "abstract": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM.", "categories": ["cs.CL"], "published": "2025-05-22 17:53:28", "updated": "2025-05-22 17:53:28", "pdf_url": "http://arxiv.org/pdf/2505.16983v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16984v1", "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning", "authors": ["Mingyang Liu", "Gabriele Farina", "Asuman Ozdaglar"], "abstract": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-22 17:53:57", "updated": "2025-05-22 17:53:57", "pdf_url": "http://arxiv.org/pdf/2505.16984v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16986v1", "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 17:54:32", "updated": "2025-05-22 17:54:32", "pdf_url": "http://arxiv.org/pdf/2505.16986v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16988v1", "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-22 17:54:38", "updated": "2025-05-22 17:54:38", "pdf_url": "http://arxiv.org/pdf/2505.16988v1", "comment": "18 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16994v1", "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning", "authors": ["Runyang You", "Yongqi Li", "Xinyu Lin", "Xin Zhang", "Wenjie Wang", "Wenjie Li", "Liqiang Nie"], "abstract": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "published": "2025-05-22 17:55:43", "updated": "2025-05-22 17:55:43", "pdf_url": "http://arxiv.org/pdf/2505.16994v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16995v1", "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization", "authors": ["Chao Zhang", "Xin Shi", "Xueqiao Zhang", "Yifan Zhu", "Yi Yang", "Yawei Luo"], "abstract": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality.", "categories": ["cs.CL"], "published": "2025-05-22 17:56:21", "updated": "2025-05-22 17:56:21", "pdf_url": "http://arxiv.org/pdf/2505.16995v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16997v1", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.", "categories": ["cs.AI", "cs.CL", "cs.MA"], "published": "2025-05-22 17:56:39", "updated": "2025-05-22 17:56:39", "pdf_url": "http://arxiv.org/pdf/2505.16997v1", "comment": "19 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16998v1", "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "abstract": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 17:57:23", "updated": "2025-05-22 17:57:23", "pdf_url": "http://arxiv.org/pdf/2505.16998v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17005v1", "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "abstract": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-22 17:58:26", "updated": "2025-05-22 17:58:26", "pdf_url": "http://arxiv.org/pdf/2505.17005v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17015v1", "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models", "authors": ["Runsen Xu", "Weiyao Wang", "Hao Tang", "Xingyu Chen", "Xiaodong Wang", "Fu-Jen Chu", "Dahua Lin", "Matt Feiszli", "Kevin J. Liang"], "abstract": "Multi-modal large language models (MLLMs) have rapidly advanced in visual\ntasks, yet their spatial understanding remains limited to single images,\nleaving them ill-suited for robotics and other real-world applications that\nrequire multi-frame reasoning. In this paper, we propose a framework to equip\nMLLMs with robust multi-frame spatial understanding by integrating depth\nperception, visual correspondence, and dynamic perception. Central to our\napproach is the MultiSPA dataset, a novel, large-scale collection of more than\n27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we\nintroduce a comprehensive benchmark that tests a wide spectrum of spatial tasks\nunder uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves\nsignificant gains over baselines and proprietary systems, demonstrating\nscalable, generalizable multi-frame reasoning. We further observe multi-task\nbenefits and early indications of emergent capabilities in challenging\nscenarios, and showcase how our model can serve as a multi-frame reward\nannotator for robotics.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-22 17:59:39", "updated": "2025-05-22 17:59:39", "pdf_url": "http://arxiv.org/pdf/2505.17015v1", "comment": "24 pages. An MLLM, dataset, and benchmark for multi-frame spatial\n  understanding. Project page: https://runsenxu.com/projects/Multi-SpatialMLLM", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17017v1", "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "abstract": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 17:59:49", "updated": "2025-05-22 17:59:49", "pdf_url": "http://arxiv.org/pdf/2505.17017v1", "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17022v1", "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "abstract": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "published": "2025-05-22 17:59:58", "updated": "2025-05-22 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.17022v1", "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "doi": null, "journal_ref": null}
