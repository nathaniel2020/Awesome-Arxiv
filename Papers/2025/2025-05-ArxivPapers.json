{"arxiv_id": "2505.12581v1", "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Lu\u00eds Carbonera"], "abstract": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-19 00:03:57", "updated": "2025-05-19 00:03:57", "pdf_url": "http://arxiv.org/pdf/2505.12581v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12583v1", "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "abstract": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-19 00:11:42", "updated": "2025-05-19 00:11:42", "pdf_url": "http://arxiv.org/pdf/2505.12583v1", "comment": "Accepted to IJCAI 2025 Survey Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12585v1", "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization", "authors": ["En Yu", "Jie Lu", "Xiaoyu Yang", "Guangquan Zhang", "Zhen Fang"], "abstract": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 00:38:18", "updated": "2025-05-19 00:38:18", "pdf_url": "http://arxiv.org/pdf/2505.12585v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12594v1", "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "abstract": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 01:14:57", "updated": "2025-05-19 01:14:57", "pdf_url": "http://arxiv.org/pdf/2505.12594v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12623v1", "title": "Lightweight and Effective Preference Construction in PIBT for Large-Scale Multi-Agent Pathfinding", "authors": ["Keisuke Okumura", "Hiroki Nagai"], "abstract": "PIBT is a computationally lightweight algorithm that can be applied to a\nvariety of multi-agent pathfinding (MAPF) problems, generating the next\ncollision-free locations of agents given another. Because of its simplicity and\nscalability, it is becoming a popular underlying scheme for recent large-scale\nMAPF methods involving several hundreds or thousands of agents. Vanilla PIBT\nmakes agents behave greedily towards their assigned goals, while agents\ntypically have multiple best actions, since the graph shortest path is not\nalways unique. Consequently, tiebreaking about how to choose between these\nactions significantly affects resulting solutions. This paper studies two\nsimple yet effective techniques for tiebreaking in PIBT, without compromising\nits computational advantage. The first technique allows an agent to\nintelligently dodge another, taking into account whether each action will\nhinder the progress of the next timestep. The second technique is to learn,\nthrough multiple PIBT runs, how an action causes regret in others and to use\nthis information to minimise regret collectively. Our empirical results\ndemonstrate that these techniques can reduce the solution cost of one-shot MAPF\nand improve the throughput of lifelong MAPF. For instance, in densely populated\none-shot cases, the combined use of these tiebreaks achieves improvements of\naround 10-20% in sum-of-costs, without significantly compromising the speed of\na PIBT-based planner.", "categories": ["cs.MA", "cs.AI"], "published": "2025-05-19 02:12:29", "updated": "2025-05-19 02:12:29", "pdf_url": "http://arxiv.org/pdf/2505.12623v1", "comment": "To be presented at SoCS-25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12626v1", "title": "scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data", "authors": ["Ping Xu", "Zhiyuan Ning", "Pengjiang Li", "Wenhao Liu", "Pengyang Wang", "Jiaxu Cui", "Yuanchun Zhou", "Pengfei Wang"], "abstract": "Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell\nclustering playing a key role in identifying cell types and marker genes.\nRecent advances, especially graph neural networks (GNNs)-based methods, have\nsignificantly improved clustering performance. However, the analysis of\nscRNA-seq data remains challenging due to noise, sparsity, and high\ndimensionality. Compounding these challenges, GNNs often suffer from\nover-smoothing, limiting their ability to capture complex biological\ninformation. In response, we propose scSiameseClu, a novel Siamese Clustering\nframework for interpreting single-cell RNA-seq data, comprising of 3 key steps:\n(1) Dual Augmentation Module, which applies biologically informed perturbations\nto the gene expression matrix and cell graph relationships to enhance\nrepresentation robustness; (2) Siamese Fusion Module, which combines\ncross-correlation refinement and adaptive information fusion to capture complex\ncellular relationships while mitigating over-smoothing; and (3) Optimal\nTransport Clustering, which utilizes Sinkhorn distance to efficiently align\ncluster assignments with predefined proportions while maintaining balance.\nComprehensive evaluations on seven real-world datasets demonstrate\nthat~\\methodname~outperforms state-of-the-art methods in single-cell\nclustering, cell type annotation, and cell type classification, providing a\npowerful tool for scRNA-seq data interpretation.", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "published": "2025-05-19 02:17:09", "updated": "2025-05-19 02:17:09", "pdf_url": "http://arxiv.org/pdf/2505.12626v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12630v1", "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "abstract": "All-in-one image restoration aims to recover clear images from various\ndegradation types and levels with a unified model. Nonetheless, the significant\nvariations among degradation types present challenges for training a universal\nmodel, often resulting in task interference, where the gradient update\ndirections of different tasks may diverge due to shared parameters. To address\nthis issue, motivated by the routing strategy, we propose DFPIR, a novel\nall-in-one image restorer that introduces Degradation-aware Feature\nPerturbations(DFP) to adjust the feature space to align with the unified\nparameter space. In this paper, the feature perturbations primarily include\nchannel-wise perturbations and attention-wise perturbations. Specifically,\nchannel-wise perturbations are implemented by shuffling the channels in\nhigh-dimensional space guided by degradation types, while attention-wise\nperturbations are achieved through selective masking in the attention space. To\nachieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)\nto implement these two functions, positioned between the encoding and decoding\nstages of the encoder-decoder architecture. Extensive experimental results\ndemonstrate that DFPIR achieves state-of-the-art performance on several\nall-in-one image restoration tasks including image denoising, image dehazing,\nimage deraining, motion deblurring, and low-light image enhancement. Our codes\nare available at https://github.com/TxpHome/DFPIR.", "categories": ["cs.CV", "cs.AI", "I.4.5"], "published": "2025-05-19 02:37:11", "updated": "2025-05-19 02:37:11", "pdf_url": "http://arxiv.org/pdf/2505.12630v1", "comment": "Accepted to CVPR 2025. 8 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12632v1", "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 02:39:03", "updated": "2025-05-19 02:39:03", "pdf_url": "http://arxiv.org/pdf/2505.12632v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12638v1", "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "abstract": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome.", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.LG"], "published": "2025-05-19 02:45:42", "updated": "2025-05-19 02:45:42", "pdf_url": "http://arxiv.org/pdf/2505.12638v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12641v1", "title": "Single Image Reflection Removal via inter-layer Complementarity", "authors": ["Yue Huang", "Zi'ang Li", "Tianle Hu", "Jie Wen", "Guanbin Li", "Jinglin Zhang", "Guoxu Zhou", "Xiaozhao Fang"], "abstract": "Although dual-stream architectures have achieved remarkable success in single\nimage reflection removal, they fail to fully exploit inter-layer\ncomplementarity in their physical modeling and network design, which limits the\nquality of image separation. To address this fundamental limitation, we propose\ntwo targeted improvements to enhance dual-stream architectures: First, we\nintroduce a novel inter-layer complementarity model where low-frequency\ncomponents extracted from the residual layer interact with the transmission\nlayer through dual-stream architecture to enhance inter-layer complementarity.\nMeanwhile, high-frequency components from the residual layer provide inverse\nmodulation to both streams, improving the detail quality of the transmission\nlayer. Second, we propose an efficient inter-layer complementarity attention\nmechanism which first cross-reorganizes dual streams at the channel level to\nobtain reorganized streams with inter-layer complementary structures, then\nperforms attention computation on the reorganized streams to achieve better\ninter-layer separation, and finally restores the original stream structure for\noutput. Experimental results demonstrate that our method achieves\nstate-of-the-art separation quality on multiple public datasets while\nsignificantly reducing both computational cost and model complexity.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 02:50:15", "updated": "2025-05-19 02:50:15", "pdf_url": "http://arxiv.org/pdf/2505.12641v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12650v1", "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "abstract": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 03:04:50", "updated": "2025-05-19 03:04:50", "pdf_url": "http://arxiv.org/pdf/2505.12650v1", "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12651v1", "title": "$\\texttt{DIAMONDs}$: A Dataset for $\\mathbb{D}$ynamic $\\mathbb{I}$nformation $\\mathbb{A}$nd $\\mathbb{M}$ental modeling $\\mathbb{O}$f $\\mathbb{N}$umeric $\\mathbb{D}$iscussions", "authors": ["Sayontan Ghosh", "Mahnaz Koupaee", "Yash Kumar Lal", "Pegah Alipoormolabashi", "Mohammad Saqib Hasan", "Jun Seok Kang", "Niranjan Balasubramanian"], "abstract": "Understanding multiparty conversations demands robust Theory of Mind (ToM)\ncapabilities, including the ability to track dynamic information, manage\nknowledge asymmetries, and distinguish relevant information across extended\nexchanges. To advance ToM evaluation in such settings, we present a carefully\ndesigned scalable methodology for generating high-quality benchmark\nconversation-question pairs with these characteristics. Using this methodology,\nwe create $\\texttt{DIAMONDs}$, a new conversational QA dataset covering common\nbusiness, financial or other group interactions. In these goal-oriented\nconversations, participants often have to track certain numerical quantities\n(say $\\textit{expected profit}$) of interest that can be derived from other\nvariable quantities (like $\\textit{marketing expenses, expected sales,\nsalary}$, etc.), whose values also change over the course of the conversation.\n$\\texttt{DIAMONDs}$ questions pose simple numerical reasoning problems over\nsuch quantities of interest (e.g., $\\textit{funds required for charity events,\nexpected company profit next quarter}$, etc.) in the context of the information\nexchanged in conversations. This allows for precisely evaluating ToM\ncapabilities for carefully tracking and reasoning over participants' knowledge\nstates.\n  Our evaluation of state-of-the-art language models reveals significant\nchallenges in handling participant-centric reasoning, specifically in\nsituations where participants have false beliefs. Models also struggle with\nconversations containing distractors and show limited ability to identify\nscenarios with insufficient information. These findings highlight current\nmodels' ToM limitations in handling real-world multi-party conversations.", "categories": ["cs.AI"], "published": "2025-05-19 03:05:13", "updated": "2025-05-19 03:05:13", "pdf_url": "http://arxiv.org/pdf/2505.12651v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12654v1", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "abstract": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:08:30", "updated": "2025-05-19 03:08:30", "pdf_url": "http://arxiv.org/pdf/2505.12654v1", "comment": "Accepected by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12655v1", "title": "Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models", "authors": ["Yisheng Zhong", "Yizhu Wen", "Junfeng Guo", "Mehran Kafai", "Heng Huang", "Hanqing Guo", "Zhuangdi Zhu"], "abstract": "Protecting cyber Intellectual Property (IP) such as web content is an\nincreasingly critical concern. The rise of large language models (LLMs) with\nonline retrieval capabilities presents a double-edged sword that enables\nconvenient access to information but often undermines the rights of original\ncontent creators. As users increasingly rely on LLM-generated responses, they\ngradually diminish direct engagement with original information sources,\nsignificantly reducing the incentives for IP creators to contribute, and\nleading to a saturating cyberspace with more AI-generated content. In response,\nwe propose a novel defense framework that empowers web content creators to\nsafeguard their web-based IP from unauthorized LLM real-time extraction by\nleveraging the semantic understanding capability of LLMs themselves. Our method\nfollows principled motivations and effectively addresses an intractable\nblack-box optimization problem. Real-world experiments demonstrated that our\nmethods improve defense success rates from 2.5% to 88.6% on different LLMs,\noutperforming traditional defenses such as configuration-based restrictions.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 03:14:08", "updated": "2025-05-19 03:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12655v1", "comment": "13 pages, 13 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12662v1", "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "abstract": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:25:18", "updated": "2025-05-19 03:25:18", "pdf_url": "http://arxiv.org/pdf/2505.12662v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12664v1", "title": "Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design", "authors": ["Ziqing Xing", "Zhaoyang Zhang", "Zirui Chen", "Hongning Ruan", "Zhaohui Yang"], "abstract": "In this paper, we incorporate physical knowledge into learning-based\nhigh-precision target sensing using the multi-view channel state information\n(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kind\nof multi-view sensing problem can be naturally cast into a conditional\ngeneration framework. To this end, we design a bipartite neural network\narchitecture, the first part of which uses an elaborately designed encoder to\nfuse the latent target features embedded in the multi-view CSI, and then the\nsecond uses them as conditioning inputs of a powerful generative model to guide\nthe target's reconstruction. Specifically, the encoder is designed to capture\nthe physical correlation between the CSI and the target, and also be adaptive\nto the numbers and positions of BS-UE pairs. Therein the view-specific nature\nof CSI is assimilated by introducing a spatial positional embedding scheme,\nwhich exploits the structure of electromagnetic(EM)-wave propagation channels.\nFinally, a conditional diffusion model with a weighted loss is employed to\ngenerate the target's point cloud from the fused features. Extensive numerical\nresults demonstrate that the proposed generative multi-view (Gen-MV) sensing\nframework exhibits excellent flexibility and significant performance\nimprovement on the reconstruction quality of target's shape and EM properties.", "categories": ["eess.SP", "cs.AI", "cs.LG"], "published": "2025-05-19 03:27:24", "updated": "2025-05-19 03:27:24", "pdf_url": "http://arxiv.org/pdf/2505.12664v1", "comment": "submitted to IEEE Transactions on Wireless Communications", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12669v1", "title": "Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment", "authors": ["Abhinaba Roy", "Geeta Puri", "Dorien Herremans"], "abstract": "We present Text2midi-InferAlign, a novel technique for improving symbolic\nmusic generation at inference time. Our method leverages text-to-audio\nalignment and music structural alignment rewards during inference to encourage\nthe generated music to be consistent with the input caption. Specifically, we\nintroduce two objectives scores: a text-audio consistency score that measures\nrhythmic alignment between the generated music and the original text caption,\nand a harmonic consistency score that penalizes generated music containing\nnotes inconsistent with the key. By optimizing these alignment-based objectives\nduring the generation process, our model produces symbolic music that is more\nclosely tied to the input captions, thereby improving the overall quality and\ncoherence of the generated compositions. Our approach can extend any existing\nautoregressive model without requiring further training or fine-tuning. We\nevaluate our work on top of Text2midi - an existing text-to-midi generation\nmodel, demonstrating significant improvements in both objective and subjective\nevaluation metrics.", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68T07", "I.2.1"], "published": "2025-05-19 03:36:06", "updated": "2025-05-19 03:36:06", "pdf_url": "http://arxiv.org/pdf/2505.12669v1", "comment": "7 pages, 1 figure, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12680v1", "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "abstract": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 03:56:05", "updated": "2025-05-19 03:56:05", "pdf_url": "http://arxiv.org/pdf/2505.12680v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12684v1", "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "abstract": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants.", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.SI"], "published": "2025-05-19 04:06:32", "updated": "2025-05-19 04:06:32", "pdf_url": "http://arxiv.org/pdf/2505.12684v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12692v1", "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "abstract": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 04:32:02", "updated": "2025-05-19 04:32:02", "pdf_url": "http://arxiv.org/pdf/2505.12692v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12701v1", "title": "Counterfactual Explanations for Continuous Action Reinforcement Learning", "authors": ["Shuyang Dong", "Shangtong Zhang", "Lu Feng"], "abstract": "Reinforcement Learning (RL) has shown great promise in domains like\nhealthcare and robotics but often struggles with adoption due to its lack of\ninterpretability. Counterfactual explanations, which address \"what if\"\nscenarios, provide a promising avenue for understanding RL decisions but remain\nunderexplored for continuous action spaces. We propose a novel approach for\ngenerating counterfactual explanations in continuous action RL by computing\nalternative action sequences that improve outcomes while minimizing deviations\nfrom the original sequence. Our approach leverages a distance metric for\ncontinuous actions and accounts for constraints such as adhering to predefined\npolicies in specific states. Evaluations in two RL domains, Diabetes Control\nand Lunar Lander, demonstrate the effectiveness, efficiency, and generalization\nof our approach, enabling more interpretable and trustworthy RL applications.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 04:41:54", "updated": "2025-05-19 04:41:54", "pdf_url": "http://arxiv.org/pdf/2505.12701v1", "comment": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12705v1", "title": "DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories", "authors": ["Joel Jang", "Seonghyeon Ye", "Zongyu Lin", "Jiannan Xiang", "Johan Bjorck", "Yu Fang", "Fengyuan Hu", "Spencer Huang", "Kaushil Kundalia", "Yen-Chen Lin", "Loic Magne", "Ajay Mandlekar", "Avnish Narayan", "You Liang Tan", "Guanzhi Wang", "Jing Wang", "Qi Wang", "Yinzhen Xu", "Xiaohui Zeng", "Kaiyuan Zheng", "Ruijie Zheng", "Ming-Yu Liu", "Luke Zettlemoyer", "Dieter Fox", "Jan Kautz", "Scott Reed", "Yuke Zhu", "Linxi Fan"], "abstract": "We introduce DreamGen, a simple yet highly effective 4-stage pipeline for\ntraining robot policies that generalize across behaviors and environments\nthrough neural trajectories - synthetic robot data generated from video world\nmodels. DreamGen leverages state-of-the-art image-to-video generative models,\nadapting them to the target robot embodiment to produce photorealistic\nsynthetic videos of familiar or novel tasks in diverse environments. Since\nthese models generate only videos, we recover pseudo-action sequences using\neither a latent action model or an inverse-dynamics model (IDM). Despite its\nsimplicity, DreamGen unlocks strong behavior and environment generalization: a\nhumanoid robot can perform 22 new behaviors in both seen and unseen\nenvironments, while requiring teleoperation data from only a single\npick-and-place task in one environment. To evaluate the pipeline\nsystematically, we introduce DreamGen Bench, a video generation benchmark that\nshows a strong correlation between benchmark performance and downstream policy\nsuccess. Our work establishes a promising new axis for scaling robot learning\nwell beyond manual data collection.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-19 04:55:39", "updated": "2025-05-19 04:55:39", "pdf_url": "http://arxiv.org/pdf/2505.12705v1", "comment": "See website for videos:\n  https://research.nvidia.com/labs/gear/dreamgen", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12707v1", "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI", "authors": ["Yingchen He", "Christian D. Weilbach", "Martyna E. Wojciechowska", "Yuxuan Zhang", "Frank Wood"], "abstract": "Advances in deep generative modelling have made it increasingly plausible to\ntrain human-level embodied agents. Yet progress has been limited by the absence\nof large-scale, real-time, multi-modal, and socially interactive datasets that\nreflect the sensory-motor complexity of natural environments. To address this,\nwe present PLAICraft, a novel data collection platform and dataset capturing\nmultiplayer Minecraft interactions across five time-aligned modalities: video,\ngame output audio, microphone input audio, mouse, and keyboard actions. Each\nmodality is logged with millisecond time precision, enabling the study of\nsynchronous, embodied behaviour in a rich, open-ended world. The dataset\ncomprises over 10,000 hours of gameplay from more than 10,000 global\nparticipants.\\footnote{We have done a privacy review for the public release of\nan initial 200-hour subset of the dataset, with plans to release most of the\ndataset over time.} Alongside the dataset, we provide an evaluation suite for\nbenchmarking model capabilities in object recognition, spatial awareness,\nlanguage grounding, and long-term memory. PLAICraft opens a path toward\ntraining and evaluating agents that act fluently and purposefully in real time,\npaving the way for truly embodied artificial intelligence.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "published": "2025-05-19 05:00:47", "updated": "2025-05-19 05:00:47", "pdf_url": "http://arxiv.org/pdf/2505.12707v1", "comment": "9 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12711v1", "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining", "authors": ["Qichen Sun", "Zhengrui Guo", "Rui Peng", "Hao Chen", "Jinzhuo Wang"], "abstract": "Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 05:07:34", "updated": "2025-05-19 05:07:34", "pdf_url": "http://arxiv.org/pdf/2505.12711v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12716v1", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "abstract": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 05:16:21", "updated": "2025-05-19 05:16:21", "pdf_url": "http://arxiv.org/pdf/2505.12716v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12731v1", "title": "Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps", "authors": ["Jie Ou", "Jinyu Guo", "Shuaihong Jiang", "Zhaokun Wang", "Libo Qin", "Shunyu Yao", "Wenhong Tian"], "abstract": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.", "categories": ["cs.AI"], "published": "2025-05-19 05:39:38", "updated": "2025-05-19 05:39:38", "pdf_url": "http://arxiv.org/pdf/2505.12731v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12734v1", "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "abstract": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences.", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "published": "2025-05-19 05:47:13", "updated": "2025-05-19 05:47:13", "pdf_url": "http://arxiv.org/pdf/2505.12734v1", "comment": "14 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12737v1", "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning", "authors": ["Hongjoon Ahn", "Heewoong Choi", "Jisu Han", "Taesup Moon"], "abstract": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical\nlearning paradigm where goal-reaching policies are trained from abundant\nunlabeled (reward-free) datasets without additional environment interaction.\nHowever, offline GCRL still struggles with long-horizon tasks, even with recent\nadvances that employ hierarchical policy structures, such as HIQL. By\nidentifying the root cause of this challenge, we observe the following\ninsights: First, performance bottlenecks mainly stem from the high-level\npolicy's inability to generate appropriate subgoals. Second, when learning the\nhigh-level policy in the long-horizon regime, the sign of the advantage signal\nfrequently becomes incorrect. Thus, we argue that improving the value function\nto produce a clear advantage signal for learning the high-level policy is\nessential. In this paper, we propose a simple yet effective solution:\nOption-aware Temporally Abstracted value learning, dubbed OTA, which\nincorporates temporal abstraction into the temporal-difference learning\nprocess. By modifying the value update to be option-aware, the proposed\nlearning scheme contracts the effective horizon length, enabling better\nadvantage estimates even in long-horizon regimes. We experimentally show that\nthe high-level policy extracted using the OTA value function achieves strong\nperformance on complex tasks from OGBench, a recently proposed offline GCRL\nbenchmark, including maze navigation and visual robotic manipulation\nenvironments.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 05:51:11", "updated": "2025-05-19 05:51:11", "pdf_url": "http://arxiv.org/pdf/2505.12737v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12738v1", "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "abstract": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs.", "categories": ["cs.LG", "cs.AI", "cs.SI"], "published": "2025-05-19 05:53:25", "updated": "2025-05-19 05:53:25", "pdf_url": "http://arxiv.org/pdf/2505.12738v1", "comment": "18 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12741v1", "title": "Dense Communication between Language Models", "authors": ["Shiguang Wu", "Yaqing Wang", "Quanming Yao"], "abstract": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility.", "categories": ["cs.AI"], "published": "2025-05-19 05:56:06", "updated": "2025-05-19 05:56:06", "pdf_url": "http://arxiv.org/pdf/2505.12741v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12744v1", "title": "Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation", "authors": ["Weiliang Tang", "Dong Jing", "Jia-Hui Pan", "Zhiwu Lu", "Yun-Hui Liu", "Li Erran Li", "Mingyu Ding", "Chi-Wing Fu"], "abstract": "Recent Large Multimodal Models have demonstrated remarkable reasoning\ncapabilities, especially in solving complex mathematical problems and realizing\naccurate spatial perception. Our key insight is that these emerging abilities\ncan naturally extend to robotic manipulation by enabling LMMs to directly infer\nthe next goal in language via reasoning, rather than relying on a separate\naction head. However, this paradigm meets two main challenges: i) How to make\nLMMs understand the spatial action space, and ii) How to fully exploit the\nreasoning capacity of LMMs in solving these tasks. To tackle the former\nchallenge, we propose a novel task formulation, which inputs the current states\nof object parts and the gripper, and reformulates rotation by a new axis\nrepresentation instead of traditional Euler angles. This representation is more\ncompatible with spatial reasoning and easier to interpret within a unified\nlanguage space. For the latter challenge, we design a pipeline to utilize\ncutting-edge LMMs to generate a small but high-quality reasoning dataset of\nmulti-round dialogues that successfully solve manipulation tasks for supervised\nfine-tuning. Then, we perform reinforcement learning by trial-and-error\ninteractions in simulation to further enhance the model's reasoning abilities\nfor robotic manipulation. Our resulting reasoning model built upon a 7B\nbackbone, named ReasonManip, demonstrates three notable advantages driven by\nits system-2 level reasoning capabilities: i) exceptional generalizability to\nout-of-distribution environments, objects, and tasks; ii) inherent sim-to-real\ntransfer ability enabled by the unified language representation shared across\ndomains; iii) transparent interpretability connecting high-level reasoning and\nlow-level control. Extensive experiments demonstrate the effectiveness of the\nproposed paradigm and its potential to advance LMM-driven robotic manipulation.", "categories": ["cs.AI"], "published": "2025-05-19 06:00:14", "updated": "2025-05-19 06:00:14", "pdf_url": "http://arxiv.org/pdf/2505.12744v1", "comment": "17 pages, 16 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12745v1", "title": "PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization", "authors": ["Dong Kyu Cho", "Inwoo Hwang", "Sanghack Lee"], "abstract": "Data augmentation is a popular tool for single source domain generalization,\nwhich expands the source domain by generating simulated ones, improving\ngeneralization on unseen target domains. In this work, we show that the\nperformance of such augmentation-based methods in the target domains\nuniversally fluctuates during training, posing challenges in model selection\nunder realistic scenarios. We argue that the fluctuation stems from the\ninability of the model to accumulate the knowledge learned from diverse\naugmentations, exacerbating feature distortion during training. Based on this\nobservation, we propose a novel generalization method, coined Parameter-Space\nEnsemble with Entropy Regularization (PEER), that uses a proxy model to learn\nthe augmented data on behalf of the main model. The main model is updated by\naveraging its parameters with the proxy model, progressively accumulating\nknowledge over the training steps. Maximizing the mutual information between\nthe output representations of the two models guides the learning process of the\nproxy model, mitigating feature distortion during training. Experimental\nresults demonstrate the effectiveness of PEER in reducing the OOD performance\nfluctuation and enhancing generalization across various datasets, including\nPACS, Digits, Office-Home, and VLCS. Notably, our method with simple random\naugmentation achieves state-of-the-art performance, surpassing prior approaches\non sDG that utilize complex data augmentation strategies.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 06:01:11", "updated": "2025-05-19 06:01:11", "pdf_url": "http://arxiv.org/pdf/2505.12745v1", "comment": "21 pages, 9 figures, Accepted at CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12746v1", "title": "Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs", "authors": ["Haruka Asanuma", "Naoko Koide-Majima", "Ken Nakamura", "Takato Horii", "Shinji Nishimoto", "Masafumi Oizumi"], "abstract": "Recent studies have revealed that human emotions exhibit a high-dimensional,\ncomplex structure. A full capturing of this complexity requires new approaches,\nas conventional models that disregard high dimensionality risk overlooking key\nnuances of human emotions. Here, we examined the extent to which the latest\ngeneration of rapidly evolving Multimodal Large Language Models (MLLMs) capture\nthese high-dimensional, intricate emotion structures, including capabilities\nand limitations. Specifically, we compared self-reported emotion ratings from\nparticipants watching videos with model-generated estimates (e.g., Gemini or\nGPT). We evaluated performance not only at the individual video level but also\nfrom emotion structures that account for inter-video relationships. At the\nlevel of simple correlation between emotion structures, our results\ndemonstrated strong similarity between human and model-inferred emotion\nstructures. To further explore whether the similarity between humans and models\nis at the signle item level or the coarse-categorical level, we applied Gromov\nWasserstein Optimal Transport. We found that although performance was not\nnecessarily high at the strict, single-item level, performance across video\ncategories that elicit similar emotions was substantial, indicating that the\nmodel could infer human emotional experiences at the category level. Our\nresults suggest that current state-of-the-art MLLMs broadly capture the complex\nhigh-dimensional emotion structures at the category level, as well as their\napparent limitations in accurately capturing entire structures at the\nsingle-item level.", "categories": ["cs.AI", "I.2.7; I.2.10; I.5.1"], "published": "2025-05-19 06:03:22", "updated": "2025-05-19 06:03:22", "pdf_url": "http://arxiv.org/pdf/2505.12746v1", "comment": "25 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12748v1", "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "abstract": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-05-19 06:08:53", "updated": "2025-05-19 06:08:53", "pdf_url": "http://arxiv.org/pdf/2505.12748v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12750v1", "title": "Malware families discovery via Open-Set Recognition on Android manifest permissions", "authors": ["Filippo Leveni", "Matteo Mistura", "Francesco Iubatti", "Carmine Giangregorio", "Nicol\u00f2 Pastore", "Cesare Alippi", "Giacomo Boracchi"], "abstract": "Malware are malicious programs that are grouped into families based on their\npenetration technique, source code, and other characteristics. Classifying\nmalware programs into their respective families is essential for building\neffective defenses against cyber threats. Machine learning models have a huge\npotential in malware detection on mobile devices, as malware families can be\nrecognized by classifying permission data extracted from Android manifest\nfiles. Still, the malware classification task is challenging due to the\nhigh-dimensional nature of permission data and the limited availability of\ntraining samples. In particular, the steady emergence of new malware families\nmakes it impossible to acquire a comprehensive training set covering all the\nmalware classes. In this work, we present a malware classification system that,\non top of classifying known malware, detects new ones. In particular, we\ncombine an open-set recognition technique developed within the computer vision\ncommunity, namely MaxLogit, with a tree-based Gradient Boosting classifier,\nwhich is particularly effective in classifying high-dimensional data. Our\nsolution turns out to be very practical, as it can be seamlessly employed in a\nstandard classification workflow, and efficient, as it adds minimal\ncomputational overhead. Experiments on public and proprietary datasets\ndemonstrate the potential of our solution, which has been deployed in a\nbusiness environment.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "published": "2025-05-19 06:19:54", "updated": "2025-05-19 06:19:54", "pdf_url": "http://arxiv.org/pdf/2505.12750v1", "comment": "Submitted to European Conference on Artificial Intelligence (ECAI\n  2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12751v1", "title": "Structure-based Anomaly Detection and Clustering", "authors": ["Filippo Leveni"], "abstract": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system.", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "published": "2025-05-19 06:20:00", "updated": "2025-05-19 06:20:00", "pdf_url": "http://arxiv.org/pdf/2505.12751v1", "comment": "Doctoral dissertation at Politecnico di Milano", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12761v1", "title": "Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding", "authors": ["Donghwa Shin", "Edwin Zhang"], "abstract": "Transformers have recently gained popularity in time series forecasting due\nto their ability to capture long-term dependencies. However, many existing\nmodels focus only on capturing temporal dependencies while omitting intricate\nrelationships between variables. Recent models have tried tackling this by\nexplicitly modeling both cross-time and cross-variate dependencies through a\nsequential or unified attention mechanism, but they are entirely channel\ndependent (CD) across all layers, making them potentially susceptible to\noverfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),\na lightweight CD module that injects cross-variate context into\nchannel-independent (CI) models by simply modifying the patch embedding\nprocess. We achieve this by adding a learnable positional encoding and a\nlightweight router-attention block to the vanilla patch embedding layer. We\nthen integrate CVPE into Time-LLM, a multimodal CI forecasting model, to\ndemonstrate its effectiveness in capturing cross-variate dependencies and\nenhance the CI model's performance. Extensive experimental results on seven\nreal-world datasets show that our enhanced Time-LLM outperforms the original\nbaseline model simply by incorporating the CVPE module, with no other changes.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 06:41:14", "updated": "2025-05-19 06:41:14", "pdf_url": "http://arxiv.org/pdf/2505.12761v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12762v1", "title": "IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment", "authors": ["Chenlin Ming", "Chendi Qu", "Mengzhang Cai", "Qizhi Pei", "Zhuoshi Pan", "Yu Li", "Xiaoming Duan", "Lijun Wu", "Conghui He"], "abstract": "Large Language Models (LLMs) have achieved impressive performance through\nSupervised Fine-tuning (SFT) on diverse instructional datasets. When training\non multiple capabilities simultaneously, the mixture training dataset, governed\nby volumes of data from different domains, is a critical factor that directly\nimpacts the final model's performance. Unlike many studies that focus on\nenhancing the quality of training datasets through data selection methods, few\nworks explore the intricate relationship between the compositional quantity of\nmixture training datasets and the emergent capabilities of LLMs. Given the\navailability of a high-quality multi-domain training dataset, understanding the\nimpact of data from each domain on the model's overall capabilities is crucial\nfor preparing SFT data and training a well-balanced model that performs\neffectively across diverse domains. In this work, we introduce IDEAL, an\ninnovative data equilibrium adaptation framework designed to effectively\noptimize volumes of data from different domains within mixture SFT datasets,\nthereby enhancing the model's alignment and performance across multiple\ncapabilities. IDEAL employs a gradient-based approach to iteratively refine the\ntraining data distribution, dynamically adjusting the volumes of\ndomain-specific data based on their impact on downstream task performance. By\nleveraging this adaptive mechanism, IDEAL ensures a balanced dataset\ncomposition, enabling the model to achieve robust generalization and consistent\nproficiency across diverse tasks. Experiments across different capabilities\ndemonstrate that IDEAL outperforms conventional uniform data allocation\nstrategies, achieving a comprehensive improvement of approximately 7% in\nmulti-task evaluation scores.", "categories": ["cs.AI"], "published": "2025-05-19 06:42:44", "updated": "2025-05-19 06:42:44", "pdf_url": "http://arxiv.org/pdf/2505.12762v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12763v1", "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "abstract": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 06:43:08", "updated": "2025-05-19 06:43:08", "pdf_url": "http://arxiv.org/pdf/2505.12763v1", "comment": "Accepted to ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12767v1", "title": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates", "authors": ["Danqing Chen", "Tobias Ladner", "Ahmed Rayen Mhadhbi", "Matthias Althoff"], "abstract": "As large language models become integral to high-stakes applications,\nensuring their robustness and fairness is critical. Despite their success,\nlarge language models remain vulnerable to adversarial attacks, where small\nperturbations, such as synonym substitutions, can alter model predictions,\nposing risks in fairness-critical areas, such as gender bias mitigation, and\nsafety-critical areas, such as toxicity detection. While formal verification\nhas been explored for neural networks, its application to large language models\nremains limited. This work presents a holistic verification framework to\ncertify the robustness of transformer-based language models, with a focus on\nensuring gender fairness and consistent outputs across different gender-related\nterms. Furthermore, we extend this methodology to toxicity detection, offering\nformal guarantees that adversarially manipulated toxic inputs are consistently\ndetected and appropriately censored, thereby ensuring the reliability of\nmoderation systems. By formalizing robustness within the embedding space, this\nwork strengthens the reliability of language models in ethical AI deployment\nand content moderation.", "categories": ["cs.AI"], "published": "2025-05-19 06:46:17", "updated": "2025-05-19 06:46:17", "pdf_url": "http://arxiv.org/pdf/2505.12767v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12774v1", "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "abstract": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation.", "categories": ["cs.GR", "cs.AI", "cs.CV"], "published": "2025-05-19 07:02:12", "updated": "2025-05-19 07:02:12", "pdf_url": "http://arxiv.org/pdf/2505.12774v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12781v1", "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:10:42", "updated": "2025-05-19 07:10:42", "pdf_url": "http://arxiv.org/pdf/2505.12781v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12788v1", "title": "Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs", "authors": ["Zhongni Hou", "Miao Su", "Xiaolong Jin", "Zixuan Li", "Long Bai", "Jiafeng Guo", "Xueqi Cheng"], "abstract": "Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of\n(subject, predicate, object, timestamp) to describe temporal facts, have\nattracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditional\nTKGs by utilizing n-tuples to incorporate auxiliary elements alongside core\nelements (i.e., subject, predicate, and object) of facts, so as to represent\nthem in a more fine-grained manner. Reasoning over N-TKGs aims to predict\npotential future facts based on historical ones. However, existing N-TKG\nreasoning methods often lack explainability due to their black-box nature.\nTherefore, we introduce a new Reinforcement Learning-based method, named\nMT-Path, which leverages the temporal information to traverse historical\nn-tuples and construct a temporal reasoning path. Specifically, in order to\nintegrate the information encapsulated within n-tuples, i.e., the\nentity-irrelevant information within the predicate, the information about core\nelements, and the complete information about the entire n-tuples, MT-Path\nutilizes a mixture policy-driven action selector, which bases on three\nlow-level policies, namely, the predicate-focused policy, the\ncore-element-focused policy and the whole-fact-focused policy. Further, MT-Path\nutilizes an auxiliary element-aware GCN to capture the rich semantic\ndependencies among facts, thereby enabling the agent to gain a deep\nunderstanding of each n-tuple. Experimental results demonstrate the\neffectiveness and the explainability of MT-Path.", "categories": ["cs.AI"], "published": "2025-05-19 07:20:33", "updated": "2025-05-19 07:20:33", "pdf_url": "http://arxiv.org/pdf/2505.12788v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12795v1", "title": "FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities", "authors": ["Shibo Hong", "Jiahao Ying", "Haiyuan Liang", "Mengdi Zhang", "Jun Kuang", "Jiazheng Zhang", "Yixin Cao"], "abstract": "Evaluating the open-ended outputs of large language models (LLMs) has become\na bottleneck as model capabilities, task diversity, and modality coverage\nrapidly expand. Existing \"LLM-as-a-Judge\" evaluators are typically narrow in a\nfew tasks, aspects, or modalities, and easily suffer from low consistency. In\nthis paper, we argue that explicit, fine-grained aspect specification is the\nkey to both generalizability and objectivity in automated evaluation. To do so,\nwe introduce a hierarchical aspect taxonomy spanning 112 aspects that unifies\nevaluation across four representative settings - Natural Language Generation,\nImage Understanding, Image Generation, and Interleaved Text-and-Image\nGeneration. Building on this taxonomy, we create FRAbench, a benchmark\ncomprising 60.4k pairwise samples with 325k aspect-level labels obtained from a\ncombination of human and LLM annotations. FRAbench provides the first\nlarge-scale, multi-modal resource for training and meta-evaluating fine-grained\nLMM judges. Leveraging FRAbench, we develop GenEval, a fine-grained evaluator\ngeneralizable across tasks and modalities. Experiments show that GenEval (i)\nattains high agreement with GPT-4o and expert annotators, (ii) transfers\nrobustly to unseen tasks and modalities, and (iii) reveals systematic\nweaknesses of current LMMs on evaluation.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-19 07:29:26", "updated": "2025-05-19 07:29:26", "pdf_url": "http://arxiv.org/pdf/2505.12795v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12800v1", "title": "OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching", "authors": ["Hieu-Nghia Huynh-Nguyen", "Ngoc Son Nguyen", "Huynh Nguyen Dang", "Thieu Vo", "Truong-Son Hy", "Van Nguyen"], "abstract": "Text-to-speech (TTS) systems have seen significant advancements in recent\nyears, driven by improvements in deep learning and neural network\narchitectures. Viewing the output speech as a data distribution, previous\napproaches often employ traditional speech representations, such as waveforms\nor spectrograms, within the Flow Matching framework. However, these methods\nhave limitations, including overlooking various speech attributes and incurring\nhigh computational costs due to additional constraints introduced during\ntraining. To address these challenges, we introduce OZSpeech, the first TTS\nmethod to explore optimal transport conditional flow matching with one-step\nsampling and a learned prior as the condition, effectively disregarding\npreceding states and reducing the number of sampling steps. Our approach\noperates on disentangled, factorized components of speech in token format,\nenabling accurate modeling of each speech attribute, which enhances the TTS\nsystem's ability to precisely clone the prompt speech. Experimental results\nshow that our method achieves promising performance over existing methods in\ncontent accuracy, naturalness, prosody generation, and speaker style\npreservation. Audio samples are available at our demo page\nhttps://ozspeech.github.io/OZSpeech_Web/.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 07:31:55", "updated": "2025-05-19 07:31:55", "pdf_url": "http://arxiv.org/pdf/2505.12800v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12805v1", "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA", "authors": ["Seanie Lee", "Sangwoo Park", "Dong Bok Lee", "Dominik Wagner", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "abstract": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable\nlow-rank matrices into frozen pre-trained weights, is widely used for efficient\nfine-tuning of language models in federated learning (FL). However, when\ncombined with differentially private stochastic gradient descent (DP-SGD), LoRA\nfaces substantial noise amplification: DP-SGD perturbs per-sample gradients,\nand the matrix multiplication of the LoRA update ($BA$) intensifies this\neffect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model\nexpressiveness, often resulting in suboptimal adaptation. To address this, we\npropose FedSVD, a simple yet effective method that introduces a global\nreparameterization based on singular value decomposition (SVD). In our\napproach, each client optimizes only the $B$ matrix and transmits it to the\nserver. The server aggregates the $B$ matrices, computes the product $BA$ using\nthe previous $A$, and refactorizes the result via SVD. This yields a new\nadaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an\nupdated $B$ containing the remaining SVD components. This reparameterization\navoids quadratic noise amplification, while allowing $A$ to better capture the\nprincipal directions of the aggregate updates. Moreover, the orthonormal\nstructure of $A$ bounds the gradient norms of $B$ and preserves more signal\nunder DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD\nconsistently improves stability and performance across a variety of privacy\nsettings and benchmarks, outperforming relevant baselines under both private\nand non-private regimes.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 07:32:56", "updated": "2025-05-19 07:32:56", "pdf_url": "http://arxiv.org/pdf/2505.12805v1", "comment": "preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12811v1", "title": "Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning", "authors": ["Wei-Chen Liao", "Ti-Rong Wu", "I-Chen Wu"], "abstract": "Multi-agent reinforcement Learning (MARL) is often challenged by the sight\nrange dilemma, where agents either receive insufficient or excessive\ninformation from their environment. In this paper, we propose a novel method,\ncalled Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes\nan Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight\nrange during training. Experiment results show several advantages of using DSR.\nFirst, we demonstrate using DSR achieves better performance in three common\nMARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse\n(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show\nthat DSR consistently improves performance across multiple MARL algorithms,\nincluding QMIX and MAPPO. Third, DSR offers suitable sight ranges for different\ntraining steps, thereby accelerating the training process. Finally, DSR\nprovides additional interpretability by indicating the optimal sight range used\nduring training. Unlike existing methods that rely on global information or\ncommunication mechanisms, our approach operates solely based on the individual\nsight ranges of agents. This approach offers a practical and efficient solution\nto the sight range dilemma, making it broadly applicable to real-world complex\nenvironments.", "categories": ["cs.MA", "cs.AI", "cs.LG"], "published": "2025-05-19 07:40:42", "updated": "2025-05-19 07:40:42", "pdf_url": "http://arxiv.org/pdf/2505.12811v1", "comment": "Accepted at AAMAS 2025. The compiled PDF includes the appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12814v1", "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "abstract": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:45:09", "updated": "2025-05-19 07:45:09", "pdf_url": "http://arxiv.org/pdf/2505.12814v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12815v1", "title": "Learning in Chaos: Efficient Autoscaling and Self-healing for Distributed Training at the Edge", "authors": ["Wenjiao Feng", "Rongxing Xiao", "Zonghang Li", "Hongfang Yu", "Gang Sun", "Long Luo", "Mohsen Guizani", "Qirong Ho"], "abstract": "Frequent node and link changes in edge AI clusters disrupt distributed\ntraining, while traditional checkpoint-based recovery and cloud-centric\nautoscaling are too slow for scale-out and ill-suited to chaotic and\nself-governed edge. This paper proposes Chaos, a resilient and scalable edge\ndistributed training system with built-in self-healing and autoscaling. It\nspeeds up scale-out by using multi-neighbor replication with fast shard\nscheduling, allowing a new node to pull the latest training state from nearby\nneighbors in parallel while balancing the traffic load between them. It also\nuses a cluster monitor to track resource and topology changes to assist\nscheduler decisions, and handles scaling events through peer negotiation\nprotocols, enabling fully self-governed autoscaling without a central admin.\nExtensive experiments show that Chaos consistently achieves much lower\nscale-out delays than Pollux, EDL, and Autoscaling, and handles scale-in,\nconnect-link, and disconnect-link events within 1 millisecond, making it\nsmoother to handle node joins, exits, and failures. It also delivers the lowest\nidle time, showing superior resource use and scalability as the cluster grows.", "categories": ["cs.DC", "cs.AI", "68T99", "I.2.11"], "published": "2025-05-19 07:52:17", "updated": "2025-05-19 07:52:17", "pdf_url": "http://arxiv.org/pdf/2505.12815v1", "comment": "13 pages, 16 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12821v1", "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "abstract": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:03:38", "updated": "2025-05-19 08:03:38", "pdf_url": "http://arxiv.org/pdf/2505.12821v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12822v1", "title": "Emergent Specialization: Rare Token Neurons in Language Models", "authors": ["Jing Liu", "Haozheng Wang", "Yueheng Li"], "abstract": "Large language models struggle with representing and generating rare tokens\ndespite their importance in specialized domains. In this study, we identify\nneuron structures with exceptionally strong influence on language model's\nprediction of rare tokens, termed as rare token neurons, and investigate the\nmechanism for their emergence and behavior. These neurons exhibit a\ncharacteristic three-phase organization (plateau, power-law, and rapid decay)\nthat emerges dynamically during training, evolving from a homogeneous initial\nstate to a functionally differentiated architecture. In the activation space,\nrare token neurons form a coordinated subnetwork that selectively co-activates\nwhile avoiding co-activation with other neurons. This functional specialization\npotentially correlates with the development of heavy-tailed weight\ndistributions, suggesting a statistical mechanical basis for emergent\nspecialization.", "categories": ["cs.AI"], "published": "2025-05-19 08:05:13", "updated": "2025-05-19 08:05:13", "pdf_url": "http://arxiv.org/pdf/2505.12822v1", "comment": "9 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12833v1", "title": "Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs", "authors": ["Zhuo Yang", "Lingli Ge", "Dong Han", "Tianfan Fu", "Yuqiang Li"], "abstract": "Many real-world scientific and industrial applications require the\noptimization of expensive black-box functions. Bayesian Optimization (BO)\nprovides an effective framework for such problems. However, traditional BO\nmethods are prone to get trapped in local optima and often lack interpretable\ninsights. To address this issue, this paper designs Reasoning BO, a novel\nframework that leverages reasoning models to guide the sampling process in BO\nwhile incorporating multi-agent systems and knowledge graphs for online\nknowledge accumulation. By integrating the reasoning and contextual\nunderstanding capabilities of Large Language Models (LLMs), we can provide\nstrong guidance to enhance the BO process. As the optimization progresses,\nReasoning BO provides real-time sampling recommendations along with critical\ninsights grounded in plausible scientific theories, aiding in the discovery of\nsuperior solutions within the search space. We systematically evaluate our\napproach across 10 diverse tasks encompassing synthetic mathematical functions\nand complex real-world applications. The framework demonstrates its capability\nto progressively refine sampling strategies through real-time insights and\nhypothesis evolution, effectively identifying higher-performing regions of the\nsearch space for focused exploration. This process highlights the powerful\nreasoning and context-learning abilities of LLMs in optimization scenarios. For\nexample, in the Direct Arylation task, our method increased the yield to 60.7%,\nwhereas traditional BO achieved only a 25.2% yield. Furthermore, our\ninvestigation reveals that smaller LLMs, when fine-tuned through reinforcement\nlearning, can attain comparable performance to their larger counterparts. This\nenhanced reasoning capability paves the way for more efficient automated\nscientific experimentation while maintaining computational feasibility.", "categories": ["cs.AI"], "published": "2025-05-19 08:20:40", "updated": "2025-05-19 08:20:40", "pdf_url": "http://arxiv.org/pdf/2505.12833v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12837v1", "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "abstract": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:25:21", "updated": "2025-05-19 08:25:21", "pdf_url": "http://arxiv.org/pdf/2505.12837v1", "comment": "20 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12843v1", "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF", "authors": ["Kangwen Zhao", "Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "abstract": "Reinforcement Learning from Human Feedback relies on reward models to align\nlarge language models with human preferences. However, RLHF often suffers from\nreward hacking, wherein policy learning exploits flaws in the trained reward\nmodel to maximize reward scores without genuinely aligning with human\npreferences. A significant example of such reward hacking is length bias, where\nreward models usually favor longer responses irrespective of actual response\nquality. Previous works on length bias have notable limitations, these\napproaches either mitigate bias without characterizing the bias form, or simply\nassume a linear length-reward relation. To accurately model the intricate\nnature of length bias and facilitate more effective bias mitigation, we propose\nFiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a\nframework that autonomously learns and corrects underlying bias patterns. Our\napproach consists of three stages: First, we train a standard reward model\nwhich inherently contains length bias. Next, we deploy a lightweight fitting\nmodel to explicitly capture the non-linear relation between length and reward.\nFinally, we incorporate this learned relation into the reward model to debias.\nExperimental results demonstrate that FiMi-RM achieves a more balanced\nlength-reward distribution. Furthermore, when applied to alignment algorithms,\nour debiased reward model improves length-controlled win rate and reduces\nverbosity without compromising its performance.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 08:29:28", "updated": "2025-05-19 08:29:28", "pdf_url": "http://arxiv.org/pdf/2505.12843v1", "comment": "Due to the word limit for arXiv abstract, the abstract here has been\n  abridged compared to the one in the PDF", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12844v1", "title": "AGI-Elo: How Far Are We From Mastering A Task?", "authors": ["Shuo Sun", "Yimin Zhao", "Christina Dao Wen Lee", "Jiawei Sun", "Chengran Yuan", "Zefan Huang", "Dongen Li", "Justin KW Yeoh", "Alok Prakash", "Thomas W. Malone", "Marcelo H. Ang Jr"], "abstract": "As the field progresses toward Artificial General Intelligence (AGI), there\nis a pressing need for more comprehensive and insightful evaluation frameworks\nthat go beyond aggregate performance metrics. This paper introduces a unified\nrating system that jointly models the difficulty of individual test cases and\nthe competency of AI models (or humans) across vision, language, and action\ndomains. Unlike existing metrics that focus solely on models, our approach\nallows for fine-grained, difficulty-aware evaluations through competitive\ninteractions between models and tasks, capturing both the long-tail\ndistribution of real-world challenges and the competency gap between current\nmodels and full task mastery. We validate the generalizability and robustness\nof our system through extensive experiments on multiple established datasets\nand models across distinct AGI domains. The resulting rating distributions\noffer novel perspectives and interpretable insights into task difficulty, model\nprogression, and the outstanding challenges that remain on the path to\nachieving full AGI task mastery.", "categories": ["cs.AI", "cs.RO"], "published": "2025-05-19 08:30:13", "updated": "2025-05-19 08:30:13", "pdf_url": "http://arxiv.org/pdf/2505.12844v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12845v1", "title": "Multi-Level Aware Preference Learning: Enhancing RLHF for Complex Multi-Instruction Tasks", "authors": ["Ruopei Sun", "Jianfeng Cai", "Jinhua Zhu", "Kangwen Zhao", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "abstract": "RLHF has emerged as a predominant approach for aligning artificial\nintelligence systems with human preferences, demonstrating exceptional and\nmeasurable efficacy in instruction following tasks; however, it exhibits\ninsufficient compliance capabilities when confronted with complex\nmulti-instruction tasks. Conventional approaches rely heavily on human\nannotation or more sophisticated large language models, thereby introducing\nsubstantial resource expenditure or potential bias concerns. Meanwhile,\nalternative synthetic methods that augment standard preference datasets often\ncompromise the model's semantic quality. Our research identifies a critical\noversight in existing techniques, which predominantly focus on comparing\nresponses while neglecting valuable latent signals embedded within prompt\ninputs, and which only focus on preference disparities at the intra-sample\nlevel, while neglecting to account for the inter-sample level preference\ndifferentials that exist among preference data. To leverage these previously\nneglected indicators, we propose a novel Multi-level Aware Preference Learning\n(MAPL) framework, capable of enhancing multi-instruction capabilities.\nSpecifically, for any given response in original preference data pairs, we\nconstruct varied prompts with a preference relation under different conditions,\nin order to learn intra-sample level preference disparities. Furthermore, for\nany given original preference pair, we synthesize multi-instruction preference\npairs to capture preference discrepancies at the inter-sample level. Building\non the two datasets constructed above, we consequently devise two sophisticated\ntraining objective functions. Subsequently, our framework integrates seamlessly\ninto both Reward Modeling and Direct Preference Optimization paradigms. Through\nrigorous evaluation across multiple benchmarks, we empirically validate the\nefficacy of our framework.", "categories": ["cs.AI"], "published": "2025-05-19 08:33:11", "updated": "2025-05-19 08:33:11", "pdf_url": "http://arxiv.org/pdf/2505.12845v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12851v1", "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting", "authors": ["Yanhua Wen", "Lu Ai", "Gang Liu", "Chuang Li", "Jianhao Wei"], "abstract": "Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 08:39:07", "updated": "2025-05-19 08:39:07", "pdf_url": "http://arxiv.org/pdf/2505.12851v1", "comment": "14 pages, 5 figures, BlockSys2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12863v1", "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "abstract": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation.", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "published": "2025-05-19 08:46:45", "updated": "2025-05-19 08:46:45", "pdf_url": "http://arxiv.org/pdf/2505.12863v1", "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12864v1", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr\u00fcwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "abstract": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "published": "2025-05-19 08:48:12", "updated": "2025-05-19 08:48:12", "pdf_url": "http://arxiv.org/pdf/2505.12864v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12869v1", "title": "Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption", "authors": ["Koki Wakiyama", "Tomohiro I", "Hiroshi Sakamoto"], "abstract": "Feature selection is a technique that extracts a meaningful subset from a set\nof features in training data. When the training data is large-scale,\nappropriate feature selection enables the removal of redundant features, which\ncan improve generalization performance, accelerate the training process, and\nenhance the interpretability of the model. This study proposes a\nprivacy-preserving computation model for feature selection. Generally, when the\ndata owner and analyst are the same, there is no need to conceal the private\ninformation. However, when they are different parties or when multiple owners\nexist, an appropriate privacy-preserving framework is required. Although\nvarious private feature selection algorithms, they all require two or more\ncomputing parties and do not guarantee security in environments where no\nexternal party can be fully trusted. To address this issue, we propose the\nfirst outsourcing algorithm for feature selection using fully homomorphic\nencryption. Compared to a prior two-party algorithm, our result improves the\ntime and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n\ndenote the number of features and data samples, respectively. We also\nimplemented the proposed algorithm and conducted comparative experiments with\nthe naive one. The experimental result shows the efficiency of our method even\nwith small datasets.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 08:55:56", "updated": "2025-05-19 08:55:56", "pdf_url": "http://arxiv.org/pdf/2505.12869v1", "comment": "14 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12871v1", "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-19 08:57:08", "updated": "2025-05-19 08:57:08", "pdf_url": "http://arxiv.org/pdf/2505.12871v1", "comment": "To appear at ICML 25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12872v1", "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging", "authors": ["Maytus Piriyajitakonkij", "Rujikorn Charakorn", "Weicheng Tao", "Wei Pan", "Mingfei Sun", "Cheston Tan", "Mengmi Zhang"], "abstract": "Early cavemen relied on gestures, vocalizations, and simple signals to\ncoordinate, plan, avoid predators, and share resources. Today, humans\ncollaborate using complex languages to achieve remarkable results. What drives\nthis evolution in communication? How does language emerge, adapt, and become\nvital for teamwork? Understanding the origins of language remains a challenge.\nA leading hypothesis in linguistics and anthropology posits that language\nevolved to meet the ecological and social demands of early human cooperation.\nLanguage did not arise in isolation, but through shared survival goals.\nInspired by this view, we investigate the emergence of language in multi-agent\nForaging Games. These environments are designed to reflect the cognitive and\necological constraints believed to have influenced the evolution of\ncommunication. Agents operate in a shared grid world with only partial\nknowledge about other agents and the environment, and must coordinate to\ncomplete games like picking up high-value targets or executing temporally\nordered actions. Using end-to-end deep reinforcement learning, agents learn\nboth actions and communication strategies from scratch. We find that agents\ndevelop communication protocols with hallmark features of natural language:\narbitrariness, interchangeability, displacement, cultural transmission, and\ncompositionality. We quantify each property and analyze how different factors,\nsuch as population size and temporal dependencies, shape specific aspects of\nthe emergent language. Our framework serves as a platform for studying how\nlanguage can evolve from partial observability, temporal reasoning, and\ncooperative goals in embodied multi-agent settings. We will release all data,\ncode, and models publicly.", "categories": ["cs.AI", "cs.LG", "cs.MA"], "published": "2025-05-19 08:57:30", "updated": "2025-05-19 08:57:30", "pdf_url": "http://arxiv.org/pdf/2505.12872v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12880v1", "title": "AdS-GNN -- a Conformally Equivariant Graph Neural Network", "authors": ["Maksim Zhdanov", "Nabil Iqbal", "Erik Bekkers", "Patrick Forr\u00e9"], "abstract": "Conformal symmetries, i.e.\\ coordinate transformations that preserve angles,\nplay a key role in many fields, including physics, mathematics, computer vision\nand (geometric) machine learning. Here we build a neural network that is\nequivariant under general conformal transformations. To achieve this, we lift\ndata from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to\nexploit a known correspondence between conformal transformations of flat space\nand isometric transformations on the AdS space. We then build upon the fact\nthat such isometric transformations have been extensively studied on general\ngeometries in the geometric deep learning literature. We employ message-passing\nlayers conditioned on the proper distance, yielding a computationally efficient\nframework. We validate our model on tasks from computer vision and statistical\nphysics, demonstrating strong performance, improved generalization capacities,\nand the ability to extract conformal data such as scaling dimensions from the\ntrained network.", "categories": ["cs.LG", "cs.AI", "hep-th"], "published": "2025-05-19 09:08:52", "updated": "2025-05-19 09:08:52", "pdf_url": "http://arxiv.org/pdf/2505.12880v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12882v1", "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "abstract": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 09:10:55", "updated": "2025-05-19 09:10:55", "pdf_url": "http://arxiv.org/pdf/2505.12882v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12884v1", "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "abstract": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-19 09:11:54", "updated": "2025-05-19 09:11:54", "pdf_url": "http://arxiv.org/pdf/2505.12884v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12886v1", "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "categories": ["cs.AI", "cs.CL", "cs.CY"], "published": "2025-05-19 09:16:40", "updated": "2025-05-19 09:16:40", "pdf_url": "http://arxiv.org/pdf/2505.12886v1", "comment": "25 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12891v1", "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "abstract": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 09:22:02", "updated": "2025-05-19 09:22:02", "pdf_url": "http://arxiv.org/pdf/2505.12891v1", "comment": "First version. There are still some examples to be added into the\n  appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12894v1", "title": "HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Keke Tang", "Chao Gao", "Zhen Wang"], "abstract": "Hypergraphs offer superior modeling capabilities for social networks,\nparticularly in capturing group phenomena that extend beyond pairwise\ninteractions in rumor propagation. Existing approaches in rumor source\ndetection predominantly focus on dyadic interactions, which inadequately\naddress the complexity of more intricate relational structures. In this study,\nwe present a novel approach for Source Detection in Hypergraphs (HyperDet) via\nInteractive Relationship Construction and Feature-rich Attention Fusion.\nSpecifically, our methodology employs an Interactive Relationship Construction\nmodule to accurately model both the static topology and dynamic interactions\namong users, followed by the Feature-rich Attention Fusion module, which\nautonomously learns node features and discriminates between nodes using a\nself-attention mechanism, thereby effectively learning node representations\nunder the framework of accurately modeled higher-order relationships. Extensive\nexperimental validation confirms the efficacy of our HyperDet approach,\nshowcasing its superiority relative to current state-of-the-art methods.", "categories": ["cs.SI", "cs.AI"], "published": "2025-05-19 09:27:46", "updated": "2025-05-19 09:27:46", "pdf_url": "http://arxiv.org/pdf/2505.12894v1", "comment": "Accepted by IJCAI25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12900v1", "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "abstract": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation.", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "published": "2025-05-19 09:35:58", "updated": "2025-05-19 09:35:58", "pdf_url": "http://arxiv.org/pdf/2505.12900v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12903v1", "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach", "authors": ["Shiao Wang", "Xiao Wang", "Liye Jin", "Bo Jiang", "Lin Zhu", "Lan Chen", "Yonghong Tian", "Bin Luo"], "abstract": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras\ncoupled with computationally intensive deep neural network architectures to\nachieve effective tracking. However, such frame-based methods inherently face\nchallenges in achieving low-latency performance and often fail in\nresource-constrained environments. Visual object tracking using bio-inspired\nevent cameras has emerged as a promising research direction in recent years,\noffering distinct advantages for low-latency applications. In this paper, we\npropose a novel Slow-Fast Tracking paradigm that flexibly adapts to different\noperational requirements, termed SFTrack. The proposed framework supports two\ncomplementary modes, i.e., a high-precision slow tracker for scenarios with\nsufficient computational resources, and an efficient fast tracker tailored for\nlatency-aware, resource-constrained environments. Specifically, our framework\nfirst performs graph-based representation learning from\nhigh-temporal-resolution event streams, and then integrates the learned\ngraph-structured information into two FlashAttention-based vision backbones,\nyielding the slow and fast trackers, respectively. The fast tracker achieves\nlow latency through a lightweight network design and by producing multiple\nbounding box outputs in a single forward pass. Finally, we seamlessly combine\nboth trackers via supervised fine-tuning and further enhance the fast tracker's\nperformance through a knowledge distillation strategy. Extensive experiments on\npublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate the\neffectiveness and efficiency of our proposed method across different real-world\nscenarios. The source code has been released on\nhttps://github.com/Event-AHU/SlowFast_Event_Track.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 09:37:23", "updated": "2025-05-19 09:37:23", "pdf_url": "http://arxiv.org/pdf/2505.12903v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12904v1", "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning", "authors": ["Hilde I. Hummel", "Arwin Gansekoele", "Sandjai Bhulai", "Rob van der Mei"], "abstract": "The increasing level of sound pollution in marine environments poses an\nincreased threat to ocean health, making it crucial to monitor underwater\nnoise. By monitoring this noise, the sources responsible for this pollution can\nbe mapped. Monitoring is performed by passively listening to these sounds. This\ngenerates a large amount of data records, capturing a mix of sound sources such\nas ship activities and marine mammal vocalizations. Although machine learning\noffers a promising solution for automatic sound classification, current\nstate-of-the-art methods implement supervised learning. This requires a large\namount of high-quality labeled data that is not publicly available. In\ncontrast, a massive amount of lower-quality unlabeled data is publicly\navailable, offering the opportunity to explore unsupervised learning\ntechniques. This research explores this possibility by implementing an\nunsupervised Contrastive Learning approach. Here, a Conformer-based encoder is\noptimized by the so-called Variance-Invariance-Covariance Regularization loss\nfunction on these lower-quality unlabeled data and the translation to the\nlabeled data is made. Through classification tasks involving recognizing ship\ntypes and marine mammal vocalizations, our method demonstrates to produce\nrobust and generalized embeddings. This shows to potential of unsupervised\nmethods for various automatic underwater acoustic analysis tasks.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 09:37:46", "updated": "2025-05-19 09:37:46", "pdf_url": "http://arxiv.org/pdf/2505.12904v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12908v1", "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "abstract": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 09:44:01", "updated": "2025-05-19 09:44:01", "pdf_url": "http://arxiv.org/pdf/2505.12908v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12909v1", "title": "Sinusoidal Initialization, Time for a New Start", "authors": ["Alberto Fern\u00e1ndez-Hern\u00e1ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ort\u00ed"], "abstract": "Initialization plays a critical role in Deep Neural Network training,\ndirectly influencing convergence, stability, and generalization. Common\napproaches such as Glorot and He initializations rely on randomness, which can\nproduce uneven weight distributions across layer connections. In this paper, we\nintroduce the Sinusoidal initialization, a novel deterministic method that\nemploys sinusoidal functions to construct structured weight matrices expressly\nto improve the spread and balance of weights throughout the network while\nsimultaneously fostering a more uniform, well-conditioned distribution of\nneuron activation states from the very first forward pass. Because Sinusoidal\ninitialization begins with weights and activations that are already evenly and\nefficiently utilized, it delivers consistently faster convergence, greater\ntraining stability, and higher final accuracy across a wide range of models,\nincluding convolutional neural networks, vision transformers, and large\nlanguage models. On average, our experiments show an increase of 4.8 % in final\nvalidation accuracy and 20.9 % in convergence speed. By replacing randomness\nwith structure, this initialization provides a stronger and more reliable\nfoundation for Deep Learning systems.", "categories": ["cs.LG", "cs.AI", "I.2; G.3; I.2.6"], "published": "2025-05-19 09:45:18", "updated": "2025-05-19 09:45:18", "pdf_url": "http://arxiv.org/pdf/2505.12909v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12910v1", "title": "SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Chao Gao", "Zhen Wang", "Keke Tang"], "abstract": "Source detection on graphs has demonstrated high efficacy in identifying\nrumor origins. Despite advances in machine learning-based methods, many fail to\ncapture intrinsic dynamics of rumor propagation. In this work, we present\nSourceDetMamba: A Graph-aware State Space Model for Source Detection in\nSequential Hypergraphs, which harnesses the recent success of the state space\nmodel Mamba, known for its superior global modeling capabilities and\ncomputational efficiency, to address this challenge. Specifically, we first\nemploy hypergraphs to model high-order interactions within social networks.\nSubsequently, temporal network snapshots generated during the propagation\nprocess are sequentially fed in reverse order into Mamba to infer underlying\npropagation dynamics. Finally, to empower the sequential model to effectively\ncapture propagation patterns while integrating structural information, we\npropose a novel graph-aware state update mechanism, wherein the state of each\nnode is propagated and refined by both temporal dependencies and topological\ncontext. Extensive evaluations on eight datasets demonstrate that\nSourceDetMamba consistently outperforms state-of-the-art approaches.", "categories": ["cs.SI", "cs.AI"], "published": "2025-05-19 09:45:27", "updated": "2025-05-19 09:45:27", "pdf_url": "http://arxiv.org/pdf/2505.12910v1", "comment": "Accepted by IJCAI25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12920v1", "title": "PyFCG: Fluid Construction Grammar in Python", "authors": ["Paul Van Eecke", "Katrien Beuls"], "abstract": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-19 10:00:01", "updated": "2025-05-19 10:00:01", "pdf_url": "http://arxiv.org/pdf/2505.12920v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12923v1", "title": "The Traitors: Deception and Trust in Multi-Agent Language Model Simulations", "authors": ["Pedro M. P. Curvo"], "abstract": "As AI systems increasingly assume roles where trust and alignment with human\nvalues are essential, understanding when and why they engage in deception has\nbecome a critical research priority. We introduce The Traitors, a multi-agent\nsimulation framework inspired by social deduction games, designed to probe\ndeception, trust formation, and strategic communication among large language\nmodel (LLM) agents under asymmetric information. A minority of agents the\ntraitors seek to mislead the majority, while the faithful must infer hidden\nidentities through dialogue and reasoning. Our contributions are: (1) we ground\nthe environment in formal frameworks from game theory, behavioral economics,\nand social cognition; (2) we develop a suite of evaluation metrics capturing\ndeception success, trust dynamics, and collective inference quality; (3) we\nimplement a fully autonomous simulation platform where LLMs reason over\npersistent memory and evolving social dynamics, with support for heterogeneous\nagent populations, specialized traits, and adaptive behaviors. Our initial\nexperiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model)\nreveal a notable asymmetry: advanced models like GPT-4o demonstrate superior\ndeceptive capabilities yet exhibit disproportionate vulnerability to others'\nfalsehoods. This suggests deception skills may scale faster than detection\nabilities. Overall, The Traitors provides a focused, configurable testbed for\ninvestigating LLM behavior in socially nuanced interactions. We position this\nwork as a contribution toward more rigorous research on deception mechanisms,\nalignment challenges, and the broader social reliability of AI systems.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-19 10:01:35", "updated": "2025-05-19 10:01:35", "pdf_url": "http://arxiv.org/pdf/2505.12923v1", "comment": "9 main pages, 31 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12925v1", "title": "CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive Programming", "authors": ["Han Deng", "Yuan Meng", "Shixiang Tang", "Wanli Ouyang", "Xinzhu Ma"], "abstract": "Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem --\nsimilar question retrieval -- to address this issue. Due to the lack of both\ndata and models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code and\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate and Simplified-to-Full), built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. In addition, we develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Code and data are available at: https://github.com/coldchair/CPRet", "categories": ["cs.SE", "cs.AI", "cs.IR", "H.3.3"], "published": "2025-05-19 10:07:51", "updated": "2025-05-19 10:07:51", "pdf_url": "http://arxiv.org/pdf/2505.12925v1", "comment": "main 9 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12929v1", "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "abstract": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:14:08", "updated": "2025-05-19 10:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12929v1", "comment": "24 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12938v1", "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "abstract": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 10:22:04", "updated": "2025-05-19 10:22:04", "pdf_url": "http://arxiv.org/pdf/2505.12938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12942v1", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "abstract": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:29:32", "updated": "2025-05-19 10:29:32", "pdf_url": "http://arxiv.org/pdf/2505.12942v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12944v1", "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "abstract": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "published": "2025-05-19 10:31:30", "updated": "2025-05-19 10:31:30", "pdf_url": "http://arxiv.org/pdf/2505.12944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12951v1", "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "abstract": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 10:44:49", "updated": "2025-05-19 10:44:49", "pdf_url": "http://arxiv.org/pdf/2505.12951v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12960v1", "title": "Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory", "authors": ["Chengping He", "Mingrui Jiang", "Keyi Shan", "Szu-Hao Yang", "Zefan Li", "Shengbo Wang", "Giacomo Pedretti", "Jim Ignowski", "Can Li"], "abstract": "Brain-inspired computing aims to mimic cognitive functions like associative\nmemory, the ability to recall complete patterns from partial cues. Memristor\ntechnology offers promising hardware for such neuromorphic systems due to its\npotential for efficient in-memory analog computing. Hopfield Neural Networks\n(HNNs) are a classic model for associative memory, but implementations on\nconventional hardware suffer from efficiency bottlenecks, while prior\nmemristor-based HNNs faced challenges with vulnerability to hardware defects\ndue to offline training, limited storage capacity, and difficulty processing\nanalog patterns. Here we introduce and experimentally demonstrate on integrated\nmemristor hardware a new hardware-adaptive learning algorithm for associative\nmemories that significantly improves defect tolerance and capacity, and\nnaturally extends to scalable multilayer architectures capable of handling both\nbinary and continuous patterns. Our approach achieves 3x effective capacity\nunder 50% device faults compared to state-of-the-art methods. Furthermore, its\nextension to multilayer architectures enables superlinear capacity scaling\n(\\(\\propto N^{1.49}\\ for binary patterns) and effective recalling of continuous\npatterns (\\propto N^{1.74}\\ scaling), as compared to linear capacity scaling\nfor previous HNNs. It also provides flexibility to adjust capacity by tuning\nhidden neurons for the same-sized patterns. By leveraging the massive\nparallelism of the hardware enabled by synchronous updates, it reduces energy\nby 8.8x and latency by 99.7% for 64-dimensional patterns over asynchronous\nschemes, with greater improvements at scale. This promises the development of\nmore reliable memristor-based associative memory systems and enables new\napplications research due to the significantly improved capacity, efficiency,\nand flexibility.", "categories": ["cs.LG", "cs.AI", "cs.ET"], "published": "2025-05-19 10:55:09", "updated": "2025-05-19 10:55:09", "pdf_url": "http://arxiv.org/pdf/2505.12960v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12963v1", "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies", "authors": ["Maksim I. Ivanov", "Olga E. Mendybaeva", "Yuri E. Karyakin", "Igor N. Glukhikh", "Aleksey V. Lebedev"], "abstract": "This article explores the use of artificial intelligence for the diagnosis of\npathologies of the temporomandibular joint (TMJ), in particular, for the\nsegmentation of the articular disc on MRI images. The relevance of the work is\ndue to the high prevalence of TMJ pathologies, as well as the need to improve\nthe accuracy and speed of diagnosis in medical institutions. During the study,\nthe existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,\nare not suitable for studying the articular disc due to the orientation towards\nbone structures. To solve the problem, an original dataset was collected from\n94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the\namount of data, augmentation methods were used. After that, the models of\nU-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and\ncompared. The evaluation was carried out according to the Dice Score,\nPrecision, Sensitivity, Specificity, and Mean Average Precision metrics. The\nresults confirm the potential of using the Roboflow model for segmentation of\nthe temporomandibular joint. In the future, it is planned to develop an\nalgorithm for measuring the distance between the jaws and determining the\nposition of the articular disc, which will improve the diagnosis of TMJ\npathologies.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-19 10:58:02", "updated": "2025-05-19 10:58:02", "pdf_url": "http://arxiv.org/pdf/2505.12963v1", "comment": "10 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12966v1", "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection", "authors": ["Zihan Xiong", "Xiaohua Wu", "Lei Chen", "Fangqi Lou"], "abstract": "Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 11:01:49", "updated": "2025-05-19 11:01:49", "pdf_url": "http://arxiv.org/pdf/2505.12966v1", "comment": "9 pages,ICMR accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12981v1", "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "abstract": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation.", "categories": ["cs.CR", "cs.AI", "cs.HC"], "published": "2025-05-19 11:17:46", "updated": "2025-05-19 11:17:46", "pdf_url": "http://arxiv.org/pdf/2505.12981v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12983v1", "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:18:54", "updated": "2025-05-19 11:18:54", "pdf_url": "http://arxiv.org/pdf/2505.12983v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12992v1", "title": "Fractured Chain-of-Thought Reasoning", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "published": "2025-05-19 11:30:41", "updated": "2025-05-19 11:30:41", "pdf_url": "http://arxiv.org/pdf/2505.12992v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12996v1", "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:34:47", "updated": "2025-05-19 11:34:47", "pdf_url": "http://arxiv.org/pdf/2505.12996v1", "comment": "12 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13010v1", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "abstract": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-19 11:54:39", "updated": "2025-05-19 11:54:39", "pdf_url": "http://arxiv.org/pdf/2505.13010v1", "comment": "7 pages, 5 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13011v1", "title": "Unveiling and Steering Connectome Organization with Interpretable Latent Variables", "authors": ["Yubin Li", "Xingyu Liu", "Guozhang Chen"], "abstract": "The brain's intricate connectome, a blueprint for its function, presents\nimmense complexity, yet it arises from a compact genetic code, hinting at\nunderlying low-dimensional organizational principles. This work bridges\nconnectomics and representation learning to uncover these principles. We\npropose a framework that combines subgraph extraction from the Drosophila\nconnectome, FlyWire, with a generative model to derive interpretable\nlow-dimensional representations of neural circuitry. Crucially, an\nexplainability module links these latent dimensions to specific structural\nfeatures, offering insights into their functional relevance. We validate our\napproach by demonstrating effective graph reconstruction and, significantly,\nthe ability to manipulate these latent codes to controllably generate\nconnectome subgraphs with predefined properties. This research offers a novel\ntool for understanding brain architecture and a potential avenue for designing\nbio-inspired artificial neural networks.", "categories": ["cs.AI"], "published": "2025-05-19 11:54:40", "updated": "2025-05-19 11:54:40", "pdf_url": "http://arxiv.org/pdf/2505.13011v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13023v1", "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "abstract": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "published": "2025-05-19 12:07:29", "updated": "2025-05-19 12:07:29", "pdf_url": "http://arxiv.org/pdf/2505.13023v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13025v1", "title": "LiBOG: Lifelong Learning for Black-Box Optimizer Generation", "authors": ["Jiyuan Pei", "Yi Mei", "Jialin Liu", "Mengjie Zhang"], "abstract": "Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in\nautomating the configuration and generation of black-box optimizers,\nsignificantly reducing the human effort required for optimizer design and\ndiscovering optimizers with higher performance than classic human-designed\noptimizers. However, existing MetaBBO methods conduct one-off training under\nthe assumption that a stationary problem distribution with extensive and\nrepresentative training problem samples is pre-available. This assumption is\noften impractical in real-world scenarios, where diverse problems following\nshifting distribution continually arise. Consequently, there is a pressing need\nfor methods that can continuously learn from new problems encountered\non-the-fly and progressively enhance their capabilities. In this work, we\nexplore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a\nnovel approach designed to learn from sequentially encountered problems and\ngenerate high-performance optimizers for Black-Box Optimization (BBO). LiBOG\nconsolidates knowledge both across tasks and within tasks to mitigate\ncatastrophic forgetting. Extensive experiments demonstrate LiBOG's\neffectiveness in learning to generate high-performance optimizers in a lifelong\nlearning manner, addressing catastrophic forgetting while maintaining\nplasticity to learn new tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:09:25", "updated": "2025-05-19 12:09:25", "pdf_url": "http://arxiv.org/pdf/2505.13025v1", "comment": "Accepted at IJCAI 2025. To appear", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13026v1", "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "abstract": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:10:17", "updated": "2025-05-19 12:10:17", "pdf_url": "http://arxiv.org/pdf/2505.13026v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13028v1", "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "authors": ["Sayon Palit", "Daniel Woods"], "abstract": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "published": "2025-05-19 12:12:00", "updated": "2025-05-19 12:12:00", "pdf_url": "http://arxiv.org/pdf/2505.13028v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13031v1", "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "authors": ["Yicheng Xiao", "Lin Song", "Yukang Chen", "Yingmin Luo", "Yuxin Chen", "Yukang Gan", "Wei Huang", "Xiu Li", "Xiaojuan Qi", "Ying Shan"], "abstract": "Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\n\\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}.", "categories": ["cs.AI"], "published": "2025-05-19 12:17:04", "updated": "2025-05-19 12:17:04", "pdf_url": "http://arxiv.org/pdf/2505.13031v1", "comment": "Code: https://github.com/EasonXiao-888/MindOmni", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13033v1", "title": "TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis", "authors": ["Vijay Ekambaram", "Subodh Kumar", "Arindam Jati", "Sumanta Mukherjee", "Tomoya Sakai", "Pankaj Dayama", "Wesley M. Gifford", "Jayant Kalagnanam"], "abstract": "The rise of time-series pre-trained models has advanced temporal\nrepresentation learning, but current state-of-the-art models are often\nlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compact\ntime-series pre-trained models with only 1M parameters, specialized to perform\nstrongly across classification, anomaly detection, imputation, and retrieval\ntasks. TSPulse introduces innovations at both the architecture and task levels.\nAt the architecture level, it employs a dual-space masked reconstruction,\nlearning from both time and frequency domains to capture complementary signals.\nThis is further enhanced by a dual-embedding disentanglement, generating both\ndetailed embeddings for fine-grained analysis and high-level semantic\nembeddings for broader task understanding. Notably, TSPulse's semantic\nembeddings are robust to shifts in time, magnitude, and noise, which is\nimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,\na fine-tuning component enabling task-specific feature attention. It also\nintroduces a multi-head triangulation technique that correlates deviations from\nmultiple prediction heads, enhancing anomaly detection by fusing complementary\nmodel outputs. Additionally, a hybrid mask pretraining is proposed to improves\nzero-shot imputation by reducing pre-training bias. These architecture and task\ninnovations collectively contribute to TSPulse's significant performance gains:\n5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly\ndetection leaderboard, +50% in zero-shot imputation, and +25% in time-series\nretrieval. Remarkably, these results are achieved with just 1M parameters,\nmaking TSPulse 10-100X smaller than existing pre-trained models. Its efficiency\nenables GPU-free inference and rapid pre-training, setting a new standard for\nefficient time-series pre-trained models. Models will be open-sourced soon.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:18:53", "updated": "2025-05-19 12:18:53", "pdf_url": "http://arxiv.org/pdf/2505.13033v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13036v1", "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "authors": ["Sai Koneru", "Maike Z\u00fcfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "abstract": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:21:29", "updated": "2025-05-19 12:21:29", "pdf_url": "http://arxiv.org/pdf/2505.13036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13043v1", "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation", "authors": ["Hao-Ran Yang", "Xiaohui Chen", "Chuan-Xian Ren"], "abstract": "Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 12:33:52", "updated": "2025-05-19 12:33:52", "pdf_url": "http://arxiv.org/pdf/2505.13043v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13044v1", "title": "CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents", "authors": ["Rebecca Westh\u00e4u\u00dfer", "Frederik Berenz", "Wolfgang Minker", "Sebastian Zepf"], "abstract": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) and are a powerful enabler for interactive systems. However,\nthey still face challenges in long-term interactions that require adaptation\ntowards the user as well as contextual knowledge and understanding of the\never-changing environment. To overcome these challenges, holistic memory\nmodeling is required to efficiently retrieve and store relevant information\nacross interaction sessions for suitable responses. Cognitive AI, which aims to\nsimulate the human thought process in a computerized model, highlights\ninteresting aspects, such as thoughts, memory mechanisms, and decision-making,\nthat can contribute towards improved memory modeling for LLMs. Inspired by\nthese cognitive AI principles, we propose our memory framework CAIM. CAIM\nconsists of three modules: 1.) The Memory Controller as the central decision\nunit; 2.) the Memory Retrieval, which filters relevant data for interaction\nupon request; and 3.) the Post-Thinking, which maintains the memory storage. We\ncompare CAIM against existing approaches, focusing on metrics such as retrieval\naccuracy, response correctness, contextual coherence, and memory storage. The\nresults demonstrate that CAIM outperforms baseline frameworks across different\nmetrics, highlighting its context-awareness and potential to improve long-term\nhuman-AI interactions.", "categories": ["cs.AI", "cs.HC"], "published": "2025-05-19 12:33:52", "updated": "2025-05-19 12:33:52", "pdf_url": "http://arxiv.org/pdf/2505.13044v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13053v1", "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "abstract": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:42:23", "updated": "2025-05-19 12:42:23", "pdf_url": "http://arxiv.org/pdf/2505.13053v1", "comment": "currently under review at Frontiers in Communication", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13073v1", "title": "Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion", "authors": ["Dengfeng Liu", "Jucai Zhai", "Xiaoguang Jiang", "Ziqun Li", "Qianjin Yu", "Feng Liu", "Rui Ye", "Huang Liu", "Zhiguo Yang", "Yongsheng Du", "Fang Tan"], "abstract": "Code completion technology based on large language model has significantly\nimproved the development efficiency of programmers. However, in practical\napplications, there remains a gap between current commonly used code completion\nevaluation metrics and users' actual perception. To address this issue, we\npropose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,\nfrom the perspective of probabilistic modeling. Furthermore, to tackle the lack\nof effective structural semantic modeling and cross-module dependency\ninformation in LLMs for repository-level code completion scenarios, we propose\na data processing method based on a Structure-Preserving and\nSemantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis\nand experimental validation, we demonstrate the superiority of the proposed\nevaluation metrics in terms of user perception consistency, as well as the\neffectiveness of the data processing method in enhancing model performance.", "categories": ["cs.SE", "cs.AI"], "published": "2025-05-19 13:09:32", "updated": "2025-05-19 13:09:32", "pdf_url": "http://arxiv.org/pdf/2505.13073v1", "comment": "14 pages,8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13076v1", "title": "The Hidden Dangers of Browsing AI Agents", "authors": ["Mykyta Mudryi", "Markiyan Chaklosh", "Grzegorz W\u00f3jcik"], "abstract": "Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 13:10:29", "updated": "2025-05-19 13:10:29", "pdf_url": "http://arxiv.org/pdf/2505.13076v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13077v1", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "abstract": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 13:11:28", "updated": "2025-05-19 13:11:28", "pdf_url": "http://arxiv.org/pdf/2505.13077v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13079v1", "title": "Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "abstract": "Transferring linguistic knowledge from a pretrained language model (PLM) to\nacoustic feature learning has proven effective in enhancing end-to-end\nautomatic speech recognition (E2E-ASR). However, aligning representations\nbetween linguistic and acoustic modalities remains a challenge due to inherent\nmodality gaps. Optimal transport (OT) has shown promise in mitigating these\ngaps by minimizing the Wasserstein distance (WD) between linguistic and\nacoustic feature distributions. However, previous OT-based methods overlook\nstructural relationships, treating feature vectors as unordered sets. To\naddress this, we propose Graph Matching Optimal Transport (GM-OT), which models\nlinguistic and acoustic sequences as structured graphs. Nodes represent feature\nembeddings, while edges capture temporal and sequential relationships. GM-OT\nminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)\n(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)\nformulation. This enables structured alignment and more efficient knowledge\ntransfer compared to existing OT-based approaches. Theoretical analysis further\nshows that prior OT-based methods in linguistic knowledge transfer can be\nviewed as a special case within our GM-OT framework. We evaluate GM-OT on\nMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge\ntransfer. Experimental results demonstrate significant performance gains over\nstate-of-the-art models, validating the effectiveness of our approach.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-19 13:13:18", "updated": "2025-05-19 13:13:18", "pdf_url": "http://arxiv.org/pdf/2505.13079v1", "comment": "To appear in Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13082v1", "title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers", "authors": ["Kyeongman Park", "Seongho Joo", "Kyomin Jung"], "abstract": "We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 13:13:46", "updated": "2025-05-19 13:13:46", "pdf_url": "http://arxiv.org/pdf/2505.13082v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13087v1", "title": "Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings", "authors": ["Adrien Lagesse", "Marc Lelarge"], "abstract": "We propose a novel benchmarking methodology for graph neural networks (GNNs)\nbased on the graph alignment problem, a combinatorial optimization task that\ngeneralizes graph isomorphism by aligning two unlabeled graphs to maximize\noverlapping edges. We frame this problem as a self-supervised learning task and\npresent several methods to generate graph alignment datasets using synthetic\nrandom graphs and real-world graph datasets from multiple domains. For a given\ngraph dataset, we generate a family of graph alignment datasets with increasing\ndifficulty, allowing us to rank the performance of various architectures. Our\nexperiments indicate that anisotropic graph neural networks outperform standard\nconvolutional architectures. To further demonstrate the utility of the graph\nalignment task, we show its effectiveness for unsupervised GNN pre-training,\nwhere the learned node embeddings outperform other positional encodings on\nthree molecular regression tasks and achieve state-of-the-art results on the\nPCQM4Mv2 dataset with significantly fewer parameters. To support\nreproducibility and further research, we provide an open-source Python package\nto generate graph alignment datasets and benchmark new GNN architectures.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 13:22:17", "updated": "2025-05-19 13:22:17", "pdf_url": "http://arxiv.org/pdf/2505.13087v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13094v1", "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech Separation", "authors": ["Guo Chen", "Kai Li", "Runxuan Yang", "Xiaolin Hu"], "abstract": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 13:25:51", "updated": "2025-05-19 13:25:51", "pdf_url": "http://arxiv.org/pdf/2505.13094v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13098v1", "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "abstract": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.", "categories": ["cs.AI", "cs.CL", "cs.DB"], "published": "2025-05-19 13:29:27", "updated": "2025-05-19 13:29:27", "pdf_url": "http://arxiv.org/pdf/2505.13098v1", "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13101v1", "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "authors": ["Shaowu Wu", "Liting Zeng", "Wei Lu", "Xiangyang Luo"], "abstract": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 13:31:48", "updated": "2025-05-19 13:31:48", "pdf_url": "http://arxiv.org/pdf/2505.13101v1", "comment": "10 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13102v1", "title": "Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast", "authors": ["Ji Qi", "Tam Thuc Do", "Mingxiao Liu", "Zhuoshi Pan", "Yuzhe Li", "Gene Cheung", "H. Vicky Zhao"], "abstract": "To forecast traffic with both spatial and temporal dimensions, we unroll a\nmixed-graph-based optimization algorithm into a lightweight and interpretable\ntransformer-like neural net. Specifically, we construct two graphs: an\nundirected graph $\\mathcal{G}^u$ capturing spatial correlations across\ngeography, and a directed graph $\\mathcal{G}^d$ capturing sequential\nrelationships over time. We formulate a prediction problem for the future\nsamples of signal $\\mathbf{x}$, assuming it is \"smooth\" with respect to both\n$\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and\n$\\ell_1$-norm variational terms to quantify and promote signal smoothness\n(low-frequency reconstruction) on a directed graph. We construct an iterative\nalgorithm based on alternating direction method of multipliers (ADMM), and\nunroll it into a feed-forward network for data-driven parameter learning. We\ninsert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$, which\nare akin to the self-attention mechanism in classical transformers. Experiments\nshow that our unrolled networks achieve competitive traffic forecast\nperformance as state-of-the-art prediction schemes, while reducing parameter\ncounts drastically. Our code is available in\nhttps://github.com/SingularityUndefined/Unrolling-GSP-STForecast.", "categories": ["cs.LG", "cs.AI", "eess.SP"], "published": "2025-05-19 13:32:34", "updated": "2025-05-19 13:32:34", "pdf_url": "http://arxiv.org/pdf/2505.13102v1", "comment": "19 pages, 5 figures, 8 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13109v1", "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "abstract": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 13:36:45", "updated": "2025-05-19 13:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13109v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13115v1", "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "abstract": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-19 13:46:35", "updated": "2025-05-19 13:46:35", "pdf_url": "http://arxiv.org/pdf/2505.13115v1", "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13116v1", "title": "Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data", "authors": ["Kathrin Lammers", "Valerie Vaquet", "Barbara Hammer"], "abstract": "As machine learning is increasingly applied in an online fashion to deal with\nevolving data streams, the fairness of these algorithms is a matter of growing\nethical and legal concern. In many use cases, class imbalance in the data also\nneeds to be dealt with to ensure predictive performance. Current fairness-aware\nstream learners typically attempt to solve these issues through in- or\npost-processing by focusing on optimizing one specific discrimination metric,\naddressing class imbalance in a separate processing step. While C-SMOTE is a\nhighly effective model-agnostic pre-processing approach to mitigate class\nimbalance, as a side effect of this method, algorithmic bias is often\nintroduced.\n  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant -\nas a pre-processing approach to simultaneously address the class imbalance and\nfairness concerns by employing situation testing and balancing\nfairness-relevant groups during oversampling. Unlike other fairness-aware\nstream learners, CFSMOTE is not optimizing for only one specific fairness\nmetric, therefore avoiding potentially problematic trade-offs. Our experiments\nshow significant improvement on several common group fairness metrics in\ncomparison to vanilla C-SMOTE while maintaining competitive performance, also\nin comparison to other fairness-aware algorithms.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 13:46:47", "updated": "2025-05-19 13:46:47", "pdf_url": "http://arxiv.org/pdf/2505.13116v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13118v1", "title": "Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals", "authors": ["Marouane Il Idrissi", "Agathe Fernandes Machado", "Ewen Gallic", "Arthur Charpentier"], "abstract": "Cooperative game theory methods, notably Shapley values, have significantly\nenhanced machine learning (ML) interpretability. However, existing explainable\nAI (XAI) frameworks mainly attribute average model predictions, overlooking\npredictive uncertainty. This work addresses that gap by proposing a novel,\nmodel-agnostic uncertainty attribution (UA) method grounded in conformal\nprediction (CP). By defining cooperative games where CP interval\nproperties-such as width and bounds-serve as value functions, we systematically\nattribute predictive uncertainty to input features. Extending beyond the\ntraditional Shapley values, we use the richer class of Harsanyi allocations,\nand in particular the proportional Shapley values, which distribute attribution\nproportionally to feature importance. We propose a Monte Carlo approximation\nmethod with robust statistical guarantees to address computational feasibility,\nsignificantly improving runtime efficiency. Our comprehensive experiments on\nsynthetic benchmarks and real-world datasets demonstrate the practical utility\nand interpretative depth of our approach. By combining cooperative game theory\nand conformal prediction, we offer a rigorous, flexible toolkit for\nunderstanding and communicating predictive uncertainty in high-stakes ML\napplications.", "categories": ["cs.AI", "cs.LG", "stat.ML"], "published": "2025-05-19 13:49:05", "updated": "2025-05-19 13:49:05", "pdf_url": "http://arxiv.org/pdf/2505.13118v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13122v1", "title": "When majority rules, minority loses: bias amplification of gradient descent", "authors": ["Fran\u00e7ois Bachoc", "J\u00e9r\u00f4me Bolte", "Ryan Boustany", "Jean-Michel Loubes"], "abstract": "Despite growing empirical evidence of bias amplification in machine learning,\nits theoretical foundations remain poorly understood. We develop a formal\nframework for majority-minority learning tasks, showing how standard training\ncan favor majority groups and produce stereotypical predictors that neglect\nminority-specific features. Assuming population and variance imbalance, our\nanalysis reveals three key findings: (i) the close proximity between\n``full-data'' and stereotypical predictors, (ii) the dominance of a region\nwhere training the entire model tends to merely learn the majority traits, and\n(iii) a lower bound on the additional training required. Our results are\nillustrated through experiments in deep learning for tabular and image\nclassification tasks.", "categories": ["cs.LG", "cs.AI", "math.OC"], "published": "2025-05-19 13:51:49", "updated": "2025-05-19 13:51:49", "pdf_url": "http://arxiv.org/pdf/2505.13122v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13123v1", "title": "Just Dance with $\u03c0$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "abstract": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 13:51:57", "updated": "2025-05-19 13:51:57", "pdf_url": "http://arxiv.org/pdf/2505.13123v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13124v1", "title": "$\u03bc$PC: Scaling Predictive Coding to 100+ Layer Networks", "authors": ["Francesco Innocenti", "El Mehdi Achour", "Christopher L. Buckley"], "abstract": "The biological implausibility of backpropagation (BP) has motivated many\nalternative, brain-inspired algorithms that attempt to rely only on local\ninformation, such as predictive coding (PC) and equilibrium propagation.\nHowever, these algorithms have notoriously struggled to train very deep\nnetworks, preventing them from competing with BP in large-scale settings.\nIndeed, scaling PC networks (PCNs) has recently been posed as a challenge for\nthe community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can\nbe trained reliably using a Depth-$\\mu$P parameterisation (Yang et al., 2023;\nBordelon et al., 2023) which we call \"$\\mu$PC\". Through an extensive analysis\nof the scaling behaviour of PCNs, we reveal several pathologies that make\nstandard PCNs difficult to train at large depths. We then show that, despite\naddressing only some of these instabilities, $\\mu$PC allows stable training of\nvery deep (up to 128-layer) residual networks on simple classification tasks\nwith competitive performance and little tuning compared to current benchmarks.\nMoreover, $\\mu$PC enables zero-shot transfer of both weight and activity\nlearning rates across widths and depths. Our results have implications for\nother local algorithms and could be extended to convolutional and transformer\narchitectures. Code for $\\mu$PC is made available as part of a JAX library for\nPCNs at https://github.com/thebuckleylab/jpc (Innocenti et al., 2024).", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.6"], "published": "2025-05-19 13:54:29", "updated": "2025-05-19 13:54:29", "pdf_url": "http://arxiv.org/pdf/2505.13124v1", "comment": "34 pages, 41 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13126v1", "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "abstract": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 13:58:15", "updated": "2025-05-19 13:58:15", "pdf_url": "http://arxiv.org/pdf/2505.13126v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13130v1", "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach", "authors": ["Muhammad Awais Amin", "Adama Ilboudo", "Abdul Samad bin Shahid", "Amjad Ali", "Waqas Haider Khan Bangyal"], "abstract": "One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:00:10", "updated": "2025-05-19 14:00:10", "pdf_url": "http://arxiv.org/pdf/2505.13130v1", "comment": null, "doi": "10.63075/2jepm102", "journal_ref": "Annual Methodological Archive Research Review 3 (05) (2025),\n  296-321"}
{"arxiv_id": "2505.13136v1", "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "abstract": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 14:07:20", "updated": "2025-05-19 14:07:20", "pdf_url": "http://arxiv.org/pdf/2505.13136v1", "comment": "under review @ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13144v1", "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning", "authors": ["Dongsu Lee", "Minhae Kwon"], "abstract": "The goal of offline reinforcement learning (RL) is to extract a\nhigh-performance policy from the fixed datasets, minimizing performance\ndegradation due to out-of-distribution (OOD) samples. Offline model-based RL\n(MBRL) is a promising approach that ameliorates OOD issues by enriching\nstate-action transitions with augmentations synthesized via a learned dynamics\nmodel. Unfortunately, seminal offline MBRL methods often struggle in\nsparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL\nframework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),\nthat generates augmented transitions in a temporally structured latent space\nrather than in raw state space. To model long-horizon behavior, TempDATA learns\na latent abstraction that captures a temporal distance from both trajectory and\ntransition levels of state space. Our experiments confirm that TempDATA\noutperforms previous offline MBRL methods and achieves matching or surpassing\nthe performance of diffusion-based trajectory augmentation and goal-conditioned\nRL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-19 14:11:14", "updated": "2025-05-19 14:11:14", "pdf_url": "http://arxiv.org/pdf/2505.13144v1", "comment": "2025 ICML", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13156v1", "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "abstract": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:17:37", "updated": "2025-05-19 14:17:37", "pdf_url": "http://arxiv.org/pdf/2505.13156v1", "comment": "23 pages, 4 figures, and 1 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13157v1", "title": "Role-Playing Evaluation for Large Language Models", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "abstract": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:18:16", "updated": "2025-05-19 14:18:16", "pdf_url": "http://arxiv.org/pdf/2505.13157v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13175v1", "title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment", "authors": ["Siming Sun", "Kai Zhang", "Xuejun Jiang", "Wenchao Meng", "Qinmin Yang"], "abstract": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting.", "categories": ["cs.AI"], "published": "2025-05-19 14:30:41", "updated": "2025-05-19 14:30:41", "pdf_url": "http://arxiv.org/pdf/2505.13175v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13176v1", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "abstract": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:30:46", "updated": "2025-05-19 14:30:46", "pdf_url": "http://arxiv.org/pdf/2505.13176v1", "comment": "Accepted by ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13180v1", "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models", "authors": ["Matteo Merler", "Nicola Dainese", "Minttu Alakuijala", "Giovanni Bonetta", "Pietro Ferrazzi", "Yu Tian", "Bernardo Magnini", "Pekka Marttinen"], "abstract": "Integrating Large Language Models with symbolic planners is a promising\ndirection for obtaining verifiable and grounded plans compared to planning in\nnatural language, with recent works extending this idea to visual domains using\nVision-Language Models (VLMs). However, rigorous comparison between\nVLM-grounded symbolic approaches and methods that plan directly with a VLM has\nbeen hindered by a lack of common environments, evaluation protocols and model\ncoverage. We introduce ViPlan, the first open-source benchmark for Visual\nPlanning with symbolic predicates and VLMs. ViPlan features a series of\nincreasingly challenging tasks in two domains: a visual variant of the classic\nBlocksworld planning problem and a simulated household robotics environment. We\nbenchmark nine open-source VLM families across multiple sizes, along with\nselected closed models, evaluating both VLM-grounded symbolic planning and\nusing the models directly to propose actions. We find symbolic planning to\noutperform direct VLM planning in Blocksworld, where accurate image grounding\nis crucial, whereas the opposite is true in the household robotics tasks, where\ncommonsense knowledge and the ability to recover from errors are beneficial.\nFinally, we show that across most models and methods, there is no significant\nbenefit to using Chain-of-Thought prompting, suggesting that current VLMs still\nstruggle with visual reasoning.", "categories": ["cs.AI"], "published": "2025-05-19 14:38:15", "updated": "2025-05-19 14:38:15", "pdf_url": "http://arxiv.org/pdf/2505.13180v1", "comment": "9 pages, 5 figures and 1 table in the main text; 43 pages, 9 figures\n  and 16 tables including supplementary material", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13182v1", "title": "Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping", "authors": ["Jianfeng Xu"], "abstract": "[Objective] This study focuses on addressing the current lack of a unified\nformal theoretical framework in machine learning, as well as the deficiencies\nin interpretability and ethical safety assurance. [Methods] A formal\ninformation model is first constructed, utilizing sets of well-formed formulas\nto explicitly define the ontological states and carrier mappings of typical\ncomponents in machine learning. Learnable and processable predicates, along\nwith learning and processing functions, are introduced to analyze the logical\ndeduction and constraint rules of the causal chains within models. [Results] A\nmeta-framework for machine learning theory (MLT-MF) is established. Based on\nthis framework, universal definitions for model interpretability and ethical\nsafety are proposed. Furthermore, three key theorems are proved: the\nequivalence of model interpretability and information recoverability, the\nassurance of ethical safety, and the estimation of generalization error.\n[Limitations] The current framework assumes ideal conditions with noiseless\ninformation-enabling mappings and primarily targets model learning and\nprocessing logic in static scenarios. It does not yet address information\nfusion and conflict resolution across ontological spaces in multimodal or\nmulti-agent systems. [Conclusions] This work overcomes the limitations of\nfragmented research and provides a unified theoretical foundation for\nsystematically addressing the critical challenges currently faced in machine\nlearning.", "categories": ["cs.LO", "cs.AI"], "published": "2025-05-19 14:39:41", "updated": "2025-05-19 14:39:41", "pdf_url": "http://arxiv.org/pdf/2505.13182v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13188v1", "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns", "authors": ["Juntian Zhu", "Miguel de Carvalho", "Zhouwang Yang", "Fengxiang He"], "abstract": "An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-19 14:45:58", "updated": "2025-05-19 14:45:58", "pdf_url": "http://arxiv.org/pdf/2505.13188v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13191v1", "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision", "authors": ["Pengcheng Pan", "Yonekura Shogo", "Yasuo Kuniyoshi"], "abstract": "Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:48:36", "updated": "2025-05-19 14:48:36", "pdf_url": "http://arxiv.org/pdf/2505.13191v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13192v1", "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "authors": ["Christoph J\u00fcrgen Hemmer", "Daniel Durstewitz"], "abstract": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.", "categories": ["cs.LG", "cs.AI", "math.DS", "nlin.CD"], "published": "2025-05-19 14:49:10", "updated": "2025-05-19 14:49:10", "pdf_url": "http://arxiv.org/pdf/2505.13192v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13195v1", "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities", "authors": ["Lili Zhang", "Haomiaomiao Wang", "Long Cheng", "Libao Deng", "Tomas Ward"], "abstract": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch.", "categories": ["cs.AI"], "published": "2025-05-19 14:50:44", "updated": "2025-05-19 14:50:44", "pdf_url": "http://arxiv.org/pdf/2505.13195v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13196v1", "title": "A Physics-Inspired Optimizer: Velocity Regularized Adam", "authors": ["Pranav Vaidhyanathan", "Lucas Schorling", "Natalia Ares", "Michael A. Osborne"], "abstract": "We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer\nfor training deep neural networks that draws on ideas from quartic terms for\nkinetic energy with its stabilizing effects on various system dynamics.\nPrevious algorithms, including the ubiquitous Adam, operate at the so called\nadaptive edge of stability regime during training leading to rapid oscillations\nand slowed convergence of loss. However, VRAdam adds a higher order penalty on\nthe learning rate based on the velocity such that the algorithm automatically\nslows down whenever weight updates become large. In practice, we observe that\nthe effective dynamic learning rate shrinks in high-velocity regimes, damping\noscillations and allowing for a more aggressive base step size when necessary\nwithout divergence. By combining this velocity-based regularizer for global\ndamping with per-parameter scaling of Adam to create a hybrid optimizer, we\ndemonstrate that VRAdam consistently exceeds the performance against standard\noptimizers including AdamW. We benchmark various tasks such as image\nclassification, language modeling, image generation and generative modeling\nusing diverse architectures and training methodologies including Convolutional\nNeural Networks (CNNs), Transformers, and GFlowNets.", "categories": ["cs.LG", "cs.AI", "quant-ph"], "published": "2025-05-19 14:51:40", "updated": "2025-05-19 14:51:40", "pdf_url": "http://arxiv.org/pdf/2505.13196v1", "comment": "L. Schorling and P. Vaidhyanathan contributed equally to this work.\n  20 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13201v1", "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "abstract": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:54:04", "updated": "2025-05-19 14:54:04", "pdf_url": "http://arxiv.org/pdf/2505.13201v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13208v1", "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "abstract": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.", "categories": ["quant-ph", "cs.AI", "cs.CL"], "published": "2025-05-19 14:57:53", "updated": "2025-05-19 14:57:53", "pdf_url": "http://arxiv.org/pdf/2505.13208v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13210v1", "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "abstract": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:58:44", "updated": "2025-05-19 14:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13210v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13211v1", "title": "MAGI-1: Autoregressive Video Generation at Scale", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "abstract": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:58:50", "updated": "2025-05-19 14:58:50", "pdf_url": "http://arxiv.org/pdf/2505.13211v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "published": "2025-05-19 15:09:23", "updated": "2025-05-19 15:09:23", "pdf_url": "http://arxiv.org/pdf/2505.13227v1", "comment": "49 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13232v1", "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "abstract": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-19 15:15:35", "updated": "2025-05-19 15:15:35", "pdf_url": "http://arxiv.org/pdf/2505.13232v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13246v1", "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "abstract": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.", "categories": ["cs.AI", "cs.HC"], "published": "2025-05-19 15:28:10", "updated": "2025-05-19 15:28:10", "pdf_url": "http://arxiv.org/pdf/2505.13246v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13253v1", "title": "Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic", "authors": ["Lennart R\u00f6stel", "Dominik Winkelbauer", "Johannes Pitz", "Leon Sievers", "Berthold B\u00e4uml"], "abstract": "In-hand manipulation and grasping are fundamental yet often separately\naddressed tasks in robotics. For deriving in-hand manipulation policies,\nreinforcement learning has recently shown great success. However, the derived\ncontrollers are not yet useful in real-world scenarios because they often\nrequire a human operator to place the objects in suitable initial (grasping)\nstates. Finding stable grasps that also promote the desired in-hand\nmanipulation goal is an open problem. In this work, we propose a method for\nbridging this gap by leveraging the critic network of a reinforcement learning\nagent trained for in-hand manipulation to score and select initial grasps. Our\nexperiments show that this method significantly increases the success rate of\nin-hand manipulation without requiring additional training. We also present an\nimplementation of a full grasp manipulation pipeline on a real-world system,\nenabling autonomous grasping and reorientation even of unwieldy objects.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-19 15:36:34", "updated": "2025-05-19 15:36:34", "pdf_url": "http://arxiv.org/pdf/2505.13253v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13257v1", "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "authors": ["Zilu Tang", "Afra Feyza Aky\u00fcrek", "Ekin Aky\u00fcrek", "Derry Wijaya"], "abstract": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:39:48", "updated": "2025-05-19 15:39:48", "pdf_url": "http://arxiv.org/pdf/2505.13257v1", "comment": "9 pages, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13264v1", "title": "Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty", "authors": ["Carlos Rodriguez-Pardo", "Louis Daumas", "Leonardo Chiani", "Massimo Tavoni"], "abstract": "Climate-economic modeling under uncertainty presents significant\ncomputational challenges that may limit policymakers' ability to address\nclimate change effectively. This paper explores neural network-based approaches\nfor solving high-dimensional optimal control problems arising from models that\nincorporate ambiguity aversion in climate mitigation decisions. We develop a\ncontinuous-time endogenous-growth economic model that accounts for multiple\nmitigation pathways, including emission-free capital and carbon intensity\nreductions. Given the inherent complexity and high dimensionality of these\nmodels, traditional numerical methods become computationally intractable. We\nbenchmark several neural network architectures against finite-difference\ngenerated solutions, evaluating their ability to capture the dynamic\ninteractions between uncertainty, technology transitions, and optimal climate\npolicy. Our findings demonstrate that appropriate neural architecture selection\nsignificantly impacts both solution accuracy and computational efficiency when\nmodeling climate-economic systems under uncertainty. These methodological\nadvances enable more sophisticated modeling of climate policy decisions,\nallowing for better representation of technology transitions and\nuncertainty-critical elements for developing effective mitigation strategies in\nthe face of climate change.", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF", "math.AP", "68T07 (Primary) 35Q91, 91B76 (Secondary)", "I.2.1; I.5.1; J.4"], "published": "2025-05-19 15:46:12", "updated": "2025-05-19 15:46:12", "pdf_url": "http://arxiv.org/pdf/2505.13264v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13268v1", "title": "Representation of perceived prosodic similarity of conversational feedback", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "abstract": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:47:51", "updated": "2025-05-19 15:47:51", "pdf_url": "http://arxiv.org/pdf/2505.13268v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13273v1", "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "abstract": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-19 15:53:32", "updated": "2025-05-19 15:53:32", "pdf_url": "http://arxiv.org/pdf/2505.13273v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13280v1", "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "authors": ["Elias Collaert", "Abel Rodr\u00edguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "abstract": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "published": "2025-05-19 16:04:43", "updated": "2025-05-19 16:04:43", "pdf_url": "http://arxiv.org/pdf/2505.13280v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13287v1", "title": "Level Generation with Quantum Reservoir Computing", "authors": ["Jo\u00e3o S. Ferreira", "Pierre Fromholz", "Hari Shaji", "James R. Wootton"], "abstract": "Reservoir computing is a form of machine learning particularly suited for\ntime series analysis, including forecasting predictions. We take an\nimplementation of \\emph{quantum} reservoir computing that was initially\ndesigned to generate variants of musical scores and adapt it to create levels\nof Super Mario Bros. Motivated by our analysis of these levels, we develop a\nnew Roblox \\textit{obby} where the courses can be generated in real time on\nsuperconducting qubit hardware, and investigate some of the constraints placed\nby such real-time generation.", "categories": ["cs.AI", "quant-ph"], "published": "2025-05-19 16:09:30", "updated": "2025-05-19 16:09:30", "pdf_url": "http://arxiv.org/pdf/2505.13287v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13291v1", "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Micha\u0142 Wili\u0144ski", "Gus Welter", "Artur Dubrawski"], "abstract": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 16:11:23", "updated": "2025-05-19 16:11:23", "pdf_url": "http://arxiv.org/pdf/2505.13291v1", "comment": "Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13292v1", "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs", "authors": ["Huaiying Luo", "Cheng Ji"], "abstract": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 16:14:27", "updated": "2025-05-19 16:14:27", "pdf_url": "http://arxiv.org/pdf/2505.13292v1", "comment": "Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13307v1", "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-19 16:25:55", "updated": "2025-05-19 16:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13307v1", "comment": "Manuscript", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13308v1", "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "abstract": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 16:26:02", "updated": "2025-05-19 16:26:02", "pdf_url": "http://arxiv.org/pdf/2505.13308v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13315v1", "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "authors": ["Reza T. Batley", "Sourav Saha"], "abstract": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.", "categories": ["cs.LG", "cs.AI", "cs.MS"], "published": "2025-05-19 16:29:07", "updated": "2025-05-19 16:29:07", "pdf_url": "http://arxiv.org/pdf/2505.13315v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13316v1", "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "abstract": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 16:29:12", "updated": "2025-05-19 16:29:12", "pdf_url": "http://arxiv.org/pdf/2505.13316v1", "comment": "6 pages, 5 figures, accepted at ICME 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13324v1", "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "abstract": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI.", "categories": ["stat.ML", "cs.AI", "cs.LG", "econ.EM", "stat.ME"], "published": "2025-05-19 16:34:36", "updated": "2025-05-19 16:34:36", "pdf_url": "http://arxiv.org/pdf/2505.13324v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13329v1", "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications", "authors": ["Fr\u00e9d\u00e9ric Berdoz", "Dustin Brunner", "Yann Vonlanthen", "Roger Wattenhofer"], "abstract": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future.", "categories": ["cs.CY", "cs.AI", "cs.CR"], "published": "2025-05-19 16:38:06", "updated": "2025-05-19 16:38:06", "pdf_url": "http://arxiv.org/pdf/2505.13329v1", "comment": "This is the extended version of the paper, accepted at IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "abstract": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "categories": ["cs.CL", "cs.AI", "eess.AS"], "published": "2025-05-19 16:47:46", "updated": "2025-05-19 16:47:46", "pdf_url": "http://arxiv.org/pdf/2505.13338v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13339v1", "title": "OPA-Pack: Object-Property-Aware Robotic Bin Packing", "authors": ["Jia-Hui Pan", "Yeok Tatt Cheah", "Zhengzhe Liu", "Ka-Hei Hui", "Xiaojie Gao", "Pheng-Ann Heng", "Yun-Hui Liu", "Chi-Wing Fu"], "abstract": "Robotic bin packing aids in a wide range of real-world scenarios such as\ne-commerce and warehouses. Yet, existing works focus mainly on considering the\nshape of objects to optimize packing compactness and neglect object properties\nsuch as fragility, edibility, and chemistry that humans typically consider when\npacking objects. This paper presents OPA-Pack (Object-Property-Aware Packing\nframework), the first framework that equips the robot with object property\nconsiderations in planning the object packing. Technical-wise, we develop a\nnovel object property recognition scheme with retrieval-augmented generation\nand chain-of-thought reasoning, and build a dataset with object property\nannotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to\njointly separate incompatible object pairs and reduce pressure on fragile\nobjects, while compacting the packing. Further, OPA-Net consists of a property\nembedding layer to encode the property of candidate objects to be packed,\ntogether with a fragility heightmap and an avoidance heightmap to keep track of\nthe packed objects. Then, we design a reward function and adopt a deep\nQ-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack\ngreatly improves the accuracy of separating incompatible object pairs (from 52%\nto 95%) and largely reduces pressure on fragile objects (by 29.4%), while\nmaintaining good packing compactness. Besides, we demonstrate the effectiveness\nof OPA-Pack on a real packing platform, showcasing its practicality in\nreal-world scenarios.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-19 16:48:14", "updated": "2025-05-19 16:48:14", "pdf_url": "http://arxiv.org/pdf/2505.13339v1", "comment": "Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13344v1", "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "abstract": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 16:50:26", "updated": "2025-05-19 16:50:26", "pdf_url": "http://arxiv.org/pdf/2505.13344v1", "comment": "https://berkegokmen1.github.io/RoPECraft/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13346v1", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "abstract": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 16:50:35", "updated": "2025-05-19 16:50:35", "pdf_url": "http://arxiv.org/pdf/2505.13346v1", "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13355v1", "title": "Multi-Armed Bandits Meet Large Language Models", "authors": ["Djallel Bouneffouf", "Raphael Feraud"], "abstract": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.", "categories": ["cs.AI"], "published": "2025-05-19 16:57:57", "updated": "2025-05-19 16:57:57", "pdf_url": "http://arxiv.org/pdf/2505.13355v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13358v1", "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "abstract": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 16:59:47", "updated": "2025-05-19 16:59:47", "pdf_url": "http://arxiv.org/pdf/2505.13358v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13372v1", "title": "Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning", "authors": ["Irene Brugnara", "Alessandro Valentini", "Andrea Micheli"], "abstract": "Recent work investigated the use of Reinforcement Learning (RL) for the\nsynthesis of heuristic guidance to improve the performance of temporal planners\nwhen a domain is fixed and a set of training problems (not plans) is given. The\nidea is to extract a heuristic from the value function of a particular\n(possibly infinite-state) MDP constructed over the training problems.\n  In this paper, we propose an evolution of this learning and planning\nframework that focuses on exploiting the information provided by symbolic\nheuristics during both the RL and planning phases. First, we formalize\ndifferent reward schemata for the synthesis and use symbolic heuristics to\nmitigate the problems caused by the truncation of episodes needed to deal with\nthe potentially infinite MDP. Second, we propose learning a residual of an\nexisting symbolic heuristic, which is a \"correction\" of the heuristic value,\ninstead of eagerly learning the whole heuristic from scratch. Finally, we use\nthe learned heuristic in combination with a symbolic heuristic using a\nmultiple-queue planning approach to balance systematic search with imperfect\nlearned information. We experimentally compare all the approaches, highlighting\ntheir strengths and weaknesses and significantly advancing the state of the art\nfor this planning and learning schema.", "categories": ["cs.AI"], "published": "2025-05-19 17:19:13", "updated": "2025-05-19 17:19:13", "pdf_url": "http://arxiv.org/pdf/2505.13372v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13379v1", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:24:16", "updated": "2025-05-19 17:24:16", "pdf_url": "http://arxiv.org/pdf/2505.13379v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13380v1", "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:24:26", "updated": "2025-05-19 17:24:26", "pdf_url": "http://arxiv.org/pdf/2505.13380v1", "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13381v1", "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "abstract": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms.", "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.7; H.5.2"], "published": "2025-05-19 17:25:07", "updated": "2025-05-19 17:25:07", "pdf_url": "http://arxiv.org/pdf/2505.13381v1", "comment": "10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM\n  Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy", "doi": "10.1145/3698205.3729542", "journal_ref": null}
{"arxiv_id": "2505.13388v1", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:29:03", "updated": "2025-05-19 17:29:03", "pdf_url": "http://arxiv.org/pdf/2505.13388v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13391v1", "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "authors": ["Miko\u0142aj Ma\u0142ki\u0144ski", "Jacek Ma\u0144dziuk"], "abstract": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods.", "categories": ["cs.AI", "cs.CV", "cs.LG"], "published": "2025-05-19 17:32:07", "updated": "2025-05-19 17:32:07", "pdf_url": "http://arxiv.org/pdf/2505.13391v1", "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13393v1", "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "authors": ["Christopher K. Frantz"], "abstract": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool.", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "published": "2025-05-19 17:33:15", "updated": "2025-05-19 17:33:15", "pdf_url": "http://arxiv.org/pdf/2505.13393v1", "comment": "24 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13400v1", "title": "Robin: A multi-agent system for automating scientific discovery", "authors": ["Ali Essam Ghareeb", "Benjamin Chang", "Ludovico Mitchener", "Angela Yiu", "Caralyn J. Szostkiewicz", "Jon M. Laurent", "Muhammed T. Razzak", "Andrew D. White", "Michaela M. Hinks", "Samuel G. Rodriques"], "abstract": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.", "categories": ["cs.AI", "cs.MA", "q-bio.QM"], "published": "2025-05-19 17:36:17", "updated": "2025-05-19 17:36:17", "pdf_url": "http://arxiv.org/pdf/2505.13400v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13406v1", "title": "AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database", "authors": ["Rong Bian", "Yu Geng", "Zijian Yang", "Bing Cheng"], "abstract": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM.", "categories": ["cs.AI"], "published": "2025-05-19 17:41:29", "updated": "2025-05-19 17:41:29", "pdf_url": "http://arxiv.org/pdf/2505.13406v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13408v1", "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "abstract": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:44:26", "updated": "2025-05-19 17:44:26", "pdf_url": "http://arxiv.org/pdf/2505.13408v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13417v1", "title": "AdaptThink: Reasoning Models Can Learn When to Think", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "abstract": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:50:52", "updated": "2025-05-19 17:50:52", "pdf_url": "http://arxiv.org/pdf/2505.13417v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13425v1", "title": "Learnware of Language Models: Specialized Small Language Models Can Do Big", "authors": ["Zhi-Hao Tan", "Zi-Chen Zhao", "Hao-Yu Shi", "Xin-Yu Zhang", "Peng Tan", "Yang Yu", "Zhi-Hua Zhou"], "abstract": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 17:54:35", "updated": "2025-05-19 17:54:35", "pdf_url": "http://arxiv.org/pdf/2505.13425v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13427v1", "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-19 17:55:08", "updated": "2025-05-19 17:55:08", "pdf_url": "http://arxiv.org/pdf/2505.13427v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13437v1", "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "abstract": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 17:58:11", "updated": "2025-05-19 17:58:11", "pdf_url": "http://arxiv.org/pdf/2505.13437v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 17:58:44", "updated": "2025-05-19 17:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13439v1", "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "abstract": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 17:59:01", "updated": "2025-05-19 17:59:01", "pdf_url": "http://arxiv.org/pdf/2505.13439v1", "comment": "24 pages, 13 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:59:31", "updated": "2025-05-19 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.13445v1", "comment": "code available at https://github.com/xyliu-cs/RISE", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "abstract": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:59:58", "updated": "2025-05-19 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.13448v1", "comment": "10 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12584v1", "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "abstract": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations.", "categories": ["cs.CL"], "published": "2025-05-19 00:14:43", "updated": "2025-05-19 00:14:43", "pdf_url": "http://arxiv.org/pdf/2505.12584v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12587v1", "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "abstract": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 00:50:49", "updated": "2025-05-19 00:50:49", "pdf_url": "http://arxiv.org/pdf/2505.12587v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12592v1", "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "abstract": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts.", "categories": ["cs.CL"], "published": "2025-05-19 01:08:26", "updated": "2025-05-19 01:08:26", "pdf_url": "http://arxiv.org/pdf/2505.12592v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12594v1", "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "abstract": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 01:14:57", "updated": "2025-05-19 01:14:57", "pdf_url": "http://arxiv.org/pdf/2505.12594v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12616v1", "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "authors": ["Shujauddin Syed", "Ted Pedersen"], "abstract": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios.", "categories": ["cs.CL", "68T50"], "published": "2025-05-19 01:58:22", "updated": "2025-05-19 01:58:22", "pdf_url": "http://arxiv.org/pdf/2505.12616v1", "comment": "SemEval-2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12621v1", "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "authors": ["Jo\u00e3o Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "abstract": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-19 02:08:20", "updated": "2025-05-19 02:08:20", "pdf_url": "http://arxiv.org/pdf/2505.12621v1", "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12625v1", "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "abstract": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment.", "categories": ["cs.CL", "cs.CR", "cs.LG"], "published": "2025-05-19 02:16:56", "updated": "2025-05-19 02:16:56", "pdf_url": "http://arxiv.org/pdf/2505.12625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12629v1", "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "abstract": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 02:35:53", "updated": "2025-05-19 02:35:53", "pdf_url": "http://arxiv.org/pdf/2505.12629v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12632v1", "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 02:39:03", "updated": "2025-05-19 02:39:03", "pdf_url": "http://arxiv.org/pdf/2505.12632v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12636v1", "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "abstract": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here.", "categories": ["cs.CL"], "published": "2025-05-19 02:44:57", "updated": "2025-05-19 02:44:57", "pdf_url": "http://arxiv.org/pdf/2505.12636v1", "comment": "Accepted by ACL 2025 main", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12654v1", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "abstract": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:08:30", "updated": "2025-05-19 03:08:30", "pdf_url": "http://arxiv.org/pdf/2505.12654v1", "comment": "Accepected by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12662v1", "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "abstract": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:25:18", "updated": "2025-05-19 03:25:18", "pdf_url": "http://arxiv.org/pdf/2505.12662v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12680v1", "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "abstract": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 03:56:05", "updated": "2025-05-19 03:56:05", "pdf_url": "http://arxiv.org/pdf/2505.12680v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12692v1", "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "abstract": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 04:32:02", "updated": "2025-05-19 04:32:02", "pdf_url": "http://arxiv.org/pdf/2505.12692v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12716v1", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "abstract": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 05:16:21", "updated": "2025-05-19 05:16:21", "pdf_url": "http://arxiv.org/pdf/2505.12716v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12717v1", "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "abstract": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks.", "categories": ["cs.CL"], "published": "2025-05-19 05:18:58", "updated": "2025-05-19 05:18:58", "pdf_url": "http://arxiv.org/pdf/2505.12717v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12718v1", "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "abstract": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content.", "categories": ["cs.CL", "cs.HC"], "published": "2025-05-19 05:19:26", "updated": "2025-05-19 05:19:26", "pdf_url": "http://arxiv.org/pdf/2505.12718v1", "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12723v1", "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "abstract": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages.", "categories": ["cs.CL"], "published": "2025-05-19 05:25:29", "updated": "2025-05-19 05:25:29", "pdf_url": "http://arxiv.org/pdf/2505.12723v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12727v1", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "abstract": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma.", "categories": ["cs.CL", "cs.CY", "cs.HC"], "published": "2025-05-19 05:31:42", "updated": "2025-05-19 05:31:42", "pdf_url": "http://arxiv.org/pdf/2505.12727v1", "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12763v1", "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "abstract": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 06:43:08", "updated": "2025-05-19 06:43:08", "pdf_url": "http://arxiv.org/pdf/2505.12763v1", "comment": "Accepted to ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12768v1", "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "abstract": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set.", "categories": ["cs.CL"], "published": "2025-05-19 06:46:47", "updated": "2025-05-19 06:46:47", "pdf_url": "http://arxiv.org/pdf/2505.12768v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12781v1", "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:10:42", "updated": "2025-05-19 07:10:42", "pdf_url": "http://arxiv.org/pdf/2505.12781v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12792v1", "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "abstract": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies.", "categories": ["cs.CL"], "published": "2025-05-19 07:24:35", "updated": "2025-05-19 07:24:35", "pdf_url": "http://arxiv.org/pdf/2505.12792v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12808v1", "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "abstract": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 07:34:25", "updated": "2025-05-19 07:34:25", "pdf_url": "http://arxiv.org/pdf/2505.12808v1", "comment": "20 pages, ongoing work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12814v1", "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "abstract": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:45:09", "updated": "2025-05-19 07:45:09", "pdf_url": "http://arxiv.org/pdf/2505.12814v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12821v1", "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "abstract": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:03:38", "updated": "2025-05-19 08:03:38", "pdf_url": "http://arxiv.org/pdf/2505.12821v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12831v1", "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "abstract": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP.", "categories": ["cs.CL"], "published": "2025-05-19 08:19:27", "updated": "2025-05-19 08:19:27", "pdf_url": "http://arxiv.org/pdf/2505.12831v1", "comment": "ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12835v1", "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "abstract": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-19 08:21:20", "updated": "2025-05-19 08:21:20", "pdf_url": "http://arxiv.org/pdf/2505.12835v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12837v1", "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "abstract": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:25:21", "updated": "2025-05-19 08:25:21", "pdf_url": "http://arxiv.org/pdf/2505.12837v1", "comment": "20 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12842v1", "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "abstract": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 08:29:05", "updated": "2025-05-19 08:29:05", "pdf_url": "http://arxiv.org/pdf/2505.12842v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12859v1", "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "abstract": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge.", "categories": ["cs.CL"], "published": "2025-05-19 08:43:54", "updated": "2025-05-19 08:43:54", "pdf_url": "http://arxiv.org/pdf/2505.12859v1", "comment": "To be presented a ACL 2025, Main, Long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12864v1", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr\u00fcwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "abstract": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "published": "2025-05-19 08:48:12", "updated": "2025-05-19 08:48:12", "pdf_url": "http://arxiv.org/pdf/2505.12864v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12871v1", "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-19 08:57:08", "updated": "2025-05-19 08:57:08", "pdf_url": "http://arxiv.org/pdf/2505.12871v1", "comment": "To appear at ICML 25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12886v1", "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "categories": ["cs.AI", "cs.CL", "cs.CY"], "published": "2025-05-19 09:16:40", "updated": "2025-05-19 09:16:40", "pdf_url": "http://arxiv.org/pdf/2505.12886v1", "comment": "25 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12888v1", "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "abstract": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines.", "categories": ["cs.CL"], "published": "2025-05-19 09:18:19", "updated": "2025-05-19 09:18:19", "pdf_url": "http://arxiv.org/pdf/2505.12888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12891v1", "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "abstract": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 09:22:02", "updated": "2025-05-19 09:22:02", "pdf_url": "http://arxiv.org/pdf/2505.12891v1", "comment": "First version. There are still some examples to be added into the\n  appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12896v1", "title": "On the Thinking-Language Modeling Gap in Large Language Models", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "abstract": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks.", "categories": ["cs.CL", "cs.LG", "stat.ML"], "published": "2025-05-19 09:31:52", "updated": "2025-05-19 09:31:52", "pdf_url": "http://arxiv.org/pdf/2505.12896v1", "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12900v1", "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "abstract": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation.", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "published": "2025-05-19 09:35:58", "updated": "2025-05-19 09:35:58", "pdf_url": "http://arxiv.org/pdf/2505.12900v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12920v1", "title": "PyFCG: Fluid Construction Grammar in Python", "authors": ["Paul Van Eecke", "Katrien Beuls"], "abstract": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-19 10:00:01", "updated": "2025-05-19 10:00:01", "pdf_url": "http://arxiv.org/pdf/2505.12920v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12929v1", "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "abstract": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:14:08", "updated": "2025-05-19 10:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12929v1", "comment": "24 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12938v1", "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "abstract": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 10:22:04", "updated": "2025-05-19 10:22:04", "pdf_url": "http://arxiv.org/pdf/2505.12938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12942v1", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "abstract": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:29:32", "updated": "2025-05-19 10:29:32", "pdf_url": "http://arxiv.org/pdf/2505.12942v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12949v1", "title": "Neural Morphological Tagging for Nguni Languages", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "abstract": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages.", "categories": ["cs.CL"], "published": "2025-05-19 10:41:47", "updated": "2025-05-19 10:41:47", "pdf_url": "http://arxiv.org/pdf/2505.12949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12950v1", "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "abstract": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE.", "categories": ["cs.CL"], "published": "2025-05-19 10:42:36", "updated": "2025-05-19 10:42:36", "pdf_url": "http://arxiv.org/pdf/2505.12950v1", "comment": "14 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12964v1", "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "abstract": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master.", "categories": ["cs.CL"], "published": "2025-05-19 11:00:43", "updated": "2025-05-19 11:00:43", "pdf_url": "http://arxiv.org/pdf/2505.12964v1", "comment": "preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12969v1", "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "abstract": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other.", "categories": ["cs.CL"], "published": "2025-05-19 11:04:52", "updated": "2025-05-19 11:04:52", "pdf_url": "http://arxiv.org/pdf/2505.12969v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12970v1", "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "authors": ["Robin Jegan", "Andreas Henrich"], "abstract": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801", "categories": ["cs.CL"], "published": "2025-05-19 11:06:50", "updated": "2025-05-19 11:06:50", "pdf_url": "http://arxiv.org/pdf/2505.12970v1", "comment": "14 pages, 1 figure", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12973v1", "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "abstract": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems.", "categories": ["cs.CL"], "published": "2025-05-19 11:11:12", "updated": "2025-05-19 11:11:12", "pdf_url": "http://arxiv.org/pdf/2505.12973v1", "comment": "8 main body pages, total 25 pages, 15 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12983v1", "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:18:54", "updated": "2025-05-19 11:18:54", "pdf_url": "http://arxiv.org/pdf/2505.12983v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12992v1", "title": "Fractured Chain-of-Thought Reasoning", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "published": "2025-05-19 11:30:41", "updated": "2025-05-19 11:30:41", "pdf_url": "http://arxiv.org/pdf/2505.12992v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12996v1", "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:34:47", "updated": "2025-05-19 11:34:47", "pdf_url": "http://arxiv.org/pdf/2505.12996v1", "comment": "12 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13004v1", "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "abstract": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x.", "categories": ["cs.CL"], "published": "2025-05-19 11:43:37", "updated": "2025-05-19 11:43:37", "pdf_url": "http://arxiv.org/pdf/2505.13004v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13006v1", "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias K\u00e4fer"], "abstract": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions.", "categories": ["cs.CL"], "published": "2025-05-19 11:46:30", "updated": "2025-05-19 11:46:30", "pdf_url": "http://arxiv.org/pdf/2505.13006v1", "comment": "Accepted by NAACL 2025 industry track", "doi": null, "journal_ref": "In Proc. NAACL-HLT 2025 Industry Track, pp. 794-808. Albuquerque,\n  NM, 2025"}
{"arxiv_id": "2505.13010v1", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "abstract": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-19 11:54:39", "updated": "2025-05-19 11:54:39", "pdf_url": "http://arxiv.org/pdf/2505.13010v1", "comment": "7 pages, 5 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13028v1", "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "authors": ["Sayon Palit", "Daniel Woods"], "abstract": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "published": "2025-05-19 12:12:00", "updated": "2025-05-19 12:12:00", "pdf_url": "http://arxiv.org/pdf/2505.13028v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13032v1", "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix", "authors": ["Ziyang Ma", "Yinghao Ma", "Yanqiao Zhu", "Chen Yang", "Yi-Wen Chao", "Ruiyang Xu", "Wenxi Chen", "Yuanzhe Chen", "Zhuo Chen", "Jian Cong", "Kai Li", "Keliang Li", "Siyou Li", "Xinfeng Li", "Xiquan Li", "Zheng Lian", "Yuzhe Liang", "Minghao Liu", "Zhikang Niu", "Tianrui Wang", "Yuping Wang", "Yuxuan Wang", "Yihao Wu", "Guanrou Yang", "Jianwei Yu", "Ruibin Yuan", "Zhisheng Zheng", "Ziya Zhou", "Haina Zhu", "Wei Xue", "Emmanouil Benetos", "Kai Yu", "Eng-Siong Chng", "Xie Chen"], "abstract": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area.", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "published": "2025-05-19 12:18:42", "updated": "2025-05-19 12:18:42", "pdf_url": "http://arxiv.org/pdf/2505.13032v1", "comment": "Open-source at https://github.com/ddlBoJack/MMAR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13034v1", "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "authors": ["M\u00e1rton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "abstract": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models.", "categories": ["cs.CL"], "published": "2025-05-19 12:19:01", "updated": "2025-05-19 12:19:01", "pdf_url": "http://arxiv.org/pdf/2505.13034v1", "comment": "9 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13036v1", "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "authors": ["Sai Koneru", "Maike Z\u00fcfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "abstract": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:21:29", "updated": "2025-05-19 12:21:29", "pdf_url": "http://arxiv.org/pdf/2505.13036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13053v1", "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "abstract": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:42:23", "updated": "2025-05-19 12:42:23", "pdf_url": "http://arxiv.org/pdf/2505.13053v1", "comment": "currently under review at Frontiers in Communication", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13069v1", "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenol\u00e9 Quellec"], "abstract": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability.", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "published": "2025-05-19 13:04:37", "updated": "2025-05-19 13:04:37", "pdf_url": "http://arxiv.org/pdf/2505.13069v1", "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13077v1", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "abstract": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 13:11:28", "updated": "2025-05-19 13:11:28", "pdf_url": "http://arxiv.org/pdf/2505.13077v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13089v1", "title": "Systematic Generalization in Language Models Scales with Information Entropy", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "\u00c9tienne Simon"], "abstract": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization.", "categories": ["cs.CL"], "published": "2025-05-19 13:23:44", "updated": "2025-05-19 13:23:44", "pdf_url": "http://arxiv.org/pdf/2505.13089v1", "comment": "Accepted to ACL 2025: Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13090v1", "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "authors": ["David Stap", "Christof Monz"], "abstract": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity.", "categories": ["cs.CL"], "published": "2025-05-19 13:24:01", "updated": "2025-05-19 13:24:01", "pdf_url": "http://arxiv.org/pdf/2505.13090v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13098v1", "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "abstract": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.", "categories": ["cs.AI", "cs.CL", "cs.DB"], "published": "2025-05-19 13:29:27", "updated": "2025-05-19 13:29:27", "pdf_url": "http://arxiv.org/pdf/2505.13098v1", "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13109v1", "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "abstract": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 13:36:45", "updated": "2025-05-19 13:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13109v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13115v1", "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "abstract": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-19 13:46:35", "updated": "2025-05-19 13:46:35", "pdf_url": "http://arxiv.org/pdf/2505.13115v1", "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13126v1", "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "abstract": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 13:58:15", "updated": "2025-05-19 13:58:15", "pdf_url": "http://arxiv.org/pdf/2505.13126v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13136v1", "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "abstract": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 14:07:20", "updated": "2025-05-19 14:07:20", "pdf_url": "http://arxiv.org/pdf/2505.13136v1", "comment": "under review @ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13141v1", "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "abstract": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English.", "categories": ["cs.CL"], "published": "2025-05-19 14:10:15", "updated": "2025-05-19 14:10:15", "pdf_url": "http://arxiv.org/pdf/2505.13141v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13147v1", "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "abstract": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP.", "categories": ["cs.CL"], "published": "2025-05-19 14:12:05", "updated": "2025-05-19 14:12:05", "pdf_url": "http://arxiv.org/pdf/2505.13147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13156v1", "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "abstract": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:17:37", "updated": "2025-05-19 14:17:37", "pdf_url": "http://arxiv.org/pdf/2505.13156v1", "comment": "23 pages, 4 figures, and 1 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13157v1", "title": "Role-Playing Evaluation for Large Language Models", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "abstract": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:18:16", "updated": "2025-05-19 14:18:16", "pdf_url": "http://arxiv.org/pdf/2505.13157v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13171v1", "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "abstract": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.", "categories": ["cs.CL"], "published": "2025-05-19 14:28:35", "updated": "2025-05-19 14:28:35", "pdf_url": "http://arxiv.org/pdf/2505.13171v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13173v1", "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.", "categories": ["cs.CL", "I.2.7"], "published": "2025-05-19 14:30:10", "updated": "2025-05-19 14:30:10", "pdf_url": "http://arxiv.org/pdf/2505.13173v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13176v1", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "abstract": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:30:46", "updated": "2025-05-19 14:30:46", "pdf_url": "http://arxiv.org/pdf/2505.13176v1", "comment": "Accepted by ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13181v1", "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "abstract": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-19 14:38:59", "updated": "2025-05-19 14:38:59", "pdf_url": "http://arxiv.org/pdf/2505.13181v1", "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13204v1", "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "abstract": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23.", "categories": ["cs.CL"], "published": "2025-05-19 14:55:41", "updated": "2025-05-19 14:55:41", "pdf_url": "http://arxiv.org/pdf/2505.13204v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13208v1", "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "abstract": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.", "categories": ["quant-ph", "cs.AI", "cs.CL"], "published": "2025-05-19 14:57:53", "updated": "2025-05-19 14:57:53", "pdf_url": "http://arxiv.org/pdf/2505.13208v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13210v1", "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "abstract": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:58:44", "updated": "2025-05-19 14:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13210v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13220v1", "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "abstract": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.", "categories": ["cs.CL"], "published": "2025-05-19 15:02:59", "updated": "2025-05-19 15:02:59", "pdf_url": "http://arxiv.org/pdf/2505.13220v1", "comment": "Accepted by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "published": "2025-05-19 15:09:23", "updated": "2025-05-19 15:09:23", "pdf_url": "http://arxiv.org/pdf/2505.13227v1", "comment": "49 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13237v1", "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information", "authors": ["Chih-Kai Yang", "Neo Ho", "Yen-Ting Piao", "Hung-yi Lee"], "abstract": "Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-19 15:20:32", "updated": "2025-05-19 15:20:32", "pdf_url": "http://arxiv.org/pdf/2505.13237v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13244v1", "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "abstract": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 15:24:53", "updated": "2025-05-19 15:24:53", "pdf_url": "http://arxiv.org/pdf/2505.13244v1", "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13251v1", "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "authors": ["Sidney Wong"], "abstract": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide.", "categories": ["cs.CL"], "published": "2025-05-19 15:34:07", "updated": "2025-05-19 15:34:07", "pdf_url": "http://arxiv.org/pdf/2505.13251v1", "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13252v1", "title": "Natural Language Planning via Coding and Inference Scaling", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "abstract": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization.", "categories": ["cs.CL"], "published": "2025-05-19 15:35:17", "updated": "2025-05-19 15:35:17", "pdf_url": "http://arxiv.org/pdf/2505.13252v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13254v1", "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "abstract": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration.", "categories": ["cs.CL"], "published": "2025-05-19 15:38:40", "updated": "2025-05-19 15:38:40", "pdf_url": "http://arxiv.org/pdf/2505.13254v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13257v1", "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "authors": ["Zilu Tang", "Afra Feyza Aky\u00fcrek", "Ekin Aky\u00fcrek", "Derry Wijaya"], "abstract": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:39:48", "updated": "2025-05-19 15:39:48", "pdf_url": "http://arxiv.org/pdf/2505.13257v1", "comment": "9 pages, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13258v1", "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "abstract": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.", "categories": ["cs.CL"], "published": "2025-05-19 15:40:29", "updated": "2025-05-19 15:40:29", "pdf_url": "http://arxiv.org/pdf/2505.13258v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13259v1", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "abstract": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "categories": ["cs.CL"], "published": "2025-05-19 15:41:32", "updated": "2025-05-19 15:41:32", "pdf_url": "http://arxiv.org/pdf/2505.13259v1", "comment": "16 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13268v1", "title": "Representation of perceived prosodic similarity of conversational feedback", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "abstract": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:47:51", "updated": "2025-05-19 15:47:51", "pdf_url": "http://arxiv.org/pdf/2505.13268v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13271v1", "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "abstract": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.", "categories": ["cs.CL"], "published": "2025-05-19 15:52:19", "updated": "2025-05-19 15:52:19", "pdf_url": "http://arxiv.org/pdf/2505.13271v1", "comment": "11 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13282v1", "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "abstract": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.", "categories": ["cs.CL"], "published": "2025-05-19 16:06:13", "updated": "2025-05-19 16:06:13", "pdf_url": "http://arxiv.org/pdf/2505.13282v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13302v1", "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "abstract": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "categories": ["cs.CL"], "published": "2025-05-19 16:20:54", "updated": "2025-05-19 16:20:54", "pdf_url": "http://arxiv.org/pdf/2505.13302v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13307v1", "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-19 16:25:55", "updated": "2025-05-19 16:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13307v1", "comment": "Manuscript", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13308v1", "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "abstract": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 16:26:02", "updated": "2025-05-19 16:26:02", "pdf_url": "http://arxiv.org/pdf/2505.13308v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13312v1", "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility.", "categories": ["cs.CL"], "published": "2025-05-19 16:26:58", "updated": "2025-05-19 16:26:58", "pdf_url": "http://arxiv.org/pdf/2505.13312v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13328v1", "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "abstract": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons.", "categories": ["cs.CL"], "published": "2025-05-19 16:36:13", "updated": "2025-05-19 16:36:13", "pdf_url": "http://arxiv.org/pdf/2505.13328v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "abstract": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "categories": ["cs.CL", "cs.AI", "eess.AS"], "published": "2025-05-19 16:47:46", "updated": "2025-05-19 16:47:46", "pdf_url": "http://arxiv.org/pdf/2505.13338v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13346v1", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "abstract": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 16:50:35", "updated": "2025-05-19 16:50:35", "pdf_url": "http://arxiv.org/pdf/2505.13346v1", "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13348v1", "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "abstract": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks.", "categories": ["cs.CL"], "published": "2025-05-19 16:51:12", "updated": "2025-05-19 16:51:12", "pdf_url": "http://arxiv.org/pdf/2505.13348v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13353v1", "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "authors": ["Adam \u0160torek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "abstract": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation.", "categories": ["cs.CL", "cs.LG", "cs.SE"], "published": "2025-05-19 16:56:31", "updated": "2025-05-19 16:56:31", "pdf_url": "http://arxiv.org/pdf/2505.13353v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13360v1", "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian K\u00e4stner", "Tongshuang Wu"], "abstract": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring.", "categories": ["cs.CL", "cs.SE"], "published": "2025-05-19 17:03:42", "updated": "2025-05-19 17:03:42", "pdf_url": "http://arxiv.org/pdf/2505.13360v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13379v1", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:24:16", "updated": "2025-05-19 17:24:16", "pdf_url": "http://arxiv.org/pdf/2505.13379v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13380v1", "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:24:26", "updated": "2025-05-19 17:24:26", "pdf_url": "http://arxiv.org/pdf/2505.13380v1", "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13388v1", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:29:03", "updated": "2025-05-19 17:29:03", "pdf_url": "http://arxiv.org/pdf/2505.13388v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13393v1", "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "authors": ["Christopher K. Frantz"], "abstract": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool.", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "published": "2025-05-19 17:33:15", "updated": "2025-05-19 17:33:15", "pdf_url": "http://arxiv.org/pdf/2505.13393v1", "comment": "24 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13398v1", "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "abstract": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 17:34:56", "updated": "2025-05-19 17:34:56", "pdf_url": "http://arxiv.org/pdf/2505.13398v1", "comment": "9 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13403v1", "title": "MR. Judge: Multimodal Reasoner as a Judge", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "abstract": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.", "categories": ["cs.CL"], "published": "2025-05-19 17:37:39", "updated": "2025-05-19 17:37:39", "pdf_url": "http://arxiv.org/pdf/2505.13403v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13404v1", "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "abstract": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary", "categories": ["cs.CL", "eess.AS"], "published": "2025-05-19 17:40:58", "updated": "2025-05-19 17:40:58", "pdf_url": "http://arxiv.org/pdf/2505.13404v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13408v1", "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "abstract": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:44:26", "updated": "2025-05-19 17:44:26", "pdf_url": "http://arxiv.org/pdf/2505.13408v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13417v1", "title": "AdaptThink: Reasoning Models Can Learn When to Think", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "abstract": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:50:52", "updated": "2025-05-19 17:50:52", "pdf_url": "http://arxiv.org/pdf/2505.13417v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13418v1", "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "abstract": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 17:51:35", "updated": "2025-05-19 17:51:35", "pdf_url": "http://arxiv.org/pdf/2505.13418v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13430v1", "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "abstract": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-19 17:55:15", "updated": "2025-05-19 17:55:15", "pdf_url": "http://arxiv.org/pdf/2505.13430v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13434v1", "title": "SMOTExT: SMOTE meets Large Language Models", "authors": ["Mateusz Bystro\u0144ski", "Miko\u0142aj Ho\u0142ysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "abstract": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints.", "categories": ["cs.CL"], "published": "2025-05-19 17:57:36", "updated": "2025-05-19 17:57:36", "pdf_url": "http://arxiv.org/pdf/2505.13434v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 17:58:44", "updated": "2025-05-19 17:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13444v1", "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "abstract": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-19 17:59:27", "updated": "2025-05-19 17:59:27", "pdf_url": "http://arxiv.org/pdf/2505.13444v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:59:31", "updated": "2025-05-19 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.13445v1", "comment": "code available at https://github.com/xyliu-cs/RISE", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "abstract": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:59:58", "updated": "2025-05-19 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.13448v1", "comment": "10 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13787v1", "title": "Preference Learning with Lie Detectors can Induce Honesty or Evasion", "authors": ["Chris Cundy", "Adam Gleave"], "abstract": "As AI systems become more capable, deceptive behaviors can undermine\nevaluation and mislead users at deployment. Recent work has shown that lie\ndetectors can accurately classify deceptive behavior, but they are not\ntypically used in the training pipeline due to concerns around contamination\nand objective hacking. We examine these concerns by incorporating a lie\ndetector into the labelling step of LLM post-training and evaluating whether\nthe learned policy is genuinely more honest, or instead learns to fool the lie\ndetector while remaining deceptive. Using DolusChat, a novel 65k-example\ndataset with paired truthful/deceptive responses, we identify three key factors\nthat determine the honesty of learned policies: amount of exploration during\npreference learning, lie detector accuracy, and KL regularization strength. We\nfind that preference learning with lie detectors and GRPO can lead to policies\nwhich evade lie detectors, with deception rates of over 85\\%. However, if the\nlie detector true positive rate (TPR) or KL regularization is sufficiently\nhigh, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)\nconsistently lead to deception rates under 25\\% for realistic TPRs. Our results\nillustrate a more complex picture than previously assumed: depending on the\ncontext, lie-detector-enhanced training can be a powerful tool for scalable\noversight, or a counterproductive method encouraging undetectable misalignment.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 00:31:53", "updated": "2025-05-20 00:31:53", "pdf_url": "http://arxiv.org/pdf/2505.13787v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13792v1", "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "abstract": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 00:49:19", "updated": "2025-05-20 00:49:19", "pdf_url": "http://arxiv.org/pdf/2505.13792v1", "comment": "10 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13794v1", "title": "LLM-based Evaluation Policy Extraction for Ecological Modeling", "authors": ["Qi Cheng", "Licheng Liu", "Qing Zhu", "Runlong Yu", "Zhenong Jin", "Yiqun Xie", "Xiaowei Jia"], "abstract": "Evaluating ecological time series is critical for benchmarking model\nperformance in many important applications, including predicting greenhouse gas\nfluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.\nTraditional numerical metrics (e.g., R-squared, root mean square error) have\nbeen widely used to quantify the similarity between modeled and observed\necosystem variables, but they often fail to capture domain-specific temporal\npatterns critical to ecological processes. As a result, these methods are often\naccompanied by expert visual inspection, which requires substantial human labor\nand limits the applicability to large-scale evaluation. To address these\nchallenges, we propose a novel framework that integrates metric learning with\nlarge language model (LLM)-based natural language policy extraction to develop\ninterpretable evaluation criteria. The proposed method processes pairwise\nannotations and implements a policy optimization mechanism to generate and\ncombine different assessment metrics. The results obtained on multiple datasets\nfor evaluating the predictions of crop gross primary production and carbon\ndioxide flux have confirmed the effectiveness of the proposed method in\ncapturing target assessment preferences, including both synthetically generated\nand expert-annotated model comparisons. The proposed framework bridges the gap\nbetween numerical metrics and expert knowledge while providing interpretable\nevaluation policies that accommodate the diverse needs of different ecosystem\nmodeling studies.", "categories": ["cs.AI"], "published": "2025-05-20 01:02:29", "updated": "2025-05-20 01:02:29", "pdf_url": "http://arxiv.org/pdf/2505.13794v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13805v1", "title": "ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech", "authors": ["Yu Pan", "Yanni Hu", "Yuguang Yang", "Jixun Yao", "Jianhao Ye", "Hongbin Zhou", "Lei Ma", "Jianjun Zhao"], "abstract": "Despite great advances, achieving high-fidelity emotional voice conversion\n(EVC) with flexible and interpretable control remains challenging. This paper\nintroduces ClapFM-EVC, a novel EVC framework capable of generating high-quality\nconverted speech driven by natural language prompts or reference speech with\nadjustable emotion intensity. We first propose EVC-CLAP, an emotional\ncontrastive language-audio pre-training model, guided by natural language\nprompts and categorical labels, to extract and align fine-grained emotional\nelements across speech and text modalities. Then, a FuEncoder with an adaptive\nintensity gate is presented to seamless fuse emotional features with Phonetic\nPosteriorGrams from a pre-trained ASR model. To further improve emotion\nexpressiveness and speech naturalness, we propose a flow matching model\nconditioned on these captured features to reconstruct Mel-spectrogram of source\nspeech. Subjective and objective evaluations validate the effectiveness of\nClapFM-EVC.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 01:34:29", "updated": "2025-05-20 01:34:29", "pdf_url": "http://arxiv.org/pdf/2505.13805v1", "comment": "Accepted by InterSpeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13808v1", "title": "RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework", "authors": ["Faramarz Safi Esfahani", "Ghassan Beydoun", "Morteza Saberi", "Brad McCusker", "Biswajeet Pradhan"], "abstract": "Metaheuristic algorithms are widely used for solving complex optimization\nproblems, yet their effectiveness is often constrained by fixed structures and\nthe need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)\naddresses this limitation by introducing a self-adaptive metaheuristic\nswitching mechanism driven by real-time performance feedback and dynamic\nalgorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)\nand the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select\nand transition between metaheuristic algorithms based on key performance\nindicators, ensuring continuous adaptation. This approach enhances convergence\nspeed, adaptability, and solution quality, outperforming traditional\nmetaheuristics in high-dimensional, dynamic, and multimodal environments.\nExperimental results on benchmark functions demonstrate that PMF significantly\nimproves optimization efficiency by mitigating stagnation and balancing\nexploration-exploitation strategies across various problem landscapes. By\nintegrating AI-driven decision-making and self-correcting mechanisms, PMF paves\nthe way for scalable, intelligent, and autonomous optimization frameworks, with\npromising applications in engineering, logistics, and complex decision-making\nsystems.", "categories": ["cs.NE", "cs.AI"], "published": "2025-05-20 01:41:22", "updated": "2025-05-20 01:41:22", "pdf_url": "http://arxiv.org/pdf/2505.13808v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13814v1", "title": "Articulatory Feature Prediction from Surface EMG during Speech Production", "authors": ["Jihwan Lee", "Kevin Huang", "Kleanthis Avramidis", "Simon Pistrosch", "Monica Gonzalez-Machorro", "Yoonjeong Lee", "Bj\u00f6rn Schuller", "Louis Goldstein", "Shrikanth Narayanan"], "abstract": "We present a model for predicting articulatory features from surface\nelectromyography (EMG) signals during speech production. The proposed model\nintegrates convolutional layers and a Transformer block, followed by separate\npredictors for articulatory features. Our approach achieves a high prediction\ncorrelation of approximately 0.9 for most articulatory features. Furthermore,\nwe demonstrate that these predicted articulatory features can be decoded into\nintelligible speech waveforms. To our knowledge, this is the first method to\ndecode speech waveforms from surface EMG via articulatory features, offering a\nnovel approach to EMG-based speech synthesis. Additionally, we analyze the\nrelationship between EMG electrode placement and articulatory feature\npredictability, providing knowledge-driven insights for optimizing EMG\nelectrode configurations. The source code and decoded speech samples are\npublicly available.", "categories": ["eess.AS", "cs.AI", "cs.SD"], "published": "2025-05-20 01:50:05", "updated": "2025-05-20 01:50:05", "pdf_url": "http://arxiv.org/pdf/2505.13814v1", "comment": "Accepted for Interspeech2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13820v1", "title": "Structured Agent Distillation for Large Language Model", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 02:01:55", "updated": "2025-05-20 02:01:55", "pdf_url": "http://arxiv.org/pdf/2505.13820v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13828v1", "title": "Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models", "authors": ["Kiarash Naghavi Khanghah", "Zhiling Chen", "Lela Romeo", "Qian Yang", "Rajiv Malhotra", "Farhad Imani", "Hongyi Xu"], "abstract": "Additive manufacturing enables the fabrication of complex designs while\nminimizing waste, but faces challenges related to defects and process\nanomalies. This study presents a novel multimodal Retrieval-Augmented\nGeneration-based framework that automates anomaly detection across various\nAdditive Manufacturing processes leveraging retrieved information from\nliterature, including images and descriptive text, rather than training\ndatasets. This framework integrates text and image retrieval from scientific\nliterature and multimodal generation models to perform zero-shot anomaly\nidentification, classification, and explanation generation in a Laser Powder\nBed Fusion setting. The proposed framework is evaluated on four L-PBF\nmanufacturing datasets from Oak Ridge National Laboratory, featuring various\nprinter makes, models, and materials. This evaluation demonstrates the\nframework's adaptability and generalizability across diverse images without\nrequiring additional training. Comparative analysis using Qwen2-VL-2B and\nGPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini\noutperforms Qwen2-VL-2B and proportional random baseline in manufacturing\nanomalies classification. Additionally, the evaluation of the RAG system\nconfirms that incorporating retrieval mechanisms improves average accuracy by\n12% by reducing the risk of hallucination and providing additional information.\nThe proposed framework can be continuously updated by integrating emerging\nresearch, allowing seamless adaptation to the evolving landscape of AM\ntechnologies. This scalable, automated, and zero-shot-capable framework\nstreamlines AM anomaly analysis, enhancing efficiency and accuracy.", "categories": ["cs.AI"], "published": "2025-05-20 02:18:22", "updated": "2025-05-20 02:18:22", "pdf_url": "http://arxiv.org/pdf/2505.13828v1", "comment": "ASME 2025 International Design Engineering Technical Conferences and\n  Computers and Information in Engineering Conference IDETC/CIE2025, August\n  17-20, 2025, Anaheim, CA (IDETC2025-168615)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13831v1", "title": "TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning", "authors": ["Zongyuan Deng", "Yujie Cai", "Qing Liu", "Shiyao Mu", "Bin Lyu", "Zhen Yang"], "abstract": "The selection of base station sites is a critical challenge in 5G network\nplanning, which requires efficient optimization of coverage, cost, user\nsatisfaction, and practical constraints. Traditional manual methods, reliant on\nhuman expertise, suffer from inefficiencies and are limited to an unsatisfied\nplanning-construction consistency. Existing AI tools, despite improving\nefficiency in certain aspects, still struggle to meet the dynamic network\nconditions and multi-objective needs of telecom operators' networks. To address\nthese challenges, we propose TelePlanNet, an AI-driven framework tailored for\nthe selection of base station sites, integrating a three-layer architecture for\nefficient planning and large-scale automation. By leveraging large language\nmodels (LLMs) for real-time user input processing and intent alignment with\nbase station planning, combined with training the planning model using the\nimproved group relative policy optimization (GRPO) reinforcement learning, the\nproposed TelePlanNet can effectively address multi-objective optimization,\nevaluates candidate sites, and delivers practical solutions. Experiments\nresults show that the proposed TelePlanNet can improve the consistency to 78%,\nwhich is superior to the manual methods, providing telecom operators with an\nefficient and scalable tool that significantly advances cellular network\nplanning.", "categories": ["cs.AI", "I.2; I.2.6; C.2.1"], "published": "2025-05-20 02:19:10", "updated": "2025-05-20 02:19:10", "pdf_url": "http://arxiv.org/pdf/2505.13831v1", "comment": "6 pages, 5 figures, 1 table, submitted to IEEE ICCC 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13834v1", "title": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams", "authors": ["Zhi Su", "Yuman Gao", "Emily Lukas", "Yunfei Li", "Jiaze Cai", "Faris Tulbah", "Fei Gao", "Chao Yu", "Zhongyu Li", "Yi Wu", "Koushil Sreenath"], "abstract": "Achieving coordinated teamwork among legged robots requires both fine-grained\nlocomotion control and long-horizon strategic decision-making. Robot soccer\noffers a compelling testbed for this challenge, combining dynamic, competitive,\nand multi-agent interactions. In this work, we present a hierarchical\nmulti-agent reinforcement learning (MARL) framework that enables fully\nautonomous and decentralized quadruped robot soccer. First, a set of highly\ndynamic low-level skills is trained for legged locomotion and ball\nmanipulation, such as walking, dribbling, and kicking. On top of these, a\nhigh-level strategic planning policy is trained with Multi-Agent Proximal\nPolicy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning\nframework allows agents to adapt to diverse opponent strategies and gives rise\nto sophisticated team behaviors, including coordinated passing, interception,\nand dynamic role allocation. With an extensive ablation study, the proposed\nlearning method shows significant advantages in the cooperative and competitive\nmulti-agent soccer game. We deploy the learned policies to real quadruped\nrobots relying solely on onboard proprioception and decentralized localization,\nwith the resulting system supporting autonomous robot-robot and robot-human\nsoccer matches on indoor and outdoor soccer courts.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 02:20:54", "updated": "2025-05-20 02:20:54", "pdf_url": "http://arxiv.org/pdf/2505.13834v1", "comment": "11 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13837v1", "title": "Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements", "authors": ["Gokul Puthumanaillam", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Melkior Ornik"], "abstract": "Robots navigating complex environments must manage uncertainty from sensor\nnoise, environmental changes, and incomplete information, with different tasks\nrequiring varying levels of precision in different areas. For example, precise\nlocalization may be crucial near obstacles but less critical in open spaces. We\npresent GUIDE (Generalized Uncertainty Integration for Decision-Making and\nExecution), a framework that integrates these task-specific requirements into\nnavigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning\nacceptable uncertainty levels to different locations, TSUMs enable robots to\nadapt uncertainty management based on context. When combined with reinforcement\nlearning, GUIDE learns policies that balance task completion and uncertainty\nmanagement without extensive reward engineering. Real-world tests show\nsignificant performance gains over methods lacking task-specific uncertainty\nawareness.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-20 02:23:15", "updated": "2025-05-20 02:23:15", "pdf_url": "http://arxiv.org/pdf/2505.13837v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "abstract": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 02:27:08", "updated": "2025-05-20 02:27:08", "pdf_url": "http://arxiv.org/pdf/2505.13840v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13847v1", "title": "Forensic deepfake audio detection using segmental speech features", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "abstract": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 02:42:46", "updated": "2025-05-20 02:42:46", "pdf_url": "http://arxiv.org/pdf/2505.13847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13851v1", "title": "A Challenge to Build Neuro-Symbolic Video Agents", "authors": ["Sahil Shah", "Harsh Goel", "Sai Shankar Narasimhan", "Minkyu Choi", "S P Sharan", "Oguzhan Akcin", "Sandeep Chinchali"], "abstract": "Modern video understanding systems excel at tasks such as scene\nclassification, object detection, and short video retrieval. However, as video\nanalysis becomes increasingly central to real-world applications, there is a\ngrowing need for proactive video agents for the systems that not only interpret\nvideo streams but also reason about events and take informed actions. A key\nobstacle in this direction is temporal reasoning: while deep learning models\nhave made remarkable progress in recognizing patterns within individual frames\nor short clips, they struggle to understand the sequencing and dependencies of\nevents over time, which is critical for action-driven decision-making.\nAddressing this limitation demands moving beyond conventional deep learning\napproaches. We posit that tackling this challenge requires a neuro-symbolic\nperspective, where video queries are decomposed into atomic events, structured\ninto coherent sequences, and validated against temporal constraints. Such an\napproach can enhance interpretability, enable structured reasoning, and provide\nstronger guarantees on system behavior, all key properties for advancing\ntrustworthy video agents. To this end, we present a grand challenge to the\nresearch community: developing the next generation of intelligent video agents\nthat integrate three core capabilities: (1) autonomous video search and\nanalysis, (2) seamless real-world interaction, and (3) advanced content\ngeneration. By addressing these pillars, we can transition from passive\nperception to intelligent video agents that reason, predict, and act, pushing\nthe boundaries of video understanding.", "categories": ["cs.AI"], "published": "2025-05-20 02:53:21", "updated": "2025-05-20 02:53:21", "pdf_url": "http://arxiv.org/pdf/2505.13851v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13855v1", "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "abstract": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 03:02:05", "updated": "2025-05-20 03:02:05", "pdf_url": "http://arxiv.org/pdf/2505.13855v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13857v1", "title": "Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer", "authors": ["Tian Sun", "Yuqi Chen", "Baihua Zheng", "Weiwei Sun"], "abstract": "In real-world applications, GPS trajectories often suffer from low sampling\nrates, with large and irregular intervals between consecutive GPS points. This\nsparse characteristic presents challenges for their direct use in GPS-based\nsystems. This paper addresses the task of map-constrained trajectory recovery,\naiming to enhance trajectory sampling rates of GPS trajectories. Previous\nstudies commonly adopt a sequence-to-sequence framework, where an encoder\ncaptures the trajectory patterns and a decoder reconstructs the target\ntrajectory. Within this framework, effectively representing the road network\nand extracting relevant trajectory features are crucial for overall\nperformance. Despite advancements in these models, they fail to fully leverage\nthe complex spatio-temporal dynamics present in both the trajectory and the\nroad network.\n  To overcome these limitations, we categorize the spatio-temporal dynamics of\ntrajectory data into two distinct aspects: spatial-temporal traffic dynamics\nand trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for\ntrajectory recovery. To capture spatio-temporal traffic dynamics, we introduce\nPD-GNN, which models periodic patterns and learns topologically aware dynamics\nconcurrently for each road segment. For spatio-temporal trajectory dynamics, we\npresent TedFormer, a time-aware Transformer that incorporates temporal dynamics\nfor each GPS location by integrating closed-form neural ordinary differential\nequations into the attention mechanism. This allows TedFormer to effectively\nhandle irregularly sampled data. Extensive experiments on three real-world\ndatasets demonstrate the superior performance of TedTrajRec. The code is\npublicly available at https://github.com/ysygMhdxw/TEDTrajRec/.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 03:09:17", "updated": "2025-05-20 03:09:17", "pdf_url": "http://arxiv.org/pdf/2505.13857v1", "comment": "Accepted as a journal paper in IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13860v1", "title": "Domain Adaptation of VLM for Soccer Video Understanding", "authors": ["Tiancheng Jiang", "Henry Wang", "Md Sirajus Salekin", "Parmida Atighehchian", "Shinan Zhang"], "abstract": "Vision Language Models (VLMs) have demonstrated strong performance in\nmulti-modal tasks by effectively aligning visual and textual representations.\nHowever, most video understanding VLM research has been domain-agnostic,\nleaving the understanding of their transfer learning capability to specialized\ndomains under-explored. In this work, we address this by exploring the\nadaptability of open-source VLMs to specific domains, and focusing on soccer as\nan initial case study. Our approach uses large-scale soccer datasets and LLM to\ncreate instruction-following data, and use them to iteratively fine-tune the\ngeneral-domain VLM in a curriculum learning fashion (first teaching the model\nkey soccer concepts to then question answering tasks). The final adapted model,\ntrained using a curated dataset of 20k video clips, exhibits significant\nimprovement in soccer-specific tasks compared to the base model, with a 37.5%\nrelative improvement for the visual question-answering task and an accuracy\nimprovement from 11.8% to 63.5% for the downstream soccer action classification\ntask.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 03:12:21", "updated": "2025-05-20 03:12:21", "pdf_url": "http://arxiv.org/pdf/2505.13860v1", "comment": "8 pages, 5 figures, accepted to the 11th IEEE International Workshop\n  on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix\n  included as ancillary PDF", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13872v1", "title": "Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving", "authors": ["Jingzheng Li", "Tiancheng Wang", "Xingyu Peng", "Jiacheng Chen", "Zhijun Chen", "Bing Li", "Xianglong Liu"], "abstract": "Autonomous Driving (AD) systems demand the high levels of safety assurance.\nDespite significant advancements in AD demonstrated on open-source benchmarks\nlike Longest6 and Bench2Drive, existing datasets still lack\nregulatory-compliant scenario libraries for closed-loop testing to\ncomprehensively evaluate the functional safety of AD. Meanwhile, real-world AD\naccidents are underrepresented in current driving datasets. This scarcity leads\nto inadequate evaluation of AD performance, posing risks to safety validation\nand practical deployment. To address these challenges, we propose Safety2Drive,\na safety-critical scenario library designed to evaluate AD systems.\nSafety2Drive offers three key contributions. (1) Safety2Drive comprehensively\ncovers the test items required by standard regulations and contains 70 AD\nfunction test items. (2) Safety2Drive supports the safety-critical scenario\ngeneralization. It has the ability to inject safety threats such as natural\nenvironment corruptions and adversarial attacks cross camera and LiDAR sensors.\n(3) Safety2Drive supports multi-dimensional evaluation. In addition to the\nevaluation of AD systems, it also supports the evaluation of various perception\ntasks, such as object detection and lane detection. Safety2Drive provides a\nparadigm from scenario construction to validation, establishing a standardized\ntest framework for the safe deployment of AD.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 03:27:06", "updated": "2025-05-20 03:27:06", "pdf_url": "http://arxiv.org/pdf/2505.13872v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13873v1", "title": "Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model", "authors": ["Peisong Niu", "Ziqing Ma", "Tian Zhou", "Weiqi Chen", "Lefei Shen", "Rong Jin", "Liang Sun"], "abstract": "Weather forecasting has long posed a significant challenge for humanity.\nWhile recent AI-based models have surpassed traditional numerical weather\nprediction (NWP) methods in global forecasting tasks, overfitting remains a\ncritical issue due to the limited availability of real-world weather data\nspanning only a few decades. Unlike fields like computer vision or natural\nlanguage processing, where data abundance can mitigate overfitting, weather\nforecasting demands innovative strategies to address this challenge with\nexisting data. In this paper, we explore pre-training methods for weather\nforecasting, finding that selecting an appropriately challenging pre-training\ntask introduces locality bias, effectively mitigating overfitting and enhancing\nperformance. We introduce Baguan, a novel data-driven model for medium-range\nweather forecasting, built on a Siamese Autoencoder pre-trained in a\nself-supervised manner and fine-tuned for different lead times. Experimental\nresults show that Baguan outperforms traditional methods, delivering more\naccurate forecasts. Additionally, the pre-trained Baguan demonstrates robust\noverfitting control and excels in downstream tasks, such as\nsubseasonal-to-seasonal (S2S) modeling and regional forecasting, after\nfine-tuning.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 03:29:23", "updated": "2025-05-20 03:29:23", "pdf_url": "http://arxiv.org/pdf/2505.13873v1", "comment": "KDD2025 research track accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13887v1", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "abstract": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 03:48:19", "updated": "2025-05-20 03:48:19", "pdf_url": "http://arxiv.org/pdf/2505.13887v1", "comment": "17 pages, 7 figures, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:2502.17110", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13898v1", "title": "Do Language Models Use Their Depth Efficiently?", "authors": ["R\u00f3bert Csord\u00e1s", "Christopher D. Manning", "Christopher Potts"], "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,\ncomparing the output of the sublayers to the residual stream reveals that\nlayers in the second half contribute much less than those in the first half,\nwith a clear phase transition between the two halves. Second, skipping layers\nin the second half has a much smaller effect on future computations and output\npredictions. Third, for multihop tasks, we are unable to find evidence that\nmodels are using increased depth to compose subresults in examples involving\nmany hops. Fourth, we seek to directly address whether deeper models are using\ntheir additional layers to perform new kinds of computation. To do this, we\ntrain linear maps from the residual stream of a shallow model to a deeper one.\nWe find that layers with the same relative depth map best to each other,\nsuggesting that the larger model simply spreads the same computations out over\nits many layers. All this evidence suggests that deeper models are not using\ntheir depth to learn new kinds of computation, but only using the greater depth\nto perform more fine-grained adjustments to the residual. This may help explain\nwhy increasing scale leads to diminishing returns for stacked Transformer\narchitectures.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "published": "2025-05-20 04:00:56", "updated": "2025-05-20 04:00:56", "pdf_url": "http://arxiv.org/pdf/2505.13898v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13904v1", "title": "Learning to Insert for Constructive Neural Vehicle Routing Solver", "authors": ["Fu Luo", "Xi Lin", "Mengyuan Zhong", "Fei Liu", "Zhenkun Wang", "Jianyong Sun", "Qingfu Zhang"], "abstract": "Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.", "categories": ["cs.LG", "cs.AI", "cs.RO", "math.OC"], "published": "2025-05-20 04:10:50", "updated": "2025-05-20 04:10:50", "pdf_url": "http://arxiv.org/pdf/2505.13904v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13906v1", "title": "XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data", "authors": ["Soyabul Islam Lincoln", "Mirza Mohd Shahriar Maswood"], "abstract": "A common neurodegenerative disease, Alzheimer's disease requires a precise\ndiagnosis and efficient treatment, particularly in light of escalating\nhealthcare expenses and the expanding use of artificial intelligence in medical\ndiagnostics. Many recent studies shows that the combination of brain Magnetic\nResonance Imaging (MRI) and deep neural networks have achieved promising\nresults for diagnosing AD. Using deep convolutional neural networks, this paper\nintroduces a novel deep learning architecture that incorporates multiresidual\nblocks, specialized spatial attention blocks, grouped query attention, and\nmulti-head attention. The study assessed the model's performance on four\npublicly accessible datasets and concentrated on identifying binary and\nmulticlass issues across various categories. This paper also takes into account\nof the explainability of AD's progression and compared with state-of-the-art\nmethods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster\nScore-CAM, and XGRADCAM. Our methodology consistently outperforms current\napproaches, achieving 99.66\\% accuracy in 4-class classification, 99.63\\% in\n3-class classification, and 100\\% in binary classification using Kaggle\ndatasets. For Open Access Series of Imaging Studies (OASIS) datasets the\naccuracies are 99.92\\%, 99.90\\%, and 99.95\\% respectively. The Alzheimer's\nDisease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in\nthree planes (axial, sagittal, and coronal) and a combination of all planes.\nThe study achieved accuracies of 99.08\\% for axis, 99.85\\% for sagittal, 99.5\\%\nfor coronal, and 99.17\\% for all axis, and 97.79\\% and 8.60\\% respectively for\nADNI-2. The network's ability to retrieve important information from MRI images\nis demonstrated by its excellent accuracy in categorizing AD stages.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-20 04:17:28", "updated": "2025-05-20 04:17:28", "pdf_url": "http://arxiv.org/pdf/2505.13906v1", "comment": "20 pages, 12 figures,", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13909v1", "title": "Efficient Agent Training for Computer Use", "authors": ["Yanheng He", "Jiahe Jin", "Pengfei Liu"], "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 04:20:18", "updated": "2025-05-20 04:20:18", "pdf_url": "http://arxiv.org/pdf/2505.13909v1", "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13911v1", "title": "Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation", "authors": ["Ruijie Zhao", "Zuopeng Tan", "Xiao Xue", "Longfei Zhao", "Bing Li", "Zicheng Liao", "Ying Ming", "Jiaru Wang", "Ran Xiao", "Sirong Piao", "Rui Zhao", "Qiqi Xu", "Wei Song"], "abstract": "Pulmonary segment segmentation is crucial for cancer localization and\nsurgical planning. However, the pixel-wise annotation of pulmonary segments is\nlaborious, as the boundaries between segments are indistinguishable in medical\nimages. To this end, we propose a weakly supervised learning (WSL) method,\ntermed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise\nclinical anatomical definition of pulmonary segments to perform pulmonary\nsegment segmentation. Since pulmonary segments reside within the lobes and are\ndetermined by the bronchovascular tree, i.e., artery, airway and vein, the\ndesign of the loss function is founded on two principles. First, segment-level\nlabels are utilized to directly supervise the output of the pulmonary segments,\nensuring that they accurately encompass the appropriate bronchovascular tree.\nSecond, lobe-level supervision indirectly oversees the pulmonary segment,\nensuring their inclusion within the corresponding lobe. Besides, we introduce a\ntwo-stage segmentation strategy that incorporates bronchovascular priori\ninformation. Furthermore, a consistency loss is proposed to enhance the\nsmoothness of segment boundaries, along with an evaluation metric designed to\nmeasure the smoothness of pulmonary segment boundaries. Visual inspection and\nevaluation metrics from experiments conducted on a private dataset demonstrate\nthe effectiveness of our method.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-20 04:23:12", "updated": "2025-05-20 04:23:12", "pdf_url": "http://arxiv.org/pdf/2505.13911v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13914v1", "title": "Parallel Belief Revision via Order Aggregation", "authors": ["Jake Chandler", "Richard Booth"], "abstract": "Despite efforts to better understand the constraints that operate on\nsingle-step parallel (aka \"package\", \"multiple\") revision, very little work has\nbeen carried out on how to extend the model to the iterated case. A recent\npaper by Delgrande & Jin outlines a range of relevant rationality postulates.\nWhile many of these are plausible, they lack an underlying unifying\nexplanation. We draw on recent work on iterated parallel contraction to offer a\ngeneral method for extending serial iterated belief revision operators to\nhandle parallel change. This method, based on a family of order aggregators\nknown as TeamQueue aggregators, provides a principled way to recover the\nindependently plausible properties that can be found in the literature, without\nyielding the more dubious ones.", "categories": ["cs.AI", "I.2.4"], "published": "2025-05-20 04:26:01", "updated": "2025-05-20 04:26:01", "pdf_url": "http://arxiv.org/pdf/2505.13914v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13921v1", "title": "APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight", "authors": ["Wanjing Huang", "Weixiang Yan", "Zhen Zhang", "Ambuj Singh"], "abstract": "Large Language Models (LLMs) demonstrate strong reasoning and task planning\ncapabilities but remain fundamentally limited in physical interaction modeling.\nExisting approaches integrate perception via Vision-Language Models (VLMs) or\nadaptive decision-making through Reinforcement Learning (RL), but they fail to\ncapture dynamic object interactions or require task-specific training, limiting\ntheir real-world applicability. We introduce APEX (Anticipatory\nPhysics-Enhanced Execution), a framework that equips LLMs with physics-driven\nforesight for real-time task planning. APEX constructs structured graphs to\nidentify and model the most relevant dynamic interactions in the environment,\nproviding LLMs with explicit physical state updates. Simultaneously, APEX\nprovides low-latency forward simulations of physically feasible actions,\nallowing LLMs to select optimal strategies based on predictive outcomes rather\nthan static observations. We evaluate APEX on three benchmarks designed to\nassess perception, prediction, and decision-making: (1) Physics Reasoning\nBenchmark, testing causal inference and object motion prediction; (2) Tetris,\nevaluating whether physics-informed prediction enhances decision-making\nperformance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,\nassessing the immediate integration of perception and action feasibility\nanalysis. APEX significantly outperforms standard LLMs and VLM-based models,\ndemonstrating the necessity of explicit physics reasoning for bridging the gap\nbetween language-based intelligence and real-world task execution. The source\ncode and experiment setup are publicly available at\nhttps://github.com/hwj20/APEX_EXP .", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 04:34:58", "updated": "2025-05-20 04:34:58", "pdf_url": "http://arxiv.org/pdf/2505.13921v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13934v1", "title": "RLVR-World: Training World Models with Reinforcement Learning", "authors": ["Jialong Wu", "Shaofeng Yin", "Ningya Feng", "Mingsheng Long"], "abstract": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 05:02:53", "updated": "2025-05-20 05:02:53", "pdf_url": "http://arxiv.org/pdf/2505.13934v1", "comment": "Code is available at project website:\n  https://thuml.github.io/RLVR-World/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13936v1", "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "abstract": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:04:15", "updated": "2025-05-20 05:04:15", "pdf_url": "http://arxiv.org/pdf/2505.13936v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13938v1", "title": "CLEVER: A Curated Benchmark for Formally Verified Code Generation", "authors": ["Amitayush Thakur", "Jasper Lee", "George Tsoukalas", "Meghana Sistla", "Matthew Zhao", "Stefan Zetzche", "Greg Durrett", "Yisong Yue", "Swarat Chaudhuri"], "abstract": "We introduce ${\\rm C{\\small LEVER}}$, a high-quality, curated benchmark of\n161 problems for end-to-end verified code generation in Lean. Each problem\nconsists of (1) the task of generating a specification that matches a held-out\nground-truth specification, and (2) the task of generating a Lean\nimplementation that provably satisfies this specification. Unlike prior\nbenchmarks, ${\\rm C{\\small LEVER}}$ avoids test-case supervision, LLM-generated\nannotations, and specifications that leak implementation logic or allow vacuous\nsolutions. All outputs are verified post-hoc using Lean's type checker to\nensure machine-checkable correctness. We use ${\\rm C{\\small LEVER}}$ to\nevaluate several few-shot and agentic approaches based on state-of-the-art\nlanguage models. These methods all struggle to achieve full verification,\nestablishing it as a challenging frontier benchmark for program synthesis and\nformal reasoning. Our benchmark can be found on\nGitHub(https://github.com/trishullab/clever) as well as\nHuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our\nevaluation code is also available\nonline(https://github.com/trishullab/clever-prover).", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.PL", "cs.SE"], "published": "2025-05-20 05:15:47", "updated": "2025-05-20 05:15:47", "pdf_url": "http://arxiv.org/pdf/2505.13938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13940v1", "title": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery", "authors": ["Kun Li", "Zhennan Wu", "Shoupeng Wang", "Wenbin Hu"], "abstract": "In the field of AI4Science, large-scale language models (LLMs) show great\npotential to parse complex scientific semantics, integrate cross-disciplinary\nknowledge, and assist critical task research. However, in the field of drug\ndiscovery, despite the optimization through professional data pre-training,\ncontext window expansion, and internet search, the existing LLMs are still\nfacing challenges such as massive multi-modal and heterogeneous data\nprocessing, domain knowledge dynamic updating delay, and insufficient\nconfidence in predicting the results of complex computational tasks. To address\nthese challenges, we propose the DrugPilot, an LLM-based agent with\nparameterized reasoning for drug discovery. DrugPilot addresses key limitations\nof traditional end-to-end LLM prediction approaches through its parametric\ninference architecture. This agent system supports major phases of the drug\ndiscovery pipeline, facilitating automated planning and execution of\nmulti-stage research tasks. To address the critical challenge of multi-modal\ndrug data analysis (incorporating both public datasets and user-submitted\ndata), we developed an interactive parameterized memory pool. This innovative\ncomponent standardizes real-world drug data into parametric representations,\nsimultaneously enabling efficient knowledge retrieval in multi-turn dialogue\nwhile mitigating the information loss inherent in text-based data transmission.\nAdditionally, we created a drug instruct dataset across 8 essential drug\ndiscovery tasks for model fine-tuning and evaluation. Based on the Berkeley\nfunction calling evaluation framework, DrugPilot demonstrated the most advanced\ntool calling capabilities on our drug discovery tool instruction dataset,\noutperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves\ntask completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and\nmulti-turn tasks, respectively.", "categories": ["cs.AI", "q-bio.BM"], "published": "2025-05-20 05:18:15", "updated": "2025-05-20 05:18:15", "pdf_url": "http://arxiv.org/pdf/2505.13940v1", "comment": "22 pages, 10 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13941v1", "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "authors": ["Haoyang Fang", "Boran Han", "Nick Erickson", "Xiyuan Zhang", "Su Zhou", "Anirudh Dagar", "Jiani Zhang", "Ali Caner Turkmen", "Cuixiong Hu", "Huzefa Rangwala", "Ying Nian Wu", "Bernie Wang", "George Karypis"], "abstract": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions.", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 05:20:53", "updated": "2025-05-20 05:20:53", "pdf_url": "http://arxiv.org/pdf/2505.13941v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13946v1", "title": "Visual Instruction Bottleneck Tuning", "authors": ["Changdae Oh", "Jiatong Li", "Shawn Im", "Yixuan Li"], "abstract": "Despite widespread adoption, multimodal large language models (MLLMs) suffer\nperformance degradation when encountering unfamiliar queries under distribution\nshifts. Existing methods to improve MLLM generalization typically require\neither more instruction data or larger advanced model architectures, both of\nwhich incur non-trivial human labor or computational costs. In this work, we\ntake an alternative approach to enhance the robustness of MLLMs under\ndistribution shifts, from a representation learning perspective. Inspired by\nthe information bottleneck (IB) principle, we derive a variational lower bound\nof the IB for MLLMs and devise a practical implementation, Visual Instruction\nBottleneck Tuning (Vittle). We then provide a theoretical justification of\nVittle by revealing its connection to an information-theoretic robustness\nmetric of MLLM. Empirical validation of three MLLMs on open-ended and\nclosed-form question answering and object hallucination detection tasks over 45\ndatasets, including 30 shift scenarios, demonstrates that Vittle consistently\nimproves the MLLM's robustness under shifts by pursuing the learning of a\nminimal sufficient representation.", "categories": ["cs.AI"], "published": "2025-05-20 05:24:53", "updated": "2025-05-20 05:24:53", "pdf_url": "http://arxiv.org/pdf/2505.13946v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13948v1", "title": "Memory-Centric Embodied Question Answer", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "abstract": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.", "categories": ["cs.CL", "cs.AI", "cs.MM"], "published": "2025-05-20 05:27:57", "updated": "2025-05-20 05:27:57", "pdf_url": "http://arxiv.org/pdf/2505.13948v1", "comment": "14pages, 7 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13949v1", "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "abstract": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:28:21", "updated": "2025-05-20 05:28:21", "pdf_url": "http://arxiv.org/pdf/2505.13949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13965v1", "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 06:05:56", "updated": "2025-05-20 06:05:56", "pdf_url": "http://arxiv.org/pdf/2505.13965v1", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13969v1", "title": "Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World", "authors": ["Junya Nakanishi", "Jun Baba", "Yuichiro Yoshikawa", "Hiroko Kamide", "Hiroshi Ishiguro"], "abstract": "This paper discusses the functional advantages of the Selection-Broadcast\nCycle structure proposed by Global Workspace Theory (GWT), inspired by human\nconsciousness, particularly focusing on its applicability to artificial\nintelligence and robotics in dynamic, real-time scenarios. While previous\nstudies often examined the Selection and Broadcast processes independently,\nthis research emphasizes their combined cyclic structure and the resulting\nbenefits for real-time cognitive systems. Specifically, the paper identifies\nthree primary benefits: Dynamic Thinking Adaptation, Experience-Based\nAdaptation, and Immediate Real-Time Adaptation. This work highlights GWT's\npotential as a cognitive architecture suitable for sophisticated\ndecision-making and adaptive performance in unsupervised, dynamic environments.\nIt suggests new directions for the development and implementation of robust,\ngeneral-purpose AI and robotics systems capable of managing complex, real-world\ntasks.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 06:07:21", "updated": "2025-05-20 06:07:21", "pdf_url": "http://arxiv.org/pdf/2505.13969v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13971v1", "title": "The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition", "authors": ["Ming Gao", "Shilong Wu", "Hang Chen", "Jun Du", "Chin-Hui Lee", "Shinji Watanabe", "Jingdong Chen", "Siniscalchi Sabato Marco", "Odette Scharenborg"], "abstract": "Meetings are a valuable yet challenging scenario for speech applications due\nto complex acoustic conditions. This paper summarizes the outcomes of the MISP\n2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,\nmulti-device meeting transcription by incorporating video modality alongside\naudio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual\nSpeech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).\nWe present the challenge's objectives, tasks, dataset, baseline systems, and\nsolutions proposed by participants. The best-performing systems achieved\nsignificant improvements over the baseline: the top AVSD model achieved a\nDiarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system\nachieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the\nbest AVDR system achieved a concatenated minimum-permutation Character Error\nRate (cpCER) of 11.56%, improving by 72.49%.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 06:11:51", "updated": "2025-05-20 06:11:51", "pdf_url": "http://arxiv.org/pdf/2505.13971v1", "comment": "Accepted by Interspeech 2025. Camera-ready version", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13973v1", "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "abstract": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 06:12:20", "updated": "2025-05-20 06:12:20", "pdf_url": "http://arxiv.org/pdf/2505.13973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13986v1", "title": "Solving Normalized Cut Problem with Constrained Action Space", "authors": ["Qize Jiang", "Linsey Pang", "Alice Gatti", "Mahima Aggarwa", "Giovanna Vantin", "Xiaosong Ma", "Weiwei Sun", "Sanjay Chawla"], "abstract": "Reinforcement Learning (RL) has emerged as an important paradigm to solve\ncombinatorial optimization problems primarily due to its ability to learn\nheuristics that can generalize across problem instances. However, integrating\nexternal knowledge that will steer combinatorial optimization problem solutions\ntowards domain appropriate outcomes remains an extremely challenging task. In\nthis paper, we propose the first RL solution that uses constrained action\nspaces to guide the normalized cut problem towards pre-defined template\ninstances. Using transportation networks as an example domain, we create a\nWedge and Ring Transformer that results in graph partitions that are shaped in\nform of Wedges and Rings and which are likely to be closer to natural optimal\npartitions. However, our approach is general as it is based on principles that\ncan be generalized to other domains.", "categories": ["math.OC", "cs.AI", "cs.LG", "I.2.8"], "published": "2025-05-20 06:33:39", "updated": "2025-05-20 06:33:39", "pdf_url": "http://arxiv.org/pdf/2505.13986v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13989v1", "title": "When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty", "authors": ["Yanzhe Wen", "Xunkai Li", "Qi Zhang", "Zhu Lei", "Guang Zeng", "Rong-Hua Li", "Guoren Wang"], "abstract": "Recently, large language models (LLMs) have significantly advanced\ntext-attributed graph (TAG) learning. However, existing methods inadequately\nhandle data uncertainty in open-world scenarios, especially concerning limited\nlabeling and unknown-class nodes. Prior solutions typically rely on isolated\nsemantic or structural approaches for unknown-class rejection, lacking\neffective annotation pipelines. To address these limitations, we propose\nOpen-world Graph Assistant (OGA), an LLM-based framework that combines adaptive\nlabel traceability, which integrates semantics and topology for unknown-class\nrejection, and a graph label annotator to enable model updates using newly\nannotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and\npracticality.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 06:37:18", "updated": "2025-05-20 06:37:18", "pdf_url": "http://arxiv.org/pdf/2505.13989v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13994v1", "title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning", "authors": ["Ruiyi Yang", "Hao Xue", "Imran Razzak", "Hakim Hacid", "Flora D. Salim"], "abstract": "Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches.", "categories": ["cs.AI", "cs.IR", "cs.MA"], "published": "2025-05-20 06:44:34", "updated": "2025-05-20 06:44:34", "pdf_url": "http://arxiv.org/pdf/2505.13994v1", "comment": "20 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13995v1", "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "abstract": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-20 06:45:17", "updated": "2025-05-20 06:45:17", "pdf_url": "http://arxiv.org/pdf/2505.13995v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14001v1", "title": "VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change", "authors": ["Sterre Lutz", "Matthijs T. J. Spaan", "Anna Lukina"], "abstract": "Autonomous systems operating in the real world encounter a range of\nuncertainties. Probabilistic neural Lyapunov certification is a powerful\napproach to proving safety of nonlinear stochastic dynamical systems. When\nfaced with changes beyond the modeled uncertainties, e.g., unidentified\nobstacles, probabilistic certificates must be transferred to the new system\ndynamics. However, even when the changes are localized in a known part of the\nstate space, state-of-the-art requires complete re-certification, which is\nparticularly costly for neural certificates. We introduce VeRecycle, the first\nframework to formally reclaim guarantees for discrete-time stochastic dynamical\nsystems. VeRecycle efficiently reuses probabilistic certificates when the\nsystem dynamics deviate only in a given subset of states. We present a general\ntheoretical justification and algorithmic implementation. Our experimental\nevaluation shows scenarios where VeRecycle both saves significant computational\neffort and achieves competitive probabilistic guarantees in compositional\nneural control.", "categories": ["cs.AI", "cs.SY", "eess.SY"], "published": "2025-05-20 06:54:19", "updated": "2025-05-20 06:54:19", "pdf_url": "http://arxiv.org/pdf/2505.14001v1", "comment": "accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14005v1", "title": "Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks", "authors": ["Han Zhang", "Yan Wang", "Guanfeng Liu", "Pengfei Ding", "Huaxiong Wang", "Kwok-Yan Lam"], "abstract": "To enhance the reliability and credibility of graph neural networks (GNNs)\nand improve the transparency of their decision logic, a new field of\nexplainability of GNNs (XGNN) has emerged. However, two major limitations\nseverely degrade the performance and hinder the generalizability of existing\nXGNN methods: they (a) fail to capture the complete decision logic of GNNs\nacross diverse distributions in the entire dataset's sample space, and (b)\nimpose strict prerequisites on edge properties and GNN internal accessibility.\nTo address these limitations, we propose OPEN, a novel c\\textbf{O}mprehensive\nand \\textbf{P}rerequisite-free \\textbf{E}xplainer for G\\textbf{N}Ns. OPEN, as\nthe first work in the literature, can infer and partition the entire dataset's\nsample space into multiple environments, each containing graphs that follow a\ndistinct distribution. OPEN further learns the decision logic of GNNs across\ndifferent distributions by sampling subgraphs from each environment and\nanalyzing their predictions, thus eliminating the need for strict\nprerequisites. Experimental results demonstrate that OPEN captures nearly\ncomplete decision logic of GNNs, outperforms state-of-the-art methods in\nfidelity while maintaining similar efficiency, and enhances robustness in\nreal-world scenarios.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 07:01:47", "updated": "2025-05-20 07:01:47", "pdf_url": "http://arxiv.org/pdf/2505.14005v1", "comment": "Accepted by IJCAI 2025 AI4Tech Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14020v1", "title": "Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning", "authors": ["Hao Dong", "Ziyue Qiao", "Zhiyuan Ning", "Qi Hao", "Yi Du", "Pengyang Wang", "Yuanchun Zhou"], "abstract": "Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs\n(KGs), incorporate the temporal feature to express the transience of knowledge\nby describing when facts occur. TKG extrapolation aims to infer possible future\nfacts based on known history, which has garnered significant attention in\nrecent years. Some existing methods treat TKG as a sequence of independent\nsubgraphs to model temporal evolution patterns, demonstrating impressive\nreasoning performance. However, they still have limitations: 1) In modeling\nsubgraph semantic evolution, they usually neglect the internal structural\ninteractions between subgraphs, which are actually crucial for encoding TKGs.\n2) They overlook the potential smooth features that do not lead to semantic\nchanges, which should be distinguished from the semantic evolution process.\nTherefore, we propose a novel Disentangled Multi-span Evolutionary Network\n(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution\nstrategy that captures local neighbor features while perceiving historical\nneighbor semantic information, thus enabling internal interactions between\nsubgraphs during the evolution process. To maximize the capture of semantic\nchange patterns, we design a disentangle component that adaptively separates\nnodes' active and stable features, used to dynamically control the influence of\nhistorical semantics on future evolution. Extensive experiments conducted on\nfour real-world TKG datasets show that DiMNet demonstrates substantial\nperformance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%\nin MRR.", "categories": ["cs.AI", "cs.IR", "cs.LG"], "published": "2025-05-20 07:22:03", "updated": "2025-05-20 07:22:03", "pdf_url": "http://arxiv.org/pdf/2505.14020v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14024v1", "title": "FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix", "authors": ["Di Wu", "Qian Li", "Heng Yang", "Yong Han"], "abstract": "Federated Learning (FL) enables geographically distributed clients to\ncollaboratively train machine learning models by sharing only their local\nmodels, ensuring data privacy. However, FL is vulnerable to untargeted attacks\nthat aim to degrade the global model's performance on the underlying data\ndistribution. Existing defense mechanisms attempt to improve FL's resilience\nagainst such attacks, but their effectiveness is limited in practical FL\nenvironments due to data heterogeneity. On the contrary, we aim to detect and\nremove the attacks to mitigate their impact. Generalization contribution plays\na crucial role in distinguishing untargeted attacks. Our observations indicate\nthat, with limited data, the divergence between embeddings representing\ndifferent classes provides a better measure of generalization than direct\naccuracy. In light of this, we propose a novel robust aggregation method,\nFedGraM, designed to defend against untargeted attacks in FL. The server\nmaintains an auxiliary dataset containing one sample per class to support\naggregation. This dataset is fed to the local models to extract embeddings.\nThen, the server calculates the norm of the Gram Matrix of the embeddings for\neach local model. The norm serves as an indicator of each model's inter-class\nseparation capability in the embedding space. FedGraM identifies and removes\npotentially malicious models by filtering out those with the largest norms,\nthen averages the remaining local models to form the global model. We conduct\nextensive experiments to evaluate the performance of FedGraM. Our empirical\nresults show that with limited data samples used to construct the auxiliary\ndataset, FedGraM achieves exceptional performance, outperforming\nstate-of-the-art defense methods.", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "published": "2025-05-20 07:26:54", "updated": "2025-05-20 07:26:54", "pdf_url": "http://arxiv.org/pdf/2505.14024v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14027v1", "title": "CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data", "authors": ["Yifan Zeng"], "abstract": "As computer networks proliferate, the gravity of network intrusions has\nescalated, emphasizing the criticality of network intrusion detection systems\nfor safeguarding security. While deep learning models have exhibited promising\nresults in intrusion detection, they face challenges in managing\nhigh-dimensional, complex traffic patterns and imbalanced data categories. This\npaper presents CSAGC-IDS, a network intrusion detection model based on deep\nlearning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced\nconvolutional conditional generative adversarial network that generates\nhigh-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS\nintegrates CSCA-CNN, a convolutional neural network enhanced through cost\nsensitive learning and channel attention mechanism, to extract features from\ncomplex traffic data for precise detection. Experiments conducted on the\nNSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of\n84.52% in five-class classification task, and an accuracy of 91.09% and an F1\nscore of 92.04% in binary classification task.Furthermore, this paper provides\nan interpretability analysis of the proposed model, using SHAP and LIME to\nexplain the decision-making mechanisms of the model.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 07:27:51", "updated": "2025-05-20 07:27:51", "pdf_url": "http://arxiv.org/pdf/2505.14027v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14029v1", "title": "AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards", "authors": ["Laura-Sophia von Hirschhausen", "Jannes S. Magnusson", "Mykyta Kovalenko", "Fredrik Boye", "Tanay Rawat", "Peter Eisert", "Anna Hilsmann", "Sebastian Pretzsch", "Sebastian Bosse"], "abstract": "Deep learning has transformed computer vision for precision agriculture, yet\napple orchard monitoring remains limited by dataset constraints. The lack of\ndiverse, realistic datasets and the difficulty of annotating dense,\nheterogeneous scenes. Existing datasets overlook different growth stages and\nstereo imagery, both essential for realistic 3D modeling of orchards and tasks\nlike fruit localization, yield estimation, and structural analysis. To address\nthese gaps, we present AppleGrowthVision, a large-scale dataset comprising two\nsubsets. The first includes 9,317 high resolution stereo images collected from\na farm in Brandenburg (Germany), covering six agriculturally validated growth\nstages over a full growth cycle. The second subset consists of 1,125 densely\nannotated images from the same farm in Brandenburg and one in Pillnitz\n(Germany), containing a total of 31,084 apple labels. AppleGrowthVision\nprovides stereo-image data with agriculturally validated growth stages,\nenabling precise phenological analysis and 3D reconstructions. Extending\nMinneApple with our data improves YOLOv8 performance by 7.69 % in terms of\nF1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by\n31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy\nusing VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges\nthe gap between agricultural science and computer vision, by enabling the\ndevelopment of robust models for fruit detection, growth modeling, and 3D\nanalysis in precision agriculture. Future work includes improving annotation,\nenhancing 3D reconstruction, and extending multimodal analysis across all\ngrowth stages.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 07:29:22", "updated": "2025-05-20 07:29:22", "pdf_url": "http://arxiv.org/pdf/2505.14029v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14036v1", "title": "Adaptive Cyclic Diffusion for Inference Scaling", "authors": ["Gyubin Lee", "Truong Nhat Nguyen Bao", "Jaesik Yoon", "Dongwoo Lee", "Minsu Kim", "Yoshua Bengio", "Sungjin Ahn"], "abstract": "Diffusion models have demonstrated strong generative capabilities across\ndomains ranging from image synthesis to complex reasoning tasks. However, most\ninference-time scaling methods rely on fixed denoising schedules, limiting\ntheir ability to allocate computation based on instance difficulty or\ntask-specific demands adaptively. We introduce the challenge of adaptive\ninference-time scaling-dynamically adjusting computational effort during\ninference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a\nflexible, search-based inference framework. ABCD refines outputs through\nbi-directional diffusion cycles while adaptively controlling exploration depth\nand termination. It comprises three components: Cyclic Diffusion Search,\nAutomatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.\nExperiments show that ABCD improves performance across diverse tasks while\nmaintaining computational efficiency.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 07:31:38", "updated": "2025-05-20 07:31:38", "pdf_url": "http://arxiv.org/pdf/2505.14036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14038v1", "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data", "authors": ["Xinzhe Zheng", "Sijie Ji", "Jiawei Sun", "Renqi Chen", "Wei Gao", "Mani Srivastava"], "abstract": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 07:36:28", "updated": "2025-05-20 07:36:28", "pdf_url": "http://arxiv.org/pdf/2505.14038v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14045v1", "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 07:43:45", "updated": "2025-05-20 07:43:45", "pdf_url": "http://arxiv.org/pdf/2505.14045v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14057v1", "title": "Field Matters: A lightweight LLM-enhanced Method for CTR Prediction", "authors": ["Yu Cui", "Feng Liu", "Jiawei Chen", "Xingyu Lou", "Changwang Zhang", "Jun Wang", "Yuegang Sun", "Xiaohu Yang", "Can Wang"], "abstract": "Click-through rate (CTR) prediction is a fundamental task in modern\nrecommender systems. In recent years, the integration of large language models\n(LLMs) has been shown to effectively enhance the performance of traditional CTR\nmethods. However, existing LLM-enhanced methods often require extensive\nprocessing of detailed textual descriptions for large-scale instances or\nuser/item entities, leading to substantial computational overhead. To address\nthis challenge, this work introduces LLaCTR, a novel and lightweight\nLLM-enhanced CTR method that employs a field-level enhancement paradigm.\nSpecifically, LLaCTR first utilizes LLMs to distill crucial and lightweight\nsemantic knowledge from small-scale feature fields through self-supervised\nfield-feature fine-tuning. Subsequently, it leverages this field-level semantic\nknowledge to enhance both feature representation and feature interactions. In\nour experiments, we integrate LLaCTR with six representative CTR models across\nfour datasets, demonstrating its superior performance in terms of both\neffectiveness and efficiency compared to existing LLM-enhanced methods. Our\ncode is available at https://anonymous.4open.science/r/LLaCTR-EC46.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-20 08:02:41", "updated": "2025-05-20 08:02:41", "pdf_url": "http://arxiv.org/pdf/2505.14057v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14064v1", "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI", "authors": ["Cosmin I. Bercea", "Jun Li", "Philipp Raffler", "Evamaria O. Riedel", "Lena Schmitzer", "Angela Kurz", "Felix Bitzer", "Paula Ro\u00dfm\u00fcller", "Julian Canisius", "Mirjam L. Beyrle", "Che Liu", "Wenjia Bai", "Bernhard Kainz", "Julia A. Schnabel", "Benedikt Wiestler"], "abstract": "In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously $unknown$ categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present $NOVA$, a challenging, real-life $evaluation-only$\nbenchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\n$extreme$ stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies.", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "published": "2025-05-20 08:10:57", "updated": "2025-05-20 08:10:57", "pdf_url": "http://arxiv.org/pdf/2505.14064v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14072v1", "title": "Personalized Student Knowledge Modeling for Future Learning Resource Prediction", "authors": ["Soroush Hashemifar", "Sherry Sahebi"], "abstract": "Despite advances in deep learning for education, student knowledge tracing\nand behavior modeling face persistent challenges: limited personalization,\ninadequate modeling of diverse learning activities (especially non-assessed\nmaterials), and overlooking the interplay between knowledge acquisition and\nbehavioral patterns. Practical limitations, such as fixed-size sequence\nsegmentation, frequently lead to the loss of contextual information vital for\npersonalized learning. Moreover, reliance on student performance on assessed\nmaterials limits the modeling scope, excluding non-assessed interactions like\nlectures. To overcome these shortcomings, we propose Knowledge Modeling and\nMaterial Prediction (KMaP), a stateful multi-task approach designed for\npersonalized and simultaneous modeling of student knowledge and behavior. KMaP\nemploys clustering-based student profiling to create personalized student\nrepresentations, improving predictions of future learning resource preferences.\nExtensive experiments on two real-world datasets confirm significant behavioral\ndifferences across student clusters and validate the efficacy of the KMaP\nmodel.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-20 08:23:50", "updated": "2025-05-20 08:23:50", "pdf_url": "http://arxiv.org/pdf/2505.14072v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14080v1", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "abstract": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "published": "2025-05-20 08:36:47", "updated": "2025-05-20 08:36:47", "pdf_url": "http://arxiv.org/pdf/2505.14080v1", "comment": null, "doi": "10.1145/3715275.3732112", "journal_ref": "FAccT '25: Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency"}
{"arxiv_id": "2505.14103v1", "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models", "authors": ["Guangke Chen", "Fu Song", "Zhe Zhao", "Xiaojun Jia", "Yang Liu", "Yanchen Qiao", "Weizhe Zhang"], "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak.", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-20 09:10:45", "updated": "2025-05-20 09:10:45", "pdf_url": "http://arxiv.org/pdf/2505.14103v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14106v1", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "abstract": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:13:22", "updated": "2025-05-20 09:13:22", "pdf_url": "http://arxiv.org/pdf/2505.14106v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14107v1", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "abstract": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:14:53", "updated": "2025-05-20 09:14:53", "pdf_url": "http://arxiv.org/pdf/2505.14107v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14117v1", "title": "Collaborative Unlabeled Data Optimization", "authors": ["Xinyi Shang", "Peng Sun", "Fengyuan Liu", "Tao Lin"], "abstract": "This paper pioneers a novel data-centric paradigm to maximize the utility of\nunlabeled data, tackling a critical question: How can we enhance the efficiency\nand sustainability of deep learning training by optimizing the data itself? We\nbegin by identifying three key limitations in existing model-centric\napproaches, all rooted in a shared bottleneck: knowledge extracted from data is\nlocked to model parameters, hindering its reusability and scalability. To this\nend, we propose CoOpt, a highly efficient, parallelized framework for\ncollaborative unlabeled data optimization, thereby effectively encoding\nknowledge into the data itself. By distributing unlabeled data and leveraging\npublicly available task-agnostic models, CoOpt facilitates scalable, reusable,\nand sustainable training pipelines. Extensive experiments across diverse\ndatasets and architectures demonstrate its efficacy and efficiency, achieving\n13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,\nwith training speedups of $1.94 \\times $ and $1.2 \\times$.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:21:40", "updated": "2025-05-20 09:21:40", "pdf_url": "http://arxiv.org/pdf/2505.14117v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14125v1", "title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning", "authors": ["Viet Anh Khoa Tran", "Emre Neftci", "Willem. A. M. Wybo"], "abstract": "Biological brains learn continually from a stream of unlabeled data, while\nintegrating specialized information from sparsely labeled examples without\ncompromising their ability to generalize. Meanwhile, machine learning methods\nare susceptible to catastrophic forgetting in this natural learning setting, as\nsupervised specialist fine-tuning degrades performance on the original task. We\nintroduce task-modulated contrastive learning (TMCL), which takes inspiration\nfrom the biophysical machinery in the neocortex, using predictive coding\nprinciples to integrate top-down information continually and without\nsupervision. We follow the idea that these principles build a view-invariant\nrepresentation space, and that this can be implemented using a contrastive\nloss. Then, whenever labeled samples of a new class occur, new affine\nmodulations are learned that improve separation of the new class from all\nothers, without affecting feedforward weights. By co-opting the view-invariance\nlearning mechanism, we then train feedforward weights to match the unmodulated\nrepresentation of a data sample to its modulated counterparts. This introduces\nmodulation invariance into the representation space, and, by also using past\nmodulations, stabilizes it. Our experiments show improvements in both\nclass-incremental and transfer learning over state-of-the-art unsupervised\napproaches, as well as over comparable supervised approaches, using as few as\n1% of available labels. Taken together, our work suggests that top-down\nmodulations play a crucial role in balancing stability and plasticity.", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "68T05 (primary), 68T07, 68T45 (secondary)", "I.2.6; I.2.10"], "published": "2025-05-20 09:31:57", "updated": "2025-05-20 09:31:57", "pdf_url": "http://arxiv.org/pdf/2505.14125v1", "comment": "33 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14128v1", "title": "A Methodological Framework for Measuring Spatial Labeling Similarity", "authors": ["Yihang Du", "Jiaying Hu", "Suyang Hou", "Yueyang Ding", "Xiaobo Sun"], "abstract": "Spatial labeling assigns labels to specific spatial locations to characterize\ntheir spatial properties and relationships, with broad applications in\nscientific research and practice. Measuring the similarity between two spatial\nlabelings is essential for understanding their differences and the contributing\nfactors, such as changes in location properties or labeling methods. An\nadequate and unbiased measurement of spatial labeling similarity should\nconsider the number of matched labels (label agreement), the topology of\nspatial label distribution, and the heterogeneous impacts of mismatched labels.\nHowever, existing methods often fail to account for all these aspects. To\naddress this gap, we propose a methodological framework to guide the\ndevelopment of methods that meet these requirements. Given two spatial\nlabelings, the framework transforms them into graphs based on location\norganization, labels, and attributes (e.g., location significance). The\ndistributions of their graph attributes are then extracted, enabling an\nefficient computation of distributional discrepancy to reflect the\ndissimilarity level between the two labelings. We further provide a concrete\nimplementation of this framework, termed Spatial Labeling Analogy Metric\n(SLAM), along with an analysis of its theoretical foundation, for evaluating\nspatial labeling results in spatial transcriptomics (ST) \\textit{as per} their\nsimilarity with ground truth labeling. Through a series of carefully designed\nexperimental cases involving both simulated and real ST data, we demonstrate\nthat SLAM provides a comprehensive and accurate reflection of labeling quality\ncompared to other well-established evaluation metrics. Our code is available at\nhttps://github.com/YihDu/SLAM.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:34:03", "updated": "2025-05-20 09:34:03", "pdf_url": "http://arxiv.org/pdf/2505.14128v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14136v1", "title": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging", "authors": ["Ryo Bertolissi", "Jonas H\u00fcbotter", "Ido Hakimi", "Andreas Krause"], "abstract": "Mixture of expert (MoE) models are a promising approach to increasing model\ncapacity without increasing inference cost, and are core components of many\nstate-of-the-art language models. However, current MoE models typically use\nonly few experts due to prohibitive training and inference cost. We propose\nTest-Time Model Merging (TTMM) which scales the MoE paradigm to an order of\nmagnitude more experts and uses model merging to avoid almost any test-time\noverhead. We show that TTMM is an approximation of test-time training (TTT),\nwhich fine-tunes an expert model for each prediction task, i.e., prompt. TTT\nhas recently been shown to significantly improve language models, but is\ncomputationally expensive. We find that performance of TTMM improves with more\nexperts and approaches the performance of TTT. Moreover, we find that with a 1B\nparameter base model, TTMM is more than 100x faster than TTT at test-time by\namortizing the cost of TTT at train-time. Thus, TTMM offers a promising\ncost-effective approach to scale test-time training.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:39:54", "updated": "2025-05-20 09:39:54", "pdf_url": "http://arxiv.org/pdf/2505.14136v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14137v1", "title": "Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games", "authors": ["Vojt\u011bch K\u016fr", "V\u00edt Musil", "Vojt\u011bch \u0158eh\u00e1k"], "abstract": "Adversarial Patrolling games form a subclass of Security games where a\nDefender moves between locations, guarding vulnerable targets. The main\nalgorithmic problem is constructing a strategy for the Defender that minimizes\nthe worst damage an Attacker can cause. We focus on the class of finite-memory\n(also known as regular) Defender's strategies that experimentally outperformed\nother competing classes. A finite-memory strategy can be seen as a positional\nstrategy on a finite set of states. Each state consists of a pair of a location\nand a certain integer value--called memory. Existing algorithms improve the\ntransitional probabilities between the states but require that the available\nmemory size itself is assigned at each location manually. Choosing the right\nmemory assignment is a well-known open and hard problem that hinders the\nusability of finite-memory strategies. We solve this issue by developing a\ngeneral method that iteratively changes the memory assignment. Our algorithm\ncan be used in connection with \\emph{any} black-box strategy optimization tool.\nWe evaluate our method on various experiments and show its robustness by\nsolving instances of various patrolling models.", "categories": ["cs.AI"], "published": "2025-05-20 09:40:53", "updated": "2025-05-20 09:40:53", "pdf_url": "http://arxiv.org/pdf/2505.14137v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14139v1", "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning", "authors": ["Marvin Alles", "Nutan Chen", "Patrick van der Smagt", "Botond Cseke"], "abstract": "The use of guidance to steer sampling toward desired outcomes has been widely\nexplored within diffusion models, especially in applications such as image and\ntrajectory generation. However, incorporating guidance during training remains\nrelatively underexplored. In this work, we introduce energy-guided flow\nmatching, a novel approach that enhances the training of flow models and\neliminates the need for guidance at inference time. We learn a conditional\nvelocity field corresponding to the flow policy by approximating an\nenergy-guided probability path as a Gaussian path. Learning guided trajectories\nis appealing for tasks where the target distribution is defined by a\ncombination of data and an energy function, as in reinforcement learning.\nDiffusion-based policies have recently attracted attention for their expressive\npower and ability to capture multi-modal action distributions. Typically, these\npolicies are optimized using weighted objectives or by back-propagating\ngradients through actions sampled by the policy. As an alternative, we propose\nFlowQ, an offline reinforcement learning algorithm based on energy-guided flow\nmatching. Our method achieves competitive performance while the policy training\ntime is constant in the number of flow sampling steps.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-20 09:43:05", "updated": "2025-05-20 09:43:05", "pdf_url": "http://arxiv.org/pdf/2505.14139v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14140v1", "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning", "authors": ["Qianyue Hao", "Sibo Li", "Jian Yuan", "Yong Li"], "abstract": "Despite rapid advancements in large language models (LLMs), the token-level\nautoregressive nature constrains their complex reasoning capabilities. To\nenhance LLM reasoning, inference-time techniques, including\nChain/Tree/Graph-of-Thought(s), successfully improve the performance, as they\nare fairly cost-effective by guiding reasoning through sophisticated logical\nstructures without modifying LLMs' parameters. However, these manually\npredefined, task-agnostic frameworks are applied uniformly across diverse\ntasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),\nwhere we train a lightweight navigator model with reinforcement learning (RL)\nto adaptively enhance LLM reasoning at inference time. Specifically, we design\nfive basic logic blocks from the perspective of human cognition. During the\nreasoning process, the trained RL navigator dynamically selects the suitable\nlogic blocks and combines them into task-specific logical structures according\nto problem characteristics. Experiments across multiple reasoning benchmarks\n(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)\nillustrate that RLoT outperforms established inference-time techniques by up to\n13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to\nmake sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL\nnavigator demonstrates strong transferability: a model trained on one specific\nLLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is\nopen-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for\nreproducibility.", "categories": ["cs.AI"], "published": "2025-05-20 09:43:33", "updated": "2025-05-20 09:43:33", "pdf_url": "http://arxiv.org/pdf/2505.14140v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14141v1", "title": "Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent", "authors": ["Fanglin Mo", "Junzhe Chen", "Haoxuan Zhu", "Xuming Hu"], "abstract": "Mobile GUI agents execute user commands by directly interacting with the\ngraphical user interface (GUI) of mobile devices, demonstrating significant\npotential to enhance user convenience. However, these agents face considerable\nchallenges in task planning, as they must continuously analyze the GUI and\ngenerate operation instructions step by step. This process often leads to\ndifficulties in making accurate task plans, as GUI agents lack a deep\nunderstanding of how to effectively use the target applications, which can\ncause them to become \"lost\" during task execution. To address the task planning\nissue, we propose SPlanner, a plug-and-play planning module to generate\nexecution plans that guide vision language model(VLMs) in executing tasks. The\nproposed planning module utilizes extended finite state machines (EFSMs) to\nmodel the control logits and configurations of mobile applications. It then\ndecomposes a user instruction into a sequence of primary function modeled in\nEFSMs, and generate the execution path by traversing the EFSMs. We further\nrefine the execution path into a natural language plan using an LLM. The final\nplan is concise and actionable, and effectively guides VLMs to generate\ninteractive GUI actions to accomplish user tasks. SPlanner demonstrates strong\nperformance on dynamic benchmarks reflecting real-world mobile usage. On the\nAndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired\nwith Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point\nimprovement compared to using Qwen2.5-VL-72B without planning assistance.", "categories": ["cs.AI", "I.2.11; H.5.2"], "published": "2025-05-20 09:45:55", "updated": "2025-05-20 09:45:55", "pdf_url": "http://arxiv.org/pdf/2505.14141v1", "comment": "10 pages. Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14143v1", "title": "Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition", "authors": ["Shuo Zhang", "Jinsong Zhang", "Zhejun Zhang", "Lei Li"], "abstract": "Multi-task learning (MTL) enables the efficient transfer of extra knowledge\nacquired from other tasks. The high correlation between multimodal sentiment\nanalysis (MSA) and multimodal emotion recognition (MER) supports their joint\ntraining. However, existing methods primarily employ hard parameter sharing,\nignoring parameter conflicts caused by complex task correlations. In this\npaper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture\nof Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts\nto distinctly model common and unique task characteristics, thereby avoiding\nparameter conflicts. Additionally, inspired by low-rank structures in the\nMixture of Experts (MoE) framework, we design low-rank expert networks to\nreduce parameter and computational overhead as the number of experts increases.\nExtensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that\nMMoLRE achieves state-of-the-art performance on the MSA task and competitive\nresults on the MER task.", "categories": ["cs.AI"], "published": "2025-05-20 09:46:56", "updated": "2025-05-20 09:46:56", "pdf_url": "http://arxiv.org/pdf/2505.14143v1", "comment": "Accepted to ICME 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14146v1", "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "abstract": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 09:53:56", "updated": "2025-05-20 09:53:56", "pdf_url": "http://arxiv.org/pdf/2505.14146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14147v1", "title": "SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning", "authors": ["Xiong Jun Wu", "Zhenduo Zhang", "ZuJie Wen", "Zhiqiang Zhang", "Wang Ren", "Lei Shi", "Cai Chen", "Deng Zhao", "Dingnan Jin", "Qing Cui", "Jun Zhou"], "abstract": "Training large reasoning models (LRMs) with reinforcement learning in STEM\ndomains is hindered by the scarcity of high-quality, diverse, and verifiable\nproblem sets. Existing synthesis methods, such as Chain-of-Thought prompting,\noften generate oversimplified or uncheckable data, limiting model advancement\non complex tasks. To address these challenges, we introduce SHARP, a unified\napproach to Synthesizing High-quality Aligned Reasoning Problems for LRMs\nreinforcement learning with verifiable rewards (RLVR). SHARP encompasses a\nstrategic set of self-alignment principles -- targeting graduate and\nOlympiad-level difficulty, rigorous logical consistency, and unambiguous,\nverifiable answers -- and a structured three-phase framework (Alignment,\nInstantiation, Inference) that ensures thematic diversity and fine-grained\ncontrol over problem generation. We implement SHARP by leveraging a\nstate-of-the-art LRM to infer and verify challenging STEM questions, then\nemploy a reinforcement learning loop to refine the model's reasoning through\nverifiable reward signals. Experiments on benchmarks such as GPQA demonstrate\nthat SHARP-augmented training substantially outperforms existing methods,\nmarkedly improving complex reasoning accuracy and pushing LRM performance\ncloser to expert-level proficiency. Our contributions include the SHARP\nstrategy, framework design, end-to-end implementation, and experimental\nevaluation of its effectiveness in elevating LRM reasoning capabilities.", "categories": ["cs.AI"], "published": "2025-05-20 09:54:42", "updated": "2025-05-20 09:54:42", "pdf_url": "http://arxiv.org/pdf/2505.14147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14148v1", "title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem", "authors": ["Fan Liu", "Zherui Yang", "Cancheng Liu", "Tianrui Song", "Xiaofeng Gao", "Hao Liu"], "abstract": "Mathematical modeling is a cornerstone of scientific discovery and\nengineering practice, enabling the translation of real-world problems into\nformal systems across domains such as physics, biology, and economics. Unlike\nmathematical reasoning, which assumes a predefined formulation, modeling\nrequires open-ended problem analysis, abstraction, and principled\nformalization. While Large Language Models (LLMs) have shown strong reasoning\ncapabilities, they fall short in rigorous model construction, limiting their\nutility in real-world problem-solving. To this end, we formalize the task of\nLLM-powered real-world mathematical modeling, where agents must analyze\nproblems, construct domain-appropriate formulations, and generate complete\nend-to-end solutions. We introduce MM-Bench, a curated benchmark of 111\nproblems from the Mathematical Contest in Modeling (MCM/ICM), spanning the\nyears 2000 to 2025 and across ten diverse domains such as physics, biology, and\neconomics. To tackle this task, we propose MM-Agent, an expert-inspired\nframework that decomposes mathematical modeling into four stages: open-ended\nproblem analysis, structured model formulation, computational problem solving,\nand report generation. Experiments on MM-Bench show that MM-Agent significantly\noutperforms baseline agents, achieving an 11.88\\% improvement over human expert\nsolutions while requiring only 15 minutes and \\$0.88 per task using GPT-4o.\nFurthermore, under official MCM/ICM protocols, MM-Agent assisted two\nundergraduate teams in winning the Finalist Award (\\textbf{top 2.0\\% among\n27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a\nmodeling copilot. Our code is available at\nhttps://github.com/usail-hkust/LLM-MM-Agent", "categories": ["cs.AI"], "published": "2025-05-20 09:55:31", "updated": "2025-05-20 09:55:31", "pdf_url": "http://arxiv.org/pdf/2505.14148v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14156v1", "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "abstract": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "categories": ["cs.CV", "cs.AI", "cs.IR", "I.2; H.3.3"], "published": "2025-05-20 10:05:06", "updated": "2025-05-20 10:05:06", "pdf_url": "http://arxiv.org/pdf/2505.14156v1", "comment": null, "doi": "10.1145/3589334.3645574", "journal_ref": null}
{"arxiv_id": "2505.14157v1", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "abstract": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:05:11", "updated": "2025-05-20 10:05:11", "pdf_url": "http://arxiv.org/pdf/2505.14157v1", "comment": "25 pages, 42 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14163v1", "title": "DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation", "authors": ["He Wang", "Alexander Hanbo Li", "Yiqun Hu", "Sheng Zhang", "Hideo Kobayashi", "Jiani Zhang", "Henry Zhu", "Chung-Wei Hang", "Patrick Ng"], "abstract": "Large language model (LLM) agents have shown promising performance in\ngenerating code for solving complex data science problems. Recent studies\nprimarily focus on enhancing in-context learning through improved search,\nsampling, and planning techniques, while overlooking the importance of the\norder in which problems are tackled during inference. In this work, we develop\na novel inference-time optimization framework, referred to as DSMentor, which\nleverages curriculum learning -- a strategy that introduces simpler task first\nand progressively moves to more complex ones as the learner improves -- to\nenhance LLM agent performance in challenging data science tasks. Our\nmentor-guided framework organizes data science tasks in order of increasing\ndifficulty and incorporates a growing long-term memory to retain prior\nexperiences, guiding the agent's learning progression and enabling more\neffective utilization of accumulated knowledge. We evaluate DSMentor through\nextensive experiments on DSEval and QRData benchmarks. Experiments show that\nDSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval\nand QRData compared to baseline agents. Furthermore, DSMentor demonstrates\nstronger causal reasoning ability, improving the pass rate by 8.8% on the\ncausality problems compared to GPT-4 using Program-of-Thoughts prompts. Our\nwork underscores the importance of developing effective strategies for\naccumulating and utilizing knowledge during inference, mirroring the human\nlearning process and opening new avenues for improving LLM performance through\ncurriculum-based inference optimization.", "categories": ["cs.AI"], "published": "2025-05-20 10:16:21", "updated": "2025-05-20 10:16:21", "pdf_url": "http://arxiv.org/pdf/2505.14163v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14178v1", "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "abstract": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:32:30", "updated": "2025-05-20 10:32:30", "pdf_url": "http://arxiv.org/pdf/2505.14178v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14179v1", "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "abstract": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-20 10:34:45", "updated": "2025-05-20 10:34:45", "pdf_url": "http://arxiv.org/pdf/2505.14179v1", "comment": null, "doi": "10.1016/j.eswa.2024.125529", "journal_ref": "Expert Systems with Applications, 2025"}
{"arxiv_id": "2505.14185v1", "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 10:41:49", "updated": "2025-05-20 10:41:49", "pdf_url": "http://arxiv.org/pdf/2505.14185v1", "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14190v1", "title": "$\u03b1$-GAN by R\u00e9nyi Cross Entropy", "authors": ["Ni Ding", "Miao Qiao", "Jiaxing Xu", "Yiping Ke", "Xiaoyu Zhang"], "abstract": "This paper proposes $\\alpha$-GAN, a generative adversarial network using\nR\\'{e}nyi measures. The value function is formulated, by R\\'{e}nyi cross\nentropy, as an expected certainty measure incurred by the discriminator's soft\ndecision as to where the sample is from, true population or the generator. The\ndiscriminator tries to maximize the R\\'{e}nyi certainty about sample source,\nwhile the generator wants to reduce it by injecting fake samples. This forms a\nmin-max problem with the solution parameterized by the R\\'{e}nyi order\n$\\alpha$. This $\\alpha$-GAN reduces to vanilla GAN at $\\alpha = 1$, where the\nvalue function is exactly the binary cross entropy. The optimization of\n$\\alpha$-GAN is over probability (vector) space. It is shown that the gradient\nis exponentially enlarged when R\\'{e}nyi order is in the range $\\alpha \\in\n(0,1)$. This makes convergence faster, which is verified by experimental\nresults. A discussion shows that choosing $\\alpha \\in (0,1)$ may be able to\nsolve some common problems, e.g., vanishing gradient. A following observation\nreveals that this range has not been fully explored in the existing R\\'{e}nyi\nversion GANs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 10:45:11", "updated": "2025-05-20 10:45:11", "pdf_url": "http://arxiv.org/pdf/2505.14190v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14193v1", "title": "Dynamic Replanning for Improved Public Transport Routing", "authors": ["Abdallah Abuaisha", "Bojie Shen", "Daniel Harabor", "Peter Stuckey", "Mark Wallace"], "abstract": "Delays in public transport are common, often impacting users through\nprolonged travel times and missed transfers. Existing solutions for handling\ndelays remain limited; backup plans based on historical data miss opportunities\nfor earlier arrivals, while snapshot planning accounts for current delays but\nnot future ones. With the growing availability of live delay data, users can\nadjust their journeys in real-time. However, the literature lacks a framework\nthat fully exploits this advantage for system-scale dynamic replanning. To\naddress this, we formalise the dynamic replanning problem in public transport\nrouting and propose two solutions: a \"pull\" approach, where users manually\nrequest replanning, and a novel \"push\" approach, where the server proactively\nmonitors and adjusts journeys. Our experiments show that the push approach\noutperforms the pull approach, achieving significant speedups. The results also\nreveal substantial arrival time savings enabled by dynamic replanning.", "categories": ["cs.AI"], "published": "2025-05-20 10:50:58", "updated": "2025-05-20 10:50:58", "pdf_url": "http://arxiv.org/pdf/2505.14193v1", "comment": "Accepted for publication at IJCAI 2025. 8 pages, 4 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14201v1", "title": "FLASH-D: FlashAttention with Hidden Softmax Division", "authors": ["Kosmas Alexandridis", "Vasileios Titopoulos", "Giorgos Dimitrakopoulos"], "abstract": "The transformer's attention mechanism has revolutionized AI and machine\nlearning, with its efficient computation being crucial to its performance.\nHowever, calculating attention involves matrix operations interspersed with\nsoftmax rescaling, which inherently slows down computation and requires\nprocessing the entire input sequence. Building on online softmax computation,\nFlashAttention integrates softmax calculation with matrix arithmetic, enabling\ntiled computation independent of sequence length. While optimized for GPUs,\nFlashAttention's simplicity makes it amenable to direct hardware acceleration.\nThis work re-evaluates the core FlashAttention kernel, presenting FLASH-D a\nmathematically equivalent, yet simplified, formulation that achieves: (a)\nhiding softmax division within other non-linear function evaluations; (b)\ninherently numerically stable computation of exponentials, eliminating the need\nfor maximum value subtraction; and (c) a reduction in computational cost\nwithout introducing numerical approximations to the FlashAttention kernel.\nImportantly, the essential FlashAttention properties that facilitate efficient\ntiled implementation are fully preserved. Hardware implementation results at\n28nm demonstrate that this proposed formulation achieves a 22.8% reduction in\narea and a 20.3% reduction in power, on average, compared to state-of-the-art\nparallel hardware architectures without any performance penalty.", "categories": ["cs.LG", "cs.AI", "cs.AR"], "published": "2025-05-20 11:01:33", "updated": "2025-05-20 11:01:33", "pdf_url": "http://arxiv.org/pdf/2505.14201v1", "comment": "IEEE/ACM International Symposium on Low Power Electronics and Design\n  (ISLPED) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14206v1", "title": "Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data", "authors": ["Flavio Di Martino", "Franca Delmastro"], "abstract": "The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 11:05:06", "updated": "2025-05-20 11:05:06", "pdf_url": "http://arxiv.org/pdf/2505.14206v1", "comment": "Submitted to ACM Transactions on Computing for Healthcare (ACM\n  HEALTH)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14209v1", "title": "Embedded Mean Field Reinforcement Learning for Perimeter-defense Game", "authors": ["Li Wang", "Xin Yu", "Xuxin Lv", "Gangzheng Ai", "Wenjun Wu"], "abstract": "With the rapid advancement of unmanned aerial vehicles (UAVs) and missile\ntechnologies, perimeter-defense game between attackers and defenders for the\nprotection of critical regions have become increasingly complex and\nstrategically significant across a wide range of domains. However, existing\nstudies predominantly focus on small-scale, simplified two-dimensional\nscenarios, often overlooking realistic environmental perturbations, motion\ndynamics, and inherent heterogeneity--factors that pose substantial challenges\nto real-world applicability. To bridge this gap, we investigate large-scale\nheterogeneous perimeter-defense game in a three-dimensional setting,\nincorporating realistic elements such as motion dynamics and wind fields. We\nderive the Nash equilibrium strategies for both attackers and defenders,\ncharacterize the victory regions, and validate our theoretical findings through\nextensive simulations. To tackle large-scale heterogeneous control challenges\nin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)\nframework. EMFAC leverages representation learning to enable high-level action\naggregation in a mean-field manner, supporting scalable coordination among\ndefenders. Furthermore, we introduce a lightweight agent-level attention\nmechanism based on reward representation, which selectively filters\nobservations and mean-field information to enhance decision-making efficiency\nand accelerate convergence in large-scale tasks. Extensive simulations across\nvarying scales demonstrate the effectiveness and adaptability of EMFAC, which\noutperforms established baselines in both convergence speed and overall\nperformance. To further validate practicality, we test EMFAC in small-scale\nreal-world experiments and conduct detailed analyses, offering deeper insights\ninto the framework's effectiveness in complex scenarios.", "categories": ["cs.AI"], "published": "2025-05-20 11:11:46", "updated": "2025-05-20 11:11:46", "pdf_url": "http://arxiv.org/pdf/2505.14209v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14212v1", "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "abstract": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:16:29", "updated": "2025-05-20 11:16:29", "pdf_url": "http://arxiv.org/pdf/2505.14212v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14216v1", "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "abstract": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 11:22:34", "updated": "2025-05-20 11:22:34", "pdf_url": "http://arxiv.org/pdf/2505.14216v1", "comment": "23 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14217v1", "title": "Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned", "authors": ["Jorge Fabila", "Lidia Garrucho", "V\u00edctor M. Campello", "Carlos Mart\u00edn-Isla", "Karim Lekadir"], "abstract": "This study explores the use of Federated Learning (FL) for tuberculosis (TB)\ndiagnosis using chest X-rays in low-resource settings across Africa. FL allows\nhospitals to collaboratively train AI models without sharing raw patient data,\naddressing privacy concerns and data scarcity that hinder traditional\ncentralized models. The research involved hospitals and research centers in\neight African countries. Most sites used local datasets, while Ghana and The\nGambia used public ones. The study compared locally trained models with a\nfederated model built across all institutions to evaluate FL's real-world\nfeasibility. Despite its promise, implementing FL in sub-Saharan Africa faces\nchallenges such as poor infrastructure, unreliable internet, limited digital\nliteracy, and weak AI regulations. Some institutions were also reluctant to\nshare model updates due to data control concerns. In conclusion, FL shows\nstrong potential for enabling AI-driven healthcare in underserved regions, but\nbroader adoption will require improvements in infrastructure, education, and\nregulatory support.", "categories": ["cs.LG", "cs.AI", "eess.IV"], "published": "2025-05-20 11:23:52", "updated": "2025-05-20 11:23:52", "pdf_url": "http://arxiv.org/pdf/2505.14217v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14226v1", "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "abstract": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:35:25", "updated": "2025-05-20 11:35:25", "pdf_url": "http://arxiv.org/pdf/2505.14226v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14227v1", "title": "VoQA: Visual-only Question Answering", "authors": ["Luyang Jiang", "Jianing An", "Jie Luo", "Wenjun Wu", "Lei Huang"], "abstract": "We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 11:37:49", "updated": "2025-05-20 11:37:49", "pdf_url": "http://arxiv.org/pdf/2505.14227v1", "comment": "18 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14233v1", "title": "Mechanistic Fine-tuning for In-context Learning", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "abstract": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:41:21", "updated": "2025-05-20 11:41:21", "pdf_url": "http://arxiv.org/pdf/2505.14233v1", "comment": "28 pages, 31 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14234v1", "title": "Fast and close Shannon entropy approximation", "authors": ["Illia Horenko", "Davide Bassetti", "Luk\u00e1\u0161 Posp\u00ed\u0161il"], "abstract": "Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy\nare key components in many tools used in physics, information theory, machine\nlearning (ML) and quantum computing. Besides of the significant amounts of SE\ncomputations required in these fields, the singularity of the SE gradient is\none of the central mathematical reason inducing the high cost, frequently low\nrobustness and slow convergence of such tools. Here we propose the Fast Entropy\nApproximation (FEA) - a non-singular rational approximation of Shannon entropy\nand its gradient that achieves a mean absolute error of $10^{-3}$, which is\napproximately $20$ times lower than comparable state-of-the-art methods. FEA\nallows around $50\\%$ faster computation, requiring only $5$ to $6$ elementary\ncomputational operations, as compared to tens of elementary operations behind\nthe fastest entropy computation algorithms with table look-ups, bitshifts, or\nseries approximations. On a set of common benchmarks for the feature selection\nproblem in machine learning, we show that the combined effect of fewer\nelementary operations, low approximation error, and a non-singular gradient\nallows significantly better model quality and enables ML feature extraction\nthat is two to three orders of magnitude faster and computationally cheaper\nwhen incorporating FEA into AI tools.", "categories": ["cs.LG", "cs.AI", "68T01 (Primary) 68Q01, 90C99 (Secondary)"], "published": "2025-05-20 11:41:26", "updated": "2025-05-20 11:41:26", "pdf_url": "http://arxiv.org/pdf/2505.14234v1", "comment": "8 pages, 1 figure", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14235v1", "title": "Toward Embodied AGI: A Review of Embodied AI and the Road Ahead", "authors": ["Yequan Wang", "Aixin Sun"], "abstract": "Artificial General Intelligence (AGI) is often envisioned as inherently\nembodied. With recent advances in robotics and foundational AI models, we stand\nat the threshold of a new era-one marked by increasingly generalized embodied\nAI systems. This paper contributes to the discourse by introducing a systematic\ntaxonomy of Embodied AGI spanning five levels (L1-L5). We review existing\nresearch and challenges at the foundational stages (L1-L2) and outline the key\ncomponents required to achieve higher-level capabilities (L3-L5). Building on\nthese insights and existing technologies, we propose a conceptual framework for\nan L3+ robotic brain, offering both a technical outlook and a foundation for\nfuture exploration.", "categories": ["cs.AI"], "published": "2025-05-20 11:42:26", "updated": "2025-05-20 11:42:26", "pdf_url": "http://arxiv.org/pdf/2505.14235v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14238v1", "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "abstract": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:43:25", "updated": "2025-05-20 11:43:25", "pdf_url": "http://arxiv.org/pdf/2505.14238v1", "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14246v1", "title": "Visual Agentic Reinforcement Fine-Tuning", "authors": ["Ziyu Liu", "Yuhang Zang", "Yushan Zou", "Zijian Liang", "Xiaoyi Dong", "Yuhang Cao", "Haodong Duan", "Dahua Lin", "Jiaqi Wang"], "abstract": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 11:59:25", "updated": "2025-05-20 11:59:25", "pdf_url": "http://arxiv.org/pdf/2505.14246v1", "comment": "project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14252v1", "title": "Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks", "authors": ["Mouad Elaarabi", "Domenico Borzacchiello", "Philippe Le Bot", "Nathan Lauzeral", "Sebastien Comas-Cardona"], "abstract": "In this work, we explore the integration of Sequence Encoding for Online\nParameter Identification with Physics-Informed Neural Networks to create a\nmodel that, once trained, can be utilized for real time applications with\nvariable parameters, boundary conditions, and initial conditions. Recently, the\ncombination of PINNs with Sparse Regression has emerged as a method for\nperforming dynamical system identification through supervised learning and\nsparse regression optimization, while also solving the dynamics using PINNs.\nHowever, this approach can be limited by variations in parameters or boundary\nand initial conditions, requiring retraining of the model whenever changes\noccur. In this work, we introduce an architecture that employs Deep Sets or\nSequence Encoders to encode dynamic parameters, boundary conditions, and\ninitial conditions, using these encoded features as inputs for the PINN,\nenabling the model to adapt to changes in parameters, BCs, and ICs. We apply\nthis approach to three different problems. First, we analyze the Rossler ODE\nsystem, demonstrating the robustness of the model with respect to noise and its\nability to generalize. Next, we explore the model's capability in a 2D\nNavier-Stokes PDE problem involving flow past a cylinder with a parametric\nsinusoidal inlet velocity function, showing that the model can encode pressure\ndata from a few points to identify the inlet velocity profile and utilize\nphysics to compute velocity and pressure throughout the domain. Finally, we\naddress a 1D heat monitoring problem using real data from the heating of glass\nfiber and thermoplastic composite plates.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 12:05:17", "updated": "2025-05-20 12:05:17", "pdf_url": "http://arxiv.org/pdf/2505.14252v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14256v1", "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "abstract": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:09:17", "updated": "2025-05-20 12:09:17", "pdf_url": "http://arxiv.org/pdf/2505.14256v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14260v1", "title": "Speculative Decoding Reimagined for Multimodal Large Language Models", "authors": ["Luxi Lin", "Zhihang Lin", "Zhanpeng Zeng", "Rongrong Ji"], "abstract": "This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 12:12:17", "updated": "2025-05-20 12:12:17", "pdf_url": "http://arxiv.org/pdf/2505.14260v1", "comment": "12 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14268v1", "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:19:10", "updated": "2025-05-20 12:19:10", "pdf_url": "http://arxiv.org/pdf/2505.14268v1", "comment": "16 pages, 14 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14273v1", "title": "X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning", "authors": ["Hiroki Shiraishi", "Hisao Ishibuchi", "Masaya Nakata"], "abstract": "Function approximation is a critical task in various fields. However,\nexisting neural network approaches struggle with locally complex or\ndiscontinuous functions due to their reliance on a single global model covering\nthe entire problem space. We propose X-KAN, a novel method that optimizes\nmultiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary\nrule-based machine learning framework called XCSF. X-KAN combines KAN's high\nexpressiveness with XCSF's adaptive partitioning capability by implementing\nlocal KAN models as rule consequents and defining local regions via rule\nantecedents. Our experimental results on artificial test functions and\nreal-world datasets demonstrate that X-KAN significantly outperforms\nconventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms\nof approximation accuracy. Notably, X-KAN effectively handles functions with\nlocally complex or discontinuous structures that are challenging for\nconventional KAN, using a compact set of rules (average 7.2 $\\pm$ 2.3 rules).\nThese results validate the effectiveness of using KAN as a local model in XCSF,\nwhich evaluates the rule fitness based on both accuracy and generality. Our\nX-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "published": "2025-05-20 12:26:03", "updated": "2025-05-20 12:26:03", "pdf_url": "http://arxiv.org/pdf/2505.14273v1", "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "doi": null, "journal_ref": "34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)"}
{"arxiv_id": "2505.14279v1", "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin M\u00fcnch"], "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:30:46", "updated": "2025-05-20 12:30:46", "pdf_url": "http://arxiv.org/pdf/2505.14279v1", "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14285v1", "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis", "authors": ["Eirini Panteli", "Paulo E. Santos", "Nabil Humphrey"], "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 12:35:43", "updated": "2025-05-20 12:35:43", "pdf_url": "http://arxiv.org/pdf/2505.14285v1", "comment": "8 pages; 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14289v1", "title": "EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection", "authors": ["Yijie Lu", "Tianjie Ju", "Manman Zhao", "Xinbei Ma", "Yuan Guo", "ZhuoSheng Zhang"], "abstract": "As multimodal agents are increasingly trained to operate graphical user\ninterfaces (GUIs) to complete user tasks, they face a growing threat from\nindirect prompt injection, attacks in which misleading instructions are\nembedded into the agent's visual environment, such as popups or chat messages,\nand misinterpreted as part of the intended task. A typical example is\nenvironmental injection, in which GUI elements are manipulated to influence\nagent behavior without directly modifying the user prompt. To address these\nemerging attacks, we propose EVA, a red teaming framework for indirect prompt\ninjection which transforms the attack into a closed loop optimization by\ncontinuously monitoring an agent's attention distribution over the GUI and\nupdating adversarial cues, keywords, phrasing, and layout, in response.\nCompared with prior one shot methods that generate fixed prompts without regard\nfor how the model allocates visual attention, EVA dynamically adapts to\nemerging attention hotspots, yielding substantially higher attack success rates\nand far greater transferability across diverse GUI scenarios. We evaluate EVA\non six widely used generalist and specialist GUI agents in realistic settings\nsuch as popup manipulation, chat based phishing, payments, and email\ncomposition. Experimental results show that EVA substantially improves success\nrates over static baselines. Under goal agnostic constraints, where the\nattacker does not know the agent's task intent, EVA still discovers effective\npatterns. Notably, we find that injection styles transfer well across models,\nrevealing shared behavioral biases in GUI agents. These results suggest that\nevolving indirect prompt injection is a powerful tool not only for red teaming\nagents, but also for uncovering common vulnerabilities in their multimodal\ndecision making.", "categories": ["cs.AI"], "published": "2025-05-20 12:41:05", "updated": "2025-05-20 12:41:05", "pdf_url": "http://arxiv.org/pdf/2505.14289v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14295v1", "title": "Benchmarking data encoding methods in Quantum Machine Learning", "authors": ["Orlane Zang", "Gr\u00e9goire Barru\u00e9", "Tony Quertier"], "abstract": "Data encoding plays a fundamental and distinctive role in Quantum Machine\nLearning (QML). While classical approaches process data directly as vectors,\nQML may require transforming classical data into quantum states through\nencoding circuits, known as quantum feature maps or quantum embeddings. This\nstep leverages the inherently high-dimensional and non-linear nature of Hilbert\nspace, enabling more efficient data separation in complex feature spaces that\nmay be inaccessible to classical methods. This encoding part significantly\naffects the performance of the QML model, so it is important to choose the\nright encoding method for the dataset to be encoded. However, this choice is\ngenerally arbitrary, since there is no \"universal\" rule for knowing which\nencoding to choose based on a specific set of data. There are currently a\nvariety of encoding methods using different quantum logic gates. We studied the\nmost commonly used types of encoding methods and benchmarked them using\ndifferent datasets.", "categories": ["quant-ph", "cs.AI"], "published": "2025-05-20 12:44:14", "updated": "2025-05-20 12:44:14", "pdf_url": "http://arxiv.org/pdf/2505.14295v1", "comment": "30 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14300v1", "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors", "authors": ["Maheep Chaudhary", "Fazl Barez"], "abstract": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 12:49:58", "updated": "2025-05-20 12:49:58", "pdf_url": "http://arxiv.org/pdf/2505.14300v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14312v1", "title": "MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains", "authors": ["Kyungeun Lee", "Moonjung Eo", "Hye-Seung Cho", "Dongmin Kim", "Ye Seul Sim", "Seoyoon Kim", "Min-Kook Suh", "Woohyung Lim"], "abstract": "Despite the widespread use of tabular data in real-world applications, most\nbenchmarks rely on average-case metrics, which fail to reveal how model\nbehavior varies across diverse data regimes. To address this, we propose\nMultiTab, a benchmark suite and evaluation framework for multi-dimensional,\ndata-aware analysis of tabular learning algorithms. Rather than comparing\nmodels only in aggregate, MultiTab categorizes 196 publicly available datasets\nalong key data characteristics, including sample size, label imbalance, and\nfeature interaction, and evaluates 13 representative models spanning a range of\ninductive biases. Our analysis shows that model performance is highly sensitive\nto such regimes: for example, models using sample-level similarity excel on\ndatasets with large sample sizes or high inter-feature correlation, while\nmodels encoding inter-feature dependencies perform best with weakly correlated\nfeatures. These findings reveal that inductive biases do not always behave as\nintended, and that regime-aware evaluation is essential for understanding and\nimproving model behavior. MultiTab enables more principled model design and\noffers practical guidance for selecting models tailored to specific data\ncharacteristics. All datasets, code, and optimization logs are publicly\navailable at https://huggingface.co/datasets/LGAI-DILab/Multitab.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 13:00:43", "updated": "2025-05-20 13:00:43", "pdf_url": "http://arxiv.org/pdf/2505.14312v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14316v1", "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion", "authors": ["Tiehan Cui", "Yanxu Mao", "Peipei Liu", "Congying Liu", "Datao You"], "abstract": "Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 13:03:15", "updated": "2025-05-20 13:03:15", "pdf_url": "http://arxiv.org/pdf/2505.14316v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14330v1", "title": "Handloom Design Generation Using Generative Networks", "authors": ["Rajat Kanti Bhattacharjee", "Meghali Nandi", "Amrit Jha", "Gunajit Kalita", "Ferdous Ahmed Barbhuiya"], "abstract": "This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-20 13:16:55", "updated": "2025-05-20 13:16:55", "pdf_url": "http://arxiv.org/pdf/2505.14330v1", "comment": null, "doi": "10.1109/ICIP40778.2020.9190925", "journal_ref": null}
{"arxiv_id": "2505.14341v1", "title": "Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image", "authors": ["Sifan Li", "Ming Tao", "Hao Zhao", "Ling Shao", "Hao Tang"], "abstract": "Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 13:27:52", "updated": "2025-05-20 13:27:52", "pdf_url": "http://arxiv.org/pdf/2505.14341v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14345v1", "title": "Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights", "authors": ["Aydin Abedinia", "Shima Tabakhi", "Vahid Seydi"], "abstract": "Recent advancements in semi-supervised deep learning have introduced\neffective strategies for leveraging both labeled and unlabeled data to improve\nclassification performance. This work proposes a semi-supervised framework that\nutilizes a distance-based weighting mechanism to prioritize critical training\nsamples based on their proximity to test data. By focusing on the most\ninformative examples, the method enhances model generalization and robustness,\nparticularly in challenging scenarios with noisy or imbalanced datasets.\nBuilding on techniques such as uncertainty consistency and graph-based\nrepresentations, the approach addresses key challenges of limited labeled data\nwhile maintaining scalability. Experiments on twelve benchmark datasets\ndemonstrate significant improvements across key metrics, including accuracy,\nprecision, and recall, consistently outperforming existing methods. This\nframework provides a robust and practical solution for semi-supervised\nlearning, with potential applications in domains such as healthcare and\nsecurity where data limitations pose significant challenges.", "categories": ["cs.LG", "cs.AI", "68T05, 62H30", "I.2.6; I.5.1; I.5.4"], "published": "2025-05-20 13:29:04", "updated": "2025-05-20 13:29:04", "pdf_url": "http://arxiv.org/pdf/2505.14345v1", "comment": "5 pages, 6 figures. This paper has been accepted for publication and\n  oral presentation at the 2025 10th IEEE International Conference on Machine\n  Learning Technologies (ICMLT 2025). The final authenticated version will be\n  available in IEEE Xplore following the conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14349v1", "title": "Upgrading Democracies with Fairer Voting Methods", "authors": ["Evangelos Pournaras", "Srijoni Majumdar", "Thomas Wellings", "Joshua C. Yang", "Fatemeh B. Heravan", "Regula H\u00e4nggli Fricker", "Dirk Helbing"], "abstract": "Voting methods are instrumental design element of democracies. Citizens use\nthem to express and aggregate their preferences to reach a collective decision.\nHowever, voting outcomes can be as sensitive to voting rules as they are to\npeople's voting choices. Despite the significance and inter-disciplinary\nscientific progress on voting methods, several democracies keep relying on\noutdated voting methods that do not fit modern, pluralistic societies well,\nwhile lacking social innovation. Here, we demonstrate how one can upgrade\nreal-world democracies, namely by using alternative preferential voting methods\nsuch as cumulative voting and the method of equal shares designed for a\nproportional representation of voters' preferences. By rigorously assessing a\nnew participatory budgeting approach applied in the city of Aarau, Switzerland,\nwe unravel the striking voting outcomes of fair voting methods: more winning\nprojects with the same budget and broader geographic and preference\nrepresentation of citizens by the elected projects, in particular for voters\nwho used to be under-represented, while promoting novel project ideas. We\nprovide profound causal evidence showing that citizens prefer proportional\nvoting methods, which possess strong legitimacy without the need of very\ntechnical specialized explanations. We also reveal strong underlying democratic\nvalues exhibited by citizens who support fair voting methods such as altruism\nand compromise. These findings come with a global momentum to unleash a new and\nlong-awaited participation blueprint of how to upgrade democracies.", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "cs.MA"], "published": "2025-05-20 13:31:43", "updated": "2025-05-20 13:31:43", "pdf_url": "http://arxiv.org/pdf/2505.14349v1", "comment": "Includes Supplementary Information", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14351v1", "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 13:35:55", "updated": "2025-05-20 13:35:55", "pdf_url": "http://arxiv.org/pdf/2505.14351v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14366v1", "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds", "authors": ["Joel Currie", "Gioele Migno", "Enrico Piacenti", "Maria Elena Giannaccini", "Patric Bach", "Davide De Tommaso", "Agnieszka Wykowska"], "abstract": "We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.", "categories": ["cs.AI", "cs.RO"], "published": "2025-05-20 13:49:09", "updated": "2025-05-20 13:49:09", "pdf_url": "http://arxiv.org/pdf/2505.14366v1", "comment": "Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late\n  Breaking Report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14377v1", "title": "When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making", "authors": ["Ulrike Kuhl", "Annika Bush"], "abstract": "Although the integration of artificial intelligence (AI) into everyday tasks\nimproves efficiency and objectivity, it also risks transmitting bias to human\ndecision-making. In this study, we conducted a controlled experiment that\nsimulated hiring decisions to examine how biased AI recommendations - augmented\nwith or without counterfactual explanations - influence human judgment over\ntime. Participants, acting as hiring managers, completed 60 decision trials\ndivided into a baseline phase without AI, followed by a phase with biased (X)AI\nrecommendations (favoring either male or female candidates), and a final\npost-interaction phase without AI. Our results indicate that the participants\nfollowed the AI recommendations 70% of the time when the qualifications of the\ngiven candidates were comparable. Yet, only a fraction of participants detected\nthe gender bias (8 out of 294). Crucially, exposure to biased AI altered\nparticipants' inherent preferences: in the post-interaction phase,\nparticipants' independent decisions aligned with the bias when no\ncounterfactual explanations were provided before, but reversed the bias when\nexplanations were given. Reported trust did not differ significantly across\nconditions. Confidence varied throughout the study phases after exposure to\nmale-biased AI, indicating nuanced effects of AI bias on decision certainty.\nOur findings point to the importance of calibrating XAI to avoid unintended\nbehavioral shifts in order to safeguard equitable decision-making and prevent\nthe adoption of algorithmic bias.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-20 14:00:28", "updated": "2025-05-20 14:00:28", "pdf_url": "http://arxiv.org/pdf/2505.14377v1", "comment": "Accepted for XAI2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14381v1", "title": "SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation", "authors": ["Yuyang Dong", "Nobuhiro Ueda", "Kriszti\u00e1n Boros", "Daiki Ito", "Takuya Sera", "Masafumi Oyamada"], "abstract": "With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions.", "categories": ["cs.AI"], "published": "2025-05-20 14:03:24", "updated": "2025-05-20 14:03:24", "pdf_url": "http://arxiv.org/pdf/2505.14381v1", "comment": "v1", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14391v1", "title": "Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning", "authors": ["Zhaohui Yang", "Chenghua He", "Xiaowen Shi", "Linjing Li", "Qiyue Yin", "Shihong Deng", "Daxin Jiang"], "abstract": "Many studies focus on data annotation techniques for training effective PRMs.\nHowever, current methods encounter a significant issue when applied to long CoT\nreasoning processes: they tend to focus solely on the first incorrect step and\nall preceding steps, assuming that all subsequent steps are incorrect. These\nmethods overlook the unique self-correction and reflection mechanisms inherent\nin long CoT, where correct reasoning steps may still occur after initial\nreasoning mistakes. To address this issue, we propose a novel data annotation\nmethod for PRMs specifically designed to score the long CoT reasoning process.\nGiven that under the reflection pattern, correct and incorrect steps often\nalternate, we introduce the concepts of Error Propagation and Error Cessation,\nenhancing PRMs' ability to identify both effective self-correction behaviors\nand reasoning based on erroneous steps. Leveraging an LLM-based judger for\nannotation, we collect 1.7 million data samples to train a 7B PRM and evaluate\nit at both solution and step levels. Experimental results demonstrate that\ncompared to existing open-source PRMs and PRMs trained on open-source datasets,\nour PRM achieves superior performance across various metrics, including search\nguidance, BoN, and F1 scores. Compared to widely used MC-based annotation\nmethods, our annotation approach not only achieves higher data efficiency but\nalso delivers superior performance. Detailed analysis is also conducted to\ndemonstrate the stability and generalizability of our method.", "categories": ["cs.AI"], "published": "2025-05-20 14:12:05", "updated": "2025-05-20 14:12:05", "pdf_url": "http://arxiv.org/pdf/2505.14391v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14394v1", "title": "Knowledge Graph Based Repository-Level Code Generation", "authors": ["Mihir Athale", "Vishal Vaddina"], "abstract": "Recent advancements in Large Language Models (LLMs) have transformed code\ngeneration from natural language queries. However, despite their extensive\nknowledge and ability to produce high-quality code, LLMs often struggle with\ncontextual accuracy, particularly in evolving codebases. Current code search\nand retrieval methods frequently lack robustness in both the quality and\ncontextual relevance of retrieved results, leading to suboptimal code\ngeneration. This paper introduces a novel knowledge graph-based approach to\nimprove code search and retrieval leading to better quality of code generation\nin the context of repository-level tasks. The proposed approach represents code\nrepositories as graphs, capturing structural and relational information for\nenhanced context-aware code generation. Our framework employs a hybrid approach\nfor code retrieval to improve contextual relevance, track inter-file modular\ndependencies, generate more robust code and ensure consistency with the\nexisting codebase. We benchmark the proposed approach on the Evolutionary Code\nBenchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,\nand demonstrate that our method significantly outperforms the baseline\napproach. These findings suggest that knowledge graph based code generation\ncould advance robust, context-sensitive coding assistance tools.", "categories": ["cs.AI"], "published": "2025-05-20 14:13:59", "updated": "2025-05-20 14:13:59", "pdf_url": "http://arxiv.org/pdf/2505.14394v1", "comment": "8 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14395v1", "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "abstract": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:14:00", "updated": "2025-05-20 14:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14395v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "authors": ["Ga\u00ebl Gendron", "Jo\u017ee M. Ro\u017eanec", "Michael Witbrock", "Gillian Dobbie"], "abstract": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"], "published": "2025-05-20 14:14:05", "updated": "2025-05-20 14:14:05", "pdf_url": "http://arxiv.org/pdf/2505.14396v1", "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14398v1", "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "abstract": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 14:14:38", "updated": "2025-05-20 14:14:38", "pdf_url": "http://arxiv.org/pdf/2505.14398v1", "comment": "Data and code are available at https://peterbaile.github.io/lag/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14403v1", "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning", "authors": ["Zhaohui Yang", "Shilei Jiang", "Chen Hu", "Linjing Li", "Shihong Deng", "Daxin Jiang"], "abstract": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.", "categories": ["cs.AI"], "published": "2025-05-20 14:16:49", "updated": "2025-05-20 14:16:49", "pdf_url": "http://arxiv.org/pdf/2505.14403v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14412v1", "title": "PRL: Prompts from Reinforcement Learning", "authors": ["Pawe\u0142 Batorski", "Adrian Kosmala", "Paul Swoboda"], "abstract": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 14:26:19", "updated": "2025-05-20 14:26:19", "pdf_url": "http://arxiv.org/pdf/2505.14412v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14419v1", "title": "SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation", "authors": ["Huimin Xu", "Xin Mao", "Feng-Lin Li", "Xiaobao Wu", "Wang Chen", "Wei Zhang", "Anh Tuan Luu"], "abstract": "Process Reward Models (PRMs) have demonstrated promising results in\nmathematical reasoning, but existing process annotation approaches, whether\nthrough human annotations or Monte Carlo simulations, remain computationally\nexpensive. In this paper, we introduce Step COmpression for Process Estimation\n(SCOPE), a novel compression-based approach that significantly reduces\nannotation costs. We first translate natural language reasoning steps into code\nand normalize them through Abstract Syntax Tree, then merge equivalent steps to\nconstruct a prefix tree. Unlike simulation-based methods that waste numerous\nsamples on estimation, SCOPE leverages a compression-based prefix tree where\neach root-to-leaf path serves as a training sample, reducing the complexity\nfrom $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K\nsamples with only 5% of the computational resources required by previous\nmethods. Empirical results demonstrate that PRMs trained on our dataset\nconsistently outperform existing automated annotation approaches on both\nBest-of-N strategy and ProcessBench.", "categories": ["cs.AI"], "published": "2025-05-20 14:31:15", "updated": "2025-05-20 14:31:15", "pdf_url": "http://arxiv.org/pdf/2505.14419v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14428v1", "title": "Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications", "authors": ["Riccardo D'Elia"], "abstract": "The objective of this proposal is to bridge the gap between Deep Learning\n(DL) and System Dynamics (SD) by developing an interpretable neural system\ndynamics framework. While DL excels at learning complex models and making\naccurate predictions, it lacks interpretability and causal reliability.\nTraditional SD approaches, on the other hand, provide transparency and causal\ninsights but are limited in scalability and require extensive domain knowledge.\nTo overcome these limitations, this project introduces a Neural System Dynamics\npipeline, integrating Concept-Based Interpretability, Mechanistic\nInterpretability, and Causal Machine Learning. This framework combines the\npredictive power of DL with the interpretability of traditional SD models,\nresulting in both causal reliability and scalability. The efficacy of the\nproposed pipeline will be validated through real-world applications of the\nEU-funded AutoMoTIF project, which is focused on autonomous multimodal\ntransportation systems. The long-term goal is to collect actionable insights\nthat support the integration of explainability and safety in autonomous\nsystems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 14:38:39", "updated": "2025-05-20 14:38:39", "pdf_url": "http://arxiv.org/pdf/2505.14428v1", "comment": "To be submitted to CEUR-WS.org for publication in the Doctoral\n  Consortium Proceedings of XAI 2025, The World Conference on Explainable\n  Artificial Intelligence", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14435v1", "title": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI", "authors": ["Annika Bush", "Meltem Aksoy", "Markus Pauly", "Greta Ontrup"], "abstract": "As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making.", "categories": ["cs.CY", "cs.AI"], "published": "2025-05-20 14:41:56", "updated": "2025-05-20 14:41:56", "pdf_url": "http://arxiv.org/pdf/2505.14435v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14436v1", "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:42:03", "updated": "2025-05-20 14:42:03", "pdf_url": "http://arxiv.org/pdf/2505.14436v1", "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14442v1", "title": "Creative Preference Optimization", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:43:41", "updated": "2025-05-20 14:43:41", "pdf_url": "http://arxiv.org/pdf/2505.14442v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14451v1", "title": "RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation", "authors": ["Md Atik Ahamed", "Qiang Ye", "Qiang Cheng"], "abstract": "Missing values in high-dimensional, mixed-type datasets pose significant\nchallenges for data imputation, particularly under Missing Not At Random (MNAR)\nmechanisms. Existing methods struggle to integrate local and global data\ncharacteristics, limiting performance in MNAR and high-dimensional settings. We\npropose an innovative framework, RefiDiff, combining local machine learning\npredictions with a novel Mamba-based denoising network capturing\ninterrelationships among distant features and samples. Our approach leverages\npre-refinement for initial warm-up imputations and post-refinement to polish\nresults, enhancing stability and accuracy. By encoding mixed-type data into\nunified tokens, RefiDiff enables robust imputation without architectural or\nhyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods\nacross missing-value settings, excelling in MNAR with a 4x faster training time\nthan SOTA DDPM-based approaches. Extensive evaluations on nine real-world\ndatasets demonstrate its robustness, scalability, and effectiveness in handling\ncomplex missingness patterns.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 14:51:07", "updated": "2025-05-20 14:51:07", "pdf_url": "http://arxiv.org/pdf/2505.14451v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14452v1", "title": "How Managers Perceive AI-Assisted Conversational Training for Workplace Communication", "authors": ["Lance T Wilhelm", "Xiaohan Ding", "Kirk McInnis Knutsen", "Buse Carik", "Eugenia H Rho"], "abstract": "Effective workplace communication is essential for managerial success, yet\nmany managers lack access to tailored and sustained training. Although\nAI-assisted communication systems may offer scalable training solutions, little\nis known about how managers envision the role of AI in helping them improve\ntheir communication skills. To investigate this, we designed a conversational\nrole-play system, CommCoach, as a functional probe to understand how managers\nanticipate using AI to practice their communication skills. Through\nsemi-structured interviews, participants emphasized the value of adaptive,\nlow-risk simulations for practicing difficult workplace conversations. They\nalso highlighted opportunities, including human-AI teaming, transparent and\ncontext-aware feedback, and greater control over AI-generated personas.\nAI-assisted communication training should balance personalization, structured\nlearning objectives, and adaptability to different user styles and contexts.\nHowever, achieving this requires carefully navigating tensions between adaptive\nand consistent AI feedback, realism and potential bias, and the open-ended\nnature of AI conversations versus structured workplace discourse.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-20 14:51:27", "updated": "2025-05-20 14:51:27", "pdf_url": "http://arxiv.org/pdf/2505.14452v1", "comment": "accepted to CUI '25", "doi": "10.1145/3719160.3736639", "journal_ref": null}
{"arxiv_id": "2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "authors": ["Chihan Huang", "Hao Tang"], "abstract": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:52:41", "updated": "2025-05-20 14:52:41", "pdf_url": "http://arxiv.org/pdf/2505.14455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14469v1", "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "abstract": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:05:03", "updated": "2025-05-20 15:05:03", "pdf_url": "http://arxiv.org/pdf/2505.14469v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14479v1", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "abstract": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:13:32", "updated": "2025-05-20 15:13:32", "pdf_url": "http://arxiv.org/pdf/2505.14479v1", "comment": "long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14489v1", "title": "Reasoning Models Better Express Their Confidence", "authors": ["Dongkeun Yoon", "Seungone Kim", "Sohee Yang", "Sunkyoung Kim", "Soyeon Kim", "Yongil Kim", "Eunbi Choi", "Yireun Kim", "Minjoon Seo"], "abstract": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:19:00", "updated": "2025-05-20 15:19:00", "pdf_url": "http://arxiv.org/pdf/2505.14489v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14499v1", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "abstract": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:28:26", "updated": "2025-05-20 15:28:26", "pdf_url": "http://arxiv.org/pdf/2505.14499v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14505v1", "title": "ModRWKV: Transformer Multimodality in Linear Time", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "abstract": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:34:36", "updated": "2025-05-20 15:34:36", "pdf_url": "http://arxiv.org/pdf/2505.14505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14510v1", "title": "BACON: A fully explainable AI model with graded logic for decision making problems", "authors": ["Haishi Bai", "Jozo Dujmovic", "Jianwu Wang"], "abstract": "As machine learning models and autonomous agents are increasingly deployed in\nhigh-stakes, real-world domains such as healthcare, security, finance, and\nrobotics, the need for transparent and trustworthy explanations has become\ncritical. To ensure end-to-end transparency of AI decisions, we need models\nthat are not only accurate but also fully explainable and human-tunable. We\nintroduce BACON, a novel framework for automatically training explainable AI\nmodels for decision making problems using graded logic. BACON achieves high\npredictive accuracy while offering full structural transparency and precise,\nlogic-based symbolic explanations, enabling effective human-AI collaboration\nand expert-guided refinement. We evaluate BACON with a diverse set of\nscenarios: classic Boolean approximation, Iris flower classification, house\npurchasing decisions and breast cancer diagnosis. In each case, BACON provides\nhigh-performance models while producing compact, human-verifiable decision\nlogic. These results demonstrate BACON's potential as a practical and\nprincipled approach for delivering crisp, trustworthy explainable AI.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-20 15:39:05", "updated": "2025-05-20 15:39:05", "pdf_url": "http://arxiv.org/pdf/2505.14510v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14513v1", "title": "Latent Flow Transformer", "authors": ["Yen-Chen Wu", "Feng-Ting Liao", "Meng-Hsi Chen", "Pei-Chen Ho", "Farhang Nabiei", "Da-shan Shiu"], "abstract": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 15:41:05", "updated": "2025-05-20 15:41:05", "pdf_url": "http://arxiv.org/pdf/2505.14513v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14523v1", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "abstract": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-20 15:46:44", "updated": "2025-05-20 15:46:44", "pdf_url": "http://arxiv.org/pdf/2505.14523v1", "comment": "To be published in ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14524v1", "title": "Guarded Query Routing for Large Language Models", "authors": ["Richard \u0160l\u00e9her", "William Brach", "Tibor Sloboda", "Kristi\u00e1n Ko\u0161\u0165\u00e1l", "Lukas Galke"], "abstract": "Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}.", "categories": ["cs.AI"], "published": "2025-05-20 15:46:59", "updated": "2025-05-20 15:46:59", "pdf_url": "http://arxiv.org/pdf/2505.14524v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14526v1", "title": "NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation", "authors": ["Matteo El-Hariry", "Antoine Richard", "Ricard M. Castan", "Luis F. W. Batista", "Matthieu Geist", "Cedric Pradalier", "Miguel Olivares-Mendez"], "abstract": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 15:48:23", "updated": "2025-05-20 15:48:23", "pdf_url": "http://arxiv.org/pdf/2505.14526v1", "comment": "Submitted for publication. Under review (2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14533v1", "title": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers", "authors": ["Mohammad Irfan Uddin", "Nishad Tasnim", "Md Omor Faruk", "Zejian Zhou"], "abstract": "Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 15:52:43", "updated": "2025-05-20 15:52:43", "pdf_url": "http://arxiv.org/pdf/2505.14533v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14539v1", "title": "A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)", "authors": ["Gaia Belardinelli", "Thomas Bolander", "Sebastian Watzl"], "abstract": "In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.", "categories": ["cs.AI"], "published": "2025-05-20 15:56:34", "updated": "2025-05-20 15:56:34", "pdf_url": "http://arxiv.org/pdf/2505.14539v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14544v1", "title": "Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study", "authors": ["Saahil Mahato"], "abstract": "Urban traffic congestion, particularly at intersections, significantly\nimpacts travel time, fuel consumption, and emissions. Traditional fixed-time\nsignal control systems often lack the adaptability to manage dynamic traffic\npatterns effectively. This study explores the application of multi-agent\nreinforcement learning (MARL) to optimize traffic signal coordination across\nmultiple intersections within a simulated environment. Utilizing Pygame, a\nsimulation was developed to model a network of interconnected intersections\nwith randomly generated vehicle flows to reflect realistic traffic variability.\nA decentralized MARL controller was implemented, in which each traffic signal\noperates as an autonomous agent, making decisions based on local observations\nand information from neighboring agents. Performance was evaluated against a\nbaseline fixed-time controller using metrics such as average vehicle wait time\nand overall throughput. The MARL approach demonstrated statistically\nsignificant improvements, including reduced average waiting times and improved\nthroughput. These findings suggest that MARL-based dynamic control strategies\nhold substantial promise for improving urban traffic management efficiency.\nMore research is recommended to address scalability and real-world\nimplementation challenges.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-20 15:59:44", "updated": "2025-05-20 15:59:44", "pdf_url": "http://arxiv.org/pdf/2505.14544v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14549v1", "title": "Can Large Language Models Really Recognize Your Name?", "authors": ["Dzung Pham", "Peter Kairouz", "Niloofar Mireshghallah", "Eugene Bagdasarian", "Chau Minh Pham", "Amir Houmansadr"], "abstract": "Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 16:05:05", "updated": "2025-05-20 16:05:05", "pdf_url": "http://arxiv.org/pdf/2505.14549v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14551v1", "title": "Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains", "authors": ["Petros Drineas", "Rohit Nema", "Rafail Ostrovsky", "Vassilis Zikas"], "abstract": "Reputation systems play an essential role in the Internet era, as they enable\npeople to decide whom to trust, by collecting and aggregating data about users'\nbehavior. Recently, several works proposed the use of reputation for the design\nand scalability improvement of decentralized (blockchain) ledgers; however,\nsuch systems are prone to manipulation and to our knowledge no game-theoretic\ntreatment exists that can support their economic robustness.\n  In this work we put forth a new model for the design of what we call, {\\em\ntrustworthy reputation systems}. Concretely, we describe a class of games,\nwhich we term {\\em trustworthy reputation games}, that enable a set of users to\nreport a function of their beliefs about the trustworthiness of each server in\na set -- i.e., their estimate of the probability that this server will behave\naccording to its specified strategy -- in a way that satisfies the following\nproperties:\n  1. It is $(\\epsilon$-)best response for any rational user in the game to play\na prescribed (truthful) strategy according to their true belief.\n  2. Assuming that the users' beliefs are not too far from the {\\em true}\ntrustworthiness of the servers, playing the above ($\\epsilon-$)Nash equilibrium\nallows anyone who observes the users' strategies to estimate the relative\ntrustworthiness of any two servers.\n  Our utilities and decoding function build on a connection between the well\nknown PageRank algorithm and the problem of trustworthiness discovery, which\ncan be of independent interest. Finally, we show how the above games are\nmotivated by and can be leveraged in proof-of-reputation (PoR) blockchains.", "categories": ["cs.GT", "cs.AI", "cs.CR"], "published": "2025-05-20 16:06:25", "updated": "2025-05-20 16:06:25", "pdf_url": "http://arxiv.org/pdf/2505.14551v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14552v1", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "abstract": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:06:32", "updated": "2025-05-20 16:06:32", "pdf_url": "http://arxiv.org/pdf/2505.14552v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14555v1", "title": "Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting", "authors": ["Yingtao Luo", "Shikai Fang", "Binqing Wu", "Qingsong Wen", "Liang Sun"], "abstract": "Weather forecasting is essential but remains computationally intensive and\nphysically incomplete in traditional numerical weather prediction (NWP)\nmethods. Deep learning (DL) models offer efficiency and accuracy but often\nignore physical laws, limiting interpretability and generalization. We propose\nPhyDL-NWP, a physics-guided deep learning framework that integrates physical\nequations with latent force parameterization into data-driven models. It\npredicts weather variables from arbitrary spatiotemporal coordinates, computes\nphysical terms via automatic differentiation, and uses a physics-informed loss\nto align predictions with governing dynamics. PhyDL-NWP enables resolution-free\ndownscaling by modeling weather as a continuous function and fine-tunes\npre-trained models with minimal overhead, achieving up to 170x faster inference\nwith only 55K parameters. Experiments show that PhyDL-NWP improves both\nforecasting performance and physical consistency.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:13:20", "updated": "2025-05-20 16:13:20", "pdf_url": "http://arxiv.org/pdf/2505.14555v1", "comment": "Published/Accepted in KDD 2025 (February Cycle)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14561v1", "title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification", "authors": ["Theo Lepage", "Reda Dehak"], "abstract": "Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS.", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "published": "2025-05-20 16:19:34", "updated": "2025-05-20 16:19:34", "pdf_url": "http://arxiv.org/pdf/2505.14561v1", "comment": "accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14564v1", "title": "Bellman operator convergence enhancements in reinforcement learning algorithms", "authors": ["David Krame Kadurha", "Domini Jocema Leko Moutouo", "Yae Ulrich Gaba"], "abstract": "This paper reviews the topological groundwork for the study of reinforcement\nlearning (RL) by focusing on the structure of state, action, and policy spaces.\nWe begin by recalling key mathematical concepts such as complete metric spaces,\nwhich form the foundation for expressing RL problems. By leveraging the Banach\ncontraction principle, we illustrate how the Banach fixed-point theorem\nexplains the convergence of RL algorithms and how Bellman operators, expressed\nas operators on Banach spaces, ensure this convergence. The work serves as a\nbridge between theoretical mathematics and practical algorithm design, offering\nnew approaches to enhance the efficiency of RL. In particular, we investigate\nalternative formulations of Bellman operators and demonstrate their impact on\nimproving convergence rates and performance in standard RL environments such as\nMountainCar, CartPole, and Acrobot. Our findings highlight how a deeper\nmathematical understanding of RL can lead to more effective algorithms for\ndecision-making problems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:24:42", "updated": "2025-05-20 16:24:42", "pdf_url": "http://arxiv.org/pdf/2505.14564v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14566v1", "title": "KIPPO: Koopman-Inspired Proximal Policy Optimization", "authors": ["Andrei Cozma", "Landon Harris", "Hairong Qi"], "abstract": "Reinforcement Learning (RL) has made significant strides in various domains,\nand policy gradient methods like Proximal Policy Optimization (PPO) have gained\npopularity due to their balance in performance, training stability, and\ncomputational efficiency. These methods directly optimize policies through\ngradient-based updates. However, developing effective control policies for\nenvironments with complex and non-linear dynamics remains a challenge. High\nvariance in gradient estimates and non-convex optimization landscapes often\nlead to unstable learning trajectories. Koopman Operator Theory has emerged as\na powerful framework for studying non-linear systems through an\ninfinite-dimensional linear operator that acts on a higher-dimensional space of\nmeasurement functions. In contrast with their non-linear counterparts, linear\nsystems are simpler, more predictable, and easier to analyze. In this paper, we\npresent Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an\napproximately linear latent-space representation of the underlying system's\ndynamics while retaining essential features for effective policy learning. This\nis achieved through a Koopman-approximation auxiliary network that can be added\nto the baseline policy optimization algorithms without altering the\narchitecture of the core policy or value function. Extensive experimental\nresults demonstrate consistent improvements over the PPO baseline with 6-60%\nincreased performance while reducing variability by up to 91% when evaluated on\nvarious continuous control tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:25:41", "updated": "2025-05-20 16:25:41", "pdf_url": "http://arxiv.org/pdf/2505.14566v1", "comment": "Accepted for IJCAI 2025. This arXiv submission is the full version of\n  the conference paper, including the appendix and supplementary material\n  omitted from the IJCAI proceedings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "abstract": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 16:28:08", "updated": "2025-05-20 16:28:08", "pdf_url": "http://arxiv.org/pdf/2505.14569v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14599v1", "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "abstract": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 16:49:40", "updated": "2025-05-20 16:49:40", "pdf_url": "http://arxiv.org/pdf/2505.14599v1", "comment": "Accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14603v1", "title": "Towards a Foundation Model for Communication Systems", "authors": ["Davide Buffelli", "Sowmen Das", "Yu-Wei Lin", "Sattar Vakili", "Chien-Yi Wang", "Masoud Attarifar", "Pritthijit Nath", "Da-shan Shiu"], "abstract": "Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.", "categories": ["cs.AI", "cs.LG", "eess.SP"], "published": "2025-05-20 16:52:11", "updated": "2025-05-20 16:52:11", "pdf_url": "http://arxiv.org/pdf/2505.14603v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14604v1", "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning", "authors": ["Haoran Zhao", "Yuchen Yan", "Yongliang Shen", "Haolei Xu", "Wenqi Zhang", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.", "categories": ["cs.AI"], "published": "2025-05-20 16:53:40", "updated": "2025-05-20 16:53:40", "pdf_url": "http://arxiv.org/pdf/2505.14604v1", "comment": "Github:https://github.com/CCAI-Lab/Self-Braking-Tuning; Project:\n  https://CCAI-Lab.github.io/SBT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14608v1", "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "abstract": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:55:44", "updated": "2025-05-20 16:55:44", "pdf_url": "http://arxiv.org/pdf/2505.14608v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14615v1", "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", "authors": ["Anjiang Wei", "Yuheng Wu", "Yingjia Wan", "Tarun Suresh", "Huanmi Tan", "Zhanke Zhou", "Sanmi Koyejo", "Ke Wang", "Alex Aiken"], "abstract": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "published": "2025-05-20 17:00:22", "updated": "2025-05-20 17:00:22", "pdf_url": "http://arxiv.org/pdf/2505.14615v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14625v1", "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 17:16:44", "updated": "2025-05-20 17:16:44", "pdf_url": "http://arxiv.org/pdf/2505.14625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14627v1", "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "authors": ["Ashutosh Adhikari", "Mirella Lapata"], "abstract": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:18:17", "updated": "2025-05-20 17:18:17", "pdf_url": "http://arxiv.org/pdf/2505.14627v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14629v1", "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "abstract": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-20 17:19:57", "updated": "2025-05-20 17:19:57", "pdf_url": "http://arxiv.org/pdf/2505.14629v1", "comment": "Accepted at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "published": "2025-05-20 17:24:09", "updated": "2025-05-20 17:24:09", "pdf_url": "http://arxiv.org/pdf/2505.14633v1", "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14646v1", "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation", "authors": ["Anna C. Doris", "Md Ferdous Alam", "Amin Heyrani Nobari", "Faez Ahmed"], "abstract": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 17:34:44", "updated": "2025-05-20 17:34:44", "pdf_url": "http://arxiv.org/pdf/2505.14646v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "abstract": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-20 17:42:34", "updated": "2025-05-20 17:42:34", "pdf_url": "http://arxiv.org/pdf/2505.14654v1", "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14656v1", "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning", "authors": ["Zihao Zhang", "Fei Liu"], "abstract": "While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch.", "categories": ["cs.AI"], "published": "2025-05-20 17:43:33", "updated": "2025-05-20 17:43:33", "pdf_url": "http://arxiv.org/pdf/2505.14656v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14659v1", "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks", "authors": ["Navneet Kaur", "Lav Gupta"], "abstract": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 17:46:09", "updated": "2025-05-20 17:46:09", "pdf_url": "http://arxiv.org/pdf/2505.14659v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14660v1", "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "authors": ["Ronald Seoh", "Dan Goldwasser"], "abstract": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 17:47:04", "updated": "2025-05-20 17:47:04", "pdf_url": "http://arxiv.org/pdf/2505.14660v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14661v1", "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems", "authors": ["Matthew Russo", "Sivaprasad Sudhir", "Gerardo Vitagliano", "Chunwei Liu", "Tim Kraska", "Samuel Madden", "Michael Cafarella"], "abstract": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.", "categories": ["cs.DB", "cs.AI", "H.2.4; I.2.5"], "published": "2025-05-20 17:49:46", "updated": "2025-05-20 17:49:46", "pdf_url": "http://arxiv.org/pdf/2505.14661v1", "comment": "16 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14664v1", "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings", "authors": ["Yilin Ye", "Junchao Huang", "Xingchen Zeng", "Jiazhi Xia", "Wei Zeng"], "abstract": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "published": "2025-05-20 17:52:03", "updated": "2025-05-20 17:52:03", "pdf_url": "http://arxiv.org/pdf/2505.14664v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:54:54", "updated": "2025-05-20 17:54:54", "pdf_url": "http://arxiv.org/pdf/2505.14667v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "categories": ["cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:55:25", "updated": "2025-05-20 17:55:25", "pdf_url": "http://arxiv.org/pdf/2505.14668v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14673v1", "title": "Training-Free Watermarking for Autoregressive Image Generation", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "abstract": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.", "categories": ["cs.CV", "cs.AI", "cs.CR"], "published": "2025-05-20 17:58:02", "updated": "2025-05-20 17:58:02", "pdf_url": "http://arxiv.org/pdf/2505.14673v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "abstract": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:59:13", "updated": "2025-05-20 17:59:13", "pdf_url": "http://arxiv.org/pdf/2505.14680v1", "comment": "SIGIR 2025 Perspective Paper", "doi": "10.1145/3726302.3730353", "journal_ref": null}
{"arxiv_id": "2505.14681v1", "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "published": "2025-05-20 17:59:16", "updated": "2025-05-20 17:59:16", "pdf_url": "http://arxiv.org/pdf/2505.14681v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14684v1", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 17:59:31", "updated": "2025-05-20 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.14684v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13792v1", "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "abstract": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 00:49:19", "updated": "2025-05-20 00:49:19", "pdf_url": "http://arxiv.org/pdf/2505.13792v1", "comment": "10 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13820v1", "title": "Structured Agent Distillation for Large Language Model", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 02:01:55", "updated": "2025-05-20 02:01:55", "pdf_url": "http://arxiv.org/pdf/2505.13820v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "abstract": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 02:27:08", "updated": "2025-05-20 02:27:08", "pdf_url": "http://arxiv.org/pdf/2505.13840v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13844v1", "title": "Improve Language Model and Brain Alignment via Associative Memory", "authors": ["Congchi Yin", "Yongpeng Zhang", "Xuyun Wen", "Piji Li"], "abstract": "Associative memory engages in the integration of relevant information for\ncomprehension in the human cognition system. In this work, we seek to improve\nalignment between language models and human brain while processing speech\ninformation by integrating associative memory. After verifying the alignment\nbetween language model and brain by mapping language model activations to brain\nactivity, the original text stimuli expanded with simulated associative memory\nare regarded as input to computational language models. We find the alignment\nbetween language model and brain is improved in brain regions closely related\nto associative memory processing. We also demonstrate large language models\nafter specific supervised fine-tuning better align with brain response, by\nbuilding the \\textit{Association} dataset containing 1000 samples of stories,\nwith instructions encouraging associative memory as input and associated\ncontent as output.", "categories": ["cs.CL"], "published": "2025-05-20 02:39:09", "updated": "2025-05-20 02:39:09", "pdf_url": "http://arxiv.org/pdf/2505.13844v1", "comment": "Accepted by Findings of ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13847v1", "title": "Forensic deepfake audio detection using segmental speech features", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "abstract": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 02:42:46", "updated": "2025-05-20 02:42:46", "pdf_url": "http://arxiv.org/pdf/2505.13847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13855v1", "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "abstract": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 03:02:05", "updated": "2025-05-20 03:02:05", "pdf_url": "http://arxiv.org/pdf/2505.13855v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13862v1", "title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks", "authors": ["Guobin Shen", "Dongcheng Zhao", "Linghao Feng", "Xiang He", "Jihang Wang", "Sicheng Shen", "Haibo Tong", "Yiting Dong", "Jindong Li", "Xiang Zheng", "Yi Zeng"], "abstract": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 03:14:57", "updated": "2025-05-20 03:14:57", "pdf_url": "http://arxiv.org/pdf/2505.13862v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13866v1", "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning", "authors": ["Jiwon Song", "Dongwon Jo", "Yulhwa Kim", "Jae-Joon Kim"], "abstract": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.", "categories": ["cs.CL"], "published": "2025-05-20 03:21:52", "updated": "2025-05-20 03:21:52", "pdf_url": "http://arxiv.org/pdf/2505.13866v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13878v1", "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models", "authors": ["Yanggan Gu", "Zhaoyi Yan", "Yuanyi Wang", "Yiming Zhang", "Qi Zhou", "Fei Wu", "Hongxia Yang"], "abstract": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 03:32:37", "updated": "2025-05-20 03:32:37", "pdf_url": "http://arxiv.org/pdf/2505.13878v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13886v1", "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning", "authors": ["Jingqi Tong", "Jixin Tang", "Hangcheng Li", "Yurong Mou", "Ming Zhang", "Jun Zhao", "Yanbo Wen", "Fan Song", "Jiahao Zhan", "Yuyang Lu", "Chaoran Tao", "Zhiyuan Guo", "Jizhou Yu", "Tianhao Cheng", "Changhao Jiang", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Weifeng Ge", "Guanhua Chen", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "abstract": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable to produce, challenging for state-of-the-art\nmodels, and diverse with 30 games and 158 tasks. Surprisingly, despite training\nsolely on game data, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33\\% across 7 diverse\nvision-language benchmarks. Our code and dataset are available at\nhttps://github.com/tongjingqi/Code2Logic.", "categories": ["cs.CL", "I.2.7; I.2.10"], "published": "2025-05-20 03:47:44", "updated": "2025-05-20 03:47:44", "pdf_url": "http://arxiv.org/pdf/2505.13886v1", "comment": "49 pages, 19 figures, submitted to NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13887v1", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "abstract": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 03:48:19", "updated": "2025-05-20 03:48:19", "pdf_url": "http://arxiv.org/pdf/2505.13887v1", "comment": "17 pages, 7 figures, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:2502.17110", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13890v1", "title": "Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM", "authors": ["Zhen Xiong", "Yujun Cai", "Zhecheng Li", "Yiwei Wang"], "abstract": "Recent advances in test-time scaling have enabled Large Language Models\n(LLMs) to display sophisticated reasoning abilities via extended\nChain-of-Thought (CoT) generation. Despite their potential, these Reasoning\nLLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as\nperformance degradation under few-shot prompting, that challenge our current\nunderstanding of RLMs. In this work, we introduce a unified graph-based\nanalytical framework for better modeling the reasoning processes of RLMs. Our\nmethod first clusters long, verbose CoT outputs into semantically coherent\nreasoning steps, then constructs directed reasoning graphs to capture\ncontextual and logical dependencies among these steps. Through comprehensive\nanalysis across models and prompting regimes, we reveal that structural\nproperties, such as exploration density, branching, and convergence ratios,\nstrongly correlate with reasoning accuracy. Our findings demonstrate how\nprompting strategies substantially reshape the internal reasoning structure of\nRLMs, directly affecting task outcomes. The proposed framework not only enables\nquantitative evaluation of reasoning quality beyond conventional metrics but\nalso provides practical insights for prompt engineering and the cognitive\nanalysis of LLMs. Code and resources will be released to facilitate future\nresearch in this direction.", "categories": ["cs.CL"], "published": "2025-05-20 03:54:57", "updated": "2025-05-20 03:54:57", "pdf_url": "http://arxiv.org/pdf/2505.13890v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13893v1", "title": "InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion", "authors": ["Yuanyi Wang", "Zhaoyi Yan", "Yiming Zhang", "Qi Zhou", "Yanggan Gu", "Fei Wu", "Hongxia Yang"], "abstract": "Recent advances in large language models (LLMs) have intensified efforts to\nfuse heterogeneous open-source models into a unified system that inherits their\ncomplementary strengths. Existing logit-based fusion methods maintain inference\nefficiency but treat vocabulary dimensions independently, overlooking semantic\ndependencies encoded by cross-dimension interactions. These dependencies\nreflect how token types interact under a model's internal reasoning and are\nessential for aligning models with diverse generation behaviors. To explicitly\nmodel these dependencies, we propose \\textbf{InfiGFusion}, the first\nstructure-aware fusion framework with a novel \\textit{Graph-on-Logits\nDistillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output\nand aggregate their outer products across sequence positions to form a global\nco-activation graph, where nodes represent vocabulary channels and edges\nquantify their joint activations. To ensure scalability and efficiency, we\ndesign a sorting-based closed-form approximation that reduces the original\n$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \\log n)$, with provable\napproximation guarantees. Experiments across multiple fusion settings show that\nGLD consistently improves fusion quality and stability. InfiGFusion outperforms\nSOTA models and fusion baselines across 11 benchmarks spanning reasoning,\ncoding, and mathematics. It shows particular strength in complex reasoning\ntasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal\nJudgement over SFT, demonstrating superior multi-step and relational inference.", "categories": ["cs.CL"], "published": "2025-05-20 03:55:35", "updated": "2025-05-20 03:55:35", "pdf_url": "http://arxiv.org/pdf/2505.13893v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13903v1", "title": "Let's Verify Math Questions Step by Step", "authors": ["Chengyu Shen", "Zhen Hao Wong", "Runming He", "Hao Liang", "Meiyi Qiang", "Zimo Meng", "Zhengyang Zhao", "Bohan Zeng", "Zhengzhou Zhu", "Bin Cui", "Wentao Zhang"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress in\nmathematical reasoning. To enable such capabilities, many existing works\ndistill strong reasoning models into long chains of thought or design\nalgorithms to construct high-quality math QA data for training. However, these\nefforts primarily focus on generating correct reasoning paths and answers,\nwhile largely overlooking the validity of the questions themselves. In this\nwork, we propose Math Question Verification (MathQ-Verify), a novel five-stage\npipeline designed to rigorously filter ill-posed or under-specified math\nproblems. MathQ-Verify first performs format-level validation to remove\nredundant instructions and ensure that each question is syntactically\nwell-formed. It then formalizes each question, decomposes it into atomic\nconditions, and verifies them against mathematical definitions. Next, it\ndetects logical contradictions among these conditions, followed by a\ngoal-oriented completeness check to ensure the question provides sufficient\ninformation for solving. To evaluate this task, we use existing benchmarks\nalong with an additional dataset we construct, containing 2,147 math questions\nwith diverse error types, each manually double-validated. Experiments show that\nMathQ-Verify achieves state-of-the-art performance across multiple benchmarks,\nimproving the F1 score by up to 25 percentage points over the direct\nverification baseline. It further attains approximately 90% precision and 63%\nrecall through a lightweight model voting scheme. MathQ-Verify offers a\nscalable and accurate solution for curating reliable mathematical datasets,\nreducing label noise and avoiding unnecessary computation on invalid questions.\nOur code and data are available at https://github.com/scuuy/MathQ-Verify.", "categories": ["cs.CL"], "published": "2025-05-20 04:07:29", "updated": "2025-05-20 04:07:29", "pdf_url": "http://arxiv.org/pdf/2505.13903v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13908v1", "title": "Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology", "authors": ["Ajitesh Bankula", "Praney Bankula"], "abstract": "Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it\nallows for models trained on resource-rich languages to be applied to\nlow-resource languages more effectively. Recently massively multilingual\npre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot\ntransfer capabilities[14] [13]. This paper investigates cross-linguistic\ntransfer through the lens of language families and morphology. Investigating\nhow language family proximity and morphological similarity affect performance\nacross NLP tasks. We further discuss our results and how it relates to findings\nfrom recent literature. Overall, we compare multilingual model performance and\nreview how linguistic distance metrics correlate with transfer outcomes. We\nalso look into emerging approaches that integrate typological and morphological\ninformation into model pre-training to improve transfer to diverse\nlanguages[18] [19].", "categories": ["cs.CL"], "published": "2025-05-20 04:19:34", "updated": "2025-05-20 04:19:34", "pdf_url": "http://arxiv.org/pdf/2505.13908v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13909v1", "title": "Efficient Agent Training for Computer Use", "authors": ["Yanheng He", "Jiahe Jin", "Pengfei Liu"], "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 04:20:18", "updated": "2025-05-20 04:20:18", "pdf_url": "http://arxiv.org/pdf/2505.13909v1", "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13913v1", "title": "Word length predicts word order: \"Min-max\"-ing drives language evolution", "authors": ["Hiram Ring"], "abstract": "Current theories of language propose an innate (Baker 2001; Chomsky 1981) or\na functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface\nstructures (i.e. word order) that we observe in languages of the world, while\nevolutionary modeling (Dunn et al. 2011) suggests that descent is the primary\nfactor influencing such patterns. Although there are hypotheses for word order\nchange from both innate and usage-based perspectives for specific languages and\nfamilies, there are key disagreements between the two major proposals for\nmechanisms that drive the evolution of language more broadly (Wasow 2002; Levy\n2008). This paper proposes a universal underlying mechanism for word order\nchange based on a large tagged parallel dataset of over 1,500 languages\nrepresenting 133 language families and 111 isolates. Results indicate that word\nclass length is significantly correlated with word order crosslinguistically,\nbut not in a straightforward manner, partially supporting opposing theories of\nprocessing, while at the same time predicting historical word order change in\ntwo different phylogenetic lines and explaining more variance than descent or\nlanguage area in regression models. Such findings suggest an integrated\n\"Min-Max\" theory of language evolution driven by competing pressures of\nprocessing and information structure, aligning with recent efficiency-oriented\n(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et\nal. 2025).", "categories": ["cs.CL"], "published": "2025-05-20 04:25:55", "updated": "2025-05-20 04:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13913v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13936v1", "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "abstract": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:04:15", "updated": "2025-05-20 05:04:15", "pdf_url": "http://arxiv.org/pdf/2505.13936v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13941v1", "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "authors": ["Haoyang Fang", "Boran Han", "Nick Erickson", "Xiyuan Zhang", "Su Zhou", "Anirudh Dagar", "Jiani Zhang", "Ali Caner Turkmen", "Cuixiong Hu", "Huzefa Rangwala", "Ying Nian Wu", "Bernie Wang", "George Karypis"], "abstract": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions.", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 05:20:53", "updated": "2025-05-20 05:20:53", "pdf_url": "http://arxiv.org/pdf/2505.13941v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13944v1", "title": "Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Nam Le", "Linh Ngo Van"], "abstract": "Memory-based approaches have shown strong performance in Continual Relation\nExtraction (CRE). However, storing examples from previous tasks increases\nmemory usage and raises privacy concerns. Recently, prompt-based methods have\nemerged as a promising alternative, as they do not rely on storing past\nsamples. Despite this progress, current prompt-based techniques face several\ncore challenges in CRE, particularly in accurately identifying task identities\nand mitigating catastrophic forgetting. Existing prompt selection strategies\noften suffer from inaccuracies, lack robust mechanisms to prevent forgetting in\nshared parameters, and struggle to handle both cross-task and within-task\nvariations. In this paper, we propose WAVE++, a novel approach inspired by the\nconnection between prefix-tuning and mixture of experts. Specifically, we\nintroduce task-specific prompt pools that enhance flexibility and adaptability\nacross diverse tasks while avoiding boundary-spanning risks; this design more\neffectively captures variations within each task and across tasks. To further\nrefine relation classification, we incorporate label descriptions that provide\nricher, more global context, enabling the model to better distinguish among\ndifferent relations. We also propose a training-free mechanism to improve task\nprediction during inference. Moreover, we integrate a generative model to\nconsolidate prior knowledge within the shared parameters, thereby removing the\nneed for explicit data storage. Extensive experiments demonstrate that WAVE++\noutperforms state-of-the-art prompt-based and rehearsal-based methods, offering\na more robust solution for continual relation extraction. Our code is publicly\navailable at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.", "categories": ["cs.CL"], "published": "2025-05-20 05:22:17", "updated": "2025-05-20 05:22:17", "pdf_url": "http://arxiv.org/pdf/2505.13944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13948v1", "title": "Memory-Centric Embodied Question Answer", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "abstract": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.", "categories": ["cs.CL", "cs.AI", "cs.MM"], "published": "2025-05-20 05:27:57", "updated": "2025-05-20 05:27:57", "pdf_url": "http://arxiv.org/pdf/2505.13948v1", "comment": "14pages, 7 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13949v1", "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "abstract": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:28:21", "updated": "2025-05-20 05:28:21", "pdf_url": "http://arxiv.org/pdf/2505.13949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13957v1", "title": "Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation", "authors": ["Jiankun Zhang", "Shenglai Zeng", "Jie Ren", "Tianqi Zheng", "Hui Liu", "Xianfeng Tang", "Hui Liu", "Yi Chang"], "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by\nintegrating external multimodal databases, but introduce unexplored privacy\nvulnerabilities. While text-based RAG privacy risks have been studied,\nmultimodal data presents unique challenges. We provide the first systematic\nanalysis of MRAG privacy vulnerabilities across vision-language and\nspeech-language modalities. Using a novel compositional structured prompt\nattack in a black-box setting, we demonstrate how attackers can extract private\ninformation by manipulating queries. Our experiments reveal that LMMs can both\ndirectly generate outputs resembling retrieved content and produce descriptions\nthat indirectly expose sensitive information, highlighting the urgent need for\nrobust privacy-preserving MRAG techniques.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 05:37:22", "updated": "2025-05-20 05:37:22", "pdf_url": "http://arxiv.org/pdf/2505.13957v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13963v1", "title": "Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability", "authors": ["Qianli Wang", "Mingyang Wang", "Nils Feldhus", "Simon Ostermann", "Yuan Cao", "Hinrich Sch\u00fctze", "Sebastian M\u00f6ller", "Vera Schmitt"], "abstract": "Quantization methods are widely used to accelerate inference and streamline\nthe deployment of large language models (LLMs). While prior research has\nextensively investigated the degradation of various LLM capabilities due to\nquantization, its effects on model explainability and interpretability, which\nare crucial for understanding decision-making processes, remain unexplored. To\naddress this gap, we conduct comprehensive experiments using three common\nquantization techniques at distinct bit widths, in conjunction with two\nexplainability methods, counterfactual examples and natural language\nexplanations, as well as two interpretability approaches, knowledge\nmemorization analysis and latent multi-hop reasoning analysis. We complement\nour analysis with a thorough user study, evaluating selected explainability\nmethods. Our findings reveal that, depending on the configuration, quantization\ncan significantly impact model explainability and interpretability. Notably,\nthe direction of this effect is not consistent, as it strongly depends on (1)\nthe quantization method, (2) the explainability or interpretability approach,\nand (3) the evaluation protocol. In some settings, human evaluation shows that\nquantization degrades explainability, while in others, it even leads to\nimprovements. Our work serves as a cautionary tale, demonstrating that\nquantization can unpredictably affect model transparency. This insight has\nimportant implications for deploying LLMs in applications where transparency is\na critical requirement.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 06:01:09", "updated": "2025-05-20 06:01:09", "pdf_url": "http://arxiv.org/pdf/2505.13963v1", "comment": "In submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13965v1", "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 06:05:56", "updated": "2025-05-20 06:05:56", "pdf_url": "http://arxiv.org/pdf/2505.13965v1", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13972v1", "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals", "authors": ["Qianli Wang", "Van Bach Nguyen", "Nils Feldhus", "Luis Felipe Villa-Arenas", "Christin Seifert", "Sebastian M\u00f6ller", "Vera Schmitt"], "abstract": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models. Through extensive experiments involving two state-of-the-art\nLLM-based methods, three datasets, five generator models, and 15 judge models,\ncomplemented by a user study (n = 90), we demonstrate that judge models with an\nindependent, non-fine-tuned relationship to the generator model provide the\nmost reliable label flipping evaluations. Relationships between the generator\nand judge models, which are closely aligned with the user study for CDA, result\nin better model performance and robustness. Nevertheless, we find that the gap\nbetween the most effective judge models and the results obtained from the user\nstudy remains considerably large. This suggests that a fully automated pipeline\nfor CDA may be inadequate and requires human intervention.", "categories": ["cs.CL"], "published": "2025-05-20 06:12:17", "updated": "2025-05-20 06:12:17", "pdf_url": "http://arxiv.org/pdf/2505.13972v1", "comment": "in submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13973v1", "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "abstract": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 06:12:20", "updated": "2025-05-20 06:12:20", "pdf_url": "http://arxiv.org/pdf/2505.13973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13975v1", "title": "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models", "authors": ["Yuxuan Jiang", "Dawei Li", "Frank Ferraro"], "abstract": "While Large Reasoning Models (LRMs) have demonstrated success in complex\nreasoning tasks through long chain-of-thought (CoT) reasoning, their inference\noften involves excessively verbose reasoning traces, resulting in substantial\ninefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a\nhybrid framework that combines inference-time pruning with tuning-based\ndistillation, two widely used strategies for efficient reasoning. DRP uses a\nteacher model to perform skill-aware step decomposition and content pruning,\nand then distills the pruned reasoning paths into a student model, enabling it\nto reason both efficiently and accurately. Across several challenging\nmathematical reasoning datasets, we find that models trained with DRP achieve\nsubstantial improvements in token efficiency without sacrificing accuracy.\nSpecifically, DRP reduces average token usage on GSM8K from 917 to 328 while\nimproving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on\nAIME with no performance drop. Further analysis shows that aligning the\nreasoning structure of training CoTs with the student's reasoning capacity is\ncritical for effective knowledge transfer and performance gains.", "categories": ["cs.CL"], "published": "2025-05-20 06:15:15", "updated": "2025-05-20 06:15:15", "pdf_url": "http://arxiv.org/pdf/2505.13975v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13979v1", "title": "Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection", "authors": ["Maya Srikanth", "Run Chen", "Julia Hirschberg"], "abstract": "Multimodal models play a key role in empathy detection, but their performance\ncan suffer when modalities provide conflicting cues. To understand these\nfailures, we examine cases where unimodal and multimodal predictions diverge.\nUsing fine-tuned models for text, audio, and video, along with a gated fusion\nmodel, we find that such disagreements often reflect underlying ambiguity, as\nevidenced by annotator uncertainty. Our analysis shows that dominant signals in\none modality can mislead fusion when unsupported by others. We also observe\nthat humans, like models, do not consistently benefit from multimodal input.\nThese insights position disagreement as a useful diagnostic signal for\nidentifying challenging examples and improving empathy system robustness.", "categories": ["cs.CL"], "published": "2025-05-20 06:25:02", "updated": "2025-05-20 06:25:02", "pdf_url": "http://arxiv.org/pdf/2505.13979v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13988v1", "title": "The Hallucination Tax of Reinforcement Finetuning", "authors": ["Linxin Song", "Taiwei Shi", "Jieyu Zhao"], "abstract": "Reinforcement finetuning (RFT) has become a standard approach for enhancing\nthe reasoning capabilities of large language models (LLMs). However, its impact\non model trustworthiness remains underexplored. In this work, we identify and\nsystematically study a critical side effect of RFT, which we term the\nhallucination tax: a degradation in refusal behavior causing models to produce\nhallucinated answers to unanswerable questions confidently. To investigate\nthis, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of\nunanswerable math problems designed to probe models' ability to recognize an\nunanswerable question by reasoning from the insufficient or ambiguous\ninformation. Our results show that standard RFT training could reduce model\nrefusal rates by more than 80%, which significantly increases model's tendency\nto hallucinate. We further demonstrate that incorporating just 10% SUM during\nRFT substantially restores appropriate refusal behavior, with minimal accuracy\ntrade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage\ninference-time compute to reason about their own uncertainty and knowledge\nboundaries, improving generalization not only to out-of-domain math problems\nbut also to factual question answering tasks.", "categories": ["cs.CL"], "published": "2025-05-20 06:36:45", "updated": "2025-05-20 06:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13988v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13990v1", "title": "DecIF: Improving Instruction-Following through Meta-Decomposition", "authors": ["Tingfeng Hui", "Pengyu Zhu", "Bowen Ping", "Ling Tang", "Yaqi Zhang", "Sen Su"], "abstract": "Instruction-following has emerged as a crucial capability for large language\nmodels (LLMs). However, existing approaches often rely on pre-existing\ndocuments or external resources to synthesize instruction-following data, which\nlimits their flexibility and generalizability. In this paper, we introduce\nDecIF, a fully autonomous, meta-decomposition guided framework that generates\ndiverse and high-quality instruction-following data using only LLMs. DecIF is\ngrounded in the principle of decomposition. For instruction generation, we\nguide LLMs to iteratively produce various types of meta-information, which are\nthen combined with response constraints to form well-structured and\nsemantically rich instructions. We further utilize LLMs to detect and resolve\npotential inconsistencies within the generated instructions. Regarding response\ngeneration, we decompose each instruction into atomic-level evaluation\ncriteria, enabling rigorous validation and the elimination of inaccurate\ninstruction-response pairs. Extensive experiments across a wide range of\nscenarios and settings demonstrate DecIF's superior performance on\ninstruction-following tasks. Further analysis highlights its strong\nflexibility, scalability, and generalizability in automatically synthesizing\nhigh-quality instruction data.", "categories": ["cs.CL"], "published": "2025-05-20 06:38:28", "updated": "2025-05-20 06:38:28", "pdf_url": "http://arxiv.org/pdf/2505.13990v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13995v1", "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "abstract": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-20 06:45:17", "updated": "2025-05-20 06:45:17", "pdf_url": "http://arxiv.org/pdf/2505.13995v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14009v1", "title": "Activation-Guided Consensus Merging for Large Language Models", "authors": ["Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Qintong Li", "Mingyang Liu", "Xiongwei Han", "Zhijiang Guo", "Han Wu", "Linqi Song"], "abstract": "Recent research has increasingly focused on reconciling the reasoning\ncapabilities of System 2 with the efficiency of System 1. While existing\ntraining-based and prompt-based approaches face significant challenges in terms\nof efficiency and stability, model merging emerges as a promising strategy to\nintegrate the diverse capabilities of different Large Language Models (LLMs)\ninto a unified model. However, conventional model merging methods often assume\nuniform importance across layers, overlooking the functional heterogeneity\ninherent in neural components. To address this limitation, we propose\n\\textbf{A}ctivation-Guided \\textbf{C}onsensus \\textbf{M}erging (\\textbf{ACM}),\na plug-and-play merging framework that determines layer-specific merging\ncoefficients based on mutual information between activations of pre-trained and\nfine-tuned models. ACM effectively preserves task-specific capabilities without\nrequiring gradient computations or additional training. Extensive experiments\non Long-to-Short (L2S) and general merging tasks demonstrate that ACM\nconsistently outperforms all baseline methods. For instance, in the case of\nQwen-7B models, TIES-Merging equipped with ACM achieves a \\textbf{55.3\\%}\nreduction in response length while simultaneously improving reasoning accuracy\nby \\textbf{1.3} points. We submit the code with the paper for reproducibility,\nand it will be publicly available.", "categories": ["cs.CL"], "published": "2025-05-20 07:04:01", "updated": "2025-05-20 07:04:01", "pdf_url": "http://arxiv.org/pdf/2505.14009v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14015v1", "title": "AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation", "authors": ["Tai D. Nguyen", "Long H. Pham", "Jun Sun"], "abstract": "The rapid advancement of domain-specific large language models (LLMs) in\nfields like law necessitates frameworks that account for nuanced regional legal\ndistinctions, which are critical for ensuring compliance and trustworthiness.\nExisting legal evaluation benchmarks often lack adaptability and fail to\naddress diverse local contexts, limiting their utility in dynamically evolving\nregulatory landscapes. To address these gaps, we propose AutoLaw, a novel\nviolation detection framework that combines adversarial data generation with a\njury-inspired deliberation process to enhance legal compliance of LLMs. Unlike\nstatic approaches, AutoLaw dynamically synthesizes case law to reflect local\nregulations and employs a pool of LLM-based \"jurors\" to simulate judicial\ndecision-making. Jurors are ranked and selected based on synthesized legal\nexpertise, enabling a deliberation process that minimizes bias and improves\ndetection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG\n(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:\nadversarial data generation improves LLM discrimination, while the jury-based\nvoting strategy significantly boosts violation detection rates. Our results\nhighlight the framework's ability to adaptively probe legal misalignments and\ndeliver reliable, context-aware judgments, offering a scalable solution for\nevaluating and enhancing LLMs in legally sensitive applications.", "categories": ["cs.CL"], "published": "2025-05-20 07:09:13", "updated": "2025-05-20 07:09:13", "pdf_url": "http://arxiv.org/pdf/2505.14015v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14038v1", "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data", "authors": ["Xinzhe Zheng", "Sijie Ji", "Jiawei Sun", "Renqi Chen", "Wei Gao", "Mani Srivastava"], "abstract": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 07:36:28", "updated": "2025-05-20 07:36:28", "pdf_url": "http://arxiv.org/pdf/2505.14038v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14045v1", "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 07:43:45", "updated": "2025-05-20 07:43:45", "pdf_url": "http://arxiv.org/pdf/2505.14045v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14052v1", "title": "Improved Methods for Model Pruning and Knowledge Distillation", "authors": ["Wei Jiang", "Anying Fu", "Youling Zhang"], "abstract": "Model pruning is a performance optimization technique for large language\nmodels like R1 or o3-mini. However, existing pruning methods often lead to\nsignificant performance degradation or require extensive retraining and\nfine-tuning. This technique aims to identify and remove neurons, connections\nunlikely leading to the contribution during the human-computer interaction\nphase. Our goal is to obtain a much smaller and faster knowledge distilled\nmodel that can quickly generate content almost as good as those of the unpruned\nones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an\nimproved pruning method that effectively reduces model size and computational\ncomplexity while maintaining performance comparable to the original unpruned\nmodel even at extreme pruned levels. The improved method is based on weights,\nbias fixed in the pre-training phase and GRPO rewards verified during the\npost-training phase as our novel pruning indicators. Preliminary experimental\nresults show that our method outperforms and be comparable to state-of-the-art\nmethods across various pruning levels and different downstream computational\nlinguistics tasks.", "categories": ["cs.CL", "cs.CE"], "published": "2025-05-20 07:53:40", "updated": "2025-05-20 07:53:40", "pdf_url": "http://arxiv.org/pdf/2505.14052v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14070v1", "title": "Enhancing LLMs via High-Knowledge Data Selection", "authors": ["Feiyu Duan", "Xuemiao Zhang", "Sirui Wang", "Haoran Que", "Yuqi Liu", "Wenge Rong", "Xunliang Cai"], "abstract": "The performance of Large Language Models (LLMs) is intrinsically linked to\nthe quality of its training data. Although several studies have proposed\nmethods for high-quality data selection, they do not consider the importance of\nknowledge richness in text corpora. In this paper, we propose a novel and\ngradient-free High-Knowledge Scorer (HKS) to select high-quality data from the\ndimension of knowledge, to alleviate the problem of knowledge scarcity in the\npre-trained corpus. We propose a comprehensive multi-domain knowledge element\npool and introduce knowledge density and coverage as metrics to assess the\nknowledge content of the text. Based on this, we propose a comprehensive\nknowledge scorer to select data with intensive knowledge, which can also be\nutilized for domain-specific high-knowledge data selection by restricting\nknowledge elements to the specific domain. We train models on a high-knowledge\nbilingual dataset, and experimental results demonstrate that our scorer\nimproves the model's performance in knowledge-intensive and general\ncomprehension tasks, and is effective in enhancing both the generic and\ndomain-specific capabilities of the model.", "categories": ["cs.CL"], "published": "2025-05-20 08:21:37", "updated": "2025-05-20 08:21:37", "pdf_url": "http://arxiv.org/pdf/2505.14070v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14071v1", "title": "Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models", "authors": ["Woody Haosheng Gan", "Deqing Fu", "Julian Asilis", "Ollie Liu", "Dani Yogatama", "Vatsal Sharan", "Robin Jia", "Willie Neiswanger"], "abstract": "Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-20 08:23:08", "updated": "2025-05-20 08:23:08", "pdf_url": "http://arxiv.org/pdf/2505.14071v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14079v1", "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "abstract": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules.", "categories": ["cs.CL"], "published": "2025-05-20 08:35:35", "updated": "2025-05-20 08:35:35", "pdf_url": "http://arxiv.org/pdf/2505.14079v1", "comment": null, "doi": null, "journal_ref": "ACL 2025"}
{"arxiv_id": "2505.14080v1", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "abstract": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "published": "2025-05-20 08:36:47", "updated": "2025-05-20 08:36:47", "pdf_url": "http://arxiv.org/pdf/2505.14080v1", "comment": null, "doi": "10.1145/3715275.3732112", "journal_ref": "FAccT '25: Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency"}
{"arxiv_id": "2505.14099v1", "title": "Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering", "authors": ["Yihua Zhu", "Qianying Liu", "Akiko Aizawa", "Hidetoshi Shimodaira"], "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions using structured knowledge from KBs. While LLM-only approaches offer\ngeneralization, they suffer from outdated knowledge, hallucinations, and lack\nof transparency. Chain-based KG-RAG methods address these issues by\nincorporating external KBs, but are limited to simple chain-structured\nquestions due to the absence of planning and logical structuring. Inspired by\nsemantic parsing methods, we propose PDRR: a four-stage framework consisting of\nPredict, Decompose, Retrieve, and Reason. Our method first predicts the\nquestion type and decomposes the question into structured triples. Then\nretrieves relevant information from KBs and guides the LLM as an agent to\nreason over and complete the decomposed triples. Experimental results\ndemonstrate that PDRR consistently outperforms existing methods across various\nLLM backbones and achieves superior performance on both chain-structured and\nnon-chain complex questions.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-20 09:01:52", "updated": "2025-05-20 09:01:52", "pdf_url": "http://arxiv.org/pdf/2505.14099v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14101v1", "title": "MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations", "authors": ["Ernests Lavrinovics", "Russa Biswas", "Katja Hose", "Johannes Bjerva"], "abstract": "Large Language Models (LLMs) have inherent limitations of faithfulness and\nfactuality, commonly referred to as hallucinations. Several benchmarks have\nbeen developed that provide a test bed for factuality evaluation within the\ncontext of English-centric datasets, while relying on supplementary informative\ncontext like web links or text passages but ignoring the available structured\nfactual resources. To this end, Knowledge Graphs (KGs) have been identified as\na useful aid for hallucination mitigation, as they provide a structured way to\nrepresent the facts about entities and their relations with minimal linguistic\noverhead. We bridge the lack of KG paths and multilinguality for factual\nlanguage modeling within the existing hallucination evaluation benchmarks and\npropose a KG-based multilingual, multihop benchmark called \\textbf{MultiHal}\nframed for generative text evaluation. As part of our data collection pipeline,\nwe mined 140k KG-paths from open-domain KGs, from which we pruned noisy\nKG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation\nshows an absolute scale increase by approximately 0.12 to 0.36 points for the\nsemantic similarity score in KG-RAG over vanilla QA across multiple languages\nand multiple models, demonstrating the potential of KG integration. We\nanticipate MultiHal will foster future research towards several graph-based\nhallucination mitigation and fact-checking tasks.", "categories": ["cs.CL"], "published": "2025-05-20 09:03:35", "updated": "2025-05-20 09:03:35", "pdf_url": "http://arxiv.org/pdf/2505.14101v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14104v1", "title": "Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents", "authors": ["Wei Fan", "Tianshi Zheng", "Yiran Hu", "Zheye Deng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Haoran Li", "Weixing Shen", "Yangqiu Song"], "abstract": "Legal rules encompass not only codified statutes but also implicit\nadjudicatory principles derived from precedents that contain discretionary\nnorms, social morality, and policy. While computational legal research has\nadvanced in applying established rules to cases, inducing legal rules from\njudicial decisions remains understudied, constrained by limitations in model\ninference efficacy and symbolic reasoning capability. The advent of Large\nLanguage Models (LLMs) offers unprecedented opportunities for automating the\nextraction of such latent principles, yet progress is stymied by the absence of\nformal task definitions, benchmark datasets, and methodologies. To address this\ngap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,\ngeneralizable doctrinal rules from sets of analogous precedents, distilling\ntheir shared preconditions, normative behaviors, and legal consequences. We\nintroduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese\ncases in total) for model tuning and 216 expert-annotated gold test sets.\nExperimental results reveal that: 1) State-of-the-art LLMs struggle with\nover-generalization and hallucination; 2) Training on our dataset markedly\nenhances LLMs capabilities in capturing nuanced rule patterns across similar\ncases.", "categories": ["cs.CL"], "published": "2025-05-20 09:10:52", "updated": "2025-05-20 09:10:52", "pdf_url": "http://arxiv.org/pdf/2505.14104v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14106v1", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "abstract": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:13:22", "updated": "2025-05-20 09:13:22", "pdf_url": "http://arxiv.org/pdf/2505.14106v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14107v1", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "abstract": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:14:53", "updated": "2025-05-20 09:14:53", "pdf_url": "http://arxiv.org/pdf/2505.14107v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14112v1", "title": "Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking", "authors": ["Tianle Gu", "Zongqi Wang", "Kexin Huang", "Yuanqi Yao", "Xiangliang Zhang", "Yujiu Yang", "Xiuying Chen"], "abstract": "Logit-based LLM watermarking traces and verifies AI-generated content by\nmaintaining green and red token lists and increasing the likelihood of green\ntokens during generation. However, it fails in low-entropy scenarios, where\npredictable outputs make green token selection difficult without disrupting\nnatural text flow. Existing approaches address this by assuming access to the\noriginal LLM to calculate entropy and selectively watermark high-entropy\ntokens. However, these methods face two major challenges: (1) high\ncomputational costs and detection delays due to reliance on the original LLM,\nand (2) potential risks of model leakage. To address these limitations, we\npropose Invisible Entropy (IE), a watermarking paradigm designed to enhance\nboth safety and efficiency. Instead of relying on the original LLM, IE\nintroduces a lightweight feature extractor and an entropy tagger to predict\nwhether the entropy of the next token is high or low. Furthermore, based on\ntheoretical analysis, we develop a threshold navigator that adaptively sets\nentropy thresholds. It identifies a threshold where the watermark ratio\ndecreases as the green token count increases, enhancing the naturalness of the\nwatermarked text and improving detection robustness. Experiments on HumanEval\nand MBPP datasets demonstrate that IE reduces parameter size by 99\\% while\nachieving performance on par with state-of-the-art methods. Our work introduces\na safe and efficient paradigm for low-entropy watermarking.\nhttps://github.com/Carol-gutianle/IE\nhttps://huggingface.co/datasets/Carol0110/IE-Tagger", "categories": ["cs.CL", "cs.CR"], "published": "2025-05-20 09:19:06", "updated": "2025-05-20 09:19:06", "pdf_url": "http://arxiv.org/pdf/2505.14112v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14116v1", "title": "Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst", "authors": ["Hongru Wang", "Deng Cai", "Wanjun Zhong", "Shijue Huang", "Jeff Z. Pan", "Zeming Liu", "Kam-Fai Wong"], "abstract": "Inference-time scaling has attracted much attention which significantly\nenhance the performance of Large Language Models (LLMs) in complex reasoning\ntasks by increasing the length of Chain-of-Thought. These longer intermediate\nreasoning rationales embody various meta-reasoning skills in human cognition,\nsuch as reflection and decomposition, being difficult to create and acquire. In\nthis work, we introduce \\textit{Self-Reasoning Language Model} (SRLM), where\nthe model itself can synthesize longer CoT data and iteratively improve\nperformance through self-training. By incorporating a few demonstration\nexamples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from\nexisting responses, which act as a reasoning catalyst, we demonstrate that SRLM\nnot only enhances the model's initial performance but also ensures more stable\nand consistent improvements in subsequent iterations. Our proposed SRLM\nachieves an average absolute improvement of more than $+2.5$ points across five\nreasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.\nMoreover, it brings more improvements with more times of sampling during\ninference, such as absolute $+7.89$ average improvement with $64$ sampling\ntimes, revealing the in-depth, diverse and creative reasoning paths in SRLM\nagainst the strong baseline.", "categories": ["cs.CL"], "published": "2025-05-20 09:21:26", "updated": "2025-05-20 09:21:26", "pdf_url": "http://arxiv.org/pdf/2505.14116v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14130v1", "title": "Probing BERT for German Compound Semantics", "authors": ["Filip Mileti\u0107", "Aaron Schmid", "Sabine Schulte im Walde"], "abstract": "This paper investigates the extent to which pretrained German BERT encodes\nknowledge of noun compound semantics. We comprehensively vary combinations of\ntarget tokens, layers, and cased vs. uncased models, and evaluate them by\npredicting the compositionality of 868 gold standard compounds. Looking at\nrepresentational patterns within the transformer architecture, we observe\ntrends comparable to equivalent prior work on English, with compositionality\ninformation most easily recoverable in the early layers. However, our strongest\nresults clearly lag behind those reported for English, suggesting an inherently\nmore difficult task in German. This may be due to the higher productivity of\ncompounding in German than in English and the associated increase in\nconstituent-level ambiguity, including in our target compound set.", "categories": ["cs.CL"], "published": "2025-05-20 09:36:17", "updated": "2025-05-20 09:36:17", "pdf_url": "http://arxiv.org/pdf/2505.14130v1", "comment": "Accepted to SwissText 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14131v1", "title": "Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "abstract": "In table question answering (TQA), tables are encoded as either texts or\nimages. Prior work suggests that passing images of tables to multi-modal large\nlanguage models (MLLMs) performs comparably to or even better than using\ntextual input with large language models (LLMs). However, the lack of\ncontrolled setups limits fine-grained distinctions between these approaches. In\nthis paper, we conduct the first controlled study on the effectiveness of\nseveral combinations of table representations and models from two perspectives:\nquestion complexity and table size. We build a new benchmark based on existing\nTQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we\nfind that the best combination of table representation and model varies across\nsetups. We propose FRES, a method selecting table representations dynamically,\nand observe a 10% average performance improvement compared to using both\nrepresentations indiscriminately.", "categories": ["cs.CL"], "published": "2025-05-20 09:36:17", "updated": "2025-05-20 09:36:17", "pdf_url": "http://arxiv.org/pdf/2505.14131v1", "comment": "Accepted at ACL25 (Findings)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14146v1", "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "abstract": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 09:53:56", "updated": "2025-05-20 09:53:56", "pdf_url": "http://arxiv.org/pdf/2505.14146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14149v1", "title": "Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information", "authors": ["Chengzhi Zhang", "Xinyi Yan", "Lei Zhao", "Yingyi Zhang"], "abstract": "The exponential increase in academic papers has significantly increased the\ntime required for researchers to access relevant literature. Keyphrase\nExtraction (KPE) offers a solution to this situation by enabling researchers to\nefficiently retrieve relevant literature. The current study on KPE from\nacademic articles aims to improve the performance of extraction models through\ninnovative approaches using Title and Abstract as input corpora. However, the\nsemantic richness of keywords is significantly constrained by the length of the\nabstract. While full-text-based KPE can address this issue, it simultaneously\nintroduces noise, which significantly diminishes KPE performance. To address\nthis issue, this paper utilized the structural features and section texts\nobtained from the section structure information of academic articles to extract\nkeyphrase from academic papers. The approach consists of two main parts: (1)\nexploring the effect of seven structural features on KPE models, and (2)\nintegrating the extraction results from all section texts used as input corpora\nfor KPE models via a keyphrase integration algorithm to obtain the keyphrase\nintegration result. Furthermore, this paper also examined the effect of the\nclassification quality of section structure on the KPE performance. The results\nshow that incorporating structural features improves KPE performance, though\ndifferent features have varying effects on model efficacy. The keyphrase\nintegration approach yields the best performance, and the classification\nquality of section structure can affect KPE performance. These findings\nindicate that using the section structure information of academic articles\ncontributes to effective KPE from academic articles. The code and dataset\nsupporting this study are available at https://github.com/yan-xinyi/SSB_KPE.", "categories": ["cs.CL", "cs.DL", "cs.IR"], "published": "2025-05-20 09:57:34", "updated": "2025-05-20 09:57:34", "pdf_url": "http://arxiv.org/pdf/2505.14149v1", "comment": null, "doi": "10.1007/s11192-025-05286-2", "journal_ref": "Scientometrics, 2025"}
{"arxiv_id": "2505.14157v1", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "abstract": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:05:11", "updated": "2025-05-20 10:05:11", "pdf_url": "http://arxiv.org/pdf/2505.14157v1", "comment": "25 pages, 42 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14158v1", "title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering", "authors": ["Sanjay Govindan", "Maurice Pagnucco", "Yang Song"], "abstract": "Large Language Models (LLMs) are trained on diverse and often conflicting\nknowledge spanning multiple domains and time periods. Some of this knowledge is\nonly valid within specific temporal contexts, such as answering the question,\n\"Who is the President of the United States in 2022?\" Ensuring LLMs generate\ntime appropriate responses is crucial for maintaining relevance and accuracy.\nIn this work we explore activation engineering as a method for temporally\naligning LLMs to improve factual recall without any training or dataset\ncreation. In this research we explore an activation engineering technique to\nground three versions of LLaMA 2 to specific points in time and examine the\neffects of varying injection layers and prompting strategies. Our experiments\ndemonstrate up to a 44% and 16% improvement in relative and explicit prompting\nrespectively, achieving comparable performance to the fine-tuning method\nproposed by Zhao et al. (2024) . Notably, our approach achieves similar results\nto the fine-tuning baseline while being significantly more computationally\nefficient and requiring no pre-aligned datasets.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:09:40", "updated": "2025-05-20 10:09:40", "pdf_url": "http://arxiv.org/pdf/2505.14158v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14160v1", "title": "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models", "authors": ["Zahraa Al Sahili", "Ioannis Patras", "Matthew Purver"], "abstract": "Multilingual vision-language models promise universal image-text retrieval,\nyet their social biases remain under-explored. We present the first systematic\naudit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and\nCAPIVARA-CLIP -- across ten languages that vary in resource availability and\ngrammatical gender. Using balanced subsets of \\textsc{FairFace} and the\n\\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and\ngender bias and measure stereotype amplification. Contrary to the assumption\nthat multilinguality mitigates bias, every model exhibits stronger gender bias\nthan its English-only baseline. CAPIVARA-CLIP shows its largest biases\nprecisely in the low-resource languages it targets, while the shared\ncross-lingual encoder of NLLB-CLIP transports English gender stereotypes into\ngender-neutral languages; loosely coupled encoders largely avoid this transfer.\nHighly gendered languages consistently magnify all measured bias types, but\neven gender-neutral languages remain vulnerable when cross-lingual weight\nsharing imports foreign stereotypes. Aggregated metrics conceal\nlanguage-specific ``hot spots,'' underscoring the need for fine-grained,\nlanguage-aware bias evaluation in future multilingual vision-language research.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:14:00", "updated": "2025-05-20 10:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14160v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14165v1", "title": "PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore", "authors": ["Zhenkai Qin", "Jiajing He", "Qiao Fang"], "abstract": "Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity\ntoward specific aspects within a text, enabling more precise opinion mining in\ndomains such as product reviews and social media. However, traditional FGSA\napproaches often require task-specific architectures and extensive annotated\ndata, limiting their generalization and scalability. To address these\nchallenges, we propose PL-FGSA, a unified prompt learning-based framework\nimplemented using the MindSpore platform, which integrates prompt design with a\nlightweight TextCNN backbone. Our method reformulates FGSA as a multi-task\nprompt-augmented generation problem, jointly tackling aspect extraction,\nsentiment classification, and causal explanation in a unified paradigm. By\nleveraging prompt-based guidance, PL-FGSA enhances interpretability and\nachieves strong performance under both full-data and low-resource conditions.\nExperiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and\nMAMS-demonstrate that our model consistently outperforms traditional\nfine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,\nrespectively. These results validate the effectiveness of prompt-based\ngeneralization and highlight the practical value of PL-FGSA for real-world\nsentiment analysis tasks.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:18:10", "updated": "2025-05-20 10:18:10", "pdf_url": "http://arxiv.org/pdf/2505.14165v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14172v1", "title": "The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models", "authors": ["Adrian Cosma", "Stefan Ruseti", "Emilian Radoi", "Mihai Dascalu"], "abstract": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge slowly, suddenly, and only late in training. We\nfurther show that percolation-based models of concept emergence explain these\npatterns, suggesting that learning character composition is not fundamentally\ndifferent from learning commonsense knowledge. To address this bottleneck, we\npropose a lightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available.", "categories": ["cs.CL"], "published": "2025-05-20 10:25:17", "updated": "2025-05-20 10:25:17", "pdf_url": "http://arxiv.org/pdf/2505.14172v1", "comment": "1 Table, 8 Figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14173v1", "title": "THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jie Zhou"], "abstract": "The sparse Mixture-of-Experts (MoE) has achieved significant progress for\nneural machine translation (NMT). However, there exist two limitations in\ncurrent MoE solutions which may lead to sub-optimal performance: 1) they\ndirectly use the task knowledge of NMT into MoE (\\emph{e.g.},\ndomain/linguistics-specific knowledge), which are generally unavailable at\npractical application and neglect the naturally grouped domain/linguistic\nproperties; 2) the expert selection only depends on the localized token\nrepresentation without considering the context, which fully grasps the state of\neach token in a global view. To address the above limitations, we propose\nTHOR-MoE via arming the MoE with hierarchical task-guided and\ncontext-responsive routing policies. Specifically, it 1) firstly predicts the\ndomain/language label and then extracts mixed domain/language representation to\nallocate task-level experts in a hierarchical manner; 2) injects the context\ninformation to enhance the token routing from the pre-selected task-level\nexperts set, which can help each token to be accurately routed to more\nspecialized and suitable experts. Extensive experiments on multi-domain\ntranslation and multilingual translation benchmarks with different\narchitectures consistently demonstrate the superior performance of THOR-MoE.\nAdditionally, the THOR-MoE operates as a plug-and-play module compatible with\nexisting Top-$k$~\\cite{shazeer2017} and Top-$p$~\\cite{huang-etal-2024-harder}\nrouting schemes, ensuring broad applicability across diverse MoE architectures.\nFor instance, compared with vanilla Top-$p$~\\cite{huang-etal-2024-harder}\nrouting, the context-aware manner can achieve an average improvement of 0.75\nBLEU with less than 22\\% activated parameters on multi-domain translation\ntasks.", "categories": ["cs.CL"], "published": "2025-05-20 10:27:19", "updated": "2025-05-20 10:27:19", "pdf_url": "http://arxiv.org/pdf/2505.14173v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14174v1", "title": "Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning", "authors": ["Yusuf Denizay D\u00f6nder", "Derek Hommel", "Andrea W Wen-Yi", "David Mimno", "Unso Eun Seo Jo"], "abstract": "LLMs are effective at code generation tasks like text-to-SQL, but is it worth\nthe cost? Many state-of-the-art approaches use non-task-specific LLM techniques\nincluding Chain-of-Thought (CoT), self-consistency, and fine-tuning. These\nmethods can be costly at inference time, sometimes requiring over a hundred LLM\ncalls with reasoning, incurring average costs of up to \\$0.46 per query, while\nfine-tuning models can cost thousands of dollars. We introduce \"N-rep\"\nconsistency, a more cost-efficient text-to-SQL approach that achieves similar\nBIRD benchmark scores as other more expensive methods, at only \\$0.039 per\nquery. N-rep leverages multiple representations of the same schema input to\nmitigate weaknesses in any single representation, making the solution more\nrobust and allowing the use of smaller and cheaper models without any reasoning\nor fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL\napproach in its cost range.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:28:46", "updated": "2025-05-20 10:28:46", "pdf_url": "http://arxiv.org/pdf/2505.14174v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14178v1", "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "abstract": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:32:30", "updated": "2025-05-20 10:32:30", "pdf_url": "http://arxiv.org/pdf/2505.14178v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14179v1", "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "abstract": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-20 10:34:45", "updated": "2025-05-20 10:34:45", "pdf_url": "http://arxiv.org/pdf/2505.14179v1", "comment": null, "doi": "10.1016/j.eswa.2024.125529", "journal_ref": "Expert Systems with Applications, 2025"}
{"arxiv_id": "2505.14181v1", "title": "SlangDIT: Benchmarking LLMs in Interpretative Slang Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jiaan Wang", "Jie Zhou"], "abstract": "The challenge of slang translation lies in capturing context-dependent\nsemantic extensions, as slang terms often convey meanings beyond their literal\ninterpretation. While slang detection, explanation, and translation have been\nstudied as isolated tasks in the era of large language models (LLMs), their\nintrinsic interdependence remains underexplored. The main reason is lacking of\na benchmark where the two tasks can be a prerequisite for the third one, which\ncan facilitate idiomatic translation. In this paper, we introduce the\ninterpretative slang translation task (named SlangDIT) consisting of three\nsub-tasks: slang detection, cross-lingual slang explanation, and slang\ntranslation within the current context, aiming to generate more accurate\ntranslation with the help of slang detection and slang explanation. To this\nend, we construct a SlangDIT dataset, containing over 25k English-Chinese\nsentence pairs. Each source sentence mentions at least one slang term and is\nlabeled with corresponding cross-lingual slang explanation. Based on the\nbenchmark, we propose a deep thinking model, named SlangOWL. It firstly\nidentifies whether the sentence contains a slang, and then judges whether the\nslang is polysemous and analyze its possible meaning. Further, the SlangOWL\nprovides the best explanation of the slang term targeting on the current\ncontext. Finally, according to the whole thought, the SlangOWL offers a\nsuitable translation. Our experiments on LLMs (\\emph{e.g.}, Qwen2.5 and\nLLama-3.1), show that our deep thinking approach indeed enhances the\nperformance of LLMs where the proposed SLangOWL significantly surpasses the\nvanilla models and supervised fine-tuned models without thinking.", "categories": ["cs.CL"], "published": "2025-05-20 10:37:34", "updated": "2025-05-20 10:37:34", "pdf_url": "http://arxiv.org/pdf/2505.14181v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14183v1", "title": "ThinkSwitcher: When to Think Hard, When to Think Fast", "authors": ["Guosheng Liang", "Longguang Zhong", "Ziyi Yang", "Xiaojun Quan"], "abstract": "Large reasoning models (LRMs) excel at solving complex tasks by leveraging\nlong chain-of-thought (CoT) reasoning. However, this often leads to\noverthinking on simple tasks, resulting in unnecessary computational overhead.\nWe observe that LRMs inherently possess the capability for efficient short CoT\nreasoning, which can be reliably elicited through prompt design. To leverage\nthis capability, we propose ThinkSwitcher, a framework that enables a single\nLRM to dynamically switch between short and long CoT modes based on task\ncomplexity. ThinkSwitcher introduces a lightweight switching module trained\nwith supervision signals derived from the relative performance of each\nreasoning mode across tasks. Experiments on multiple reasoning benchmarks show\nthat ThinkSwitcher reduces computational cost by 20-30% while maintaining high\naccuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher\nas a scalable and efficient solution for unified LRM deployment.", "categories": ["cs.CL"], "published": "2025-05-20 10:40:41", "updated": "2025-05-20 10:40:41", "pdf_url": "http://arxiv.org/pdf/2505.14183v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14185v1", "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 10:41:49", "updated": "2025-05-20 10:41:49", "pdf_url": "http://arxiv.org/pdf/2505.14185v1", "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14195v1", "title": "Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification", "authors": ["Tuc Nguyen", "Yifan Hu", "Thai Le"], "abstract": "Recent advancements in large language models (LLMs) have been fueled by large\nscale training corpora drawn from diverse sources such as websites, news\narticles, and books. These datasets often contain explicit user information,\nsuch as person names and addresses, that LLMs may unintentionally reproduce in\ntheir generated outputs. Beyond such explicit content, LLMs can also leak\nidentity revealing cues through implicit signals such as distinctive writing\nstyles, raising significant concerns about authorship privacy. There are three\nmajor automated tasks in authorship privacy, namely authorship obfuscation\n(AO), authorship mimicking (AM), and authorship verification (AV). Prior\nresearch has studied AO, AM, and AV independently. However, their interplays\nremain under explored, which leaves a major research gap, especially in the era\nof LLMs, where they are profoundly shaping how we curate and share user\ngenerated content, and the distinction between machine generated and human\nauthored text is also increasingly blurred. This work then presents the first\nunified framework for analyzing the dynamic relationships among LLM enabled AO,\nAM, and AV in the context of authorship privacy. We quantify how they interact\nwith each other to transform human authored text, examining effects at a single\npoint in time and iteratively over time. We also examine the role of\ndemographic metadata, such as gender, academic background, in modulating their\nperformances, inter-task dynamics, and privacy risks. All source code will be\npublicly available.", "categories": ["cs.CL"], "published": "2025-05-20 10:52:12", "updated": "2025-05-20 10:52:12", "pdf_url": "http://arxiv.org/pdf/2505.14195v1", "comment": "17 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14212v1", "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "abstract": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:16:29", "updated": "2025-05-20 11:16:29", "pdf_url": "http://arxiv.org/pdf/2505.14212v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14216v1", "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "abstract": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 11:22:34", "updated": "2025-05-20 11:22:34", "pdf_url": "http://arxiv.org/pdf/2505.14216v1", "comment": "23 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14226v1", "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "abstract": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:35:25", "updated": "2025-05-20 11:35:25", "pdf_url": "http://arxiv.org/pdf/2505.14226v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14233v1", "title": "Mechanistic Fine-tuning for In-context Learning", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "abstract": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:41:21", "updated": "2025-05-20 11:41:21", "pdf_url": "http://arxiv.org/pdf/2505.14233v1", "comment": "28 pages, 31 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14238v1", "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "abstract": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:43:25", "updated": "2025-05-20 11:43:25", "pdf_url": "http://arxiv.org/pdf/2505.14238v1", "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14242v1", "title": "Technical Report on classification of literature related to children speech disorder", "authors": ["Ziang Wang", "Amir Aryani"], "abstract": "This technical report presents a natural language processing (NLP)-based\napproach for systematically classifying scientific literature on childhood\nspeech disorders. We retrieved and filtered 4,804 relevant articles published\nafter 2015 from the PubMed database using domain-specific keywords. After\ncleaning and pre-processing the abstracts, we applied two topic modeling\ntechniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify\nlatent thematic structures in the corpus. Our models uncovered 14 clinically\nmeaningful clusters, such as infantile hyperactivity and abnormal epileptic\nbehavior. To improve relevance and precision, we incorporated a custom stop\nword list tailored to speech pathology. Evaluation results showed that the LDA\nmodel achieved a coherence score of 0.42 and a perplexity of -7.5, indicating\nstrong topic coherence and predictive performance. The BERTopic model exhibited\na low proportion of outlier topics (less than 20%), demonstrating its capacity\nto classify heterogeneous literature effectively. These results provide a\nfoundation for automating literature reviews in speech-language pathology.", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-20 11:52:17", "updated": "2025-05-20 11:52:17", "pdf_url": "http://arxiv.org/pdf/2505.14242v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14244v1", "title": "TransBench: Benchmarking Machine Translation for Industrial-Scale Applications", "authors": ["Haijun Li", "Tianqi Shi", "Zifu Shang", "Yuxuan Han", "Xueyu Zhao", "Hao Wang", "Yu Qian", "Zhiqiang Qian", "Linlong Xu", "Minghao Wu", "Chenyang Lyu", "Longyue Wang", "Gongbo Tang", "Weihua Luo", "Zhao Xu", "Kaifu Zhang"], "abstract": "Machine translation (MT) has become indispensable for cross-border\ncommunication in globalized industries like e-commerce, finance, and legal\nservices, with recent advancements in large language models (LLMs)\nsignificantly enhancing translation quality. However, applying general-purpose\nMT models to industrial scenarios reveals critical limitations due to\ndomain-specific terminology, cultural nuances, and stylistic conventions absent\nin generic benchmarks. Existing evaluation frameworks inadequately assess\nperformance in specialized contexts, creating a gap between academic benchmarks\nand real-world efficacy. To address this, we propose a three-level translation\ncapability framework: (1) Basic Linguistic Competence, (2) Domain-Specific\nProficiency, and (3) Cultural Adaptation, emphasizing the need for holistic\nevaluation across these dimensions. We introduce TransBench, a benchmark\ntailored for industrial MT, initially targeting international e-commerce with\n17,000 professionally translated sentences spanning 4 main scenarios and 33\nlanguage pairs. TransBench integrates traditional metrics (BLEU, TER) with\nMarco-MOS, a domain-specific evaluation model, and provides guidelines for\nreproducible benchmark construction. Our contributions include: (1) a\nstructured framework for industrial MT evaluation, (2) the first publicly\navailable benchmark for e-commerce translation, (3) novel metrics probing\nmulti-level translation quality, and (4) open-sourced evaluation tools. This\nwork bridges the evaluation gap, enabling researchers and practitioners to\nsystematically assess and enhance MT systems for industry-specific needs.", "categories": ["cs.CL"], "published": "2025-05-20 11:54:58", "updated": "2025-05-20 11:54:58", "pdf_url": "http://arxiv.org/pdf/2505.14244v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14256v1", "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "abstract": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:09:17", "updated": "2025-05-20 12:09:17", "pdf_url": "http://arxiv.org/pdf/2505.14256v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14264v1", "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum", "authors": ["Jian Xiong", "Jingbo Zhou", "Jingyong Ye", "Dejing Dou"], "abstract": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 12:13:44", "updated": "2025-05-20 12:13:44", "pdf_url": "http://arxiv.org/pdf/2505.14264v1", "comment": "14 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14268v1", "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:19:10", "updated": "2025-05-20 12:19:10", "pdf_url": "http://arxiv.org/pdf/2505.14268v1", "comment": "16 pages, 14 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14271v1", "title": "FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning", "authors": ["Minh Ngoc Ta", "Dong Cao Van", "Duc-Anh Hoang", "Minh Le-Anh", "Truong Nguyen", "My Anh Tran Nguyen", "Yuxia Wang", "Preslav Nakov", "Sang Dinh"], "abstract": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nAI-generated, and human-AI collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, meanwhile identifying the underlying AI model family. Unlike\nexisting binary classifiers, FAID is built to capture both authorship and\nmodel-specific characteristics. Our method combines multi-level contrastive\nlearning with multi-task auxiliary classification to learn subtle stylistic\ncues. By modeling AI families as distinct stylistic entities, FAID offers\nimproved interpretability. We incorporate an adaptation to address\ndistributional shifts without retraining for unseen data. Experimental results\ndemonstrate that FAID outperforms several baseline approaches, particularly\nenhancing the generalization accuracy on unseen domains and new AI models. It\nprovide a potential solution for improving transparency and accountability in\nAI-assisted writing.", "categories": ["cs.CL"], "published": "2025-05-20 12:23:31", "updated": "2025-05-20 12:23:31", "pdf_url": "http://arxiv.org/pdf/2505.14271v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14272v1", "title": "Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "abstract": "Considering the importance of detecting hateful language, labeled hate speech\ndata is expensive and time-consuming to collect, particularly for low-resource\nlanguages. Prior work has demonstrated the effectiveness of cross-lingual\ntransfer learning and data augmentation in improving performance on tasks with\nlimited labeled data. To develop an efficient and scalable cross-lingual\ntransfer learning approach, we leverage nearest-neighbor retrieval to augment\nminimal labeled data in the target language, thereby enhancing detection\nperformance. Specifically, we assume access to a small set of labeled training\ninstances in the target language and use these to retrieve the most relevant\nlabeled examples from a large multilingual hate speech detection pool. We\nevaluate our approach on eight languages and demonstrate that it consistently\noutperforms models trained solely on the target language data. Furthermore, in\nmost cases, our method surpasses the current state-of-the-art. Notably, our\napproach is highly data-efficient, retrieving as small as 200 instances in some\ncases while maintaining superior performance. Moreover, it is scalable, as the\nretrieval pool can be easily expanded, and the method can be readily adapted to\nnew languages and tasks. We also apply maximum marginal relevance to mitigate\nredundancy and filter out highly similar retrieved instances, resulting in\nimprovements in some languages.", "categories": ["cs.CL", "cs.CY", "cs.MM"], "published": "2025-05-20 12:25:33", "updated": "2025-05-20 12:25:33", "pdf_url": "http://arxiv.org/pdf/2505.14272v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14279v1", "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin M\u00fcnch"], "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:30:46", "updated": "2025-05-20 12:30:46", "pdf_url": "http://arxiv.org/pdf/2505.14279v1", "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14286v1", "title": "Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs", "authors": ["Rao Ma", "Mengjie Qian", "Vyas Raina", "Mark Gales", "Kate Knill"], "abstract": "The combination of pre-trained speech encoders with large language models has\nenabled the development of speech LLMs that can handle a wide range of spoken\nlanguage processing tasks. While these models are powerful and flexible, this\nvery flexibility may make them more vulnerable to adversarial attacks. To\nexamine the extent of this problem, in this work we investigate universal\nacoustic adversarial attacks on speech LLMs. Here a fixed, universal,\nadversarial audio segment is prepended to the original input audio. We\ninitially investigate attacks that cause the model to either produce no output\nor to perform a modified task overriding the original prompt. We then extend\nthe nature of the attack to be selective so that it activates only when\nspecific input attributes, such as a speaker gender or spoken language, are\npresent. Inputs without the targeted attribute should be unaffected, allowing\nfine-grained control over the model outputs. Our findings reveal critical\nvulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar\nspeech LLMs may be susceptible to universal adversarial attacks. This\nhighlights the need for more robust training strategies and improved resistance\nto adversarial attacks.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-20 12:35:59", "updated": "2025-05-20 12:35:59", "pdf_url": "http://arxiv.org/pdf/2505.14286v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14297v1", "title": "Cross-Lingual Optimization for Language Transfer in Large Language Models", "authors": ["Jungseob Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "abstract": "Adapting large language models to other languages typically employs\nsupervised fine-tuning (SFT) as a standard approach. However, it often suffers\nfrom an overemphasis on English performance, a phenomenon that is especially\npronounced in data-constrained environments. To overcome these challenges, we\npropose \\textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an\nEnglish-centric LLM to a target language while preserving its English\ncapabilities. CLO utilizes publicly available English SFT data and a\ntranslation model to enable cross-lingual transfer. We conduct experiments\nusing five models on six languages, each possessing varying levels of resource.\nOur results show that CLO consistently outperforms SFT in both acquiring target\nlanguage proficiency and maintaining English performance. Remarkably, in\nlow-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400\nsamples, demonstrating that CLO can achieve better performance with less data.\nFurthermore, we find that SFT is particularly sensitive to data quantity in\nmedium and low-resource languages, whereas CLO remains robust. Our\ncomprehensive analysis emphasizes the limitations of SFT and incorporates\nadditional training strategies in CLO to enhance efficiency.", "categories": ["cs.CL"], "published": "2025-05-20 12:45:09", "updated": "2025-05-20 12:45:09", "pdf_url": "http://arxiv.org/pdf/2505.14297v1", "comment": "Accepted for publication at ACL 2025. Jungseob Lee and Seongtae Hong\n  contributed equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14300v1", "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors", "authors": ["Maheep Chaudhary", "Fazl Barez"], "abstract": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 12:49:58", "updated": "2025-05-20 12:49:58", "pdf_url": "http://arxiv.org/pdf/2505.14300v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14302v1", "title": "Scaling Law for Quantization-Aware Training", "authors": ["Mengzhao Chen", "Chaoyi Zhang", "Jing Liu", "Yutao Zeng", "Zeyue Xue", "Zhiheng Liu", "Yunshui Li", "Jin Ma", "Jie Huang", "Xun Zhou", "Ping Luo"], "abstract": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 12:54:43", "updated": "2025-05-20 12:54:43", "pdf_url": "http://arxiv.org/pdf/2505.14302v1", "comment": "A unified scaling law for QAT that models quantization error as a\n  function of model size, training data volume, and quantization group size", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14305v1", "title": "JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling", "authors": ["Jinwang Song", "Hongying Zan", "Kunli Zhang", "Lingling Mu", "Yingjie Han", "Haobo Hua", "Min Peng"], "abstract": "Text-to-SQL, which maps natural language to SQL queries, has benefited\ngreatly from recent advances in Large Language Models (LLMs). While LLMs offer\nvarious paradigms for this task, including prompting and supervised fine-tuning\n(SFT), SFT approaches still face challenges such as complex multi-stage\npipelines and poor robustness to noisy schema information. To address these\nlimitations, we present JOLT-SQL, a streamlined single-stage SFT framework that\njointly optimizes schema linking and SQL generation via a unified loss.\nJOLT-SQL employs discriminative schema linking, enhanced by local bidirectional\nattention, alongside a confusion-aware noisy schema sampling strategy with\nselective attention to improve robustness under noisy schema conditions.\nExperiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL\nachieves state-of-the-art execution accuracy among comparable-size open-source\nmodels, while significantly improving both training and inference efficiency.", "categories": ["cs.CL"], "published": "2025-05-20 12:55:10", "updated": "2025-05-20 12:55:10", "pdf_url": "http://arxiv.org/pdf/2505.14305v1", "comment": "Work in progress. 13 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14309v1", "title": "Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency", "authors": ["Ehsan Doostmohammadi", "Marco Kuhlmann"], "abstract": "Retrieval-augmented language models have demonstrated performance comparable\nto much larger models while requiring fewer computational resources. The\neffectiveness of these models crucially depends on the overlap between query\nand retrieved context, but the optimal degree of this overlap remains\nunexplored. In this paper, we systematically investigate how varying levels of\nquery--context overlap affect model performance during both training and\ninference. Our experiments reveal that increased overlap initially has minimal\neffect, but substantially improves test-time perplexity and accelerates model\nlearning above a critical threshold. Building on these findings, we demonstrate\nthat deliberately increasing overlap through synthetic context can enhance data\nefficiency and reduce training time by approximately 40\\% without compromising\nperformance. We specifically generate synthetic context through paraphrasing\nqueries. We validate our perplexity-based findings on question-answering tasks,\nconfirming that the benefits of retrieval-augmented language modeling extend to\npractical applications. Our results provide empirical evidence of significant\noptimization potential for retrieval mechanisms in language model pretraining.", "categories": ["cs.CL"], "published": "2025-05-20 12:58:07", "updated": "2025-05-20 12:58:07", "pdf_url": "http://arxiv.org/pdf/2505.14309v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14311v1", "title": "HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing", "authors": ["Shamsuddeen Hassan Muhammad", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Falalu Ibrahim Lawan", "Babangida Sani", "Sukairaj Hafiz Imam", "Yusuf Aliyu", "Sani Abdullahi Sani", "Ali Usman Umar", "Kenneth Church", "Vukosi Marivate"], "abstract": "Hausa Natural Language Processing (NLP) has gained increasing attention in\nrecent years, yet remains understudied as a low-resource language despite\nhaving over 120 million first-language (L1) and 80 million second-language (L2)\nspeakers worldwide. While significant advances have been made in high-resource\nlanguages, Hausa NLP faces persistent challenges, including limited open-source\ndatasets and inadequate model representation. This paper presents an overview\nof the current state of Hausa NLP, systematically examining existing resources,\nresearch contributions, and gaps across fundamental NLP tasks: text\nclassification, machine translation, named entity recognition, speech\nrecognition, and question answering. We introduce HausaNLP\n(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\ntools, and research works to enhance accessibility and drive further\ndevelopment. Furthermore, we discuss challenges in integrating Hausa into large\nlanguage models (LLMs), addressing issues of suboptimal tokenization and\ndialectal variation. Finally, we propose strategic research directions\nemphasizing dataset expansion, improved language modeling approaches, and\nstrengthened community collaboration to advance Hausa NLP. Our work provides\nboth a foundation for accelerating Hausa NLP progress and valuable insights for\nbroader multilingual NLP research.", "categories": ["cs.CL"], "published": "2025-05-20 12:59:55", "updated": "2025-05-20 12:59:55", "pdf_url": "http://arxiv.org/pdf/2505.14311v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14313v1", "title": "A MIND for Reasoning: Meta-learning for In-context Deduction", "authors": ["Leonardo Bertolazzi", "Manuel Vargas Guzm\u00e1n", "Raffaella Bernardi", "Maciej Malicki", "Jakub Szymanik"], "abstract": "Large language models (LLMs) are increasingly evaluated on formal tasks,\nwhere strong reasoning abilities define the state of the art. However, their\nability to generalize to out-of-distribution problems remains limited. In this\npaper, we investigate how LLMs can achieve a systematic understanding of\ndeductive rules. Our focus is on the task of identifying the appropriate subset\nof premises within a knowledge base needed to derive a given hypothesis. To\ntackle this challenge, we propose Meta-learning for In-context Deduction\n(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND\nis to enable models to generalize more effectively to unseen knowledge bases\nand to systematically apply inference rules. Our results show that MIND\nsignificantly improves generalization in small LMs ranging from 1.5B to 7B\nparameters. The benefits are especially pronounced in smaller models and\nlow-data settings. Remarkably, small models fine-tuned with MIND outperform\nstate-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.", "categories": ["cs.CL"], "published": "2025-05-20 13:00:48", "updated": "2025-05-20 13:00:48", "pdf_url": "http://arxiv.org/pdf/2505.14313v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14318v1", "title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection", "authors": ["Wenjun Hou", "Yi Cheng", "Kaishuai Xu", "Heng Li", "Yan Hu", "Wenjie Li", "Jiang Liu"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration and inefficient\nutilization of learned representations. To address this limitation, we propose\nRADAR, a framework for enhancing radiology report generation with supplementary\nknowledge injection. RADAR improves report generation by systematically\nleveraging both the internal knowledge of an LLM and externally retrieved\ninformation. Specifically, it first extracts the model's acquired knowledge\nthat aligns with expert image-based classification outputs. It then retrieves\nrelevant supplementary knowledge to further enrich this information. Finally,\nby aggregating both sources, RADAR generates more accurate and informative\nradiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU\nX-ray demonstrate that our model outperforms state-of-the-art LLMs in both\nlanguage quality and clinical accuracy", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-20 13:05:41", "updated": "2025-05-20 13:05:41", "pdf_url": "http://arxiv.org/pdf/2505.14318v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14347v1", "title": "QA-prompting: Improving Summarization with Large Language Models using Question-Answering", "authors": ["Neelabh Sinha"], "abstract": "Language Models (LMs) have revolutionized natural language processing,\nenabling high-quality text generation through prompting and in-context\nlearning. However, models often struggle with long-context summarization due to\npositional biases, leading to suboptimal extraction of critical information.\nThere are techniques to improve this with fine-tuning, pipelining, or using\ncomplex techniques, which have their own challenges. To solve these challenges,\nwe propose QA-prompting - a simple prompting method for summarization that\nutilizes question-answering as an intermediate step prior to summary\ngeneration. Our method extracts key information and enriches the context of\ntext to mitigate positional biases and improve summarization in a single LM\ncall per task without requiring fine-tuning or pipelining. Experiments on\nmultiple datasets belonging to different domains using ten state-of-the-art\npre-trained models demonstrate that QA-prompting outperforms baseline and other\nstate-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This\nprovides an effective and scalable solution for summarization and highlights\nthe importance of domain-specific question selection for optimal performance.", "categories": ["cs.CL"], "published": "2025-05-20 13:29:36", "updated": "2025-05-20 13:29:36", "pdf_url": "http://arxiv.org/pdf/2505.14347v1", "comment": "Submitted to ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14350v1", "title": "OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "abstract": "Fine-tuning Large Language Models (LLMs) has become increasingly challenging\ndue to their massive scale and associated computational costs.\nParameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as\ncomputational alternatives; however, their implementations still require\nsignificant resources. In this paper, we present OSoRA (Output-Dimension and\nSingular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.\nOSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value\nDecomposition (SVD) with learnable scaling vectors in a unified framework. It\nfirst performs an SVD of pre-trained weight matrices, then optimizes an\noutput-dimension vector during training, while keeping the corresponding\nsingular vector matrices frozen. OSoRA substantially reduces computational\nresource requirements by minimizing the number of trainable parameters during\nfine-tuning. Comprehensive evaluations across mathematical reasoning, common\nsense reasoning, and other benchmarks demonstrate that OSoRA achieves\ncomparable or superior performance to state-of-the-art methods like LoRA and\nVeRA, while maintaining a linear parameter scaling even as the rank increases\nto higher dimensions. Our ablation studies further confirm that jointly\ntraining both the singular values and the output-dimension vector is critical\nfor optimal performance.", "categories": ["cs.CL"], "published": "2025-05-20 13:34:06", "updated": "2025-05-20 13:34:06", "pdf_url": "http://arxiv.org/pdf/2505.14350v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14351v1", "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 13:35:55", "updated": "2025-05-20 13:35:55", "pdf_url": "http://arxiv.org/pdf/2505.14351v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14354v1", "title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications", "authors": ["Xin Li", "Mengbing Liu", "Li Wei", "Jiancheng An", "M\u00e9rouane Debbah", "Chau Yuen"], "abstract": "Large Language Models (LLMs) have achieved impressive results across a broad\narray of tasks, yet their capacity for complex, domain-specific mathematical\nreasoning-particularly in wireless communications-remains underexplored. In\nthis work, we introduce WirelessMathBench, a novel benchmark specifically\ndesigned to evaluate LLMs on mathematical modeling challenges to wireless\ncommunications engineering. Our benchmark consists of 587 meticulously curated\nquestions sourced from 40 state-of-the-art research papers, encompassing a\ndiverse spectrum of tasks ranging from basic multiple-choice questions to\ncomplex equation completion tasks, including both partial and full completions,\nall of which rigorously adhere to physical and dimensional constraints. Through\nextensive experimentation with leading LLMs, we observe that while many models\nexcel in basic recall tasks, their performance degrades significantly when\nreconstructing partially or fully obscured equations, exposing fundamental\nlimitations in current LLMs. Even DeepSeek-R1, the best performer on our\nbenchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%\nsuccess rate in full equation completion. By publicly releasing\nWirelessMathBench along with the evaluation toolkit, we aim to advance the\ndevelopment of more robust, domain-aware LLMs for wireless system analysis and\nbroader engineering applications.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 13:38:10", "updated": "2025-05-20 13:38:10", "pdf_url": "http://arxiv.org/pdf/2505.14354v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14356v1", "title": "PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs", "authors": ["Sho Inoue", "Shai Wang", "Haizhou Li"], "abstract": "Despite significant progress in neural spoken dialog systems,\npersonality-aware conversation agents -- capable of adapting behavior based on\npersonalities -- remain underexplored due to the absence of personality\nannotations in speech datasets. We propose a pipeline that preprocesses raw\naudio recordings to create a dialogue dataset annotated with timestamps,\nresponse types, and emotion/sentiment labels. We employ an automatic speech\nrecognition (ASR) system to extract transcripts and timestamps, then generate\nconversation-level annotations. Leveraging these annotations, we design a\nsystem that employs large language models to predict conversational\npersonality. Human evaluators were engaged to identify conversational\ncharacteristics and assign personality labels. Our analysis demonstrates that\nthe proposed system achieves stronger alignment with human judgments compared\nto existing approaches.", "categories": ["cs.SD", "cs.CL", "eess.AS"], "published": "2025-05-20 13:41:32", "updated": "2025-05-20 13:41:32", "pdf_url": "http://arxiv.org/pdf/2505.14356v1", "comment": "This is accepted to Interspeech 2025; Added an extra page for\n  supplementary figures; Project page:\n  https://github.com/shinshoji01/Personality-Prediction-for-Conversation-Agents", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14367v1", "title": "Dual Decomposition of Weights and Singular Value Low Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for\nadapting Large Language Models (LLMs) to downstream tasks, among which Low-rank\nAdaptation (LoRA) represents one of the most widely adopted methodologies.\nHowever, existing LoRA-based approaches exhibit two fundamental limitations:\nunstable training dynamics and inefficient knowledge transfer from pre-trained\nmodels, both stemming from random initialization of adapter parameters. To\novercome these challenges, we propose DuDe, a novel approach that decomposes\nweight matrices into magnitude and direction components, employing Singular\nValue Decomposition (SVD) for principled initialization. Our comprehensive\nevaluation demonstrates DuDe's superior performance and robustness, achieving\nup to 48.35\\% accuracy on MMLU and 62.53\\% ($\\pm$ 1.59) accuracy on GSM8K. Our\ntheoretical analysis and empirical validation collectively demonstrate that\nDuDe's decomposition strategy enhances optimization stability and better\npreserves pre-trained representations, particularly for domain-specific tasks\nrequiring specialized knowledge. The combination of robust empirical\nperformance and rigorous theoretical foundations establishes DuDe as a\nsignificant contribution to PEFT methodologies for LLMs.", "categories": ["cs.CL"], "published": "2025-05-20 13:49:15", "updated": "2025-05-20 13:49:15", "pdf_url": "http://arxiv.org/pdf/2505.14367v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14368v1", "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs", "authors": ["Jiawen Wang", "Pritha Gupta", "Ivan Habernal", "Eyke H\u00fcllermeier"], "abstract": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 13:50:43", "updated": "2025-05-20 13:50:43", "pdf_url": "http://arxiv.org/pdf/2505.14368v1", "comment": "8 pages, 3 figures, EMNLP 2025 under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14376v1", "title": "AutoRev: Automatic Peer Review System for Academic Research Papers", "authors": ["Maitreya Prafulla Chitale", "Ketaki Mangesh Shetye", "Harshit Gupta", "Manav Chaudhary", "Vasudeva Varma"], "abstract": "Generating a review for an academic research paper is a complex task that\nrequires a deep understanding of the document's content and the\ninterdependencies between its sections. It demands not only insight into\ntechnical details but also an appreciation of the paper's overall coherence and\nstructure. Recent methods have predominantly focused on fine-tuning large\nlanguage models (LLMs) to address this challenge. However, they often overlook\nthe computational and performance limitations imposed by long input token\nlengths. To address this, we introduce AutoRev, an Automatic Peer Review System\nfor Academic Research Papers. Our novel framework represents an academic\ndocument as a graph, enabling the extraction of the most critical passages that\ncontribute significantly to the review. This graph-based approach demonstrates\neffectiveness for review generation and is potentially adaptable to various\ndownstream tasks, such as question answering, summarization, and document\nrepresentation. When applied to review generation, our method outperforms SOTA\nbaselines by an average of 58.72% across all evaluation metrics. We hope that\nour work will stimulate further research in applying graph-based extraction\ntechniques to other downstream tasks in NLP. We plan to make our code public\nupon acceptance.", "categories": ["cs.CL"], "published": "2025-05-20 13:59:58", "updated": "2025-05-20 13:59:58", "pdf_url": "http://arxiv.org/pdf/2505.14376v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14393v1", "title": "Editing Across Languages: A Survey of Multilingual Knowledge Editing", "authors": ["Nadir Durrani", "Basel Mousi", "Fahim Dalvi"], "abstract": "While Knowledge Editing has been extensively studied in monolingual settings,\nit remains underexplored in multilingual contexts. This survey systematizes\nrecent research on Multilingual Knowledge Editing (MKE), a growing subdomain of\nmodel editing focused on ensuring factual edits generalize reliably across\nlanguages. We present a comprehensive taxonomy of MKE methods, covering\nparameter-based, memory-based, fine-tuning, and hypernetwork approaches. We\nsurvey available benchmarks,summarize key findings on method effectiveness and\ntransfer patterns, identify challenges in cross-lingual propagation, and\nhighlight open problems related to language anisotropy, evaluation coverage,\nand edit scalability. Our analysis consolidates a rapidly evolving area and\nlays the groundwork for future progress in editable language-aware LLMs.", "categories": ["cs.CL"], "published": "2025-05-20 14:13:04", "updated": "2025-05-20 14:13:04", "pdf_url": "http://arxiv.org/pdf/2505.14393v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14395v1", "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "abstract": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:14:00", "updated": "2025-05-20 14:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14395v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "authors": ["Ga\u00ebl Gendron", "Jo\u017ee M. Ro\u017eanec", "Michael Witbrock", "Gillian Dobbie"], "abstract": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"], "published": "2025-05-20 14:14:05", "updated": "2025-05-20 14:14:05", "pdf_url": "http://arxiv.org/pdf/2505.14396v1", "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14398v1", "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "abstract": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 14:14:38", "updated": "2025-05-20 14:14:38", "pdf_url": "http://arxiv.org/pdf/2505.14398v1", "comment": "Data and code are available at https://peterbaile.github.io/lag/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14402v1", "title": "OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking", "authors": ["Heng Yang", "Jack Cole", "Yuan Li", "Renzhi Chen", "Geyong Min", "Ke Li"], "abstract": "The code of nature, embedded in DNA and RNA genomes since the origin of life,\nholds immense potential to impact both humans and ecosystems through genome\nmodeling. Genomic Foundation Models (GFMs) have emerged as a transformative\napproach to decoding the genome. As GFMs scale up and reshape the landscape of\nAI-driven genomics, the field faces an urgent need for rigorous and\nreproducible evaluation. We present OmniGenBench, a modular benchmarking\nplatform designed to unify the data, model, benchmarking, and interpretability\nlayers across GFMs. OmniGenBench enables standardized, one-command evaluation\nof any GFM across five benchmark suites, with seamless integration of over 31\nopen-source models. Through automated pipelines and community-extensible\nfeatures, the platform addresses critical reproducibility challenges, including\ndata transparency, model interoperability, benchmark fragmentation, and\nblack-box interpretability. OmniGenBench aims to serve as foundational\ninfrastructure for reproducible genomic AI research, accelerating trustworthy\ndiscovery and collaborative innovation in the era of genome-scale modeling.", "categories": ["q-bio.GN", "cs.CL"], "published": "2025-05-20 14:16:25", "updated": "2025-05-20 14:16:25", "pdf_url": "http://arxiv.org/pdf/2505.14402v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14406v1", "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis", "authors": ["Haoming Huang", "Yibo Yan", "Jiahao Huo", "Xin Zou", "Xinfeng Li", "Kun Wang", "Xuming Hu"], "abstract": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the internal workings of attention heads,\ntracing how competing knowledge pathways contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation.", "categories": ["cs.CL"], "published": "2025-05-20 14:20:30", "updated": "2025-05-20 14:20:30", "pdf_url": "http://arxiv.org/pdf/2505.14406v1", "comment": "18 pages, 6 figures, EMNLP under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14410v1", "title": "Pairwise Evaluation of Accent Similarity in Speech Synthesis", "authors": ["Jinzuomu Zhong", "Suyuan Liu", "Dan Wells", "Korin Richmond"], "abstract": "Despite growing interest in generating high-fidelity accents, evaluating\naccent similarity in speech synthesis has been underexplored. We aim to enhance\nboth subjective and objective evaluation methods for accent similarity.\nSubjectively, we refine the XAB listening test by adding components that\nachieve higher statistical significance with fewer listeners and lower costs.\nOur method involves providing listeners with transcriptions, having them\nhighlight perceived accent differences, and implementing meticulous screening\nfor reliability. Objectively, we utilise pronunciation-related metrics, based\non distances between vowel formants and phonetic posteriorgrams, to evaluate\naccent generation. Comparative experiments reveal that these metrics, alongside\naccent similarity, speaker similarity, and Mel Cepstral Distortion, can be\nused. Moreover, our findings underscore significant limitations of common\nmetrics like Word Error Rate in assessing underrepresented accents.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 14:23:50", "updated": "2025-05-20 14:23:50", "pdf_url": "http://arxiv.org/pdf/2505.14410v1", "comment": "Accepted by INTERSPEECH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14412v1", "title": "PRL: Prompts from Reinforcement Learning", "authors": ["Pawe\u0142 Batorski", "Adrian Kosmala", "Paul Swoboda"], "abstract": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 14:26:19", "updated": "2025-05-20 14:26:19", "pdf_url": "http://arxiv.org/pdf/2505.14412v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14418v1", "title": "Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents", "authors": ["Pengzhou Cheng", "Haowen Hu", "Zheng Wu", "Zongru Wu", "Tianjie Ju", "Daizong Ding", "Zhuosheng Zhang", "Gongshen Liu"], "abstract": "Graphical user interface (GUI) agents powered by multimodal large language\nmodels (MLLMs) have shown greater promise for human-interaction. However, due\nto the high fine-tuning cost, users often rely on open-source GUI agents or\nAPIs offered by AI providers, which introduces a critical but underexplored\nsupply chain threat: backdoor attacks. In this work, we first unveil that\nMLLM-powered GUI agents naturally expose multiple interaction-level triggers,\nsuch as historical steps, environment states, and task progress. Based on this\nobservation, we introduce AgentGhost, an effective and stealthy framework for\nred-teaming backdoor attacks. Specifically, we first construct composite\ntriggers by combining goal and interaction levels, allowing GUI agents to\nunintentionally activate backdoors while ensuring task utility. Then, we\nformulate backdoor injection as a Min-Max optimization problem that uses\nsupervised contrastive learning to maximize the feature difference across\nsample classes at the representation space, improving flexibility of the\nbackdoor. Meanwhile, it adopts supervised fine-tuning to minimize the\ndiscrepancy between backdoor and clean behavior generation, enhancing\neffectiveness and utility. Extensive evaluations of various agent models in two\nestablished mobile benchmarks show that AgentGhost is effective and generic,\nwith attack accuracy that reaches 99.7\\% on three attack objectives, and shows\nstealthiness with only 1\\% utility degradation. Furthermore, we tailor a\ndefense method against AgentGhost that reduces the attack accuracy to 22.1\\%.\nOur code is available at \\texttt{anonymous}.", "categories": ["cs.CL"], "published": "2025-05-20 14:29:18", "updated": "2025-05-20 14:29:18", "pdf_url": "http://arxiv.org/pdf/2505.14418v1", "comment": "25 pages, 10 figures, 12 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14420v1", "title": "SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection", "authors": ["Huopu Zhang", "Yanguang Liu", "Mengnan Du"], "abstract": "Predicting earnings surprises through the analysis of earnings conference\ncall transcripts has attracted increasing attention from the financial research\ncommunity. Conference calls serve as critical communication channels between\ncompany executives, analysts, and shareholders, offering valuable\nforward-looking information. However, these transcripts present significant\nanalytical challenges, typically containing over 5,000 words with substantial\nredundancy and industry-specific terminology that creates obstacles for\nlanguage models. In this work, we propose the Sparse Autoencoder for Financial\nRepresentation Enhancement (SAE-FiRE) framework to address these limitations by\nextracting key information while eliminating redundancy. SAE-FiRE employs\nSparse Autoencoders (SAEs) to efficiently identify patterns and filter out\nnoises, and focusing specifically on capturing nuanced financial signals that\nhave predictive power for earnings surprises. Experimental results indicate\nthat the proposed method can significantly outperform comparing baselines.", "categories": ["q-fin.CP", "cs.CL", "cs.LG"], "published": "2025-05-20 14:31:23", "updated": "2025-05-20 14:31:23", "pdf_url": "http://arxiv.org/pdf/2505.14420v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14423v1", "title": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs", "authors": ["Ona de Gibert", "Joseph Attieh", "Teemu Vahtola", "Mikko Aulamo", "Zihao Li", "Ra\u00fal V\u00e1zquez", "Tiancheng Hu", "J\u00f6rg Tiedemann"], "abstract": "We investigate the potential of LLM-generated synthetic data for improving\nlow-resource machine translation (MT). Focusing on seven diverse target\nlanguages, we construct a document-level synthetic corpus from English\nEuroparl, and extend it via pivoting to 147 additional language pairs.\nAutomatic and human evaluation confirm its high overall quality. We study its\npractical application by (i) identifying effective training regimes, (ii)\ncomparing our data with the HPLT dataset, and (iii) testing its utility beyond\nEnglish-centric MT. Finally, we introduce SynOPUS, a public repository for\nsynthetic parallel datasets. Our findings show that LLM-generated synthetic\ndata, even when noisy, can substantially improve MT performance for\nlow-resource languages.", "categories": ["cs.CL"], "published": "2025-05-20 14:31:54", "updated": "2025-05-20 14:31:54", "pdf_url": "http://arxiv.org/pdf/2505.14423v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14425v1", "title": "From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "abstract": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization.", "categories": ["cs.CL"], "published": "2025-05-20 14:33:29", "updated": "2025-05-20 14:33:29", "pdf_url": "http://arxiv.org/pdf/2505.14425v1", "comment": "4 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14432v1", "title": "Rank-K: Test-Time Reasoning for Listwise Reranking", "authors": ["Eugene Yang", "Andrew Yates", "Kathryn Ricci", "Orion Weller", "Vivek Chari", "Benjamin Van Durme", "Dawn Lawrie"], "abstract": "Retrieve-and-rerank is a popular retrieval pipeline because of its ability to\nmake slow but effective rerankers efficient enough at query time by reducing\nthe number of comparisons. Recent works in neural rerankers take advantage of\nlarge language models for their capability in reasoning between queries and\npassages and have achieved state-of-the-art retrieval effectiveness. However,\nsuch rerankers are resource-intensive, even after heavy optimization. In this\nwork, we introduce Rank-K, a listwise passage reranking model that leverages\nthe reasoning capability of the reasoning language model at query time that\nprovides test time scalability to serve hard queries. We show that Rank-K\nimproves retrieval effectiveness by 23\\% over the RankZephyr, the\nstate-of-the-art listwise reranker, when reranking a BM25 initial ranked list\nand 19\\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is\ninherently a multilingual model, we found that it ranks passages based on\nqueries in different languages as effectively as it does in monolingual\nretrieval.", "categories": ["cs.IR", "cs.CL"], "published": "2025-05-20 14:39:34", "updated": "2025-05-20 14:39:34", "pdf_url": "http://arxiv.org/pdf/2505.14432v1", "comment": "15 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14436v1", "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:42:03", "updated": "2025-05-20 14:42:03", "pdf_url": "http://arxiv.org/pdf/2505.14436v1", "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14438v1", "title": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models", "authors": ["Yuanbo Fang", "Haoze Sun", "Jun Liu", "Tao Zhang", "Zenan Zhou", "Weipeng Chen", "Xiaofen Xing", "Xiangmin Xu"], "abstract": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench.", "categories": ["cs.SD", "cs.CL", "eess.AS"], "published": "2025-05-20 14:42:20", "updated": "2025-05-20 14:42:20", "pdf_url": "http://arxiv.org/pdf/2505.14438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14442v1", "title": "Creative Preference Optimization", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:43:41", "updated": "2025-05-20 14:43:41", "pdf_url": "http://arxiv.org/pdf/2505.14442v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14449v1", "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "abstract": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 14:50:44", "updated": "2025-05-20 14:50:44", "pdf_url": "http://arxiv.org/pdf/2505.14449v1", "comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "authors": ["Chihan Huang", "Hao Tang"], "abstract": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:52:41", "updated": "2025-05-20 14:52:41", "pdf_url": "http://arxiv.org/pdf/2505.14455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14462v1", "title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding", "authors": ["Jiaang Li", "Yifei Yuan", "Wenyan Li", "Mohammad Aliannejadi", "Daniel Hershcovich", "Anders S\u00f8gaard", "Ivan Vuli\u0107", "Wenxuan Zhang", "Paul Pu Liang", "Yang Deng", "Serge Belongie"], "abstract": "As vision-language models (VLMs) become increasingly integrated into daily\nlife, the need for accurate visual culture understanding is becoming critical.\nYet, these models frequently fall short in interpreting cultural nuances\neffectively. Prior work has demonstrated the effectiveness of\nretrieval-augmented generation (RAG) in enhancing cultural understanding in\ntext-only settings, while its application in multimodal scenarios remains\nunderexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented\nVisual culturE uNdErstAnding), a new benchmark designed to advance visual\nculture understanding through retrieval, focusing on two tasks: culture-focused\nvisual question answering (cVQA) and culture-informed image captioning (cIC).\nRAVENEA extends existing datasets by integrating over 10,000 Wikipedia\ndocuments curated and ranked by human annotators. With RAVENEA, we train and\nevaluate seven multimodal retrievers for each image query, and measure the\ndownstream impact of retrieval-augmented inputs across fourteen\nstate-of-the-art VLMs. Our results show that lightweight VLMs, when augmented\nwith culture-aware retrieval, outperform their non-augmented counterparts (by\nat least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the\nvalue of retrieval-augmented methods and culturally inclusive benchmarks for\nmultimodal understanding.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-20 14:57:16", "updated": "2025-05-20 14:57:16", "pdf_url": "http://arxiv.org/pdf/2505.14462v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14464v1", "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters", "authors": ["Xiaoyu Tian", "Yunjie Ji", "Haotian Wang", "Shuaiting Chen", "Sitong Zhao", "Yiping Peng", "Han Zhao", "Xiangang Li"], "abstract": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging Face\\footnote{Datasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},\n\\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.", "categories": ["cs.CL"], "published": "2025-05-20 15:00:51", "updated": "2025-05-20 15:00:51", "pdf_url": "http://arxiv.org/pdf/2505.14464v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14467v1", "title": "Void in Language Models", "authors": ["Mani Shemiranifar"], "abstract": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks.", "categories": ["cs.CL"], "published": "2025-05-20 15:01:56", "updated": "2025-05-20 15:01:56", "pdf_url": "http://arxiv.org/pdf/2505.14467v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14469v1", "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "abstract": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:05:03", "updated": "2025-05-20 15:05:03", "pdf_url": "http://arxiv.org/pdf/2505.14469v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14470v1", "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "authors": ["Nadav Har-Tuv", "Or Tal", "Yossi Adi"], "abstract": "We present PAST, a novel end-to-end framework that jointly models phonetic\ninformation alongside signal reconstruction, eliminating the need for external\npretrained models. Unlike previous approaches that rely on pretrained\nself-supervised models, PAST employs supervised phonetic data, directly\nintegrating domain knowledge into the tokenization process via auxiliary tasks.\nAdditionally, we introduce a streamable, causal variant of PAST, enabling\nreal-time speech applications. Results demonstrate that PAST surpasses existing\nevaluated baseline tokenizers across common evaluation metrics, including\nphonetic representation and speech reconstruction. Notably, PAST also achieves\nsuperior performance when serving as a speech representation for speech\nlanguage models, further highlighting its effectiveness as a foundation for\nspoken language generation. To foster further research, we release the full\nimplementation. For code, model checkpoints, and samples see:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/PAST", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "published": "2025-05-20 15:05:14", "updated": "2025-05-20 15:05:14", "pdf_url": "http://arxiv.org/pdf/2505.14470v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14471v1", "title": "Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning", "authors": ["Tong Li", "Jiachuan Wang", "Yongqi Zhang", "Shuangyin Li", "Lei Chen"], "abstract": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss", "categories": ["cs.CL"], "published": "2025-05-20 15:05:27", "updated": "2025-05-20 15:05:27", "pdf_url": "http://arxiv.org/pdf/2505.14471v1", "comment": "Manuscripts, accepted to KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14479v1", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "abstract": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:13:32", "updated": "2025-05-20 15:13:32", "pdf_url": "http://arxiv.org/pdf/2505.14479v1", "comment": "long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14481v1", "title": "PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models", "authors": ["He Zhu", "Junyou Su", "Minxi Chen", "Wen Wang", "Yijie Deng", "Guanhua Chen", "Wenjia Zhang"], "abstract": "In the field of urban planning, existing Vision-Language Models (VLMs)\nfrequently fail to effectively analyze and evaluate planning maps, despite the\ncritical importance of these visual elements for urban planners and related\neducational contexts. Planning maps, which visualize land use, infrastructure\nlayouts, and functional zoning, require specialized understanding of spatial\nconfigurations, regulatory requirements, and multi-scale analysis. To address\nthis challenge, we introduce PlanGPT-VL, the first domain-specific\nVision-Language Model tailored specifically for urban planning maps. PlanGPT-VL\nemploys three innovative approaches: (1) PlanAnno-V framework for high-quality\nVQA data synthesis, (2) Critical Point Thinking to reduce hallucinations\nthrough structured verification, and (3) comprehensive training methodology\ncombining Supervised Fine-Tuning with frozen vision encoder parameters. Through\nsystematic evaluation on our proposed PlanBench-V benchmark, we demonstrate\nthat PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs\nin specialized planning map interpretation tasks, offering urban planning\nprofessionals a reliable tool for map analysis, assessment, and educational\napplications while maintaining high factual accuracy. Our lightweight 7B\nparameter model achieves comparable performance to models exceeding 72B\nparameters, demonstrating efficient domain specialization without sacrificing\nperformance.", "categories": ["cs.CL"], "published": "2025-05-20 15:14:47", "updated": "2025-05-20 15:14:47", "pdf_url": "http://arxiv.org/pdf/2505.14481v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14483v1", "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance", "authors": ["Agam Goyal", "Xianyang Zhan", "Yilun Chen", "Koustuv Saha", "Eshwar Chandrasekharan"], "abstract": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities.", "categories": ["cs.CL"], "published": "2025-05-20 15:16:06", "updated": "2025-05-20 15:16:06", "pdf_url": "http://arxiv.org/pdf/2505.14483v1", "comment": "Preprint: 15 pages, 4 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14489v1", "title": "Reasoning Models Better Express Their Confidence", "authors": ["Dongkeun Yoon", "Seungone Kim", "Sohee Yang", "Sunkyoung Kim", "Soyeon Kim", "Yongil Kim", "Eunbi Choi", "Yireun Kim", "Minjoon Seo"], "abstract": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:19:00", "updated": "2025-05-20 15:19:00", "pdf_url": "http://arxiv.org/pdf/2505.14489v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14499v1", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "abstract": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:28:26", "updated": "2025-05-20 15:28:26", "pdf_url": "http://arxiv.org/pdf/2505.14499v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14505v1", "title": "ModRWKV: Transformer Multimodality in Linear Time", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "abstract": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:34:36", "updated": "2025-05-20 15:34:36", "pdf_url": "http://arxiv.org/pdf/2505.14505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14518v1", "title": "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "abstract": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 15:44:01", "updated": "2025-05-20 15:44:01", "pdf_url": "http://arxiv.org/pdf/2505.14518v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14523v1", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "abstract": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-20 15:46:44", "updated": "2025-05-20 15:46:44", "pdf_url": "http://arxiv.org/pdf/2505.14523v1", "comment": "To be published in ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14530v1", "title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs", "authors": ["Zhipeng Yang", "Junzhuo Li", "Siyu Xia", "Xuming Hu"], "abstract": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 15:49:15", "updated": "2025-05-20 15:49:15", "pdf_url": "http://arxiv.org/pdf/2505.14530v1", "comment": "27 pages, 17 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14536v1", "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders", "authors": ["Agam Goyal", "Vedant Rathi", "William Yeh", "Yian Wang", "Yuen Chen", "Hari Sundaram"], "abstract": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment.", "categories": ["cs.CL"], "published": "2025-05-20 15:55:31", "updated": "2025-05-20 15:55:31", "pdf_url": "http://arxiv.org/pdf/2505.14536v1", "comment": "Preprint: 19 pages, 7 figures, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14552v1", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "abstract": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:06:32", "updated": "2025-05-20 16:06:32", "pdf_url": "http://arxiv.org/pdf/2505.14552v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14553v1", "title": "Pivot Language for Low-Resource Machine Translation", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "abstract": "Certain pairs of languages suffer from lack of a parallel corpus which is\nlarge in size and diverse in domain. One of the ways this is overcome is via\nuse of a pivot language. In this paper we use Hindi as a pivot language to\ntranslate Nepali into English. We describe what makes Hindi a good candidate\nfor the pivot. We discuss ways in which a pivot language can be used, and use\ntwo such approaches - the Transfer Method (fully supervised) and\nBacktranslation (semi-supervised) - to translate Nepali into English. Using the\nformer, we are able to achieve a devtest Set SacreBLEU score of 14.2, which\nimproves the baseline fully supervised score reported by (Guzman et al., 2019)\nby 6.6 points. While we are slightly below the semi-supervised baseline score\nof 15.1, we discuss what may have caused this under-performance, and suggest\nscope for future work.", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "published": "2025-05-20 16:10:10", "updated": "2025-05-20 16:10:10", "pdf_url": "http://arxiv.org/pdf/2505.14553v1", "comment": "7 pages, 3 figures, paper dated May 13, 2019", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "abstract": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 16:28:08", "updated": "2025-05-20 16:28:08", "pdf_url": "http://arxiv.org/pdf/2505.14569v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14577v1", "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring", "authors": ["Sohaila Eltanbouly", "Salam Albatarni", "Tamer Elsayed"], "abstract": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant.", "categories": ["cs.CL"], "published": "2025-05-20 16:34:37", "updated": "2025-05-20 16:34:37", "pdf_url": "http://arxiv.org/pdf/2505.14577v1", "comment": "Accepted at ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14582v1", "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning", "authors": ["Shangziqi Zhao", "Jiahao Yuan", "Guisong Yang", "Usman Naseem"], "abstract": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity.", "categories": ["cs.CL"], "published": "2025-05-20 16:38:32", "updated": "2025-05-20 16:38:32", "pdf_url": "http://arxiv.org/pdf/2505.14582v1", "comment": "17 pages,4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14585v1", "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "abstract": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "categories": ["cs.CL"], "published": "2025-05-20 16:40:09", "updated": "2025-05-20 16:40:09", "pdf_url": "http://arxiv.org/pdf/2505.14585v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14590v1", "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "abstract": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.", "categories": ["cs.CL"], "published": "2025-05-20 16:41:45", "updated": "2025-05-20 16:41:45", "pdf_url": "http://arxiv.org/pdf/2505.14590v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14597v1", "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals", "authors": ["Xianzhen Luo", "Qingfu Zhu", "Zhiming Zhang", "Mingzheng Xu", "Tianhao Cheng", "Yixuan Wang", "Zheng Chu", "Shijie Xuyang", "Zhiyuan Ma", "YuanTao Fan", "Wanxiang Che"], "abstract": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance.", "categories": ["cs.CL"], "published": "2025-05-20 16:48:57", "updated": "2025-05-20 16:48:57", "pdf_url": "http://arxiv.org/pdf/2505.14597v1", "comment": "Code & Model is https://github.com/Luowaterbi/CTF-Instruct", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14599v1", "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "abstract": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 16:49:40", "updated": "2025-05-20 16:49:40", "pdf_url": "http://arxiv.org/pdf/2505.14599v1", "comment": "Accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14607v1", "title": "sudoLLM : On Multi-role Alignment of Language Models", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Joy Mahapatra", "Utpal Garain"], "abstract": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.", "categories": ["cs.CL", "cs.CR", "I.2.7"], "published": "2025-05-20 16:54:34", "updated": "2025-05-20 16:54:34", "pdf_url": "http://arxiv.org/pdf/2505.14607v1", "comment": "Under review. Code and data to be released later", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14608v1", "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "abstract": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:55:44", "updated": "2025-05-20 16:55:44", "pdf_url": "http://arxiv.org/pdf/2505.14608v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14615v1", "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", "authors": ["Anjiang Wei", "Yuheng Wu", "Yingjia Wan", "Tarun Suresh", "Huanmi Tan", "Zhanke Zhou", "Sanmi Koyejo", "Ke Wang", "Alex Aiken"], "abstract": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "published": "2025-05-20 17:00:22", "updated": "2025-05-20 17:00:22", "pdf_url": "http://arxiv.org/pdf/2505.14615v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14617v1", "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models", "authors": ["Sahar Abdelnabi", "Ahmed Salem"], "abstract": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation.", "categories": ["cs.CL", "cs.CY"], "published": "2025-05-20 17:03:12", "updated": "2025-05-20 17:03:12", "pdf_url": "http://arxiv.org/pdf/2505.14617v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14620v1", "title": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs", "authors": ["Morgan Lindsay Heisler", "Linzi Xing", "Ge Shi", "Hanieh Sadri", "Gursimran Singh", "Weiwei Zhang", "Tao Ye", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "abstract": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 17:11:18", "updated": "2025-05-20 17:11:18", "pdf_url": "http://arxiv.org/pdf/2505.14620v1", "comment": "Accepted at ACM KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14625v1", "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 17:16:44", "updated": "2025-05-20 17:16:44", "pdf_url": "http://arxiv.org/pdf/2505.14625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14627v1", "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "authors": ["Ashutosh Adhikari", "Mirella Lapata"], "abstract": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:18:17", "updated": "2025-05-20 17:18:17", "pdf_url": "http://arxiv.org/pdf/2505.14627v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14629v1", "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "abstract": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-20 17:19:57", "updated": "2025-05-20 17:19:57", "pdf_url": "http://arxiv.org/pdf/2505.14629v1", "comment": "Accepted at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14631v1", "title": "Think Only When You Need with Large Hybrid-Reasoning Models", "authors": ["Lingjie Jiang", "Xun Wu", "Shaohan Huang", "Qingxiu Dong", "Zewen Chi", "Li Dong", "Xingxing Zhang", "Tengchao Lv", "Lei Cui", "Furu Wei"], "abstract": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.", "categories": ["cs.CL"], "published": "2025-05-20 17:23:25", "updated": "2025-05-20 17:23:25", "pdf_url": "http://arxiv.org/pdf/2505.14631v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "published": "2025-05-20 17:24:09", "updated": "2025-05-20 17:24:09", "pdf_url": "http://arxiv.org/pdf/2505.14633v1", "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14638v1", "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference", "authors": ["Tomer Gafni", "Asaf Karnieli", "Yair Hanani"], "abstract": "Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model.", "categories": ["cs.CV", "cs.CL", "cs.LG"], "published": "2025-05-20 17:26:12", "updated": "2025-05-20 17:26:12", "pdf_url": "http://arxiv.org/pdf/2505.14638v1", "comment": "Accepted at eLVM Workshop, CVPR, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14652v1", "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "abstract": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.", "categories": ["cs.CL"], "published": "2025-05-20 17:41:33", "updated": "2025-05-20 17:41:33", "pdf_url": "http://arxiv.org/pdf/2505.14652v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "abstract": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-20 17:42:34", "updated": "2025-05-20 17:42:34", "pdf_url": "http://arxiv.org/pdf/2505.14654v1", "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14660v1", "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "authors": ["Ronald Seoh", "Dan Goldwasser"], "abstract": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 17:47:04", "updated": "2025-05-20 17:47:04", "pdf_url": "http://arxiv.org/pdf/2505.14660v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:54:54", "updated": "2025-05-20 17:54:54", "pdf_url": "http://arxiv.org/pdf/2505.14667v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "categories": ["cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:55:25", "updated": "2025-05-20 17:55:25", "pdf_url": "http://arxiv.org/pdf/2505.14668v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14674v1", "title": "Reward Reasoning Model", "authors": ["Jiaxin Guo", "Zewen Chi", "Li Dong", "Qingxiu Dong", "Xun Wu", "Shaohan Huang", "Furu Wei"], "abstract": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.", "categories": ["cs.CL"], "published": "2025-05-20 17:58:03", "updated": "2025-05-20 17:58:03", "pdf_url": "http://arxiv.org/pdf/2505.14674v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14679v1", "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "abstract": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.", "categories": ["cs.CL"], "published": "2025-05-20 17:59:04", "updated": "2025-05-20 17:59:04", "pdf_url": "http://arxiv.org/pdf/2505.14679v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "abstract": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:59:13", "updated": "2025-05-20 17:59:13", "pdf_url": "http://arxiv.org/pdf/2505.14680v1", "comment": "SIGIR 2025 Perspective Paper", "doi": "10.1145/3726302.3730353", "journal_ref": null}
{"arxiv_id": "2505.14681v1", "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "published": "2025-05-20 17:59:16", "updated": "2025-05-20 17:59:16", "pdf_url": "http://arxiv.org/pdf/2505.14681v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14684v1", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 17:59:31", "updated": "2025-05-20 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.14684v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14685v1", "title": "Language Models use Lookbacks to Track Beliefs", "authors": ["Nikhil Prakash", "Natalie Shapira", "Arnab Sen Sharma", "Christoph Riedl", "Yonatan Belinkov", "Tamar Rott Shaham", "David Bau", "Atticus Geiger"], "abstract": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs.", "categories": ["cs.CL"], "published": "2025-05-20 17:59:45", "updated": "2025-05-20 17:59:45", "pdf_url": "http://arxiv.org/pdf/2505.14685v1", "comment": "32 pages, 32 figures. Code and data at https://belief.baulab.info/", "doi": null, "journal_ref": null}
