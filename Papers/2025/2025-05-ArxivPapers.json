{"arxiv_id": "2505.09040v1", "title": "RT-cache: Efficient Robot Trajectory Retrieval System", "authors": ["Owen Kwon", "Abraham George", "Alison Bartsch", "Amir Barati Farimani"], "abstract": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "published": "2025-05-14 00:41:44", "updated": "2025-05-14 00:41:44", "pdf_url": "http://arxiv.org/pdf/2505.09040v1", "comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.09062v1", "title": "Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models", "authors": ["Junda Zhao", "Yuliang Song", "Eldan Cohen"], "abstract": "Recent advancements in source code summarization have leveraged\ntransformer-based pre-trained models, including Large Language Models of Code\n(LLMCs), to automate and improve the generation of code summaries. However,\nexisting methods often focus on generating a single high-quality summary for a\ngiven source code, neglecting scenarios where the generated summary might be\ninadequate and alternative options are needed. In this paper, we introduce\nVariational Prefix Tuning (VPT), a novel approach that enhances pre-trained\nmodels' ability to generate diverse yet accurate sets of summaries, allowing\nthe user to choose the most suitable one for the given source code. Our method\nintegrates a Conditional Variational Autoencoder (CVAE) framework as a modular\ncomponent into pre-trained models, enabling us to model the distribution of\nobserved target summaries and sample continuous embeddings to be used as\nprefixes to steer the generation of diverse outputs during decoding.\nImportantly, we construct our method in a parameter-efficient manner,\neliminating the need for expensive model retraining, especially when using\nLLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset\nof generated summaries, optimizing both the diversity and the accuracy of the\noptions presented to users. We present extensive experimental evaluations using\nwidely used datasets and current state-of-the-art pre-trained code\nsummarization models to demonstrate the effectiveness of our approach and its\nadaptability across models.", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.7"], "published": "2025-05-14 01:46:56", "updated": "2025-05-14 01:46:56", "pdf_url": "http://arxiv.org/pdf/2505.09062v1", "comment": "Accepted by the Journal of Systems and Software", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.09081v1", "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation", "authors": ["Gaurav Koley"], "abstract": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.", "categories": ["cs.SI", "cs.AI", "cs.MA"], "published": "2025-05-14 02:29:46", "updated": "2025-05-14 02:29:46", "pdf_url": "http://arxiv.org/pdf/2505.09081v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.09082v1", "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM", "authors": ["Sophie Zhang", "Zhiming Lin"], "abstract": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-14 02:35:47", "updated": "2025-05-14 02:35:47", "pdf_url": "http://arxiv.org/pdf/2505.09082v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.09085v1", "title": "Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision", "authors": ["Jiaxuan Chen", "Yu Qi", "Yueming Wang", "Gang Pan"], "abstract": "Recent advancements in deep neural networks (DNNs), particularly large-scale\nlanguage models, have demonstrated remarkable capabilities in image and natural\nlanguage understanding. Although scaling up model parameters with increasing\nvolume of training data has progressively improved DNN capabilities, achieving\ncomplex cognitive abilities - such as understanding abstract concepts,\nreasoning, and adapting to novel scenarios, which are intrinsic to human\ncognition - remains a major challenge. In this study, we show that\nbrain-in-the-loop supervised learning, utilizing a small set of brain signals,\ncan effectively transfer human conceptual structures to DNNs, significantly\nenhancing their comprehension of abstract and even unseen concepts.\nExperimental results further indicate that the enhanced cognitive capabilities\nlead to substantial performance gains in challenging tasks, including\nfew-shot/zero-shot learning and out-of-distribution recognition, while also\nyielding highly interpretable concept representations. These findings highlight\nthat human-in-the-loop supervision can effectively augment the complex\ncognitive abilities of large models, offering a promising pathway toward\ndeveloping more human-like cognitive abilities in artificial systems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-14 02:39:10", "updated": "2025-05-14 02:39:10", "pdf_url": "http://arxiv.org/pdf/2505.09085v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.09091v1", "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis", "authors": ["Zeeshan Ahmad", "Shudi Bao", "Meng Chen"], "abstract": "In recent years, generative adversarial networks (GANs) have made significant\nprogress in generating audio sequences. However, these models typically rely on\nbandwidth-limited mel-spectrograms, which constrain the resolution of generated\naudio sequences, and lead to mode collapse during conditional generation. To\naddress this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),\na novel GAN architecture that incorporates a kernel-based periodic ReLU\nactivation function to induce periodic bias in audio generation. This\ninnovative approach enhances the model's ability to capture and reproduce\nintricate audio patterns. In particular, our proposed model features a DPN\nmodule for multi-resolution generation utilizing deformable convolution\noperations, allowing for adaptive receptive fields that improve the quality and\nfidelity of the synthetic audio. Additionally, we enhance the discriminator\nnetwork using deformable convolution to better distinguish between real and\ngenerated samples, further refining the audio quality. We trained two versions\nof the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M\nparameters). For evaluation, we use five different datasets, covering both\nspeech synthesis and music generation tasks, to demonstrate the efficiency of\nthe DPN-GAN. The experimental results demonstrate that DPN-GAN delivers\nsuperior performance on both out-of-distribution and noisy data, showcasing its\nrobustness and adaptability. Trained across various datasets, DPN-GAN\noutperforms state-of-the-art GAN architectures on standard evaluation metrics,\nand exhibits increased robustness in synthesized audio.", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "published": "2025-05-14 02:52:16", "updated": "2025-05-14 02:52:16", "pdf_url": "http://arxiv.org/pdf/2505.09091v1", "comment": null, "doi": "10.1109/ACCESS.2025.3561857", "journal_ref": "IEEE Access, vol. 13, pp. 69324-69340, 2025"}
{"arxiv_id": "2505.09108v1", "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown Environments", "authors": ["Fernando Cladera", "Zachary Ravichandran", "Jason Hughes", "Varun Murali", "Carlos Nieto-Granda", "M. Ani Hsieh", "George J. Pappas", "Camillo J. Taylor", "Vijay Kumar"], "abstract": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-14 03:33:46", "updated": "2025-05-14 03:33:46", "pdf_url": "http://arxiv.org/pdf/2505.09108v1", "comment": "19 pages, 24 figures, 7 tables. Submitted to T-FR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.09114v1", "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer", "authors": ["Minh Hoang Nguyen", "Linh Le Pham Van", "Thommen George Karimpanal", "Sunil Gupta", "Hung Le"], "abstract": "Decision Transformers (DT) play a crucial role in modern reinforcement\nlearning, leveraging offline datasets to achieve impressive results across\nvarious domains. However, DT requires high-quality, comprehensive data to\nperform optimally. In real-world applications, the lack of training data and\nthe scarcity of optimal behaviours make training on offline datasets\nchallenging, as suboptimal data can hinder performance. To address this, we\npropose the Counterfactual Reasoning Decision Transformer (CRDT), a novel\nframework inspired by counterfactual reasoning. CRDT enhances DT ability to\nreason beyond known data by generating and utilizing counterfactual\nexperiences, enabling improved decision-making in unseen scenarios. Experiments\nacross Atari and D4RL benchmarks, including scenarios with limited data and\naltered dynamics, demonstrate that CRDT outperforms conventional DT approaches.\nAdditionally, reasoning counterfactually allows the DT agent to obtain\nstitching abilities, combining suboptimal trajectories, without architectural\nmodifications. These results highlight the potential of counterfactual\nreasoning to enhance reinforcement learning agents' performance and\ngeneralization capabilities.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-14 03:45:16", "updated": "2025-05-14 03:45:16", "pdf_url": "http://arxiv.org/pdf/2505.09114v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.09115v1", "title": "PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence", "authors": ["Yu Lun Hsu", "Yun-Rung Chou", "Chiao-Ju Chang", "Yu-Cheng Chang", "Zer-Wei Lee", "Rokas Gipi\u0161kis", "Rachel Li", "Chih-Yuan Shih", "Jen-Kuei Peng", "Hsien-Liang Huang", "Jaw-Shiun Tsai", "Mike Y. Chen"], "abstract": "Advance Care Planning (ACP) allows individuals to specify their preferred\nend-of-life life-sustaining treatments before they become incapacitated by\ninjury or terminal illness (e.g., coma, cancer, dementia). While online ACP\noffers high accessibility, it lacks key benefits of clinical consultations,\nincluding personalized value exploration, immediate clarification of decision\nconsequences. To bridge this gap, we conducted two formative studies: 1)\nshadowed and interviewed 3 ACP teams consisting of physicians, nurses, and\nsocial workers (18 patients total), and 2) interviewed 14 users of ACP\nwebsites. Building on these insights, we designed PreCare in collaboration with\n6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed\nto guide users through exploring personal values, gaining ACP knowledge, and\nsupporting informed decision-making. A usability study (n=12) showed that\nPreCare achieved a System Usability Scale (SUS) rating of excellent. A\ncomparative evaluation (n=12) showed that PreCare's AI assistants significantly\nimproved exploration of personal values, knowledge, and decisional confidence,\nand was preferred by 92% of participants.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-14 03:53:35", "updated": "2025-05-14 03:53:35", "pdf_url": "http://arxiv.org/pdf/2505.09115v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.09129v1", "title": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes", "authors": ["Wei Meng"], "abstract": "The deployment of traditional deep learning models in high-risk security\ntasks in an unlabeled, data-non-exploitable video intelligence environment\nfaces significant challenges. In this paper, we propose a lightweight anomaly\ndetection framework based on color features for surveillance video clips in a\nhigh sensitivity tactical mission, aiming to quickly identify and interpret\npotential threat events under resource-constrained and data-sensitive\nconditions. The method fuses unsupervised KMeans clustering with RGB channel\nhistogram modeling to achieve composite detection of structural anomalies and\ncolor mutation signals in key frames. The experiment takes an operation\nsurveillance video occurring in an African country as a research sample, and\nsuccessfully identifies multiple highly anomalous frames related to high-energy\nlight sources, target presence, and reflective interference under the condition\nof no access to the original data. The results show that this method can be\neffectively used for tactical assassination warning, suspicious object\nscreening and environmental drastic change monitoring with strong deployability\nand tactical interpretation value. The study emphasizes the importance of color\nfeatures as low semantic battlefield signal carriers, and its battlefield\nintelligent perception capability will be further extended by combining graph\nneural networks and temporal modeling in the future.", "categories": ["cs.CV", "cs.AI", "es: 68T10, 68T05, 62H35, 68U10", "I.4.9; I.5.1; I.2.10"], "published": "2025-05-14 04:24:37", "updated": "2025-05-14 04:24:37", "pdf_url": "http://arxiv.org/pdf/2505.09129v1", "comment": "17 pages, 3 figures, 3 tables. The paper proposes a lightweight\n  weakly-supervised color intelligence model for tactical video anomaly\n  detection, tested on anonymized African surveillance data", "doi": null, "journal_ref": null}
