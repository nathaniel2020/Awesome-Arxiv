{"arxiv_id": "2505.12581v1", "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Lu\u00eds Carbonera"], "abstract": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-19 00:03:57", "updated": "2025-05-19 00:03:57", "pdf_url": "http://arxiv.org/pdf/2505.12581v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12583v1", "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "abstract": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-19 00:11:42", "updated": "2025-05-19 00:11:42", "pdf_url": "http://arxiv.org/pdf/2505.12583v1", "comment": "Accepted to IJCAI 2025 Survey Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12585v1", "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization", "authors": ["En Yu", "Jie Lu", "Xiaoyu Yang", "Guangquan Zhang", "Zhen Fang"], "abstract": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 00:38:18", "updated": "2025-05-19 00:38:18", "pdf_url": "http://arxiv.org/pdf/2505.12585v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12594v1", "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "abstract": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 01:14:57", "updated": "2025-05-19 01:14:57", "pdf_url": "http://arxiv.org/pdf/2505.12594v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12623v1", "title": "Lightweight and Effective Preference Construction in PIBT for Large-Scale Multi-Agent Pathfinding", "authors": ["Keisuke Okumura", "Hiroki Nagai"], "abstract": "PIBT is a computationally lightweight algorithm that can be applied to a\nvariety of multi-agent pathfinding (MAPF) problems, generating the next\ncollision-free locations of agents given another. Because of its simplicity and\nscalability, it is becoming a popular underlying scheme for recent large-scale\nMAPF methods involving several hundreds or thousands of agents. Vanilla PIBT\nmakes agents behave greedily towards their assigned goals, while agents\ntypically have multiple best actions, since the graph shortest path is not\nalways unique. Consequently, tiebreaking about how to choose between these\nactions significantly affects resulting solutions. This paper studies two\nsimple yet effective techniques for tiebreaking in PIBT, without compromising\nits computational advantage. The first technique allows an agent to\nintelligently dodge another, taking into account whether each action will\nhinder the progress of the next timestep. The second technique is to learn,\nthrough multiple PIBT runs, how an action causes regret in others and to use\nthis information to minimise regret collectively. Our empirical results\ndemonstrate that these techniques can reduce the solution cost of one-shot MAPF\nand improve the throughput of lifelong MAPF. For instance, in densely populated\none-shot cases, the combined use of these tiebreaks achieves improvements of\naround 10-20% in sum-of-costs, without significantly compromising the speed of\na PIBT-based planner.", "categories": ["cs.MA", "cs.AI"], "published": "2025-05-19 02:12:29", "updated": "2025-05-19 02:12:29", "pdf_url": "http://arxiv.org/pdf/2505.12623v1", "comment": "To be presented at SoCS-25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12626v1", "title": "scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data", "authors": ["Ping Xu", "Zhiyuan Ning", "Pengjiang Li", "Wenhao Liu", "Pengyang Wang", "Jiaxu Cui", "Yuanchun Zhou", "Pengfei Wang"], "abstract": "Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell\nclustering playing a key role in identifying cell types and marker genes.\nRecent advances, especially graph neural networks (GNNs)-based methods, have\nsignificantly improved clustering performance. However, the analysis of\nscRNA-seq data remains challenging due to noise, sparsity, and high\ndimensionality. Compounding these challenges, GNNs often suffer from\nover-smoothing, limiting their ability to capture complex biological\ninformation. In response, we propose scSiameseClu, a novel Siamese Clustering\nframework for interpreting single-cell RNA-seq data, comprising of 3 key steps:\n(1) Dual Augmentation Module, which applies biologically informed perturbations\nto the gene expression matrix and cell graph relationships to enhance\nrepresentation robustness; (2) Siamese Fusion Module, which combines\ncross-correlation refinement and adaptive information fusion to capture complex\ncellular relationships while mitigating over-smoothing; and (3) Optimal\nTransport Clustering, which utilizes Sinkhorn distance to efficiently align\ncluster assignments with predefined proportions while maintaining balance.\nComprehensive evaluations on seven real-world datasets demonstrate\nthat~\\methodname~outperforms state-of-the-art methods in single-cell\nclustering, cell type annotation, and cell type classification, providing a\npowerful tool for scRNA-seq data interpretation.", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "published": "2025-05-19 02:17:09", "updated": "2025-05-19 02:17:09", "pdf_url": "http://arxiv.org/pdf/2505.12626v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12630v1", "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "abstract": "All-in-one image restoration aims to recover clear images from various\ndegradation types and levels with a unified model. Nonetheless, the significant\nvariations among degradation types present challenges for training a universal\nmodel, often resulting in task interference, where the gradient update\ndirections of different tasks may diverge due to shared parameters. To address\nthis issue, motivated by the routing strategy, we propose DFPIR, a novel\nall-in-one image restorer that introduces Degradation-aware Feature\nPerturbations(DFP) to adjust the feature space to align with the unified\nparameter space. In this paper, the feature perturbations primarily include\nchannel-wise perturbations and attention-wise perturbations. Specifically,\nchannel-wise perturbations are implemented by shuffling the channels in\nhigh-dimensional space guided by degradation types, while attention-wise\nperturbations are achieved through selective masking in the attention space. To\nachieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)\nto implement these two functions, positioned between the encoding and decoding\nstages of the encoder-decoder architecture. Extensive experimental results\ndemonstrate that DFPIR achieves state-of-the-art performance on several\nall-in-one image restoration tasks including image denoising, image dehazing,\nimage deraining, motion deblurring, and low-light image enhancement. Our codes\nare available at https://github.com/TxpHome/DFPIR.", "categories": ["cs.CV", "cs.AI", "I.4.5"], "published": "2025-05-19 02:37:11", "updated": "2025-05-19 02:37:11", "pdf_url": "http://arxiv.org/pdf/2505.12630v1", "comment": "Accepted to CVPR 2025. 8 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12632v1", "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 02:39:03", "updated": "2025-05-19 02:39:03", "pdf_url": "http://arxiv.org/pdf/2505.12632v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12638v1", "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "abstract": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome.", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.LG"], "published": "2025-05-19 02:45:42", "updated": "2025-05-19 02:45:42", "pdf_url": "http://arxiv.org/pdf/2505.12638v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12641v1", "title": "Single Image Reflection Removal via inter-layer Complementarity", "authors": ["Yue Huang", "Zi'ang Li", "Tianle Hu", "Jie Wen", "Guanbin Li", "Jinglin Zhang", "Guoxu Zhou", "Xiaozhao Fang"], "abstract": "Although dual-stream architectures have achieved remarkable success in single\nimage reflection removal, they fail to fully exploit inter-layer\ncomplementarity in their physical modeling and network design, which limits the\nquality of image separation. To address this fundamental limitation, we propose\ntwo targeted improvements to enhance dual-stream architectures: First, we\nintroduce a novel inter-layer complementarity model where low-frequency\ncomponents extracted from the residual layer interact with the transmission\nlayer through dual-stream architecture to enhance inter-layer complementarity.\nMeanwhile, high-frequency components from the residual layer provide inverse\nmodulation to both streams, improving the detail quality of the transmission\nlayer. Second, we propose an efficient inter-layer complementarity attention\nmechanism which first cross-reorganizes dual streams at the channel level to\nobtain reorganized streams with inter-layer complementary structures, then\nperforms attention computation on the reorganized streams to achieve better\ninter-layer separation, and finally restores the original stream structure for\noutput. Experimental results demonstrate that our method achieves\nstate-of-the-art separation quality on multiple public datasets while\nsignificantly reducing both computational cost and model complexity.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 02:50:15", "updated": "2025-05-19 02:50:15", "pdf_url": "http://arxiv.org/pdf/2505.12641v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12650v1", "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "abstract": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 03:04:50", "updated": "2025-05-19 03:04:50", "pdf_url": "http://arxiv.org/pdf/2505.12650v1", "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12651v1", "title": "$\\texttt{DIAMONDs}$: A Dataset for $\\mathbb{D}$ynamic $\\mathbb{I}$nformation $\\mathbb{A}$nd $\\mathbb{M}$ental modeling $\\mathbb{O}$f $\\mathbb{N}$umeric $\\mathbb{D}$iscussions", "authors": ["Sayontan Ghosh", "Mahnaz Koupaee", "Yash Kumar Lal", "Pegah Alipoormolabashi", "Mohammad Saqib Hasan", "Jun Seok Kang", "Niranjan Balasubramanian"], "abstract": "Understanding multiparty conversations demands robust Theory of Mind (ToM)\ncapabilities, including the ability to track dynamic information, manage\nknowledge asymmetries, and distinguish relevant information across extended\nexchanges. To advance ToM evaluation in such settings, we present a carefully\ndesigned scalable methodology for generating high-quality benchmark\nconversation-question pairs with these characteristics. Using this methodology,\nwe create $\\texttt{DIAMONDs}$, a new conversational QA dataset covering common\nbusiness, financial or other group interactions. In these goal-oriented\nconversations, participants often have to track certain numerical quantities\n(say $\\textit{expected profit}$) of interest that can be derived from other\nvariable quantities (like $\\textit{marketing expenses, expected sales,\nsalary}$, etc.), whose values also change over the course of the conversation.\n$\\texttt{DIAMONDs}$ questions pose simple numerical reasoning problems over\nsuch quantities of interest (e.g., $\\textit{funds required for charity events,\nexpected company profit next quarter}$, etc.) in the context of the information\nexchanged in conversations. This allows for precisely evaluating ToM\ncapabilities for carefully tracking and reasoning over participants' knowledge\nstates.\n  Our evaluation of state-of-the-art language models reveals significant\nchallenges in handling participant-centric reasoning, specifically in\nsituations where participants have false beliefs. Models also struggle with\nconversations containing distractors and show limited ability to identify\nscenarios with insufficient information. These findings highlight current\nmodels' ToM limitations in handling real-world multi-party conversations.", "categories": ["cs.AI"], "published": "2025-05-19 03:05:13", "updated": "2025-05-19 03:05:13", "pdf_url": "http://arxiv.org/pdf/2505.12651v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12654v1", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "abstract": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:08:30", "updated": "2025-05-19 03:08:30", "pdf_url": "http://arxiv.org/pdf/2505.12654v1", "comment": "Accepected by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12655v1", "title": "Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models", "authors": ["Yisheng Zhong", "Yizhu Wen", "Junfeng Guo", "Mehran Kafai", "Heng Huang", "Hanqing Guo", "Zhuangdi Zhu"], "abstract": "Protecting cyber Intellectual Property (IP) such as web content is an\nincreasingly critical concern. The rise of large language models (LLMs) with\nonline retrieval capabilities presents a double-edged sword that enables\nconvenient access to information but often undermines the rights of original\ncontent creators. As users increasingly rely on LLM-generated responses, they\ngradually diminish direct engagement with original information sources,\nsignificantly reducing the incentives for IP creators to contribute, and\nleading to a saturating cyberspace with more AI-generated content. In response,\nwe propose a novel defense framework that empowers web content creators to\nsafeguard their web-based IP from unauthorized LLM real-time extraction by\nleveraging the semantic understanding capability of LLMs themselves. Our method\nfollows principled motivations and effectively addresses an intractable\nblack-box optimization problem. Real-world experiments demonstrated that our\nmethods improve defense success rates from 2.5% to 88.6% on different LLMs,\noutperforming traditional defenses such as configuration-based restrictions.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 03:14:08", "updated": "2025-05-19 03:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12655v1", "comment": "13 pages, 13 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12662v1", "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "abstract": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:25:18", "updated": "2025-05-19 03:25:18", "pdf_url": "http://arxiv.org/pdf/2505.12662v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12664v1", "title": "Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design", "authors": ["Ziqing Xing", "Zhaoyang Zhang", "Zirui Chen", "Hongning Ruan", "Zhaohui Yang"], "abstract": "In this paper, we incorporate physical knowledge into learning-based\nhigh-precision target sensing using the multi-view channel state information\n(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kind\nof multi-view sensing problem can be naturally cast into a conditional\ngeneration framework. To this end, we design a bipartite neural network\narchitecture, the first part of which uses an elaborately designed encoder to\nfuse the latent target features embedded in the multi-view CSI, and then the\nsecond uses them as conditioning inputs of a powerful generative model to guide\nthe target's reconstruction. Specifically, the encoder is designed to capture\nthe physical correlation between the CSI and the target, and also be adaptive\nto the numbers and positions of BS-UE pairs. Therein the view-specific nature\nof CSI is assimilated by introducing a spatial positional embedding scheme,\nwhich exploits the structure of electromagnetic(EM)-wave propagation channels.\nFinally, a conditional diffusion model with a weighted loss is employed to\ngenerate the target's point cloud from the fused features. Extensive numerical\nresults demonstrate that the proposed generative multi-view (Gen-MV) sensing\nframework exhibits excellent flexibility and significant performance\nimprovement on the reconstruction quality of target's shape and EM properties.", "categories": ["eess.SP", "cs.AI", "cs.LG"], "published": "2025-05-19 03:27:24", "updated": "2025-05-19 03:27:24", "pdf_url": "http://arxiv.org/pdf/2505.12664v1", "comment": "submitted to IEEE Transactions on Wireless Communications", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12669v1", "title": "Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment", "authors": ["Abhinaba Roy", "Geeta Puri", "Dorien Herremans"], "abstract": "We present Text2midi-InferAlign, a novel technique for improving symbolic\nmusic generation at inference time. Our method leverages text-to-audio\nalignment and music structural alignment rewards during inference to encourage\nthe generated music to be consistent with the input caption. Specifically, we\nintroduce two objectives scores: a text-audio consistency score that measures\nrhythmic alignment between the generated music and the original text caption,\nand a harmonic consistency score that penalizes generated music containing\nnotes inconsistent with the key. By optimizing these alignment-based objectives\nduring the generation process, our model produces symbolic music that is more\nclosely tied to the input captions, thereby improving the overall quality and\ncoherence of the generated compositions. Our approach can extend any existing\nautoregressive model without requiring further training or fine-tuning. We\nevaluate our work on top of Text2midi - an existing text-to-midi generation\nmodel, demonstrating significant improvements in both objective and subjective\nevaluation metrics.", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68T07", "I.2.1"], "published": "2025-05-19 03:36:06", "updated": "2025-05-19 03:36:06", "pdf_url": "http://arxiv.org/pdf/2505.12669v1", "comment": "7 pages, 1 figure, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12680v1", "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "abstract": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 03:56:05", "updated": "2025-05-19 03:56:05", "pdf_url": "http://arxiv.org/pdf/2505.12680v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12684v1", "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "abstract": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants.", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.SI"], "published": "2025-05-19 04:06:32", "updated": "2025-05-19 04:06:32", "pdf_url": "http://arxiv.org/pdf/2505.12684v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12692v1", "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "abstract": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 04:32:02", "updated": "2025-05-19 04:32:02", "pdf_url": "http://arxiv.org/pdf/2505.12692v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12701v1", "title": "Counterfactual Explanations for Continuous Action Reinforcement Learning", "authors": ["Shuyang Dong", "Shangtong Zhang", "Lu Feng"], "abstract": "Reinforcement Learning (RL) has shown great promise in domains like\nhealthcare and robotics but often struggles with adoption due to its lack of\ninterpretability. Counterfactual explanations, which address \"what if\"\nscenarios, provide a promising avenue for understanding RL decisions but remain\nunderexplored for continuous action spaces. We propose a novel approach for\ngenerating counterfactual explanations in continuous action RL by computing\nalternative action sequences that improve outcomes while minimizing deviations\nfrom the original sequence. Our approach leverages a distance metric for\ncontinuous actions and accounts for constraints such as adhering to predefined\npolicies in specific states. Evaluations in two RL domains, Diabetes Control\nand Lunar Lander, demonstrate the effectiveness, efficiency, and generalization\nof our approach, enabling more interpretable and trustworthy RL applications.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 04:41:54", "updated": "2025-05-19 04:41:54", "pdf_url": "http://arxiv.org/pdf/2505.12701v1", "comment": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12705v1", "title": "DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories", "authors": ["Joel Jang", "Seonghyeon Ye", "Zongyu Lin", "Jiannan Xiang", "Johan Bjorck", "Yu Fang", "Fengyuan Hu", "Spencer Huang", "Kaushil Kundalia", "Yen-Chen Lin", "Loic Magne", "Ajay Mandlekar", "Avnish Narayan", "You Liang Tan", "Guanzhi Wang", "Jing Wang", "Qi Wang", "Yinzhen Xu", "Xiaohui Zeng", "Kaiyuan Zheng", "Ruijie Zheng", "Ming-Yu Liu", "Luke Zettlemoyer", "Dieter Fox", "Jan Kautz", "Scott Reed", "Yuke Zhu", "Linxi Fan"], "abstract": "We introduce DreamGen, a simple yet highly effective 4-stage pipeline for\ntraining robot policies that generalize across behaviors and environments\nthrough neural trajectories - synthetic robot data generated from video world\nmodels. DreamGen leverages state-of-the-art image-to-video generative models,\nadapting them to the target robot embodiment to produce photorealistic\nsynthetic videos of familiar or novel tasks in diverse environments. Since\nthese models generate only videos, we recover pseudo-action sequences using\neither a latent action model or an inverse-dynamics model (IDM). Despite its\nsimplicity, DreamGen unlocks strong behavior and environment generalization: a\nhumanoid robot can perform 22 new behaviors in both seen and unseen\nenvironments, while requiring teleoperation data from only a single\npick-and-place task in one environment. To evaluate the pipeline\nsystematically, we introduce DreamGen Bench, a video generation benchmark that\nshows a strong correlation between benchmark performance and downstream policy\nsuccess. Our work establishes a promising new axis for scaling robot learning\nwell beyond manual data collection.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-19 04:55:39", "updated": "2025-05-19 04:55:39", "pdf_url": "http://arxiv.org/pdf/2505.12705v1", "comment": "See website for videos:\n  https://research.nvidia.com/labs/gear/dreamgen", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12707v1", "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI", "authors": ["Yingchen He", "Christian D. Weilbach", "Martyna E. Wojciechowska", "Yuxuan Zhang", "Frank Wood"], "abstract": "Advances in deep generative modelling have made it increasingly plausible to\ntrain human-level embodied agents. Yet progress has been limited by the absence\nof large-scale, real-time, multi-modal, and socially interactive datasets that\nreflect the sensory-motor complexity of natural environments. To address this,\nwe present PLAICraft, a novel data collection platform and dataset capturing\nmultiplayer Minecraft interactions across five time-aligned modalities: video,\ngame output audio, microphone input audio, mouse, and keyboard actions. Each\nmodality is logged with millisecond time precision, enabling the study of\nsynchronous, embodied behaviour in a rich, open-ended world. The dataset\ncomprises over 10,000 hours of gameplay from more than 10,000 global\nparticipants.\\footnote{We have done a privacy review for the public release of\nan initial 200-hour subset of the dataset, with plans to release most of the\ndataset over time.} Alongside the dataset, we provide an evaluation suite for\nbenchmarking model capabilities in object recognition, spatial awareness,\nlanguage grounding, and long-term memory. PLAICraft opens a path toward\ntraining and evaluating agents that act fluently and purposefully in real time,\npaving the way for truly embodied artificial intelligence.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "published": "2025-05-19 05:00:47", "updated": "2025-05-19 05:00:47", "pdf_url": "http://arxiv.org/pdf/2505.12707v1", "comment": "9 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12711v1", "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining", "authors": ["Qichen Sun", "Zhengrui Guo", "Rui Peng", "Hao Chen", "Jinzhuo Wang"], "abstract": "Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 05:07:34", "updated": "2025-05-19 05:07:34", "pdf_url": "http://arxiv.org/pdf/2505.12711v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12716v1", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "abstract": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 05:16:21", "updated": "2025-05-19 05:16:21", "pdf_url": "http://arxiv.org/pdf/2505.12716v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12731v1", "title": "Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps", "authors": ["Jie Ou", "Jinyu Guo", "Shuaihong Jiang", "Zhaokun Wang", "Libo Qin", "Shunyu Yao", "Wenhong Tian"], "abstract": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.", "categories": ["cs.AI"], "published": "2025-05-19 05:39:38", "updated": "2025-05-19 05:39:38", "pdf_url": "http://arxiv.org/pdf/2505.12731v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12734v1", "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "abstract": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences.", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "published": "2025-05-19 05:47:13", "updated": "2025-05-19 05:47:13", "pdf_url": "http://arxiv.org/pdf/2505.12734v1", "comment": "14 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12737v1", "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning", "authors": ["Hongjoon Ahn", "Heewoong Choi", "Jisu Han", "Taesup Moon"], "abstract": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical\nlearning paradigm where goal-reaching policies are trained from abundant\nunlabeled (reward-free) datasets without additional environment interaction.\nHowever, offline GCRL still struggles with long-horizon tasks, even with recent\nadvances that employ hierarchical policy structures, such as HIQL. By\nidentifying the root cause of this challenge, we observe the following\ninsights: First, performance bottlenecks mainly stem from the high-level\npolicy's inability to generate appropriate subgoals. Second, when learning the\nhigh-level policy in the long-horizon regime, the sign of the advantage signal\nfrequently becomes incorrect. Thus, we argue that improving the value function\nto produce a clear advantage signal for learning the high-level policy is\nessential. In this paper, we propose a simple yet effective solution:\nOption-aware Temporally Abstracted value learning, dubbed OTA, which\nincorporates temporal abstraction into the temporal-difference learning\nprocess. By modifying the value update to be option-aware, the proposed\nlearning scheme contracts the effective horizon length, enabling better\nadvantage estimates even in long-horizon regimes. We experimentally show that\nthe high-level policy extracted using the OTA value function achieves strong\nperformance on complex tasks from OGBench, a recently proposed offline GCRL\nbenchmark, including maze navigation and visual robotic manipulation\nenvironments.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 05:51:11", "updated": "2025-05-19 05:51:11", "pdf_url": "http://arxiv.org/pdf/2505.12737v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12738v1", "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "abstract": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs.", "categories": ["cs.LG", "cs.AI", "cs.SI"], "published": "2025-05-19 05:53:25", "updated": "2025-05-19 05:53:25", "pdf_url": "http://arxiv.org/pdf/2505.12738v1", "comment": "18 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12741v1", "title": "Dense Communication between Language Models", "authors": ["Shiguang Wu", "Yaqing Wang", "Quanming Yao"], "abstract": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility.", "categories": ["cs.AI"], "published": "2025-05-19 05:56:06", "updated": "2025-05-19 05:56:06", "pdf_url": "http://arxiv.org/pdf/2505.12741v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12744v1", "title": "Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation", "authors": ["Weiliang Tang", "Dong Jing", "Jia-Hui Pan", "Zhiwu Lu", "Yun-Hui Liu", "Li Erran Li", "Mingyu Ding", "Chi-Wing Fu"], "abstract": "Recent Large Multimodal Models have demonstrated remarkable reasoning\ncapabilities, especially in solving complex mathematical problems and realizing\naccurate spatial perception. Our key insight is that these emerging abilities\ncan naturally extend to robotic manipulation by enabling LMMs to directly infer\nthe next goal in language via reasoning, rather than relying on a separate\naction head. However, this paradigm meets two main challenges: i) How to make\nLMMs understand the spatial action space, and ii) How to fully exploit the\nreasoning capacity of LMMs in solving these tasks. To tackle the former\nchallenge, we propose a novel task formulation, which inputs the current states\nof object parts and the gripper, and reformulates rotation by a new axis\nrepresentation instead of traditional Euler angles. This representation is more\ncompatible with spatial reasoning and easier to interpret within a unified\nlanguage space. For the latter challenge, we design a pipeline to utilize\ncutting-edge LMMs to generate a small but high-quality reasoning dataset of\nmulti-round dialogues that successfully solve manipulation tasks for supervised\nfine-tuning. Then, we perform reinforcement learning by trial-and-error\ninteractions in simulation to further enhance the model's reasoning abilities\nfor robotic manipulation. Our resulting reasoning model built upon a 7B\nbackbone, named ReasonManip, demonstrates three notable advantages driven by\nits system-2 level reasoning capabilities: i) exceptional generalizability to\nout-of-distribution environments, objects, and tasks; ii) inherent sim-to-real\ntransfer ability enabled by the unified language representation shared across\ndomains; iii) transparent interpretability connecting high-level reasoning and\nlow-level control. Extensive experiments demonstrate the effectiveness of the\nproposed paradigm and its potential to advance LMM-driven robotic manipulation.", "categories": ["cs.AI"], "published": "2025-05-19 06:00:14", "updated": "2025-05-19 06:00:14", "pdf_url": "http://arxiv.org/pdf/2505.12744v1", "comment": "17 pages, 16 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12745v1", "title": "PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization", "authors": ["Dong Kyu Cho", "Inwoo Hwang", "Sanghack Lee"], "abstract": "Data augmentation is a popular tool for single source domain generalization,\nwhich expands the source domain by generating simulated ones, improving\ngeneralization on unseen target domains. In this work, we show that the\nperformance of such augmentation-based methods in the target domains\nuniversally fluctuates during training, posing challenges in model selection\nunder realistic scenarios. We argue that the fluctuation stems from the\ninability of the model to accumulate the knowledge learned from diverse\naugmentations, exacerbating feature distortion during training. Based on this\nobservation, we propose a novel generalization method, coined Parameter-Space\nEnsemble with Entropy Regularization (PEER), that uses a proxy model to learn\nthe augmented data on behalf of the main model. The main model is updated by\naveraging its parameters with the proxy model, progressively accumulating\nknowledge over the training steps. Maximizing the mutual information between\nthe output representations of the two models guides the learning process of the\nproxy model, mitigating feature distortion during training. Experimental\nresults demonstrate the effectiveness of PEER in reducing the OOD performance\nfluctuation and enhancing generalization across various datasets, including\nPACS, Digits, Office-Home, and VLCS. Notably, our method with simple random\naugmentation achieves state-of-the-art performance, surpassing prior approaches\non sDG that utilize complex data augmentation strategies.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 06:01:11", "updated": "2025-05-19 06:01:11", "pdf_url": "http://arxiv.org/pdf/2505.12745v1", "comment": "21 pages, 9 figures, Accepted at CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12746v1", "title": "Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs", "authors": ["Haruka Asanuma", "Naoko Koide-Majima", "Ken Nakamura", "Takato Horii", "Shinji Nishimoto", "Masafumi Oizumi"], "abstract": "Recent studies have revealed that human emotions exhibit a high-dimensional,\ncomplex structure. A full capturing of this complexity requires new approaches,\nas conventional models that disregard high dimensionality risk overlooking key\nnuances of human emotions. Here, we examined the extent to which the latest\ngeneration of rapidly evolving Multimodal Large Language Models (MLLMs) capture\nthese high-dimensional, intricate emotion structures, including capabilities\nand limitations. Specifically, we compared self-reported emotion ratings from\nparticipants watching videos with model-generated estimates (e.g., Gemini or\nGPT). We evaluated performance not only at the individual video level but also\nfrom emotion structures that account for inter-video relationships. At the\nlevel of simple correlation between emotion structures, our results\ndemonstrated strong similarity between human and model-inferred emotion\nstructures. To further explore whether the similarity between humans and models\nis at the signle item level or the coarse-categorical level, we applied Gromov\nWasserstein Optimal Transport. We found that although performance was not\nnecessarily high at the strict, single-item level, performance across video\ncategories that elicit similar emotions was substantial, indicating that the\nmodel could infer human emotional experiences at the category level. Our\nresults suggest that current state-of-the-art MLLMs broadly capture the complex\nhigh-dimensional emotion structures at the category level, as well as their\napparent limitations in accurately capturing entire structures at the\nsingle-item level.", "categories": ["cs.AI", "I.2.7; I.2.10; I.5.1"], "published": "2025-05-19 06:03:22", "updated": "2025-05-19 06:03:22", "pdf_url": "http://arxiv.org/pdf/2505.12746v1", "comment": "25 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12748v1", "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "abstract": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-05-19 06:08:53", "updated": "2025-05-19 06:08:53", "pdf_url": "http://arxiv.org/pdf/2505.12748v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12750v1", "title": "Malware families discovery via Open-Set Recognition on Android manifest permissions", "authors": ["Filippo Leveni", "Matteo Mistura", "Francesco Iubatti", "Carmine Giangregorio", "Nicol\u00f2 Pastore", "Cesare Alippi", "Giacomo Boracchi"], "abstract": "Malware are malicious programs that are grouped into families based on their\npenetration technique, source code, and other characteristics. Classifying\nmalware programs into their respective families is essential for building\neffective defenses against cyber threats. Machine learning models have a huge\npotential in malware detection on mobile devices, as malware families can be\nrecognized by classifying permission data extracted from Android manifest\nfiles. Still, the malware classification task is challenging due to the\nhigh-dimensional nature of permission data and the limited availability of\ntraining samples. In particular, the steady emergence of new malware families\nmakes it impossible to acquire a comprehensive training set covering all the\nmalware classes. In this work, we present a malware classification system that,\non top of classifying known malware, detects new ones. In particular, we\ncombine an open-set recognition technique developed within the computer vision\ncommunity, namely MaxLogit, with a tree-based Gradient Boosting classifier,\nwhich is particularly effective in classifying high-dimensional data. Our\nsolution turns out to be very practical, as it can be seamlessly employed in a\nstandard classification workflow, and efficient, as it adds minimal\ncomputational overhead. Experiments on public and proprietary datasets\ndemonstrate the potential of our solution, which has been deployed in a\nbusiness environment.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "published": "2025-05-19 06:19:54", "updated": "2025-05-19 06:19:54", "pdf_url": "http://arxiv.org/pdf/2505.12750v1", "comment": "Submitted to European Conference on Artificial Intelligence (ECAI\n  2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12751v1", "title": "Structure-based Anomaly Detection and Clustering", "authors": ["Filippo Leveni"], "abstract": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system.", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "published": "2025-05-19 06:20:00", "updated": "2025-05-19 06:20:00", "pdf_url": "http://arxiv.org/pdf/2505.12751v1", "comment": "Doctoral dissertation at Politecnico di Milano", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12761v1", "title": "Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding", "authors": ["Donghwa Shin", "Edwin Zhang"], "abstract": "Transformers have recently gained popularity in time series forecasting due\nto their ability to capture long-term dependencies. However, many existing\nmodels focus only on capturing temporal dependencies while omitting intricate\nrelationships between variables. Recent models have tried tackling this by\nexplicitly modeling both cross-time and cross-variate dependencies through a\nsequential or unified attention mechanism, but they are entirely channel\ndependent (CD) across all layers, making them potentially susceptible to\noverfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),\na lightweight CD module that injects cross-variate context into\nchannel-independent (CI) models by simply modifying the patch embedding\nprocess. We achieve this by adding a learnable positional encoding and a\nlightweight router-attention block to the vanilla patch embedding layer. We\nthen integrate CVPE into Time-LLM, a multimodal CI forecasting model, to\ndemonstrate its effectiveness in capturing cross-variate dependencies and\nenhance the CI model's performance. Extensive experimental results on seven\nreal-world datasets show that our enhanced Time-LLM outperforms the original\nbaseline model simply by incorporating the CVPE module, with no other changes.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 06:41:14", "updated": "2025-05-19 06:41:14", "pdf_url": "http://arxiv.org/pdf/2505.12761v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12762v1", "title": "IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment", "authors": ["Chenlin Ming", "Chendi Qu", "Mengzhang Cai", "Qizhi Pei", "Zhuoshi Pan", "Yu Li", "Xiaoming Duan", "Lijun Wu", "Conghui He"], "abstract": "Large Language Models (LLMs) have achieved impressive performance through\nSupervised Fine-tuning (SFT) on diverse instructional datasets. When training\non multiple capabilities simultaneously, the mixture training dataset, governed\nby volumes of data from different domains, is a critical factor that directly\nimpacts the final model's performance. Unlike many studies that focus on\nenhancing the quality of training datasets through data selection methods, few\nworks explore the intricate relationship between the compositional quantity of\nmixture training datasets and the emergent capabilities of LLMs. Given the\navailability of a high-quality multi-domain training dataset, understanding the\nimpact of data from each domain on the model's overall capabilities is crucial\nfor preparing SFT data and training a well-balanced model that performs\neffectively across diverse domains. In this work, we introduce IDEAL, an\ninnovative data equilibrium adaptation framework designed to effectively\noptimize volumes of data from different domains within mixture SFT datasets,\nthereby enhancing the model's alignment and performance across multiple\ncapabilities. IDEAL employs a gradient-based approach to iteratively refine the\ntraining data distribution, dynamically adjusting the volumes of\ndomain-specific data based on their impact on downstream task performance. By\nleveraging this adaptive mechanism, IDEAL ensures a balanced dataset\ncomposition, enabling the model to achieve robust generalization and consistent\nproficiency across diverse tasks. Experiments across different capabilities\ndemonstrate that IDEAL outperforms conventional uniform data allocation\nstrategies, achieving a comprehensive improvement of approximately 7% in\nmulti-task evaluation scores.", "categories": ["cs.AI"], "published": "2025-05-19 06:42:44", "updated": "2025-05-19 06:42:44", "pdf_url": "http://arxiv.org/pdf/2505.12762v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12763v1", "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "abstract": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 06:43:08", "updated": "2025-05-19 06:43:08", "pdf_url": "http://arxiv.org/pdf/2505.12763v1", "comment": "Accepted to ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12767v1", "title": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates", "authors": ["Danqing Chen", "Tobias Ladner", "Ahmed Rayen Mhadhbi", "Matthias Althoff"], "abstract": "As large language models become integral to high-stakes applications,\nensuring their robustness and fairness is critical. Despite their success,\nlarge language models remain vulnerable to adversarial attacks, where small\nperturbations, such as synonym substitutions, can alter model predictions,\nposing risks in fairness-critical areas, such as gender bias mitigation, and\nsafety-critical areas, such as toxicity detection. While formal verification\nhas been explored for neural networks, its application to large language models\nremains limited. This work presents a holistic verification framework to\ncertify the robustness of transformer-based language models, with a focus on\nensuring gender fairness and consistent outputs across different gender-related\nterms. Furthermore, we extend this methodology to toxicity detection, offering\nformal guarantees that adversarially manipulated toxic inputs are consistently\ndetected and appropriately censored, thereby ensuring the reliability of\nmoderation systems. By formalizing robustness within the embedding space, this\nwork strengthens the reliability of language models in ethical AI deployment\nand content moderation.", "categories": ["cs.AI"], "published": "2025-05-19 06:46:17", "updated": "2025-05-19 06:46:17", "pdf_url": "http://arxiv.org/pdf/2505.12767v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12774v1", "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "abstract": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation.", "categories": ["cs.GR", "cs.AI", "cs.CV"], "published": "2025-05-19 07:02:12", "updated": "2025-05-19 07:02:12", "pdf_url": "http://arxiv.org/pdf/2505.12774v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12781v1", "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:10:42", "updated": "2025-05-19 07:10:42", "pdf_url": "http://arxiv.org/pdf/2505.12781v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12788v1", "title": "Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs", "authors": ["Zhongni Hou", "Miao Su", "Xiaolong Jin", "Zixuan Li", "Long Bai", "Jiafeng Guo", "Xueqi Cheng"], "abstract": "Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of\n(subject, predicate, object, timestamp) to describe temporal facts, have\nattracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditional\nTKGs by utilizing n-tuples to incorporate auxiliary elements alongside core\nelements (i.e., subject, predicate, and object) of facts, so as to represent\nthem in a more fine-grained manner. Reasoning over N-TKGs aims to predict\npotential future facts based on historical ones. However, existing N-TKG\nreasoning methods often lack explainability due to their black-box nature.\nTherefore, we introduce a new Reinforcement Learning-based method, named\nMT-Path, which leverages the temporal information to traverse historical\nn-tuples and construct a temporal reasoning path. Specifically, in order to\nintegrate the information encapsulated within n-tuples, i.e., the\nentity-irrelevant information within the predicate, the information about core\nelements, and the complete information about the entire n-tuples, MT-Path\nutilizes a mixture policy-driven action selector, which bases on three\nlow-level policies, namely, the predicate-focused policy, the\ncore-element-focused policy and the whole-fact-focused policy. Further, MT-Path\nutilizes an auxiliary element-aware GCN to capture the rich semantic\ndependencies among facts, thereby enabling the agent to gain a deep\nunderstanding of each n-tuple. Experimental results demonstrate the\neffectiveness and the explainability of MT-Path.", "categories": ["cs.AI"], "published": "2025-05-19 07:20:33", "updated": "2025-05-19 07:20:33", "pdf_url": "http://arxiv.org/pdf/2505.12788v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12795v1", "title": "FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities", "authors": ["Shibo Hong", "Jiahao Ying", "Haiyuan Liang", "Mengdi Zhang", "Jun Kuang", "Jiazheng Zhang", "Yixin Cao"], "abstract": "Evaluating the open-ended outputs of large language models (LLMs) has become\na bottleneck as model capabilities, task diversity, and modality coverage\nrapidly expand. Existing \"LLM-as-a-Judge\" evaluators are typically narrow in a\nfew tasks, aspects, or modalities, and easily suffer from low consistency. In\nthis paper, we argue that explicit, fine-grained aspect specification is the\nkey to both generalizability and objectivity in automated evaluation. To do so,\nwe introduce a hierarchical aspect taxonomy spanning 112 aspects that unifies\nevaluation across four representative settings - Natural Language Generation,\nImage Understanding, Image Generation, and Interleaved Text-and-Image\nGeneration. Building on this taxonomy, we create FRAbench, a benchmark\ncomprising 60.4k pairwise samples with 325k aspect-level labels obtained from a\ncombination of human and LLM annotations. FRAbench provides the first\nlarge-scale, multi-modal resource for training and meta-evaluating fine-grained\nLMM judges. Leveraging FRAbench, we develop GenEval, a fine-grained evaluator\ngeneralizable across tasks and modalities. Experiments show that GenEval (i)\nattains high agreement with GPT-4o and expert annotators, (ii) transfers\nrobustly to unseen tasks and modalities, and (iii) reveals systematic\nweaknesses of current LMMs on evaluation.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-19 07:29:26", "updated": "2025-05-19 07:29:26", "pdf_url": "http://arxiv.org/pdf/2505.12795v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12800v1", "title": "OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching", "authors": ["Hieu-Nghia Huynh-Nguyen", "Ngoc Son Nguyen", "Huynh Nguyen Dang", "Thieu Vo", "Truong-Son Hy", "Van Nguyen"], "abstract": "Text-to-speech (TTS) systems have seen significant advancements in recent\nyears, driven by improvements in deep learning and neural network\narchitectures. Viewing the output speech as a data distribution, previous\napproaches often employ traditional speech representations, such as waveforms\nor spectrograms, within the Flow Matching framework. However, these methods\nhave limitations, including overlooking various speech attributes and incurring\nhigh computational costs due to additional constraints introduced during\ntraining. To address these challenges, we introduce OZSpeech, the first TTS\nmethod to explore optimal transport conditional flow matching with one-step\nsampling and a learned prior as the condition, effectively disregarding\npreceding states and reducing the number of sampling steps. Our approach\noperates on disentangled, factorized components of speech in token format,\nenabling accurate modeling of each speech attribute, which enhances the TTS\nsystem's ability to precisely clone the prompt speech. Experimental results\nshow that our method achieves promising performance over existing methods in\ncontent accuracy, naturalness, prosody generation, and speaker style\npreservation. Audio samples are available at our demo page\nhttps://ozspeech.github.io/OZSpeech_Web/.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 07:31:55", "updated": "2025-05-19 07:31:55", "pdf_url": "http://arxiv.org/pdf/2505.12800v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12805v1", "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA", "authors": ["Seanie Lee", "Sangwoo Park", "Dong Bok Lee", "Dominik Wagner", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "abstract": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable\nlow-rank matrices into frozen pre-trained weights, is widely used for efficient\nfine-tuning of language models in federated learning (FL). However, when\ncombined with differentially private stochastic gradient descent (DP-SGD), LoRA\nfaces substantial noise amplification: DP-SGD perturbs per-sample gradients,\nand the matrix multiplication of the LoRA update ($BA$) intensifies this\neffect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model\nexpressiveness, often resulting in suboptimal adaptation. To address this, we\npropose FedSVD, a simple yet effective method that introduces a global\nreparameterization based on singular value decomposition (SVD). In our\napproach, each client optimizes only the $B$ matrix and transmits it to the\nserver. The server aggregates the $B$ matrices, computes the product $BA$ using\nthe previous $A$, and refactorizes the result via SVD. This yields a new\nadaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an\nupdated $B$ containing the remaining SVD components. This reparameterization\navoids quadratic noise amplification, while allowing $A$ to better capture the\nprincipal directions of the aggregate updates. Moreover, the orthonormal\nstructure of $A$ bounds the gradient norms of $B$ and preserves more signal\nunder DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD\nconsistently improves stability and performance across a variety of privacy\nsettings and benchmarks, outperforming relevant baselines under both private\nand non-private regimes.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 07:32:56", "updated": "2025-05-19 07:32:56", "pdf_url": "http://arxiv.org/pdf/2505.12805v1", "comment": "preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12811v1", "title": "Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning", "authors": ["Wei-Chen Liao", "Ti-Rong Wu", "I-Chen Wu"], "abstract": "Multi-agent reinforcement Learning (MARL) is often challenged by the sight\nrange dilemma, where agents either receive insufficient or excessive\ninformation from their environment. In this paper, we propose a novel method,\ncalled Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes\nan Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight\nrange during training. Experiment results show several advantages of using DSR.\nFirst, we demonstrate using DSR achieves better performance in three common\nMARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse\n(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show\nthat DSR consistently improves performance across multiple MARL algorithms,\nincluding QMIX and MAPPO. Third, DSR offers suitable sight ranges for different\ntraining steps, thereby accelerating the training process. Finally, DSR\nprovides additional interpretability by indicating the optimal sight range used\nduring training. Unlike existing methods that rely on global information or\ncommunication mechanisms, our approach operates solely based on the individual\nsight ranges of agents. This approach offers a practical and efficient solution\nto the sight range dilemma, making it broadly applicable to real-world complex\nenvironments.", "categories": ["cs.MA", "cs.AI", "cs.LG"], "published": "2025-05-19 07:40:42", "updated": "2025-05-19 07:40:42", "pdf_url": "http://arxiv.org/pdf/2505.12811v1", "comment": "Accepted at AAMAS 2025. The compiled PDF includes the appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12814v1", "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "abstract": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:45:09", "updated": "2025-05-19 07:45:09", "pdf_url": "http://arxiv.org/pdf/2505.12814v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12815v1", "title": "Learning in Chaos: Efficient Autoscaling and Self-healing for Distributed Training at the Edge", "authors": ["Wenjiao Feng", "Rongxing Xiao", "Zonghang Li", "Hongfang Yu", "Gang Sun", "Long Luo", "Mohsen Guizani", "Qirong Ho"], "abstract": "Frequent node and link changes in edge AI clusters disrupt distributed\ntraining, while traditional checkpoint-based recovery and cloud-centric\nautoscaling are too slow for scale-out and ill-suited to chaotic and\nself-governed edge. This paper proposes Chaos, a resilient and scalable edge\ndistributed training system with built-in self-healing and autoscaling. It\nspeeds up scale-out by using multi-neighbor replication with fast shard\nscheduling, allowing a new node to pull the latest training state from nearby\nneighbors in parallel while balancing the traffic load between them. It also\nuses a cluster monitor to track resource and topology changes to assist\nscheduler decisions, and handles scaling events through peer negotiation\nprotocols, enabling fully self-governed autoscaling without a central admin.\nExtensive experiments show that Chaos consistently achieves much lower\nscale-out delays than Pollux, EDL, and Autoscaling, and handles scale-in,\nconnect-link, and disconnect-link events within 1 millisecond, making it\nsmoother to handle node joins, exits, and failures. It also delivers the lowest\nidle time, showing superior resource use and scalability as the cluster grows.", "categories": ["cs.DC", "cs.AI", "68T99", "I.2.11"], "published": "2025-05-19 07:52:17", "updated": "2025-05-19 07:52:17", "pdf_url": "http://arxiv.org/pdf/2505.12815v1", "comment": "13 pages, 16 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12821v1", "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "abstract": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:03:38", "updated": "2025-05-19 08:03:38", "pdf_url": "http://arxiv.org/pdf/2505.12821v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12822v1", "title": "Emergent Specialization: Rare Token Neurons in Language Models", "authors": ["Jing Liu", "Haozheng Wang", "Yueheng Li"], "abstract": "Large language models struggle with representing and generating rare tokens\ndespite their importance in specialized domains. In this study, we identify\nneuron structures with exceptionally strong influence on language model's\nprediction of rare tokens, termed as rare token neurons, and investigate the\nmechanism for their emergence and behavior. These neurons exhibit a\ncharacteristic three-phase organization (plateau, power-law, and rapid decay)\nthat emerges dynamically during training, evolving from a homogeneous initial\nstate to a functionally differentiated architecture. In the activation space,\nrare token neurons form a coordinated subnetwork that selectively co-activates\nwhile avoiding co-activation with other neurons. This functional specialization\npotentially correlates with the development of heavy-tailed weight\ndistributions, suggesting a statistical mechanical basis for emergent\nspecialization.", "categories": ["cs.AI"], "published": "2025-05-19 08:05:13", "updated": "2025-05-19 08:05:13", "pdf_url": "http://arxiv.org/pdf/2505.12822v1", "comment": "9 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12833v1", "title": "Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs", "authors": ["Zhuo Yang", "Lingli Ge", "Dong Han", "Tianfan Fu", "Yuqiang Li"], "abstract": "Many real-world scientific and industrial applications require the\noptimization of expensive black-box functions. Bayesian Optimization (BO)\nprovides an effective framework for such problems. However, traditional BO\nmethods are prone to get trapped in local optima and often lack interpretable\ninsights. To address this issue, this paper designs Reasoning BO, a novel\nframework that leverages reasoning models to guide the sampling process in BO\nwhile incorporating multi-agent systems and knowledge graphs for online\nknowledge accumulation. By integrating the reasoning and contextual\nunderstanding capabilities of Large Language Models (LLMs), we can provide\nstrong guidance to enhance the BO process. As the optimization progresses,\nReasoning BO provides real-time sampling recommendations along with critical\ninsights grounded in plausible scientific theories, aiding in the discovery of\nsuperior solutions within the search space. We systematically evaluate our\napproach across 10 diverse tasks encompassing synthetic mathematical functions\nand complex real-world applications. The framework demonstrates its capability\nto progressively refine sampling strategies through real-time insights and\nhypothesis evolution, effectively identifying higher-performing regions of the\nsearch space for focused exploration. This process highlights the powerful\nreasoning and context-learning abilities of LLMs in optimization scenarios. For\nexample, in the Direct Arylation task, our method increased the yield to 60.7%,\nwhereas traditional BO achieved only a 25.2% yield. Furthermore, our\ninvestigation reveals that smaller LLMs, when fine-tuned through reinforcement\nlearning, can attain comparable performance to their larger counterparts. This\nenhanced reasoning capability paves the way for more efficient automated\nscientific experimentation while maintaining computational feasibility.", "categories": ["cs.AI"], "published": "2025-05-19 08:20:40", "updated": "2025-05-19 08:20:40", "pdf_url": "http://arxiv.org/pdf/2505.12833v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12837v1", "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "abstract": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:25:21", "updated": "2025-05-19 08:25:21", "pdf_url": "http://arxiv.org/pdf/2505.12837v1", "comment": "20 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12843v1", "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF", "authors": ["Kangwen Zhao", "Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "abstract": "Reinforcement Learning from Human Feedback relies on reward models to align\nlarge language models with human preferences. However, RLHF often suffers from\nreward hacking, wherein policy learning exploits flaws in the trained reward\nmodel to maximize reward scores without genuinely aligning with human\npreferences. A significant example of such reward hacking is length bias, where\nreward models usually favor longer responses irrespective of actual response\nquality. Previous works on length bias have notable limitations, these\napproaches either mitigate bias without characterizing the bias form, or simply\nassume a linear length-reward relation. To accurately model the intricate\nnature of length bias and facilitate more effective bias mitigation, we propose\nFiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a\nframework that autonomously learns and corrects underlying bias patterns. Our\napproach consists of three stages: First, we train a standard reward model\nwhich inherently contains length bias. Next, we deploy a lightweight fitting\nmodel to explicitly capture the non-linear relation between length and reward.\nFinally, we incorporate this learned relation into the reward model to debias.\nExperimental results demonstrate that FiMi-RM achieves a more balanced\nlength-reward distribution. Furthermore, when applied to alignment algorithms,\nour debiased reward model improves length-controlled win rate and reduces\nverbosity without compromising its performance.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 08:29:28", "updated": "2025-05-19 08:29:28", "pdf_url": "http://arxiv.org/pdf/2505.12843v1", "comment": "Due to the word limit for arXiv abstract, the abstract here has been\n  abridged compared to the one in the PDF", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12844v1", "title": "AGI-Elo: How Far Are We From Mastering A Task?", "authors": ["Shuo Sun", "Yimin Zhao", "Christina Dao Wen Lee", "Jiawei Sun", "Chengran Yuan", "Zefan Huang", "Dongen Li", "Justin KW Yeoh", "Alok Prakash", "Thomas W. Malone", "Marcelo H. Ang Jr"], "abstract": "As the field progresses toward Artificial General Intelligence (AGI), there\nis a pressing need for more comprehensive and insightful evaluation frameworks\nthat go beyond aggregate performance metrics. This paper introduces a unified\nrating system that jointly models the difficulty of individual test cases and\nthe competency of AI models (or humans) across vision, language, and action\ndomains. Unlike existing metrics that focus solely on models, our approach\nallows for fine-grained, difficulty-aware evaluations through competitive\ninteractions between models and tasks, capturing both the long-tail\ndistribution of real-world challenges and the competency gap between current\nmodels and full task mastery. We validate the generalizability and robustness\nof our system through extensive experiments on multiple established datasets\nand models across distinct AGI domains. The resulting rating distributions\noffer novel perspectives and interpretable insights into task difficulty, model\nprogression, and the outstanding challenges that remain on the path to\nachieving full AGI task mastery.", "categories": ["cs.AI", "cs.RO"], "published": "2025-05-19 08:30:13", "updated": "2025-05-19 08:30:13", "pdf_url": "http://arxiv.org/pdf/2505.12844v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12845v1", "title": "Multi-Level Aware Preference Learning: Enhancing RLHF for Complex Multi-Instruction Tasks", "authors": ["Ruopei Sun", "Jianfeng Cai", "Jinhua Zhu", "Kangwen Zhao", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "abstract": "RLHF has emerged as a predominant approach for aligning artificial\nintelligence systems with human preferences, demonstrating exceptional and\nmeasurable efficacy in instruction following tasks; however, it exhibits\ninsufficient compliance capabilities when confronted with complex\nmulti-instruction tasks. Conventional approaches rely heavily on human\nannotation or more sophisticated large language models, thereby introducing\nsubstantial resource expenditure or potential bias concerns. Meanwhile,\nalternative synthetic methods that augment standard preference datasets often\ncompromise the model's semantic quality. Our research identifies a critical\noversight in existing techniques, which predominantly focus on comparing\nresponses while neglecting valuable latent signals embedded within prompt\ninputs, and which only focus on preference disparities at the intra-sample\nlevel, while neglecting to account for the inter-sample level preference\ndifferentials that exist among preference data. To leverage these previously\nneglected indicators, we propose a novel Multi-level Aware Preference Learning\n(MAPL) framework, capable of enhancing multi-instruction capabilities.\nSpecifically, for any given response in original preference data pairs, we\nconstruct varied prompts with a preference relation under different conditions,\nin order to learn intra-sample level preference disparities. Furthermore, for\nany given original preference pair, we synthesize multi-instruction preference\npairs to capture preference discrepancies at the inter-sample level. Building\non the two datasets constructed above, we consequently devise two sophisticated\ntraining objective functions. Subsequently, our framework integrates seamlessly\ninto both Reward Modeling and Direct Preference Optimization paradigms. Through\nrigorous evaluation across multiple benchmarks, we empirically validate the\nefficacy of our framework.", "categories": ["cs.AI"], "published": "2025-05-19 08:33:11", "updated": "2025-05-19 08:33:11", "pdf_url": "http://arxiv.org/pdf/2505.12845v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12851v1", "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting", "authors": ["Yanhua Wen", "Lu Ai", "Gang Liu", "Chuang Li", "Jianhao Wei"], "abstract": "Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 08:39:07", "updated": "2025-05-19 08:39:07", "pdf_url": "http://arxiv.org/pdf/2505.12851v1", "comment": "14 pages, 5 figures, BlockSys2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12863v1", "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "abstract": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation.", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "published": "2025-05-19 08:46:45", "updated": "2025-05-19 08:46:45", "pdf_url": "http://arxiv.org/pdf/2505.12863v1", "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12864v1", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr\u00fcwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "abstract": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "published": "2025-05-19 08:48:12", "updated": "2025-05-19 08:48:12", "pdf_url": "http://arxiv.org/pdf/2505.12864v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12869v1", "title": "Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption", "authors": ["Koki Wakiyama", "Tomohiro I", "Hiroshi Sakamoto"], "abstract": "Feature selection is a technique that extracts a meaningful subset from a set\nof features in training data. When the training data is large-scale,\nappropriate feature selection enables the removal of redundant features, which\ncan improve generalization performance, accelerate the training process, and\nenhance the interpretability of the model. This study proposes a\nprivacy-preserving computation model for feature selection. Generally, when the\ndata owner and analyst are the same, there is no need to conceal the private\ninformation. However, when they are different parties or when multiple owners\nexist, an appropriate privacy-preserving framework is required. Although\nvarious private feature selection algorithms, they all require two or more\ncomputing parties and do not guarantee security in environments where no\nexternal party can be fully trusted. To address this issue, we propose the\nfirst outsourcing algorithm for feature selection using fully homomorphic\nencryption. Compared to a prior two-party algorithm, our result improves the\ntime and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n\ndenote the number of features and data samples, respectively. We also\nimplemented the proposed algorithm and conducted comparative experiments with\nthe naive one. The experimental result shows the efficiency of our method even\nwith small datasets.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 08:55:56", "updated": "2025-05-19 08:55:56", "pdf_url": "http://arxiv.org/pdf/2505.12869v1", "comment": "14 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12871v1", "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-19 08:57:08", "updated": "2025-05-19 08:57:08", "pdf_url": "http://arxiv.org/pdf/2505.12871v1", "comment": "To appear at ICML 25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12872v1", "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging", "authors": ["Maytus Piriyajitakonkij", "Rujikorn Charakorn", "Weicheng Tao", "Wei Pan", "Mingfei Sun", "Cheston Tan", "Mengmi Zhang"], "abstract": "Early cavemen relied on gestures, vocalizations, and simple signals to\ncoordinate, plan, avoid predators, and share resources. Today, humans\ncollaborate using complex languages to achieve remarkable results. What drives\nthis evolution in communication? How does language emerge, adapt, and become\nvital for teamwork? Understanding the origins of language remains a challenge.\nA leading hypothesis in linguistics and anthropology posits that language\nevolved to meet the ecological and social demands of early human cooperation.\nLanguage did not arise in isolation, but through shared survival goals.\nInspired by this view, we investigate the emergence of language in multi-agent\nForaging Games. These environments are designed to reflect the cognitive and\necological constraints believed to have influenced the evolution of\ncommunication. Agents operate in a shared grid world with only partial\nknowledge about other agents and the environment, and must coordinate to\ncomplete games like picking up high-value targets or executing temporally\nordered actions. Using end-to-end deep reinforcement learning, agents learn\nboth actions and communication strategies from scratch. We find that agents\ndevelop communication protocols with hallmark features of natural language:\narbitrariness, interchangeability, displacement, cultural transmission, and\ncompositionality. We quantify each property and analyze how different factors,\nsuch as population size and temporal dependencies, shape specific aspects of\nthe emergent language. Our framework serves as a platform for studying how\nlanguage can evolve from partial observability, temporal reasoning, and\ncooperative goals in embodied multi-agent settings. We will release all data,\ncode, and models publicly.", "categories": ["cs.AI", "cs.LG", "cs.MA"], "published": "2025-05-19 08:57:30", "updated": "2025-05-19 08:57:30", "pdf_url": "http://arxiv.org/pdf/2505.12872v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12880v1", "title": "AdS-GNN -- a Conformally Equivariant Graph Neural Network", "authors": ["Maksim Zhdanov", "Nabil Iqbal", "Erik Bekkers", "Patrick Forr\u00e9"], "abstract": "Conformal symmetries, i.e.\\ coordinate transformations that preserve angles,\nplay a key role in many fields, including physics, mathematics, computer vision\nand (geometric) machine learning. Here we build a neural network that is\nequivariant under general conformal transformations. To achieve this, we lift\ndata from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to\nexploit a known correspondence between conformal transformations of flat space\nand isometric transformations on the AdS space. We then build upon the fact\nthat such isometric transformations have been extensively studied on general\ngeometries in the geometric deep learning literature. We employ message-passing\nlayers conditioned on the proper distance, yielding a computationally efficient\nframework. We validate our model on tasks from computer vision and statistical\nphysics, demonstrating strong performance, improved generalization capacities,\nand the ability to extract conformal data such as scaling dimensions from the\ntrained network.", "categories": ["cs.LG", "cs.AI", "hep-th"], "published": "2025-05-19 09:08:52", "updated": "2025-05-19 09:08:52", "pdf_url": "http://arxiv.org/pdf/2505.12880v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12882v1", "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "abstract": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 09:10:55", "updated": "2025-05-19 09:10:55", "pdf_url": "http://arxiv.org/pdf/2505.12882v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12884v1", "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "abstract": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-19 09:11:54", "updated": "2025-05-19 09:11:54", "pdf_url": "http://arxiv.org/pdf/2505.12884v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12886v1", "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "categories": ["cs.AI", "cs.CL", "cs.CY"], "published": "2025-05-19 09:16:40", "updated": "2025-05-19 09:16:40", "pdf_url": "http://arxiv.org/pdf/2505.12886v1", "comment": "25 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12891v1", "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "abstract": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 09:22:02", "updated": "2025-05-19 09:22:02", "pdf_url": "http://arxiv.org/pdf/2505.12891v1", "comment": "First version. There are still some examples to be added into the\n  appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12894v1", "title": "HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Keke Tang", "Chao Gao", "Zhen Wang"], "abstract": "Hypergraphs offer superior modeling capabilities for social networks,\nparticularly in capturing group phenomena that extend beyond pairwise\ninteractions in rumor propagation. Existing approaches in rumor source\ndetection predominantly focus on dyadic interactions, which inadequately\naddress the complexity of more intricate relational structures. In this study,\nwe present a novel approach for Source Detection in Hypergraphs (HyperDet) via\nInteractive Relationship Construction and Feature-rich Attention Fusion.\nSpecifically, our methodology employs an Interactive Relationship Construction\nmodule to accurately model both the static topology and dynamic interactions\namong users, followed by the Feature-rich Attention Fusion module, which\nautonomously learns node features and discriminates between nodes using a\nself-attention mechanism, thereby effectively learning node representations\nunder the framework of accurately modeled higher-order relationships. Extensive\nexperimental validation confirms the efficacy of our HyperDet approach,\nshowcasing its superiority relative to current state-of-the-art methods.", "categories": ["cs.SI", "cs.AI"], "published": "2025-05-19 09:27:46", "updated": "2025-05-19 09:27:46", "pdf_url": "http://arxiv.org/pdf/2505.12894v1", "comment": "Accepted by IJCAI25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12900v1", "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "abstract": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation.", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "published": "2025-05-19 09:35:58", "updated": "2025-05-19 09:35:58", "pdf_url": "http://arxiv.org/pdf/2505.12900v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12903v1", "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach", "authors": ["Shiao Wang", "Xiao Wang", "Liye Jin", "Bo Jiang", "Lin Zhu", "Lan Chen", "Yonghong Tian", "Bin Luo"], "abstract": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras\ncoupled with computationally intensive deep neural network architectures to\nachieve effective tracking. However, such frame-based methods inherently face\nchallenges in achieving low-latency performance and often fail in\nresource-constrained environments. Visual object tracking using bio-inspired\nevent cameras has emerged as a promising research direction in recent years,\noffering distinct advantages for low-latency applications. In this paper, we\npropose a novel Slow-Fast Tracking paradigm that flexibly adapts to different\noperational requirements, termed SFTrack. The proposed framework supports two\ncomplementary modes, i.e., a high-precision slow tracker for scenarios with\nsufficient computational resources, and an efficient fast tracker tailored for\nlatency-aware, resource-constrained environments. Specifically, our framework\nfirst performs graph-based representation learning from\nhigh-temporal-resolution event streams, and then integrates the learned\ngraph-structured information into two FlashAttention-based vision backbones,\nyielding the slow and fast trackers, respectively. The fast tracker achieves\nlow latency through a lightweight network design and by producing multiple\nbounding box outputs in a single forward pass. Finally, we seamlessly combine\nboth trackers via supervised fine-tuning and further enhance the fast tracker's\nperformance through a knowledge distillation strategy. Extensive experiments on\npublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate the\neffectiveness and efficiency of our proposed method across different real-world\nscenarios. The source code has been released on\nhttps://github.com/Event-AHU/SlowFast_Event_Track.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 09:37:23", "updated": "2025-05-19 09:37:23", "pdf_url": "http://arxiv.org/pdf/2505.12903v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12904v1", "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning", "authors": ["Hilde I. Hummel", "Arwin Gansekoele", "Sandjai Bhulai", "Rob van der Mei"], "abstract": "The increasing level of sound pollution in marine environments poses an\nincreased threat to ocean health, making it crucial to monitor underwater\nnoise. By monitoring this noise, the sources responsible for this pollution can\nbe mapped. Monitoring is performed by passively listening to these sounds. This\ngenerates a large amount of data records, capturing a mix of sound sources such\nas ship activities and marine mammal vocalizations. Although machine learning\noffers a promising solution for automatic sound classification, current\nstate-of-the-art methods implement supervised learning. This requires a large\namount of high-quality labeled data that is not publicly available. In\ncontrast, a massive amount of lower-quality unlabeled data is publicly\navailable, offering the opportunity to explore unsupervised learning\ntechniques. This research explores this possibility by implementing an\nunsupervised Contrastive Learning approach. Here, a Conformer-based encoder is\noptimized by the so-called Variance-Invariance-Covariance Regularization loss\nfunction on these lower-quality unlabeled data and the translation to the\nlabeled data is made. Through classification tasks involving recognizing ship\ntypes and marine mammal vocalizations, our method demonstrates to produce\nrobust and generalized embeddings. This shows to potential of unsupervised\nmethods for various automatic underwater acoustic analysis tasks.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 09:37:46", "updated": "2025-05-19 09:37:46", "pdf_url": "http://arxiv.org/pdf/2505.12904v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12908v1", "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "abstract": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 09:44:01", "updated": "2025-05-19 09:44:01", "pdf_url": "http://arxiv.org/pdf/2505.12908v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12909v1", "title": "Sinusoidal Initialization, Time for a New Start", "authors": ["Alberto Fern\u00e1ndez-Hern\u00e1ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ort\u00ed"], "abstract": "Initialization plays a critical role in Deep Neural Network training,\ndirectly influencing convergence, stability, and generalization. Common\napproaches such as Glorot and He initializations rely on randomness, which can\nproduce uneven weight distributions across layer connections. In this paper, we\nintroduce the Sinusoidal initialization, a novel deterministic method that\nemploys sinusoidal functions to construct structured weight matrices expressly\nto improve the spread and balance of weights throughout the network while\nsimultaneously fostering a more uniform, well-conditioned distribution of\nneuron activation states from the very first forward pass. Because Sinusoidal\ninitialization begins with weights and activations that are already evenly and\nefficiently utilized, it delivers consistently faster convergence, greater\ntraining stability, and higher final accuracy across a wide range of models,\nincluding convolutional neural networks, vision transformers, and large\nlanguage models. On average, our experiments show an increase of 4.8 % in final\nvalidation accuracy and 20.9 % in convergence speed. By replacing randomness\nwith structure, this initialization provides a stronger and more reliable\nfoundation for Deep Learning systems.", "categories": ["cs.LG", "cs.AI", "I.2; G.3; I.2.6"], "published": "2025-05-19 09:45:18", "updated": "2025-05-19 09:45:18", "pdf_url": "http://arxiv.org/pdf/2505.12909v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12910v1", "title": "SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Chao Gao", "Zhen Wang", "Keke Tang"], "abstract": "Source detection on graphs has demonstrated high efficacy in identifying\nrumor origins. Despite advances in machine learning-based methods, many fail to\ncapture intrinsic dynamics of rumor propagation. In this work, we present\nSourceDetMamba: A Graph-aware State Space Model for Source Detection in\nSequential Hypergraphs, which harnesses the recent success of the state space\nmodel Mamba, known for its superior global modeling capabilities and\ncomputational efficiency, to address this challenge. Specifically, we first\nemploy hypergraphs to model high-order interactions within social networks.\nSubsequently, temporal network snapshots generated during the propagation\nprocess are sequentially fed in reverse order into Mamba to infer underlying\npropagation dynamics. Finally, to empower the sequential model to effectively\ncapture propagation patterns while integrating structural information, we\npropose a novel graph-aware state update mechanism, wherein the state of each\nnode is propagated and refined by both temporal dependencies and topological\ncontext. Extensive evaluations on eight datasets demonstrate that\nSourceDetMamba consistently outperforms state-of-the-art approaches.", "categories": ["cs.SI", "cs.AI"], "published": "2025-05-19 09:45:27", "updated": "2025-05-19 09:45:27", "pdf_url": "http://arxiv.org/pdf/2505.12910v1", "comment": "Accepted by IJCAI25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12920v1", "title": "PyFCG: Fluid Construction Grammar in Python", "authors": ["Paul Van Eecke", "Katrien Beuls"], "abstract": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-19 10:00:01", "updated": "2025-05-19 10:00:01", "pdf_url": "http://arxiv.org/pdf/2505.12920v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12923v1", "title": "The Traitors: Deception and Trust in Multi-Agent Language Model Simulations", "authors": ["Pedro M. P. Curvo"], "abstract": "As AI systems increasingly assume roles where trust and alignment with human\nvalues are essential, understanding when and why they engage in deception has\nbecome a critical research priority. We introduce The Traitors, a multi-agent\nsimulation framework inspired by social deduction games, designed to probe\ndeception, trust formation, and strategic communication among large language\nmodel (LLM) agents under asymmetric information. A minority of agents the\ntraitors seek to mislead the majority, while the faithful must infer hidden\nidentities through dialogue and reasoning. Our contributions are: (1) we ground\nthe environment in formal frameworks from game theory, behavioral economics,\nand social cognition; (2) we develop a suite of evaluation metrics capturing\ndeception success, trust dynamics, and collective inference quality; (3) we\nimplement a fully autonomous simulation platform where LLMs reason over\npersistent memory and evolving social dynamics, with support for heterogeneous\nagent populations, specialized traits, and adaptive behaviors. Our initial\nexperiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model)\nreveal a notable asymmetry: advanced models like GPT-4o demonstrate superior\ndeceptive capabilities yet exhibit disproportionate vulnerability to others'\nfalsehoods. This suggests deception skills may scale faster than detection\nabilities. Overall, The Traitors provides a focused, configurable testbed for\ninvestigating LLM behavior in socially nuanced interactions. We position this\nwork as a contribution toward more rigorous research on deception mechanisms,\nalignment challenges, and the broader social reliability of AI systems.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-19 10:01:35", "updated": "2025-05-19 10:01:35", "pdf_url": "http://arxiv.org/pdf/2505.12923v1", "comment": "9 main pages, 31 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12925v1", "title": "CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive Programming", "authors": ["Han Deng", "Yuan Meng", "Shixiang Tang", "Wanli Ouyang", "Xinzhu Ma"], "abstract": "Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem --\nsimilar question retrieval -- to address this issue. Due to the lack of both\ndata and models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code and\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate and Simplified-to-Full), built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. In addition, we develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Code and data are available at: https://github.com/coldchair/CPRet", "categories": ["cs.SE", "cs.AI", "cs.IR", "H.3.3"], "published": "2025-05-19 10:07:51", "updated": "2025-05-19 10:07:51", "pdf_url": "http://arxiv.org/pdf/2505.12925v1", "comment": "main 9 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12929v1", "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "abstract": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:14:08", "updated": "2025-05-19 10:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12929v1", "comment": "24 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12938v1", "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "abstract": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 10:22:04", "updated": "2025-05-19 10:22:04", "pdf_url": "http://arxiv.org/pdf/2505.12938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12942v1", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "abstract": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:29:32", "updated": "2025-05-19 10:29:32", "pdf_url": "http://arxiv.org/pdf/2505.12942v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12944v1", "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "abstract": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "published": "2025-05-19 10:31:30", "updated": "2025-05-19 10:31:30", "pdf_url": "http://arxiv.org/pdf/2505.12944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12951v1", "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "abstract": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 10:44:49", "updated": "2025-05-19 10:44:49", "pdf_url": "http://arxiv.org/pdf/2505.12951v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12960v1", "title": "Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory", "authors": ["Chengping He", "Mingrui Jiang", "Keyi Shan", "Szu-Hao Yang", "Zefan Li", "Shengbo Wang", "Giacomo Pedretti", "Jim Ignowski", "Can Li"], "abstract": "Brain-inspired computing aims to mimic cognitive functions like associative\nmemory, the ability to recall complete patterns from partial cues. Memristor\ntechnology offers promising hardware for such neuromorphic systems due to its\npotential for efficient in-memory analog computing. Hopfield Neural Networks\n(HNNs) are a classic model for associative memory, but implementations on\nconventional hardware suffer from efficiency bottlenecks, while prior\nmemristor-based HNNs faced challenges with vulnerability to hardware defects\ndue to offline training, limited storage capacity, and difficulty processing\nanalog patterns. Here we introduce and experimentally demonstrate on integrated\nmemristor hardware a new hardware-adaptive learning algorithm for associative\nmemories that significantly improves defect tolerance and capacity, and\nnaturally extends to scalable multilayer architectures capable of handling both\nbinary and continuous patterns. Our approach achieves 3x effective capacity\nunder 50% device faults compared to state-of-the-art methods. Furthermore, its\nextension to multilayer architectures enables superlinear capacity scaling\n(\\(\\propto N^{1.49}\\ for binary patterns) and effective recalling of continuous\npatterns (\\propto N^{1.74}\\ scaling), as compared to linear capacity scaling\nfor previous HNNs. It also provides flexibility to adjust capacity by tuning\nhidden neurons for the same-sized patterns. By leveraging the massive\nparallelism of the hardware enabled by synchronous updates, it reduces energy\nby 8.8x and latency by 99.7% for 64-dimensional patterns over asynchronous\nschemes, with greater improvements at scale. This promises the development of\nmore reliable memristor-based associative memory systems and enables new\napplications research due to the significantly improved capacity, efficiency,\nand flexibility.", "categories": ["cs.LG", "cs.AI", "cs.ET"], "published": "2025-05-19 10:55:09", "updated": "2025-05-19 10:55:09", "pdf_url": "http://arxiv.org/pdf/2505.12960v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12963v1", "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies", "authors": ["Maksim I. Ivanov", "Olga E. Mendybaeva", "Yuri E. Karyakin", "Igor N. Glukhikh", "Aleksey V. Lebedev"], "abstract": "This article explores the use of artificial intelligence for the diagnosis of\npathologies of the temporomandibular joint (TMJ), in particular, for the\nsegmentation of the articular disc on MRI images. The relevance of the work is\ndue to the high prevalence of TMJ pathologies, as well as the need to improve\nthe accuracy and speed of diagnosis in medical institutions. During the study,\nthe existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,\nare not suitable for studying the articular disc due to the orientation towards\nbone structures. To solve the problem, an original dataset was collected from\n94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the\namount of data, augmentation methods were used. After that, the models of\nU-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and\ncompared. The evaluation was carried out according to the Dice Score,\nPrecision, Sensitivity, Specificity, and Mean Average Precision metrics. The\nresults confirm the potential of using the Roboflow model for segmentation of\nthe temporomandibular joint. In the future, it is planned to develop an\nalgorithm for measuring the distance between the jaws and determining the\nposition of the articular disc, which will improve the diagnosis of TMJ\npathologies.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-19 10:58:02", "updated": "2025-05-19 10:58:02", "pdf_url": "http://arxiv.org/pdf/2505.12963v1", "comment": "10 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12966v1", "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection", "authors": ["Zihan Xiong", "Xiaohua Wu", "Lei Chen", "Fangqi Lou"], "abstract": "Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 11:01:49", "updated": "2025-05-19 11:01:49", "pdf_url": "http://arxiv.org/pdf/2505.12966v1", "comment": "9 pages,ICMR accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12981v1", "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "abstract": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation.", "categories": ["cs.CR", "cs.AI", "cs.HC"], "published": "2025-05-19 11:17:46", "updated": "2025-05-19 11:17:46", "pdf_url": "http://arxiv.org/pdf/2505.12981v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12983v1", "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:18:54", "updated": "2025-05-19 11:18:54", "pdf_url": "http://arxiv.org/pdf/2505.12983v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12992v1", "title": "Fractured Chain-of-Thought Reasoning", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "published": "2025-05-19 11:30:41", "updated": "2025-05-19 11:30:41", "pdf_url": "http://arxiv.org/pdf/2505.12992v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12996v1", "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:34:47", "updated": "2025-05-19 11:34:47", "pdf_url": "http://arxiv.org/pdf/2505.12996v1", "comment": "12 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13010v1", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "abstract": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-19 11:54:39", "updated": "2025-05-19 11:54:39", "pdf_url": "http://arxiv.org/pdf/2505.13010v1", "comment": "7 pages, 5 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13011v1", "title": "Unveiling and Steering Connectome Organization with Interpretable Latent Variables", "authors": ["Yubin Li", "Xingyu Liu", "Guozhang Chen"], "abstract": "The brain's intricate connectome, a blueprint for its function, presents\nimmense complexity, yet it arises from a compact genetic code, hinting at\nunderlying low-dimensional organizational principles. This work bridges\nconnectomics and representation learning to uncover these principles. We\npropose a framework that combines subgraph extraction from the Drosophila\nconnectome, FlyWire, with a generative model to derive interpretable\nlow-dimensional representations of neural circuitry. Crucially, an\nexplainability module links these latent dimensions to specific structural\nfeatures, offering insights into their functional relevance. We validate our\napproach by demonstrating effective graph reconstruction and, significantly,\nthe ability to manipulate these latent codes to controllably generate\nconnectome subgraphs with predefined properties. This research offers a novel\ntool for understanding brain architecture and a potential avenue for designing\nbio-inspired artificial neural networks.", "categories": ["cs.AI"], "published": "2025-05-19 11:54:40", "updated": "2025-05-19 11:54:40", "pdf_url": "http://arxiv.org/pdf/2505.13011v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13023v1", "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "abstract": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "published": "2025-05-19 12:07:29", "updated": "2025-05-19 12:07:29", "pdf_url": "http://arxiv.org/pdf/2505.13023v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13025v1", "title": "LiBOG: Lifelong Learning for Black-Box Optimizer Generation", "authors": ["Jiyuan Pei", "Yi Mei", "Jialin Liu", "Mengjie Zhang"], "abstract": "Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in\nautomating the configuration and generation of black-box optimizers,\nsignificantly reducing the human effort required for optimizer design and\ndiscovering optimizers with higher performance than classic human-designed\noptimizers. However, existing MetaBBO methods conduct one-off training under\nthe assumption that a stationary problem distribution with extensive and\nrepresentative training problem samples is pre-available. This assumption is\noften impractical in real-world scenarios, where diverse problems following\nshifting distribution continually arise. Consequently, there is a pressing need\nfor methods that can continuously learn from new problems encountered\non-the-fly and progressively enhance their capabilities. In this work, we\nexplore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a\nnovel approach designed to learn from sequentially encountered problems and\ngenerate high-performance optimizers for Black-Box Optimization (BBO). LiBOG\nconsolidates knowledge both across tasks and within tasks to mitigate\ncatastrophic forgetting. Extensive experiments demonstrate LiBOG's\neffectiveness in learning to generate high-performance optimizers in a lifelong\nlearning manner, addressing catastrophic forgetting while maintaining\nplasticity to learn new tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:09:25", "updated": "2025-05-19 12:09:25", "pdf_url": "http://arxiv.org/pdf/2505.13025v1", "comment": "Accepted at IJCAI 2025. To appear", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13026v1", "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "abstract": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:10:17", "updated": "2025-05-19 12:10:17", "pdf_url": "http://arxiv.org/pdf/2505.13026v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13028v1", "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "authors": ["Sayon Palit", "Daniel Woods"], "abstract": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "published": "2025-05-19 12:12:00", "updated": "2025-05-19 12:12:00", "pdf_url": "http://arxiv.org/pdf/2505.13028v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13031v1", "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "authors": ["Yicheng Xiao", "Lin Song", "Yukang Chen", "Yingmin Luo", "Yuxin Chen", "Yukang Gan", "Wei Huang", "Xiu Li", "Xiaojuan Qi", "Ying Shan"], "abstract": "Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\n\\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}.", "categories": ["cs.AI"], "published": "2025-05-19 12:17:04", "updated": "2025-05-19 12:17:04", "pdf_url": "http://arxiv.org/pdf/2505.13031v1", "comment": "Code: https://github.com/EasonXiao-888/MindOmni", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13033v1", "title": "TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis", "authors": ["Vijay Ekambaram", "Subodh Kumar", "Arindam Jati", "Sumanta Mukherjee", "Tomoya Sakai", "Pankaj Dayama", "Wesley M. Gifford", "Jayant Kalagnanam"], "abstract": "The rise of time-series pre-trained models has advanced temporal\nrepresentation learning, but current state-of-the-art models are often\nlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compact\ntime-series pre-trained models with only 1M parameters, specialized to perform\nstrongly across classification, anomaly detection, imputation, and retrieval\ntasks. TSPulse introduces innovations at both the architecture and task levels.\nAt the architecture level, it employs a dual-space masked reconstruction,\nlearning from both time and frequency domains to capture complementary signals.\nThis is further enhanced by a dual-embedding disentanglement, generating both\ndetailed embeddings for fine-grained analysis and high-level semantic\nembeddings for broader task understanding. Notably, TSPulse's semantic\nembeddings are robust to shifts in time, magnitude, and noise, which is\nimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,\na fine-tuning component enabling task-specific feature attention. It also\nintroduces a multi-head triangulation technique that correlates deviations from\nmultiple prediction heads, enhancing anomaly detection by fusing complementary\nmodel outputs. Additionally, a hybrid mask pretraining is proposed to improves\nzero-shot imputation by reducing pre-training bias. These architecture and task\ninnovations collectively contribute to TSPulse's significant performance gains:\n5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly\ndetection leaderboard, +50% in zero-shot imputation, and +25% in time-series\nretrieval. Remarkably, these results are achieved with just 1M parameters,\nmaking TSPulse 10-100X smaller than existing pre-trained models. Its efficiency\nenables GPU-free inference and rapid pre-training, setting a new standard for\nefficient time-series pre-trained models. Models will be open-sourced soon.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 12:18:53", "updated": "2025-05-19 12:18:53", "pdf_url": "http://arxiv.org/pdf/2505.13033v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13036v1", "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "authors": ["Sai Koneru", "Maike Z\u00fcfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "abstract": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:21:29", "updated": "2025-05-19 12:21:29", "pdf_url": "http://arxiv.org/pdf/2505.13036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13043v1", "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation", "authors": ["Hao-Ran Yang", "Xiaohui Chen", "Chuan-Xian Ren"], "abstract": "Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 12:33:52", "updated": "2025-05-19 12:33:52", "pdf_url": "http://arxiv.org/pdf/2505.13043v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13044v1", "title": "CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents", "authors": ["Rebecca Westh\u00e4u\u00dfer", "Frederik Berenz", "Wolfgang Minker", "Sebastian Zepf"], "abstract": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) and are a powerful enabler for interactive systems. However,\nthey still face challenges in long-term interactions that require adaptation\ntowards the user as well as contextual knowledge and understanding of the\never-changing environment. To overcome these challenges, holistic memory\nmodeling is required to efficiently retrieve and store relevant information\nacross interaction sessions for suitable responses. Cognitive AI, which aims to\nsimulate the human thought process in a computerized model, highlights\ninteresting aspects, such as thoughts, memory mechanisms, and decision-making,\nthat can contribute towards improved memory modeling for LLMs. Inspired by\nthese cognitive AI principles, we propose our memory framework CAIM. CAIM\nconsists of three modules: 1.) The Memory Controller as the central decision\nunit; 2.) the Memory Retrieval, which filters relevant data for interaction\nupon request; and 3.) the Post-Thinking, which maintains the memory storage. We\ncompare CAIM against existing approaches, focusing on metrics such as retrieval\naccuracy, response correctness, contextual coherence, and memory storage. The\nresults demonstrate that CAIM outperforms baseline frameworks across different\nmetrics, highlighting its context-awareness and potential to improve long-term\nhuman-AI interactions.", "categories": ["cs.AI", "cs.HC"], "published": "2025-05-19 12:33:52", "updated": "2025-05-19 12:33:52", "pdf_url": "http://arxiv.org/pdf/2505.13044v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13053v1", "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "abstract": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:42:23", "updated": "2025-05-19 12:42:23", "pdf_url": "http://arxiv.org/pdf/2505.13053v1", "comment": "currently under review at Frontiers in Communication", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13073v1", "title": "Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion", "authors": ["Dengfeng Liu", "Jucai Zhai", "Xiaoguang Jiang", "Ziqun Li", "Qianjin Yu", "Feng Liu", "Rui Ye", "Huang Liu", "Zhiguo Yang", "Yongsheng Du", "Fang Tan"], "abstract": "Code completion technology based on large language model has significantly\nimproved the development efficiency of programmers. However, in practical\napplications, there remains a gap between current commonly used code completion\nevaluation metrics and users' actual perception. To address this issue, we\npropose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,\nfrom the perspective of probabilistic modeling. Furthermore, to tackle the lack\nof effective structural semantic modeling and cross-module dependency\ninformation in LLMs for repository-level code completion scenarios, we propose\na data processing method based on a Structure-Preserving and\nSemantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis\nand experimental validation, we demonstrate the superiority of the proposed\nevaluation metrics in terms of user perception consistency, as well as the\neffectiveness of the data processing method in enhancing model performance.", "categories": ["cs.SE", "cs.AI"], "published": "2025-05-19 13:09:32", "updated": "2025-05-19 13:09:32", "pdf_url": "http://arxiv.org/pdf/2505.13073v1", "comment": "14 pages,8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13076v1", "title": "The Hidden Dangers of Browsing AI Agents", "authors": ["Mykyta Mudryi", "Markiyan Chaklosh", "Grzegorz W\u00f3jcik"], "abstract": "Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 13:10:29", "updated": "2025-05-19 13:10:29", "pdf_url": "http://arxiv.org/pdf/2505.13076v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13077v1", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "abstract": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 13:11:28", "updated": "2025-05-19 13:11:28", "pdf_url": "http://arxiv.org/pdf/2505.13077v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13079v1", "title": "Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "abstract": "Transferring linguistic knowledge from a pretrained language model (PLM) to\nacoustic feature learning has proven effective in enhancing end-to-end\nautomatic speech recognition (E2E-ASR). However, aligning representations\nbetween linguistic and acoustic modalities remains a challenge due to inherent\nmodality gaps. Optimal transport (OT) has shown promise in mitigating these\ngaps by minimizing the Wasserstein distance (WD) between linguistic and\nacoustic feature distributions. However, previous OT-based methods overlook\nstructural relationships, treating feature vectors as unordered sets. To\naddress this, we propose Graph Matching Optimal Transport (GM-OT), which models\nlinguistic and acoustic sequences as structured graphs. Nodes represent feature\nembeddings, while edges capture temporal and sequential relationships. GM-OT\nminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)\n(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)\nformulation. This enables structured alignment and more efficient knowledge\ntransfer compared to existing OT-based approaches. Theoretical analysis further\nshows that prior OT-based methods in linguistic knowledge transfer can be\nviewed as a special case within our GM-OT framework. We evaluate GM-OT on\nMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge\ntransfer. Experimental results demonstrate significant performance gains over\nstate-of-the-art models, validating the effectiveness of our approach.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-19 13:13:18", "updated": "2025-05-19 13:13:18", "pdf_url": "http://arxiv.org/pdf/2505.13079v1", "comment": "To appear in Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13082v1", "title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers", "authors": ["Kyeongman Park", "Seongho Joo", "Kyomin Jung"], "abstract": "We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 13:13:46", "updated": "2025-05-19 13:13:46", "pdf_url": "http://arxiv.org/pdf/2505.13082v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13087v1", "title": "Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings", "authors": ["Adrien Lagesse", "Marc Lelarge"], "abstract": "We propose a novel benchmarking methodology for graph neural networks (GNNs)\nbased on the graph alignment problem, a combinatorial optimization task that\ngeneralizes graph isomorphism by aligning two unlabeled graphs to maximize\noverlapping edges. We frame this problem as a self-supervised learning task and\npresent several methods to generate graph alignment datasets using synthetic\nrandom graphs and real-world graph datasets from multiple domains. For a given\ngraph dataset, we generate a family of graph alignment datasets with increasing\ndifficulty, allowing us to rank the performance of various architectures. Our\nexperiments indicate that anisotropic graph neural networks outperform standard\nconvolutional architectures. To further demonstrate the utility of the graph\nalignment task, we show its effectiveness for unsupervised GNN pre-training,\nwhere the learned node embeddings outperform other positional encodings on\nthree molecular regression tasks and achieve state-of-the-art results on the\nPCQM4Mv2 dataset with significantly fewer parameters. To support\nreproducibility and further research, we provide an open-source Python package\nto generate graph alignment datasets and benchmark new GNN architectures.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 13:22:17", "updated": "2025-05-19 13:22:17", "pdf_url": "http://arxiv.org/pdf/2505.13087v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13094v1", "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech Separation", "authors": ["Guo Chen", "Kai Li", "Runxuan Yang", "Xiaolin Hu"], "abstract": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-19 13:25:51", "updated": "2025-05-19 13:25:51", "pdf_url": "http://arxiv.org/pdf/2505.13094v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13098v1", "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "abstract": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.", "categories": ["cs.AI", "cs.CL", "cs.DB"], "published": "2025-05-19 13:29:27", "updated": "2025-05-19 13:29:27", "pdf_url": "http://arxiv.org/pdf/2505.13098v1", "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13101v1", "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "authors": ["Shaowu Wu", "Liting Zeng", "Wei Lu", "Xiangyang Luo"], "abstract": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 13:31:48", "updated": "2025-05-19 13:31:48", "pdf_url": "http://arxiv.org/pdf/2505.13101v1", "comment": "10 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13102v1", "title": "Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast", "authors": ["Ji Qi", "Tam Thuc Do", "Mingxiao Liu", "Zhuoshi Pan", "Yuzhe Li", "Gene Cheung", "H. Vicky Zhao"], "abstract": "To forecast traffic with both spatial and temporal dimensions, we unroll a\nmixed-graph-based optimization algorithm into a lightweight and interpretable\ntransformer-like neural net. Specifically, we construct two graphs: an\nundirected graph $\\mathcal{G}^u$ capturing spatial correlations across\ngeography, and a directed graph $\\mathcal{G}^d$ capturing sequential\nrelationships over time. We formulate a prediction problem for the future\nsamples of signal $\\mathbf{x}$, assuming it is \"smooth\" with respect to both\n$\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and\n$\\ell_1$-norm variational terms to quantify and promote signal smoothness\n(low-frequency reconstruction) on a directed graph. We construct an iterative\nalgorithm based on alternating direction method of multipliers (ADMM), and\nunroll it into a feed-forward network for data-driven parameter learning. We\ninsert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$, which\nare akin to the self-attention mechanism in classical transformers. Experiments\nshow that our unrolled networks achieve competitive traffic forecast\nperformance as state-of-the-art prediction schemes, while reducing parameter\ncounts drastically. Our code is available in\nhttps://github.com/SingularityUndefined/Unrolling-GSP-STForecast.", "categories": ["cs.LG", "cs.AI", "eess.SP"], "published": "2025-05-19 13:32:34", "updated": "2025-05-19 13:32:34", "pdf_url": "http://arxiv.org/pdf/2505.13102v1", "comment": "19 pages, 5 figures, 8 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13109v1", "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "abstract": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 13:36:45", "updated": "2025-05-19 13:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13109v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13115v1", "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "abstract": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-19 13:46:35", "updated": "2025-05-19 13:46:35", "pdf_url": "http://arxiv.org/pdf/2505.13115v1", "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13116v1", "title": "Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data", "authors": ["Kathrin Lammers", "Valerie Vaquet", "Barbara Hammer"], "abstract": "As machine learning is increasingly applied in an online fashion to deal with\nevolving data streams, the fairness of these algorithms is a matter of growing\nethical and legal concern. In many use cases, class imbalance in the data also\nneeds to be dealt with to ensure predictive performance. Current fairness-aware\nstream learners typically attempt to solve these issues through in- or\npost-processing by focusing on optimizing one specific discrimination metric,\naddressing class imbalance in a separate processing step. While C-SMOTE is a\nhighly effective model-agnostic pre-processing approach to mitigate class\nimbalance, as a side effect of this method, algorithmic bias is often\nintroduced.\n  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant -\nas a pre-processing approach to simultaneously address the class imbalance and\nfairness concerns by employing situation testing and balancing\nfairness-relevant groups during oversampling. Unlike other fairness-aware\nstream learners, CFSMOTE is not optimizing for only one specific fairness\nmetric, therefore avoiding potentially problematic trade-offs. Our experiments\nshow significant improvement on several common group fairness metrics in\ncomparison to vanilla C-SMOTE while maintaining competitive performance, also\nin comparison to other fairness-aware algorithms.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 13:46:47", "updated": "2025-05-19 13:46:47", "pdf_url": "http://arxiv.org/pdf/2505.13116v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13118v1", "title": "Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals", "authors": ["Marouane Il Idrissi", "Agathe Fernandes Machado", "Ewen Gallic", "Arthur Charpentier"], "abstract": "Cooperative game theory methods, notably Shapley values, have significantly\nenhanced machine learning (ML) interpretability. However, existing explainable\nAI (XAI) frameworks mainly attribute average model predictions, overlooking\npredictive uncertainty. This work addresses that gap by proposing a novel,\nmodel-agnostic uncertainty attribution (UA) method grounded in conformal\nprediction (CP). By defining cooperative games where CP interval\nproperties-such as width and bounds-serve as value functions, we systematically\nattribute predictive uncertainty to input features. Extending beyond the\ntraditional Shapley values, we use the richer class of Harsanyi allocations,\nand in particular the proportional Shapley values, which distribute attribution\nproportionally to feature importance. We propose a Monte Carlo approximation\nmethod with robust statistical guarantees to address computational feasibility,\nsignificantly improving runtime efficiency. Our comprehensive experiments on\nsynthetic benchmarks and real-world datasets demonstrate the practical utility\nand interpretative depth of our approach. By combining cooperative game theory\nand conformal prediction, we offer a rigorous, flexible toolkit for\nunderstanding and communicating predictive uncertainty in high-stakes ML\napplications.", "categories": ["cs.AI", "cs.LG", "stat.ML"], "published": "2025-05-19 13:49:05", "updated": "2025-05-19 13:49:05", "pdf_url": "http://arxiv.org/pdf/2505.13118v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13122v1", "title": "When majority rules, minority loses: bias amplification of gradient descent", "authors": ["Fran\u00e7ois Bachoc", "J\u00e9r\u00f4me Bolte", "Ryan Boustany", "Jean-Michel Loubes"], "abstract": "Despite growing empirical evidence of bias amplification in machine learning,\nits theoretical foundations remain poorly understood. We develop a formal\nframework for majority-minority learning tasks, showing how standard training\ncan favor majority groups and produce stereotypical predictors that neglect\nminority-specific features. Assuming population and variance imbalance, our\nanalysis reveals three key findings: (i) the close proximity between\n``full-data'' and stereotypical predictors, (ii) the dominance of a region\nwhere training the entire model tends to merely learn the majority traits, and\n(iii) a lower bound on the additional training required. Our results are\nillustrated through experiments in deep learning for tabular and image\nclassification tasks.", "categories": ["cs.LG", "cs.AI", "math.OC"], "published": "2025-05-19 13:51:49", "updated": "2025-05-19 13:51:49", "pdf_url": "http://arxiv.org/pdf/2505.13122v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13123v1", "title": "Just Dance with $\u03c0$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "abstract": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 13:51:57", "updated": "2025-05-19 13:51:57", "pdf_url": "http://arxiv.org/pdf/2505.13123v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13124v1", "title": "$\u03bc$PC: Scaling Predictive Coding to 100+ Layer Networks", "authors": ["Francesco Innocenti", "El Mehdi Achour", "Christopher L. Buckley"], "abstract": "The biological implausibility of backpropagation (BP) has motivated many\nalternative, brain-inspired algorithms that attempt to rely only on local\ninformation, such as predictive coding (PC) and equilibrium propagation.\nHowever, these algorithms have notoriously struggled to train very deep\nnetworks, preventing them from competing with BP in large-scale settings.\nIndeed, scaling PC networks (PCNs) has recently been posed as a challenge for\nthe community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can\nbe trained reliably using a Depth-$\\mu$P parameterisation (Yang et al., 2023;\nBordelon et al., 2023) which we call \"$\\mu$PC\". Through an extensive analysis\nof the scaling behaviour of PCNs, we reveal several pathologies that make\nstandard PCNs difficult to train at large depths. We then show that, despite\naddressing only some of these instabilities, $\\mu$PC allows stable training of\nvery deep (up to 128-layer) residual networks on simple classification tasks\nwith competitive performance and little tuning compared to current benchmarks.\nMoreover, $\\mu$PC enables zero-shot transfer of both weight and activity\nlearning rates across widths and depths. Our results have implications for\nother local algorithms and could be extended to convolutional and transformer\narchitectures. Code for $\\mu$PC is made available as part of a JAX library for\nPCNs at https://github.com/thebuckleylab/jpc (Innocenti et al., 2024).", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.6"], "published": "2025-05-19 13:54:29", "updated": "2025-05-19 13:54:29", "pdf_url": "http://arxiv.org/pdf/2505.13124v1", "comment": "34 pages, 41 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13126v1", "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "abstract": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 13:58:15", "updated": "2025-05-19 13:58:15", "pdf_url": "http://arxiv.org/pdf/2505.13126v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13130v1", "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach", "authors": ["Muhammad Awais Amin", "Adama Ilboudo", "Abdul Samad bin Shahid", "Amjad Ali", "Waqas Haider Khan Bangyal"], "abstract": "One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:00:10", "updated": "2025-05-19 14:00:10", "pdf_url": "http://arxiv.org/pdf/2505.13130v1", "comment": null, "doi": "10.63075/2jepm102", "journal_ref": "Annual Methodological Archive Research Review 3 (05) (2025),\n  296-321"}
{"arxiv_id": "2505.13136v1", "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "abstract": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 14:07:20", "updated": "2025-05-19 14:07:20", "pdf_url": "http://arxiv.org/pdf/2505.13136v1", "comment": "under review @ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13144v1", "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning", "authors": ["Dongsu Lee", "Minhae Kwon"], "abstract": "The goal of offline reinforcement learning (RL) is to extract a\nhigh-performance policy from the fixed datasets, minimizing performance\ndegradation due to out-of-distribution (OOD) samples. Offline model-based RL\n(MBRL) is a promising approach that ameliorates OOD issues by enriching\nstate-action transitions with augmentations synthesized via a learned dynamics\nmodel. Unfortunately, seminal offline MBRL methods often struggle in\nsparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL\nframework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),\nthat generates augmented transitions in a temporally structured latent space\nrather than in raw state space. To model long-horizon behavior, TempDATA learns\na latent abstraction that captures a temporal distance from both trajectory and\ntransition levels of state space. Our experiments confirm that TempDATA\noutperforms previous offline MBRL methods and achieves matching or surpassing\nthe performance of diffusion-based trajectory augmentation and goal-conditioned\nRL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-19 14:11:14", "updated": "2025-05-19 14:11:14", "pdf_url": "http://arxiv.org/pdf/2505.13144v1", "comment": "2025 ICML", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13156v1", "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "abstract": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:17:37", "updated": "2025-05-19 14:17:37", "pdf_url": "http://arxiv.org/pdf/2505.13156v1", "comment": "23 pages, 4 figures, and 1 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13157v1", "title": "Role-Playing Evaluation for Large Language Models", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "abstract": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:18:16", "updated": "2025-05-19 14:18:16", "pdf_url": "http://arxiv.org/pdf/2505.13157v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13175v1", "title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment", "authors": ["Siming Sun", "Kai Zhang", "Xuejun Jiang", "Wenchao Meng", "Qinmin Yang"], "abstract": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting.", "categories": ["cs.AI"], "published": "2025-05-19 14:30:41", "updated": "2025-05-19 14:30:41", "pdf_url": "http://arxiv.org/pdf/2505.13175v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13176v1", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "abstract": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:30:46", "updated": "2025-05-19 14:30:46", "pdf_url": "http://arxiv.org/pdf/2505.13176v1", "comment": "Accepted by ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13180v1", "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models", "authors": ["Matteo Merler", "Nicola Dainese", "Minttu Alakuijala", "Giovanni Bonetta", "Pietro Ferrazzi", "Yu Tian", "Bernardo Magnini", "Pekka Marttinen"], "abstract": "Integrating Large Language Models with symbolic planners is a promising\ndirection for obtaining verifiable and grounded plans compared to planning in\nnatural language, with recent works extending this idea to visual domains using\nVision-Language Models (VLMs). However, rigorous comparison between\nVLM-grounded symbolic approaches and methods that plan directly with a VLM has\nbeen hindered by a lack of common environments, evaluation protocols and model\ncoverage. We introduce ViPlan, the first open-source benchmark for Visual\nPlanning with symbolic predicates and VLMs. ViPlan features a series of\nincreasingly challenging tasks in two domains: a visual variant of the classic\nBlocksworld planning problem and a simulated household robotics environment. We\nbenchmark nine open-source VLM families across multiple sizes, along with\nselected closed models, evaluating both VLM-grounded symbolic planning and\nusing the models directly to propose actions. We find symbolic planning to\noutperform direct VLM planning in Blocksworld, where accurate image grounding\nis crucial, whereas the opposite is true in the household robotics tasks, where\ncommonsense knowledge and the ability to recover from errors are beneficial.\nFinally, we show that across most models and methods, there is no significant\nbenefit to using Chain-of-Thought prompting, suggesting that current VLMs still\nstruggle with visual reasoning.", "categories": ["cs.AI"], "published": "2025-05-19 14:38:15", "updated": "2025-05-19 14:38:15", "pdf_url": "http://arxiv.org/pdf/2505.13180v1", "comment": "9 pages, 5 figures and 1 table in the main text; 43 pages, 9 figures\n  and 16 tables including supplementary material", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13182v1", "title": "Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping", "authors": ["Jianfeng Xu"], "abstract": "[Objective] This study focuses on addressing the current lack of a unified\nformal theoretical framework in machine learning, as well as the deficiencies\nin interpretability and ethical safety assurance. [Methods] A formal\ninformation model is first constructed, utilizing sets of well-formed formulas\nto explicitly define the ontological states and carrier mappings of typical\ncomponents in machine learning. Learnable and processable predicates, along\nwith learning and processing functions, are introduced to analyze the logical\ndeduction and constraint rules of the causal chains within models. [Results] A\nmeta-framework for machine learning theory (MLT-MF) is established. Based on\nthis framework, universal definitions for model interpretability and ethical\nsafety are proposed. Furthermore, three key theorems are proved: the\nequivalence of model interpretability and information recoverability, the\nassurance of ethical safety, and the estimation of generalization error.\n[Limitations] The current framework assumes ideal conditions with noiseless\ninformation-enabling mappings and primarily targets model learning and\nprocessing logic in static scenarios. It does not yet address information\nfusion and conflict resolution across ontological spaces in multimodal or\nmulti-agent systems. [Conclusions] This work overcomes the limitations of\nfragmented research and provides a unified theoretical foundation for\nsystematically addressing the critical challenges currently faced in machine\nlearning.", "categories": ["cs.LO", "cs.AI"], "published": "2025-05-19 14:39:41", "updated": "2025-05-19 14:39:41", "pdf_url": "http://arxiv.org/pdf/2505.13182v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13188v1", "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns", "authors": ["Juntian Zhu", "Miguel de Carvalho", "Zhouwang Yang", "Fengxiang He"], "abstract": "An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-19 14:45:58", "updated": "2025-05-19 14:45:58", "pdf_url": "http://arxiv.org/pdf/2505.13188v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13191v1", "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision", "authors": ["Pengcheng Pan", "Yonekura Shogo", "Yasuo Kuniyoshi"], "abstract": "Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:48:36", "updated": "2025-05-19 14:48:36", "pdf_url": "http://arxiv.org/pdf/2505.13191v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13192v1", "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "authors": ["Christoph J\u00fcrgen Hemmer", "Daniel Durstewitz"], "abstract": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.", "categories": ["cs.LG", "cs.AI", "math.DS", "nlin.CD"], "published": "2025-05-19 14:49:10", "updated": "2025-05-19 14:49:10", "pdf_url": "http://arxiv.org/pdf/2505.13192v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13195v1", "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities", "authors": ["Lili Zhang", "Haomiaomiao Wang", "Long Cheng", "Libao Deng", "Tomas Ward"], "abstract": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch.", "categories": ["cs.AI"], "published": "2025-05-19 14:50:44", "updated": "2025-05-19 14:50:44", "pdf_url": "http://arxiv.org/pdf/2505.13195v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13196v1", "title": "A Physics-Inspired Optimizer: Velocity Regularized Adam", "authors": ["Pranav Vaidhyanathan", "Lucas Schorling", "Natalia Ares", "Michael A. Osborne"], "abstract": "We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer\nfor training deep neural networks that draws on ideas from quartic terms for\nkinetic energy with its stabilizing effects on various system dynamics.\nPrevious algorithms, including the ubiquitous Adam, operate at the so called\nadaptive edge of stability regime during training leading to rapid oscillations\nand slowed convergence of loss. However, VRAdam adds a higher order penalty on\nthe learning rate based on the velocity such that the algorithm automatically\nslows down whenever weight updates become large. In practice, we observe that\nthe effective dynamic learning rate shrinks in high-velocity regimes, damping\noscillations and allowing for a more aggressive base step size when necessary\nwithout divergence. By combining this velocity-based regularizer for global\ndamping with per-parameter scaling of Adam to create a hybrid optimizer, we\ndemonstrate that VRAdam consistently exceeds the performance against standard\noptimizers including AdamW. We benchmark various tasks such as image\nclassification, language modeling, image generation and generative modeling\nusing diverse architectures and training methodologies including Convolutional\nNeural Networks (CNNs), Transformers, and GFlowNets.", "categories": ["cs.LG", "cs.AI", "quant-ph"], "published": "2025-05-19 14:51:40", "updated": "2025-05-19 14:51:40", "pdf_url": "http://arxiv.org/pdf/2505.13196v1", "comment": "L. Schorling and P. Vaidhyanathan contributed equally to this work.\n  20 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13201v1", "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "abstract": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:54:04", "updated": "2025-05-19 14:54:04", "pdf_url": "http://arxiv.org/pdf/2505.13201v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13208v1", "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "abstract": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.", "categories": ["quant-ph", "cs.AI", "cs.CL"], "published": "2025-05-19 14:57:53", "updated": "2025-05-19 14:57:53", "pdf_url": "http://arxiv.org/pdf/2505.13208v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13210v1", "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "abstract": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:58:44", "updated": "2025-05-19 14:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13210v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13211v1", "title": "MAGI-1: Autoregressive Video Generation at Scale", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "abstract": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 14:58:50", "updated": "2025-05-19 14:58:50", "pdf_url": "http://arxiv.org/pdf/2505.13211v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "published": "2025-05-19 15:09:23", "updated": "2025-05-19 15:09:23", "pdf_url": "http://arxiv.org/pdf/2505.13227v1", "comment": "49 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13232v1", "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "abstract": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-19 15:15:35", "updated": "2025-05-19 15:15:35", "pdf_url": "http://arxiv.org/pdf/2505.13232v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13246v1", "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "abstract": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.", "categories": ["cs.AI", "cs.HC"], "published": "2025-05-19 15:28:10", "updated": "2025-05-19 15:28:10", "pdf_url": "http://arxiv.org/pdf/2505.13246v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13253v1", "title": "Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic", "authors": ["Lennart R\u00f6stel", "Dominik Winkelbauer", "Johannes Pitz", "Leon Sievers", "Berthold B\u00e4uml"], "abstract": "In-hand manipulation and grasping are fundamental yet often separately\naddressed tasks in robotics. For deriving in-hand manipulation policies,\nreinforcement learning has recently shown great success. However, the derived\ncontrollers are not yet useful in real-world scenarios because they often\nrequire a human operator to place the objects in suitable initial (grasping)\nstates. Finding stable grasps that also promote the desired in-hand\nmanipulation goal is an open problem. In this work, we propose a method for\nbridging this gap by leveraging the critic network of a reinforcement learning\nagent trained for in-hand manipulation to score and select initial grasps. Our\nexperiments show that this method significantly increases the success rate of\nin-hand manipulation without requiring additional training. We also present an\nimplementation of a full grasp manipulation pipeline on a real-world system,\nenabling autonomous grasping and reorientation even of unwieldy objects.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-19 15:36:34", "updated": "2025-05-19 15:36:34", "pdf_url": "http://arxiv.org/pdf/2505.13253v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13257v1", "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "authors": ["Zilu Tang", "Afra Feyza Aky\u00fcrek", "Ekin Aky\u00fcrek", "Derry Wijaya"], "abstract": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:39:48", "updated": "2025-05-19 15:39:48", "pdf_url": "http://arxiv.org/pdf/2505.13257v1", "comment": "9 pages, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13264v1", "title": "Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty", "authors": ["Carlos Rodriguez-Pardo", "Louis Daumas", "Leonardo Chiani", "Massimo Tavoni"], "abstract": "Climate-economic modeling under uncertainty presents significant\ncomputational challenges that may limit policymakers' ability to address\nclimate change effectively. This paper explores neural network-based approaches\nfor solving high-dimensional optimal control problems arising from models that\nincorporate ambiguity aversion in climate mitigation decisions. We develop a\ncontinuous-time endogenous-growth economic model that accounts for multiple\nmitigation pathways, including emission-free capital and carbon intensity\nreductions. Given the inherent complexity and high dimensionality of these\nmodels, traditional numerical methods become computationally intractable. We\nbenchmark several neural network architectures against finite-difference\ngenerated solutions, evaluating their ability to capture the dynamic\ninteractions between uncertainty, technology transitions, and optimal climate\npolicy. Our findings demonstrate that appropriate neural architecture selection\nsignificantly impacts both solution accuracy and computational efficiency when\nmodeling climate-economic systems under uncertainty. These methodological\nadvances enable more sophisticated modeling of climate policy decisions,\nallowing for better representation of technology transitions and\nuncertainty-critical elements for developing effective mitigation strategies in\nthe face of climate change.", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF", "math.AP", "68T07 (Primary) 35Q91, 91B76 (Secondary)", "I.2.1; I.5.1; J.4"], "published": "2025-05-19 15:46:12", "updated": "2025-05-19 15:46:12", "pdf_url": "http://arxiv.org/pdf/2505.13264v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13268v1", "title": "Representation of perceived prosodic similarity of conversational feedback", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "abstract": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:47:51", "updated": "2025-05-19 15:47:51", "pdf_url": "http://arxiv.org/pdf/2505.13268v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13273v1", "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "abstract": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-19 15:53:32", "updated": "2025-05-19 15:53:32", "pdf_url": "http://arxiv.org/pdf/2505.13273v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13280v1", "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "authors": ["Elias Collaert", "Abel Rodr\u00edguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "abstract": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "published": "2025-05-19 16:04:43", "updated": "2025-05-19 16:04:43", "pdf_url": "http://arxiv.org/pdf/2505.13280v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13287v1", "title": "Level Generation with Quantum Reservoir Computing", "authors": ["Jo\u00e3o S. Ferreira", "Pierre Fromholz", "Hari Shaji", "James R. Wootton"], "abstract": "Reservoir computing is a form of machine learning particularly suited for\ntime series analysis, including forecasting predictions. We take an\nimplementation of \\emph{quantum} reservoir computing that was initially\ndesigned to generate variants of musical scores and adapt it to create levels\nof Super Mario Bros. Motivated by our analysis of these levels, we develop a\nnew Roblox \\textit{obby} where the courses can be generated in real time on\nsuperconducting qubit hardware, and investigate some of the constraints placed\nby such real-time generation.", "categories": ["cs.AI", "quant-ph"], "published": "2025-05-19 16:09:30", "updated": "2025-05-19 16:09:30", "pdf_url": "http://arxiv.org/pdf/2505.13287v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13291v1", "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Micha\u0142 Wili\u0144ski", "Gus Welter", "Artur Dubrawski"], "abstract": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 16:11:23", "updated": "2025-05-19 16:11:23", "pdf_url": "http://arxiv.org/pdf/2505.13291v1", "comment": "Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13292v1", "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs", "authors": ["Huaiying Luo", "Cheng Ji"], "abstract": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-19 16:14:27", "updated": "2025-05-19 16:14:27", "pdf_url": "http://arxiv.org/pdf/2505.13292v1", "comment": "Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13307v1", "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-19 16:25:55", "updated": "2025-05-19 16:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13307v1", "comment": "Manuscript", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13308v1", "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "abstract": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 16:26:02", "updated": "2025-05-19 16:26:02", "pdf_url": "http://arxiv.org/pdf/2505.13308v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13315v1", "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "authors": ["Reza T. Batley", "Sourav Saha"], "abstract": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.", "categories": ["cs.LG", "cs.AI", "cs.MS"], "published": "2025-05-19 16:29:07", "updated": "2025-05-19 16:29:07", "pdf_url": "http://arxiv.org/pdf/2505.13315v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13316v1", "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "abstract": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 16:29:12", "updated": "2025-05-19 16:29:12", "pdf_url": "http://arxiv.org/pdf/2505.13316v1", "comment": "6 pages, 5 figures, accepted at ICME 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13324v1", "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "abstract": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI.", "categories": ["stat.ML", "cs.AI", "cs.LG", "econ.EM", "stat.ME"], "published": "2025-05-19 16:34:36", "updated": "2025-05-19 16:34:36", "pdf_url": "http://arxiv.org/pdf/2505.13324v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13329v1", "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications", "authors": ["Fr\u00e9d\u00e9ric Berdoz", "Dustin Brunner", "Yann Vonlanthen", "Roger Wattenhofer"], "abstract": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future.", "categories": ["cs.CY", "cs.AI", "cs.CR"], "published": "2025-05-19 16:38:06", "updated": "2025-05-19 16:38:06", "pdf_url": "http://arxiv.org/pdf/2505.13329v1", "comment": "This is the extended version of the paper, accepted at IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "abstract": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "categories": ["cs.CL", "cs.AI", "eess.AS"], "published": "2025-05-19 16:47:46", "updated": "2025-05-19 16:47:46", "pdf_url": "http://arxiv.org/pdf/2505.13338v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13339v1", "title": "OPA-Pack: Object-Property-Aware Robotic Bin Packing", "authors": ["Jia-Hui Pan", "Yeok Tatt Cheah", "Zhengzhe Liu", "Ka-Hei Hui", "Xiaojie Gao", "Pheng-Ann Heng", "Yun-Hui Liu", "Chi-Wing Fu"], "abstract": "Robotic bin packing aids in a wide range of real-world scenarios such as\ne-commerce and warehouses. Yet, existing works focus mainly on considering the\nshape of objects to optimize packing compactness and neglect object properties\nsuch as fragility, edibility, and chemistry that humans typically consider when\npacking objects. This paper presents OPA-Pack (Object-Property-Aware Packing\nframework), the first framework that equips the robot with object property\nconsiderations in planning the object packing. Technical-wise, we develop a\nnovel object property recognition scheme with retrieval-augmented generation\nand chain-of-thought reasoning, and build a dataset with object property\nannotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to\njointly separate incompatible object pairs and reduce pressure on fragile\nobjects, while compacting the packing. Further, OPA-Net consists of a property\nembedding layer to encode the property of candidate objects to be packed,\ntogether with a fragility heightmap and an avoidance heightmap to keep track of\nthe packed objects. Then, we design a reward function and adopt a deep\nQ-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack\ngreatly improves the accuracy of separating incompatible object pairs (from 52%\nto 95%) and largely reduces pressure on fragile objects (by 29.4%), while\nmaintaining good packing compactness. Besides, we demonstrate the effectiveness\nof OPA-Pack on a real packing platform, showcasing its practicality in\nreal-world scenarios.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-19 16:48:14", "updated": "2025-05-19 16:48:14", "pdf_url": "http://arxiv.org/pdf/2505.13339v1", "comment": "Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13344v1", "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "abstract": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 16:50:26", "updated": "2025-05-19 16:50:26", "pdf_url": "http://arxiv.org/pdf/2505.13344v1", "comment": "https://berkegokmen1.github.io/RoPECraft/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13346v1", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "abstract": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 16:50:35", "updated": "2025-05-19 16:50:35", "pdf_url": "http://arxiv.org/pdf/2505.13346v1", "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13355v1", "title": "Multi-Armed Bandits Meet Large Language Models", "authors": ["Djallel Bouneffouf", "Raphael Feraud"], "abstract": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.", "categories": ["cs.AI"], "published": "2025-05-19 16:57:57", "updated": "2025-05-19 16:57:57", "pdf_url": "http://arxiv.org/pdf/2505.13355v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13358v1", "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "abstract": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 16:59:47", "updated": "2025-05-19 16:59:47", "pdf_url": "http://arxiv.org/pdf/2505.13358v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13372v1", "title": "Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning", "authors": ["Irene Brugnara", "Alessandro Valentini", "Andrea Micheli"], "abstract": "Recent work investigated the use of Reinforcement Learning (RL) for the\nsynthesis of heuristic guidance to improve the performance of temporal planners\nwhen a domain is fixed and a set of training problems (not plans) is given. The\nidea is to extract a heuristic from the value function of a particular\n(possibly infinite-state) MDP constructed over the training problems.\n  In this paper, we propose an evolution of this learning and planning\nframework that focuses on exploiting the information provided by symbolic\nheuristics during both the RL and planning phases. First, we formalize\ndifferent reward schemata for the synthesis and use symbolic heuristics to\nmitigate the problems caused by the truncation of episodes needed to deal with\nthe potentially infinite MDP. Second, we propose learning a residual of an\nexisting symbolic heuristic, which is a \"correction\" of the heuristic value,\ninstead of eagerly learning the whole heuristic from scratch. Finally, we use\nthe learned heuristic in combination with a symbolic heuristic using a\nmultiple-queue planning approach to balance systematic search with imperfect\nlearned information. We experimentally compare all the approaches, highlighting\ntheir strengths and weaknesses and significantly advancing the state of the art\nfor this planning and learning schema.", "categories": ["cs.AI"], "published": "2025-05-19 17:19:13", "updated": "2025-05-19 17:19:13", "pdf_url": "http://arxiv.org/pdf/2505.13372v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13379v1", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:24:16", "updated": "2025-05-19 17:24:16", "pdf_url": "http://arxiv.org/pdf/2505.13379v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13380v1", "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:24:26", "updated": "2025-05-19 17:24:26", "pdf_url": "http://arxiv.org/pdf/2505.13380v1", "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13381v1", "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "abstract": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms.", "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.7; H.5.2"], "published": "2025-05-19 17:25:07", "updated": "2025-05-19 17:25:07", "pdf_url": "http://arxiv.org/pdf/2505.13381v1", "comment": "10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM\n  Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy", "doi": "10.1145/3698205.3729542", "journal_ref": null}
{"arxiv_id": "2505.13388v1", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:29:03", "updated": "2025-05-19 17:29:03", "pdf_url": "http://arxiv.org/pdf/2505.13388v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13391v1", "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "authors": ["Miko\u0142aj Ma\u0142ki\u0144ski", "Jacek Ma\u0144dziuk"], "abstract": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods.", "categories": ["cs.AI", "cs.CV", "cs.LG"], "published": "2025-05-19 17:32:07", "updated": "2025-05-19 17:32:07", "pdf_url": "http://arxiv.org/pdf/2505.13391v1", "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13393v1", "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "authors": ["Christopher K. Frantz"], "abstract": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool.", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "published": "2025-05-19 17:33:15", "updated": "2025-05-19 17:33:15", "pdf_url": "http://arxiv.org/pdf/2505.13393v1", "comment": "24 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13400v1", "title": "Robin: A multi-agent system for automating scientific discovery", "authors": ["Ali Essam Ghareeb", "Benjamin Chang", "Ludovico Mitchener", "Angela Yiu", "Caralyn J. Szostkiewicz", "Jon M. Laurent", "Muhammed T. Razzak", "Andrew D. White", "Michaela M. Hinks", "Samuel G. Rodriques"], "abstract": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.", "categories": ["cs.AI", "cs.MA", "q-bio.QM"], "published": "2025-05-19 17:36:17", "updated": "2025-05-19 17:36:17", "pdf_url": "http://arxiv.org/pdf/2505.13400v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13406v1", "title": "AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database", "authors": ["Rong Bian", "Yu Geng", "Zijian Yang", "Bing Cheng"], "abstract": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM.", "categories": ["cs.AI"], "published": "2025-05-19 17:41:29", "updated": "2025-05-19 17:41:29", "pdf_url": "http://arxiv.org/pdf/2505.13406v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13408v1", "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "abstract": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:44:26", "updated": "2025-05-19 17:44:26", "pdf_url": "http://arxiv.org/pdf/2505.13408v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13417v1", "title": "AdaptThink: Reasoning Models Can Learn When to Think", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "abstract": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:50:52", "updated": "2025-05-19 17:50:52", "pdf_url": "http://arxiv.org/pdf/2505.13417v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13425v1", "title": "Learnware of Language Models: Specialized Small Language Models Can Do Big", "authors": ["Zhi-Hao Tan", "Zi-Chen Zhao", "Hao-Yu Shi", "Xin-Yu Zhang", "Peng Tan", "Yang Yu", "Zhi-Hua Zhou"], "abstract": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-19 17:54:35", "updated": "2025-05-19 17:54:35", "pdf_url": "http://arxiv.org/pdf/2505.13425v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13427v1", "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-19 17:55:08", "updated": "2025-05-19 17:55:08", "pdf_url": "http://arxiv.org/pdf/2505.13427v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13437v1", "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "abstract": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-19 17:58:11", "updated": "2025-05-19 17:58:11", "pdf_url": "http://arxiv.org/pdf/2505.13437v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 17:58:44", "updated": "2025-05-19 17:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13439v1", "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "abstract": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-19 17:59:01", "updated": "2025-05-19 17:59:01", "pdf_url": "http://arxiv.org/pdf/2505.13439v1", "comment": "24 pages, 13 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:59:31", "updated": "2025-05-19 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.13445v1", "comment": "code available at https://github.com/xyliu-cs/RISE", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "abstract": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:59:58", "updated": "2025-05-19 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.13448v1", "comment": "10 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12584v1", "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "abstract": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations.", "categories": ["cs.CL"], "published": "2025-05-19 00:14:43", "updated": "2025-05-19 00:14:43", "pdf_url": "http://arxiv.org/pdf/2505.12584v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12587v1", "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "abstract": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 00:50:49", "updated": "2025-05-19 00:50:49", "pdf_url": "http://arxiv.org/pdf/2505.12587v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12592v1", "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "abstract": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts.", "categories": ["cs.CL"], "published": "2025-05-19 01:08:26", "updated": "2025-05-19 01:08:26", "pdf_url": "http://arxiv.org/pdf/2505.12592v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12594v1", "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "abstract": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 01:14:57", "updated": "2025-05-19 01:14:57", "pdf_url": "http://arxiv.org/pdf/2505.12594v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12616v1", "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "authors": ["Shujauddin Syed", "Ted Pedersen"], "abstract": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios.", "categories": ["cs.CL", "68T50"], "published": "2025-05-19 01:58:22", "updated": "2025-05-19 01:58:22", "pdf_url": "http://arxiv.org/pdf/2505.12616v1", "comment": "SemEval-2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12621v1", "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "authors": ["Jo\u00e3o Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "abstract": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-19 02:08:20", "updated": "2025-05-19 02:08:20", "pdf_url": "http://arxiv.org/pdf/2505.12621v1", "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12625v1", "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "abstract": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment.", "categories": ["cs.CL", "cs.CR", "cs.LG"], "published": "2025-05-19 02:16:56", "updated": "2025-05-19 02:16:56", "pdf_url": "http://arxiv.org/pdf/2505.12625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12629v1", "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "abstract": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 02:35:53", "updated": "2025-05-19 02:35:53", "pdf_url": "http://arxiv.org/pdf/2505.12629v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12632v1", "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 02:39:03", "updated": "2025-05-19 02:39:03", "pdf_url": "http://arxiv.org/pdf/2505.12632v1", "comment": "CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12636v1", "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "abstract": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here.", "categories": ["cs.CL"], "published": "2025-05-19 02:44:57", "updated": "2025-05-19 02:44:57", "pdf_url": "http://arxiv.org/pdf/2505.12636v1", "comment": "Accepted by ACL 2025 main", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12654v1", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "abstract": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:08:30", "updated": "2025-05-19 03:08:30", "pdf_url": "http://arxiv.org/pdf/2505.12654v1", "comment": "Accepected by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12662v1", "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "abstract": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 03:25:18", "updated": "2025-05-19 03:25:18", "pdf_url": "http://arxiv.org/pdf/2505.12662v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12680v1", "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "abstract": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-19 03:56:05", "updated": "2025-05-19 03:56:05", "pdf_url": "http://arxiv.org/pdf/2505.12680v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12692v1", "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "abstract": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 04:32:02", "updated": "2025-05-19 04:32:02", "pdf_url": "http://arxiv.org/pdf/2505.12692v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12716v1", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "abstract": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 05:16:21", "updated": "2025-05-19 05:16:21", "pdf_url": "http://arxiv.org/pdf/2505.12716v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12717v1", "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "abstract": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks.", "categories": ["cs.CL"], "published": "2025-05-19 05:18:58", "updated": "2025-05-19 05:18:58", "pdf_url": "http://arxiv.org/pdf/2505.12717v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12718v1", "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "abstract": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content.", "categories": ["cs.CL", "cs.HC"], "published": "2025-05-19 05:19:26", "updated": "2025-05-19 05:19:26", "pdf_url": "http://arxiv.org/pdf/2505.12718v1", "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12723v1", "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "abstract": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages.", "categories": ["cs.CL"], "published": "2025-05-19 05:25:29", "updated": "2025-05-19 05:25:29", "pdf_url": "http://arxiv.org/pdf/2505.12723v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12727v1", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "abstract": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma.", "categories": ["cs.CL", "cs.CY", "cs.HC"], "published": "2025-05-19 05:31:42", "updated": "2025-05-19 05:31:42", "pdf_url": "http://arxiv.org/pdf/2505.12727v1", "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12763v1", "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "abstract": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 06:43:08", "updated": "2025-05-19 06:43:08", "pdf_url": "http://arxiv.org/pdf/2505.12763v1", "comment": "Accepted to ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12768v1", "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "abstract": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set.", "categories": ["cs.CL"], "published": "2025-05-19 06:46:47", "updated": "2025-05-19 06:46:47", "pdf_url": "http://arxiv.org/pdf/2505.12768v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12781v1", "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:10:42", "updated": "2025-05-19 07:10:42", "pdf_url": "http://arxiv.org/pdf/2505.12781v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12792v1", "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "abstract": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies.", "categories": ["cs.CL"], "published": "2025-05-19 07:24:35", "updated": "2025-05-19 07:24:35", "pdf_url": "http://arxiv.org/pdf/2505.12792v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12808v1", "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "abstract": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 07:34:25", "updated": "2025-05-19 07:34:25", "pdf_url": "http://arxiv.org/pdf/2505.12808v1", "comment": "20 pages, ongoing work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12814v1", "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "abstract": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 07:45:09", "updated": "2025-05-19 07:45:09", "pdf_url": "http://arxiv.org/pdf/2505.12814v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12821v1", "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "abstract": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:03:38", "updated": "2025-05-19 08:03:38", "pdf_url": "http://arxiv.org/pdf/2505.12821v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12831v1", "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "abstract": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP.", "categories": ["cs.CL"], "published": "2025-05-19 08:19:27", "updated": "2025-05-19 08:19:27", "pdf_url": "http://arxiv.org/pdf/2505.12831v1", "comment": "ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12835v1", "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "abstract": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-19 08:21:20", "updated": "2025-05-19 08:21:20", "pdf_url": "http://arxiv.org/pdf/2505.12835v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12837v1", "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "abstract": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 08:25:21", "updated": "2025-05-19 08:25:21", "pdf_url": "http://arxiv.org/pdf/2505.12837v1", "comment": "20 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12842v1", "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "abstract": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 08:29:05", "updated": "2025-05-19 08:29:05", "pdf_url": "http://arxiv.org/pdf/2505.12842v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12859v1", "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "abstract": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge.", "categories": ["cs.CL"], "published": "2025-05-19 08:43:54", "updated": "2025-05-19 08:43:54", "pdf_url": "http://arxiv.org/pdf/2505.12859v1", "comment": "To be presented a ACL 2025, Main, Long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12864v1", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr\u00fcwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "abstract": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "published": "2025-05-19 08:48:12", "updated": "2025-05-19 08:48:12", "pdf_url": "http://arxiv.org/pdf/2505.12864v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12871v1", "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-19 08:57:08", "updated": "2025-05-19 08:57:08", "pdf_url": "http://arxiv.org/pdf/2505.12871v1", "comment": "To appear at ICML 25", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12886v1", "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "categories": ["cs.AI", "cs.CL", "cs.CY"], "published": "2025-05-19 09:16:40", "updated": "2025-05-19 09:16:40", "pdf_url": "http://arxiv.org/pdf/2505.12886v1", "comment": "25 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12888v1", "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "abstract": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines.", "categories": ["cs.CL"], "published": "2025-05-19 09:18:19", "updated": "2025-05-19 09:18:19", "pdf_url": "http://arxiv.org/pdf/2505.12888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12891v1", "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "abstract": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 09:22:02", "updated": "2025-05-19 09:22:02", "pdf_url": "http://arxiv.org/pdf/2505.12891v1", "comment": "First version. There are still some examples to be added into the\n  appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12896v1", "title": "On the Thinking-Language Modeling Gap in Large Language Models", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "abstract": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks.", "categories": ["cs.CL", "cs.LG", "stat.ML"], "published": "2025-05-19 09:31:52", "updated": "2025-05-19 09:31:52", "pdf_url": "http://arxiv.org/pdf/2505.12896v1", "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12900v1", "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "abstract": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation.", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "published": "2025-05-19 09:35:58", "updated": "2025-05-19 09:35:58", "pdf_url": "http://arxiv.org/pdf/2505.12900v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12920v1", "title": "PyFCG: Fluid Construction Grammar in Python", "authors": ["Paul Van Eecke", "Katrien Beuls"], "abstract": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-19 10:00:01", "updated": "2025-05-19 10:00:01", "pdf_url": "http://arxiv.org/pdf/2505.12920v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12929v1", "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "abstract": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:14:08", "updated": "2025-05-19 10:14:08", "pdf_url": "http://arxiv.org/pdf/2505.12929v1", "comment": "24 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12938v1", "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "abstract": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 10:22:04", "updated": "2025-05-19 10:22:04", "pdf_url": "http://arxiv.org/pdf/2505.12938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12942v1", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "abstract": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 10:29:32", "updated": "2025-05-19 10:29:32", "pdf_url": "http://arxiv.org/pdf/2505.12942v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12949v1", "title": "Neural Morphological Tagging for Nguni Languages", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "abstract": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages.", "categories": ["cs.CL"], "published": "2025-05-19 10:41:47", "updated": "2025-05-19 10:41:47", "pdf_url": "http://arxiv.org/pdf/2505.12949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12950v1", "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "abstract": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE.", "categories": ["cs.CL"], "published": "2025-05-19 10:42:36", "updated": "2025-05-19 10:42:36", "pdf_url": "http://arxiv.org/pdf/2505.12950v1", "comment": "14 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12964v1", "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "abstract": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master.", "categories": ["cs.CL"], "published": "2025-05-19 11:00:43", "updated": "2025-05-19 11:00:43", "pdf_url": "http://arxiv.org/pdf/2505.12964v1", "comment": "preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12969v1", "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "abstract": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other.", "categories": ["cs.CL"], "published": "2025-05-19 11:04:52", "updated": "2025-05-19 11:04:52", "pdf_url": "http://arxiv.org/pdf/2505.12969v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12970v1", "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "authors": ["Robin Jegan", "Andreas Henrich"], "abstract": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801", "categories": ["cs.CL"], "published": "2025-05-19 11:06:50", "updated": "2025-05-19 11:06:50", "pdf_url": "http://arxiv.org/pdf/2505.12970v1", "comment": "14 pages, 1 figure", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12973v1", "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "abstract": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems.", "categories": ["cs.CL"], "published": "2025-05-19 11:11:12", "updated": "2025-05-19 11:11:12", "pdf_url": "http://arxiv.org/pdf/2505.12973v1", "comment": "8 main body pages, total 25 pages, 15 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12983v1", "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:18:54", "updated": "2025-05-19 11:18:54", "pdf_url": "http://arxiv.org/pdf/2505.12983v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12992v1", "title": "Fractured Chain-of-Thought Reasoning", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "published": "2025-05-19 11:30:41", "updated": "2025-05-19 11:30:41", "pdf_url": "http://arxiv.org/pdf/2505.12992v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.12996v1", "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 11:34:47", "updated": "2025-05-19 11:34:47", "pdf_url": "http://arxiv.org/pdf/2505.12996v1", "comment": "12 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13004v1", "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "abstract": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x.", "categories": ["cs.CL"], "published": "2025-05-19 11:43:37", "updated": "2025-05-19 11:43:37", "pdf_url": "http://arxiv.org/pdf/2505.13004v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13006v1", "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias K\u00e4fer"], "abstract": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions.", "categories": ["cs.CL"], "published": "2025-05-19 11:46:30", "updated": "2025-05-19 11:46:30", "pdf_url": "http://arxiv.org/pdf/2505.13006v1", "comment": "Accepted by NAACL 2025 industry track", "doi": null, "journal_ref": "In Proc. NAACL-HLT 2025 Industry Track, pp. 794-808. Albuquerque,\n  NM, 2025"}
{"arxiv_id": "2505.13010v1", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "abstract": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-19 11:54:39", "updated": "2025-05-19 11:54:39", "pdf_url": "http://arxiv.org/pdf/2505.13010v1", "comment": "7 pages, 5 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13028v1", "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "authors": ["Sayon Palit", "Daniel Woods"], "abstract": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "published": "2025-05-19 12:12:00", "updated": "2025-05-19 12:12:00", "pdf_url": "http://arxiv.org/pdf/2505.13028v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13032v1", "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix", "authors": ["Ziyang Ma", "Yinghao Ma", "Yanqiao Zhu", "Chen Yang", "Yi-Wen Chao", "Ruiyang Xu", "Wenxi Chen", "Yuanzhe Chen", "Zhuo Chen", "Jian Cong", "Kai Li", "Keliang Li", "Siyou Li", "Xinfeng Li", "Xiquan Li", "Zheng Lian", "Yuzhe Liang", "Minghao Liu", "Zhikang Niu", "Tianrui Wang", "Yuping Wang", "Yuxuan Wang", "Yihao Wu", "Guanrou Yang", "Jianwei Yu", "Ruibin Yuan", "Zhisheng Zheng", "Ziya Zhou", "Haina Zhu", "Wei Xue", "Emmanouil Benetos", "Kai Yu", "Eng-Siong Chng", "Xie Chen"], "abstract": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area.", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "published": "2025-05-19 12:18:42", "updated": "2025-05-19 12:18:42", "pdf_url": "http://arxiv.org/pdf/2505.13032v1", "comment": "Open-source at https://github.com/ddlBoJack/MMAR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13034v1", "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "authors": ["M\u00e1rton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "abstract": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models.", "categories": ["cs.CL"], "published": "2025-05-19 12:19:01", "updated": "2025-05-19 12:19:01", "pdf_url": "http://arxiv.org/pdf/2505.13034v1", "comment": "9 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13036v1", "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "authors": ["Sai Koneru", "Maike Z\u00fcfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "abstract": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:21:29", "updated": "2025-05-19 12:21:29", "pdf_url": "http://arxiv.org/pdf/2505.13036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13053v1", "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "abstract": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 12:42:23", "updated": "2025-05-19 12:42:23", "pdf_url": "http://arxiv.org/pdf/2505.13053v1", "comment": "currently under review at Frontiers in Communication", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13069v1", "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenol\u00e9 Quellec"], "abstract": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability.", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "published": "2025-05-19 13:04:37", "updated": "2025-05-19 13:04:37", "pdf_url": "http://arxiv.org/pdf/2505.13069v1", "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13077v1", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "abstract": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 13:11:28", "updated": "2025-05-19 13:11:28", "pdf_url": "http://arxiv.org/pdf/2505.13077v1", "comment": "Accepted to ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13089v1", "title": "Systematic Generalization in Language Models Scales with Information Entropy", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "\u00c9tienne Simon"], "abstract": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization.", "categories": ["cs.CL"], "published": "2025-05-19 13:23:44", "updated": "2025-05-19 13:23:44", "pdf_url": "http://arxiv.org/pdf/2505.13089v1", "comment": "Accepted to ACL 2025: Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13090v1", "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "authors": ["David Stap", "Christof Monz"], "abstract": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity.", "categories": ["cs.CL"], "published": "2025-05-19 13:24:01", "updated": "2025-05-19 13:24:01", "pdf_url": "http://arxiv.org/pdf/2505.13090v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13098v1", "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "abstract": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.", "categories": ["cs.AI", "cs.CL", "cs.DB"], "published": "2025-05-19 13:29:27", "updated": "2025-05-19 13:29:27", "pdf_url": "http://arxiv.org/pdf/2505.13098v1", "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13109v1", "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "abstract": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 13:36:45", "updated": "2025-05-19 13:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13109v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13115v1", "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "abstract": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-19 13:46:35", "updated": "2025-05-19 13:46:35", "pdf_url": "http://arxiv.org/pdf/2505.13115v1", "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13126v1", "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "abstract": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 13:58:15", "updated": "2025-05-19 13:58:15", "pdf_url": "http://arxiv.org/pdf/2505.13126v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13136v1", "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "abstract": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 14:07:20", "updated": "2025-05-19 14:07:20", "pdf_url": "http://arxiv.org/pdf/2505.13136v1", "comment": "under review @ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13141v1", "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "abstract": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English.", "categories": ["cs.CL"], "published": "2025-05-19 14:10:15", "updated": "2025-05-19 14:10:15", "pdf_url": "http://arxiv.org/pdf/2505.13141v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13147v1", "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "abstract": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP.", "categories": ["cs.CL"], "published": "2025-05-19 14:12:05", "updated": "2025-05-19 14:12:05", "pdf_url": "http://arxiv.org/pdf/2505.13147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13156v1", "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "abstract": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:17:37", "updated": "2025-05-19 14:17:37", "pdf_url": "http://arxiv.org/pdf/2505.13156v1", "comment": "23 pages, 4 figures, and 1 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13157v1", "title": "Role-Playing Evaluation for Large Language Models", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "abstract": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:18:16", "updated": "2025-05-19 14:18:16", "pdf_url": "http://arxiv.org/pdf/2505.13157v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13171v1", "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "abstract": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.", "categories": ["cs.CL"], "published": "2025-05-19 14:28:35", "updated": "2025-05-19 14:28:35", "pdf_url": "http://arxiv.org/pdf/2505.13171v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13173v1", "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.", "categories": ["cs.CL", "I.2.7"], "published": "2025-05-19 14:30:10", "updated": "2025-05-19 14:30:10", "pdf_url": "http://arxiv.org/pdf/2505.13173v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13176v1", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "abstract": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:30:46", "updated": "2025-05-19 14:30:46", "pdf_url": "http://arxiv.org/pdf/2505.13176v1", "comment": "Accepted by ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13181v1", "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "abstract": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-19 14:38:59", "updated": "2025-05-19 14:38:59", "pdf_url": "http://arxiv.org/pdf/2505.13181v1", "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13204v1", "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "abstract": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23.", "categories": ["cs.CL"], "published": "2025-05-19 14:55:41", "updated": "2025-05-19 14:55:41", "pdf_url": "http://arxiv.org/pdf/2505.13204v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13208v1", "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "abstract": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.", "categories": ["quant-ph", "cs.AI", "cs.CL"], "published": "2025-05-19 14:57:53", "updated": "2025-05-19 14:57:53", "pdf_url": "http://arxiv.org/pdf/2505.13208v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13210v1", "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "abstract": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 14:58:44", "updated": "2025-05-19 14:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13210v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13220v1", "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "abstract": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.", "categories": ["cs.CL"], "published": "2025-05-19 15:02:59", "updated": "2025-05-19 15:02:59", "pdf_url": "http://arxiv.org/pdf/2505.13220v1", "comment": "Accepted by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "published": "2025-05-19 15:09:23", "updated": "2025-05-19 15:09:23", "pdf_url": "http://arxiv.org/pdf/2505.13227v1", "comment": "49 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13237v1", "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information", "authors": ["Chih-Kai Yang", "Neo Ho", "Yen-Ting Piao", "Hung-yi Lee"], "abstract": "Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-19 15:20:32", "updated": "2025-05-19 15:20:32", "pdf_url": "http://arxiv.org/pdf/2505.13237v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13244v1", "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "abstract": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 15:24:53", "updated": "2025-05-19 15:24:53", "pdf_url": "http://arxiv.org/pdf/2505.13244v1", "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13251v1", "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "authors": ["Sidney Wong"], "abstract": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide.", "categories": ["cs.CL"], "published": "2025-05-19 15:34:07", "updated": "2025-05-19 15:34:07", "pdf_url": "http://arxiv.org/pdf/2505.13251v1", "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13252v1", "title": "Natural Language Planning via Coding and Inference Scaling", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "abstract": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization.", "categories": ["cs.CL"], "published": "2025-05-19 15:35:17", "updated": "2025-05-19 15:35:17", "pdf_url": "http://arxiv.org/pdf/2505.13252v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13254v1", "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "abstract": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration.", "categories": ["cs.CL"], "published": "2025-05-19 15:38:40", "updated": "2025-05-19 15:38:40", "pdf_url": "http://arxiv.org/pdf/2505.13254v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13257v1", "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "authors": ["Zilu Tang", "Afra Feyza Aky\u00fcrek", "Ekin Aky\u00fcrek", "Derry Wijaya"], "abstract": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:39:48", "updated": "2025-05-19 15:39:48", "pdf_url": "http://arxiv.org/pdf/2505.13257v1", "comment": "9 pages, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13258v1", "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "abstract": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.", "categories": ["cs.CL"], "published": "2025-05-19 15:40:29", "updated": "2025-05-19 15:40:29", "pdf_url": "http://arxiv.org/pdf/2505.13258v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13259v1", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "abstract": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "categories": ["cs.CL"], "published": "2025-05-19 15:41:32", "updated": "2025-05-19 15:41:32", "pdf_url": "http://arxiv.org/pdf/2505.13259v1", "comment": "16 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13268v1", "title": "Representation of perceived prosodic similarity of conversational feedback", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "abstract": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 15:47:51", "updated": "2025-05-19 15:47:51", "pdf_url": "http://arxiv.org/pdf/2505.13268v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13271v1", "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "abstract": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.", "categories": ["cs.CL"], "published": "2025-05-19 15:52:19", "updated": "2025-05-19 15:52:19", "pdf_url": "http://arxiv.org/pdf/2505.13271v1", "comment": "11 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13282v1", "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "abstract": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.", "categories": ["cs.CL"], "published": "2025-05-19 16:06:13", "updated": "2025-05-19 16:06:13", "pdf_url": "http://arxiv.org/pdf/2505.13282v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13302v1", "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "abstract": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "categories": ["cs.CL"], "published": "2025-05-19 16:20:54", "updated": "2025-05-19 16:20:54", "pdf_url": "http://arxiv.org/pdf/2505.13302v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13307v1", "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-19 16:25:55", "updated": "2025-05-19 16:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13307v1", "comment": "Manuscript", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13308v1", "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "abstract": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 16:26:02", "updated": "2025-05-19 16:26:02", "pdf_url": "http://arxiv.org/pdf/2505.13308v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13312v1", "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility.", "categories": ["cs.CL"], "published": "2025-05-19 16:26:58", "updated": "2025-05-19 16:26:58", "pdf_url": "http://arxiv.org/pdf/2505.13312v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13328v1", "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "abstract": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons.", "categories": ["cs.CL"], "published": "2025-05-19 16:36:13", "updated": "2025-05-19 16:36:13", "pdf_url": "http://arxiv.org/pdf/2505.13328v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "abstract": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "categories": ["cs.CL", "cs.AI", "eess.AS"], "published": "2025-05-19 16:47:46", "updated": "2025-05-19 16:47:46", "pdf_url": "http://arxiv.org/pdf/2505.13338v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13346v1", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "abstract": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 16:50:35", "updated": "2025-05-19 16:50:35", "pdf_url": "http://arxiv.org/pdf/2505.13346v1", "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13348v1", "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "abstract": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks.", "categories": ["cs.CL"], "published": "2025-05-19 16:51:12", "updated": "2025-05-19 16:51:12", "pdf_url": "http://arxiv.org/pdf/2505.13348v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13353v1", "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "authors": ["Adam \u0160torek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "abstract": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation.", "categories": ["cs.CL", "cs.LG", "cs.SE"], "published": "2025-05-19 16:56:31", "updated": "2025-05-19 16:56:31", "pdf_url": "http://arxiv.org/pdf/2505.13353v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13360v1", "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian K\u00e4stner", "Tongshuang Wu"], "abstract": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring.", "categories": ["cs.CL", "cs.SE"], "published": "2025-05-19 17:03:42", "updated": "2025-05-19 17:03:42", "pdf_url": "http://arxiv.org/pdf/2505.13360v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13379v1", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:24:16", "updated": "2025-05-19 17:24:16", "pdf_url": "http://arxiv.org/pdf/2505.13379v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13380v1", "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:24:26", "updated": "2025-05-19 17:24:26", "pdf_url": "http://arxiv.org/pdf/2505.13380v1", "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13388v1", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:29:03", "updated": "2025-05-19 17:29:03", "pdf_url": "http://arxiv.org/pdf/2505.13388v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13393v1", "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "authors": ["Christopher K. Frantz"], "abstract": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool.", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "published": "2025-05-19 17:33:15", "updated": "2025-05-19 17:33:15", "pdf_url": "http://arxiv.org/pdf/2505.13393v1", "comment": "24 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13398v1", "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "abstract": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-19 17:34:56", "updated": "2025-05-19 17:34:56", "pdf_url": "http://arxiv.org/pdf/2505.13398v1", "comment": "9 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13403v1", "title": "MR. Judge: Multimodal Reasoner as a Judge", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "abstract": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.", "categories": ["cs.CL"], "published": "2025-05-19 17:37:39", "updated": "2025-05-19 17:37:39", "pdf_url": "http://arxiv.org/pdf/2505.13403v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13404v1", "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "abstract": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary", "categories": ["cs.CL", "eess.AS"], "published": "2025-05-19 17:40:58", "updated": "2025-05-19 17:40:58", "pdf_url": "http://arxiv.org/pdf/2505.13404v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13408v1", "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "abstract": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:44:26", "updated": "2025-05-19 17:44:26", "pdf_url": "http://arxiv.org/pdf/2505.13408v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13417v1", "title": "AdaptThink: Reasoning Models Can Learn When to Think", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "abstract": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-19 17:50:52", "updated": "2025-05-19 17:50:52", "pdf_url": "http://arxiv.org/pdf/2505.13417v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13418v1", "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "abstract": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-19 17:51:35", "updated": "2025-05-19 17:51:35", "pdf_url": "http://arxiv.org/pdf/2505.13418v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13430v1", "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "abstract": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-19 17:55:15", "updated": "2025-05-19 17:55:15", "pdf_url": "http://arxiv.org/pdf/2505.13430v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13434v1", "title": "SMOTExT: SMOTE meets Large Language Models", "authors": ["Mateusz Bystro\u0144ski", "Miko\u0142aj Ho\u0142ysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "abstract": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints.", "categories": ["cs.CL"], "published": "2025-05-19 17:57:36", "updated": "2025-05-19 17:57:36", "pdf_url": "http://arxiv.org/pdf/2505.13434v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-19 17:58:44", "updated": "2025-05-19 17:58:44", "pdf_url": "http://arxiv.org/pdf/2505.13438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13444v1", "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "abstract": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-19 17:59:27", "updated": "2025-05-19 17:59:27", "pdf_url": "http://arxiv.org/pdf/2505.13444v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-19 17:59:31", "updated": "2025-05-19 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.13445v1", "comment": "code available at https://github.com/xyliu-cs/RISE", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "abstract": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-19 17:59:58", "updated": "2025-05-19 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.13448v1", "comment": "10 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13787v1", "title": "Preference Learning with Lie Detectors can Induce Honesty or Evasion", "authors": ["Chris Cundy", "Adam Gleave"], "abstract": "As AI systems become more capable, deceptive behaviors can undermine\nevaluation and mislead users at deployment. Recent work has shown that lie\ndetectors can accurately classify deceptive behavior, but they are not\ntypically used in the training pipeline due to concerns around contamination\nand objective hacking. We examine these concerns by incorporating a lie\ndetector into the labelling step of LLM post-training and evaluating whether\nthe learned policy is genuinely more honest, or instead learns to fool the lie\ndetector while remaining deceptive. Using DolusChat, a novel 65k-example\ndataset with paired truthful/deceptive responses, we identify three key factors\nthat determine the honesty of learned policies: amount of exploration during\npreference learning, lie detector accuracy, and KL regularization strength. We\nfind that preference learning with lie detectors and GRPO can lead to policies\nwhich evade lie detectors, with deception rates of over 85\\%. However, if the\nlie detector true positive rate (TPR) or KL regularization is sufficiently\nhigh, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)\nconsistently lead to deception rates under 25\\% for realistic TPRs. Our results\nillustrate a more complex picture than previously assumed: depending on the\ncontext, lie-detector-enhanced training can be a powerful tool for scalable\noversight, or a counterproductive method encouraging undetectable misalignment.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 00:31:53", "updated": "2025-05-20 00:31:53", "pdf_url": "http://arxiv.org/pdf/2505.13787v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13792v1", "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "abstract": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 00:49:19", "updated": "2025-05-20 00:49:19", "pdf_url": "http://arxiv.org/pdf/2505.13792v1", "comment": "10 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13794v1", "title": "LLM-based Evaluation Policy Extraction for Ecological Modeling", "authors": ["Qi Cheng", "Licheng Liu", "Qing Zhu", "Runlong Yu", "Zhenong Jin", "Yiqun Xie", "Xiaowei Jia"], "abstract": "Evaluating ecological time series is critical for benchmarking model\nperformance in many important applications, including predicting greenhouse gas\nfluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.\nTraditional numerical metrics (e.g., R-squared, root mean square error) have\nbeen widely used to quantify the similarity between modeled and observed\necosystem variables, but they often fail to capture domain-specific temporal\npatterns critical to ecological processes. As a result, these methods are often\naccompanied by expert visual inspection, which requires substantial human labor\nand limits the applicability to large-scale evaluation. To address these\nchallenges, we propose a novel framework that integrates metric learning with\nlarge language model (LLM)-based natural language policy extraction to develop\ninterpretable evaluation criteria. The proposed method processes pairwise\nannotations and implements a policy optimization mechanism to generate and\ncombine different assessment metrics. The results obtained on multiple datasets\nfor evaluating the predictions of crop gross primary production and carbon\ndioxide flux have confirmed the effectiveness of the proposed method in\ncapturing target assessment preferences, including both synthetically generated\nand expert-annotated model comparisons. The proposed framework bridges the gap\nbetween numerical metrics and expert knowledge while providing interpretable\nevaluation policies that accommodate the diverse needs of different ecosystem\nmodeling studies.", "categories": ["cs.AI"], "published": "2025-05-20 01:02:29", "updated": "2025-05-20 01:02:29", "pdf_url": "http://arxiv.org/pdf/2505.13794v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13805v1", "title": "ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech", "authors": ["Yu Pan", "Yanni Hu", "Yuguang Yang", "Jixun Yao", "Jianhao Ye", "Hongbin Zhou", "Lei Ma", "Jianjun Zhao"], "abstract": "Despite great advances, achieving high-fidelity emotional voice conversion\n(EVC) with flexible and interpretable control remains challenging. This paper\nintroduces ClapFM-EVC, a novel EVC framework capable of generating high-quality\nconverted speech driven by natural language prompts or reference speech with\nadjustable emotion intensity. We first propose EVC-CLAP, an emotional\ncontrastive language-audio pre-training model, guided by natural language\nprompts and categorical labels, to extract and align fine-grained emotional\nelements across speech and text modalities. Then, a FuEncoder with an adaptive\nintensity gate is presented to seamless fuse emotional features with Phonetic\nPosteriorGrams from a pre-trained ASR model. To further improve emotion\nexpressiveness and speech naturalness, we propose a flow matching model\nconditioned on these captured features to reconstruct Mel-spectrogram of source\nspeech. Subjective and objective evaluations validate the effectiveness of\nClapFM-EVC.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 01:34:29", "updated": "2025-05-20 01:34:29", "pdf_url": "http://arxiv.org/pdf/2505.13805v1", "comment": "Accepted by InterSpeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13808v1", "title": "RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework", "authors": ["Faramarz Safi Esfahani", "Ghassan Beydoun", "Morteza Saberi", "Brad McCusker", "Biswajeet Pradhan"], "abstract": "Metaheuristic algorithms are widely used for solving complex optimization\nproblems, yet their effectiveness is often constrained by fixed structures and\nthe need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)\naddresses this limitation by introducing a self-adaptive metaheuristic\nswitching mechanism driven by real-time performance feedback and dynamic\nalgorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)\nand the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select\nand transition between metaheuristic algorithms based on key performance\nindicators, ensuring continuous adaptation. This approach enhances convergence\nspeed, adaptability, and solution quality, outperforming traditional\nmetaheuristics in high-dimensional, dynamic, and multimodal environments.\nExperimental results on benchmark functions demonstrate that PMF significantly\nimproves optimization efficiency by mitigating stagnation and balancing\nexploration-exploitation strategies across various problem landscapes. By\nintegrating AI-driven decision-making and self-correcting mechanisms, PMF paves\nthe way for scalable, intelligent, and autonomous optimization frameworks, with\npromising applications in engineering, logistics, and complex decision-making\nsystems.", "categories": ["cs.NE", "cs.AI"], "published": "2025-05-20 01:41:22", "updated": "2025-05-20 01:41:22", "pdf_url": "http://arxiv.org/pdf/2505.13808v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13814v1", "title": "Articulatory Feature Prediction from Surface EMG during Speech Production", "authors": ["Jihwan Lee", "Kevin Huang", "Kleanthis Avramidis", "Simon Pistrosch", "Monica Gonzalez-Machorro", "Yoonjeong Lee", "Bj\u00f6rn Schuller", "Louis Goldstein", "Shrikanth Narayanan"], "abstract": "We present a model for predicting articulatory features from surface\nelectromyography (EMG) signals during speech production. The proposed model\nintegrates convolutional layers and a Transformer block, followed by separate\npredictors for articulatory features. Our approach achieves a high prediction\ncorrelation of approximately 0.9 for most articulatory features. Furthermore,\nwe demonstrate that these predicted articulatory features can be decoded into\nintelligible speech waveforms. To our knowledge, this is the first method to\ndecode speech waveforms from surface EMG via articulatory features, offering a\nnovel approach to EMG-based speech synthesis. Additionally, we analyze the\nrelationship between EMG electrode placement and articulatory feature\npredictability, providing knowledge-driven insights for optimizing EMG\nelectrode configurations. The source code and decoded speech samples are\npublicly available.", "categories": ["eess.AS", "cs.AI", "cs.SD"], "published": "2025-05-20 01:50:05", "updated": "2025-05-20 01:50:05", "pdf_url": "http://arxiv.org/pdf/2505.13814v1", "comment": "Accepted for Interspeech2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13820v1", "title": "Structured Agent Distillation for Large Language Model", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 02:01:55", "updated": "2025-05-20 02:01:55", "pdf_url": "http://arxiv.org/pdf/2505.13820v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13828v1", "title": "Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models", "authors": ["Kiarash Naghavi Khanghah", "Zhiling Chen", "Lela Romeo", "Qian Yang", "Rajiv Malhotra", "Farhad Imani", "Hongyi Xu"], "abstract": "Additive manufacturing enables the fabrication of complex designs while\nminimizing waste, but faces challenges related to defects and process\nanomalies. This study presents a novel multimodal Retrieval-Augmented\nGeneration-based framework that automates anomaly detection across various\nAdditive Manufacturing processes leveraging retrieved information from\nliterature, including images and descriptive text, rather than training\ndatasets. This framework integrates text and image retrieval from scientific\nliterature and multimodal generation models to perform zero-shot anomaly\nidentification, classification, and explanation generation in a Laser Powder\nBed Fusion setting. The proposed framework is evaluated on four L-PBF\nmanufacturing datasets from Oak Ridge National Laboratory, featuring various\nprinter makes, models, and materials. This evaluation demonstrates the\nframework's adaptability and generalizability across diverse images without\nrequiring additional training. Comparative analysis using Qwen2-VL-2B and\nGPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini\noutperforms Qwen2-VL-2B and proportional random baseline in manufacturing\nanomalies classification. Additionally, the evaluation of the RAG system\nconfirms that incorporating retrieval mechanisms improves average accuracy by\n12% by reducing the risk of hallucination and providing additional information.\nThe proposed framework can be continuously updated by integrating emerging\nresearch, allowing seamless adaptation to the evolving landscape of AM\ntechnologies. This scalable, automated, and zero-shot-capable framework\nstreamlines AM anomaly analysis, enhancing efficiency and accuracy.", "categories": ["cs.AI"], "published": "2025-05-20 02:18:22", "updated": "2025-05-20 02:18:22", "pdf_url": "http://arxiv.org/pdf/2505.13828v1", "comment": "ASME 2025 International Design Engineering Technical Conferences and\n  Computers and Information in Engineering Conference IDETC/CIE2025, August\n  17-20, 2025, Anaheim, CA (IDETC2025-168615)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13831v1", "title": "TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning", "authors": ["Zongyuan Deng", "Yujie Cai", "Qing Liu", "Shiyao Mu", "Bin Lyu", "Zhen Yang"], "abstract": "The selection of base station sites is a critical challenge in 5G network\nplanning, which requires efficient optimization of coverage, cost, user\nsatisfaction, and practical constraints. Traditional manual methods, reliant on\nhuman expertise, suffer from inefficiencies and are limited to an unsatisfied\nplanning-construction consistency. Existing AI tools, despite improving\nefficiency in certain aspects, still struggle to meet the dynamic network\nconditions and multi-objective needs of telecom operators' networks. To address\nthese challenges, we propose TelePlanNet, an AI-driven framework tailored for\nthe selection of base station sites, integrating a three-layer architecture for\nefficient planning and large-scale automation. By leveraging large language\nmodels (LLMs) for real-time user input processing and intent alignment with\nbase station planning, combined with training the planning model using the\nimproved group relative policy optimization (GRPO) reinforcement learning, the\nproposed TelePlanNet can effectively address multi-objective optimization,\nevaluates candidate sites, and delivers practical solutions. Experiments\nresults show that the proposed TelePlanNet can improve the consistency to 78%,\nwhich is superior to the manual methods, providing telecom operators with an\nefficient and scalable tool that significantly advances cellular network\nplanning.", "categories": ["cs.AI", "I.2; I.2.6; C.2.1"], "published": "2025-05-20 02:19:10", "updated": "2025-05-20 02:19:10", "pdf_url": "http://arxiv.org/pdf/2505.13831v1", "comment": "6 pages, 5 figures, 1 table, submitted to IEEE ICCC 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13834v1", "title": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams", "authors": ["Zhi Su", "Yuman Gao", "Emily Lukas", "Yunfei Li", "Jiaze Cai", "Faris Tulbah", "Fei Gao", "Chao Yu", "Zhongyu Li", "Yi Wu", "Koushil Sreenath"], "abstract": "Achieving coordinated teamwork among legged robots requires both fine-grained\nlocomotion control and long-horizon strategic decision-making. Robot soccer\noffers a compelling testbed for this challenge, combining dynamic, competitive,\nand multi-agent interactions. In this work, we present a hierarchical\nmulti-agent reinforcement learning (MARL) framework that enables fully\nautonomous and decentralized quadruped robot soccer. First, a set of highly\ndynamic low-level skills is trained for legged locomotion and ball\nmanipulation, such as walking, dribbling, and kicking. On top of these, a\nhigh-level strategic planning policy is trained with Multi-Agent Proximal\nPolicy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning\nframework allows agents to adapt to diverse opponent strategies and gives rise\nto sophisticated team behaviors, including coordinated passing, interception,\nand dynamic role allocation. With an extensive ablation study, the proposed\nlearning method shows significant advantages in the cooperative and competitive\nmulti-agent soccer game. We deploy the learned policies to real quadruped\nrobots relying solely on onboard proprioception and decentralized localization,\nwith the resulting system supporting autonomous robot-robot and robot-human\nsoccer matches on indoor and outdoor soccer courts.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 02:20:54", "updated": "2025-05-20 02:20:54", "pdf_url": "http://arxiv.org/pdf/2505.13834v1", "comment": "11 pages, 12 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13837v1", "title": "Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements", "authors": ["Gokul Puthumanaillam", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Melkior Ornik"], "abstract": "Robots navigating complex environments must manage uncertainty from sensor\nnoise, environmental changes, and incomplete information, with different tasks\nrequiring varying levels of precision in different areas. For example, precise\nlocalization may be crucial near obstacles but less critical in open spaces. We\npresent GUIDE (Generalized Uncertainty Integration for Decision-Making and\nExecution), a framework that integrates these task-specific requirements into\nnavigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning\nacceptable uncertainty levels to different locations, TSUMs enable robots to\nadapt uncertainty management based on context. When combined with reinforcement\nlearning, GUIDE learns policies that balance task completion and uncertainty\nmanagement without extensive reward engineering. Real-world tests show\nsignificant performance gains over methods lacking task-specific uncertainty\nawareness.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-05-20 02:23:15", "updated": "2025-05-20 02:23:15", "pdf_url": "http://arxiv.org/pdf/2505.13837v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "abstract": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 02:27:08", "updated": "2025-05-20 02:27:08", "pdf_url": "http://arxiv.org/pdf/2505.13840v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13847v1", "title": "Forensic deepfake audio detection using segmental speech features", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "abstract": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 02:42:46", "updated": "2025-05-20 02:42:46", "pdf_url": "http://arxiv.org/pdf/2505.13847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13851v1", "title": "A Challenge to Build Neuro-Symbolic Video Agents", "authors": ["Sahil Shah", "Harsh Goel", "Sai Shankar Narasimhan", "Minkyu Choi", "S P Sharan", "Oguzhan Akcin", "Sandeep Chinchali"], "abstract": "Modern video understanding systems excel at tasks such as scene\nclassification, object detection, and short video retrieval. However, as video\nanalysis becomes increasingly central to real-world applications, there is a\ngrowing need for proactive video agents for the systems that not only interpret\nvideo streams but also reason about events and take informed actions. A key\nobstacle in this direction is temporal reasoning: while deep learning models\nhave made remarkable progress in recognizing patterns within individual frames\nor short clips, they struggle to understand the sequencing and dependencies of\nevents over time, which is critical for action-driven decision-making.\nAddressing this limitation demands moving beyond conventional deep learning\napproaches. We posit that tackling this challenge requires a neuro-symbolic\nperspective, where video queries are decomposed into atomic events, structured\ninto coherent sequences, and validated against temporal constraints. Such an\napproach can enhance interpretability, enable structured reasoning, and provide\nstronger guarantees on system behavior, all key properties for advancing\ntrustworthy video agents. To this end, we present a grand challenge to the\nresearch community: developing the next generation of intelligent video agents\nthat integrate three core capabilities: (1) autonomous video search and\nanalysis, (2) seamless real-world interaction, and (3) advanced content\ngeneration. By addressing these pillars, we can transition from passive\nperception to intelligent video agents that reason, predict, and act, pushing\nthe boundaries of video understanding.", "categories": ["cs.AI"], "published": "2025-05-20 02:53:21", "updated": "2025-05-20 02:53:21", "pdf_url": "http://arxiv.org/pdf/2505.13851v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13855v1", "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "abstract": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 03:02:05", "updated": "2025-05-20 03:02:05", "pdf_url": "http://arxiv.org/pdf/2505.13855v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13857v1", "title": "Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer", "authors": ["Tian Sun", "Yuqi Chen", "Baihua Zheng", "Weiwei Sun"], "abstract": "In real-world applications, GPS trajectories often suffer from low sampling\nrates, with large and irregular intervals between consecutive GPS points. This\nsparse characteristic presents challenges for their direct use in GPS-based\nsystems. This paper addresses the task of map-constrained trajectory recovery,\naiming to enhance trajectory sampling rates of GPS trajectories. Previous\nstudies commonly adopt a sequence-to-sequence framework, where an encoder\ncaptures the trajectory patterns and a decoder reconstructs the target\ntrajectory. Within this framework, effectively representing the road network\nand extracting relevant trajectory features are crucial for overall\nperformance. Despite advancements in these models, they fail to fully leverage\nthe complex spatio-temporal dynamics present in both the trajectory and the\nroad network.\n  To overcome these limitations, we categorize the spatio-temporal dynamics of\ntrajectory data into two distinct aspects: spatial-temporal traffic dynamics\nand trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for\ntrajectory recovery. To capture spatio-temporal traffic dynamics, we introduce\nPD-GNN, which models periodic patterns and learns topologically aware dynamics\nconcurrently for each road segment. For spatio-temporal trajectory dynamics, we\npresent TedFormer, a time-aware Transformer that incorporates temporal dynamics\nfor each GPS location by integrating closed-form neural ordinary differential\nequations into the attention mechanism. This allows TedFormer to effectively\nhandle irregularly sampled data. Extensive experiments on three real-world\ndatasets demonstrate the superior performance of TedTrajRec. The code is\npublicly available at https://github.com/ysygMhdxw/TEDTrajRec/.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 03:09:17", "updated": "2025-05-20 03:09:17", "pdf_url": "http://arxiv.org/pdf/2505.13857v1", "comment": "Accepted as a journal paper in IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13860v1", "title": "Domain Adaptation of VLM for Soccer Video Understanding", "authors": ["Tiancheng Jiang", "Henry Wang", "Md Sirajus Salekin", "Parmida Atighehchian", "Shinan Zhang"], "abstract": "Vision Language Models (VLMs) have demonstrated strong performance in\nmulti-modal tasks by effectively aligning visual and textual representations.\nHowever, most video understanding VLM research has been domain-agnostic,\nleaving the understanding of their transfer learning capability to specialized\ndomains under-explored. In this work, we address this by exploring the\nadaptability of open-source VLMs to specific domains, and focusing on soccer as\nan initial case study. Our approach uses large-scale soccer datasets and LLM to\ncreate instruction-following data, and use them to iteratively fine-tune the\ngeneral-domain VLM in a curriculum learning fashion (first teaching the model\nkey soccer concepts to then question answering tasks). The final adapted model,\ntrained using a curated dataset of 20k video clips, exhibits significant\nimprovement in soccer-specific tasks compared to the base model, with a 37.5%\nrelative improvement for the visual question-answering task and an accuracy\nimprovement from 11.8% to 63.5% for the downstream soccer action classification\ntask.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 03:12:21", "updated": "2025-05-20 03:12:21", "pdf_url": "http://arxiv.org/pdf/2505.13860v1", "comment": "8 pages, 5 figures, accepted to the 11th IEEE International Workshop\n  on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix\n  included as ancillary PDF", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13872v1", "title": "Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving", "authors": ["Jingzheng Li", "Tiancheng Wang", "Xingyu Peng", "Jiacheng Chen", "Zhijun Chen", "Bing Li", "Xianglong Liu"], "abstract": "Autonomous Driving (AD) systems demand the high levels of safety assurance.\nDespite significant advancements in AD demonstrated on open-source benchmarks\nlike Longest6 and Bench2Drive, existing datasets still lack\nregulatory-compliant scenario libraries for closed-loop testing to\ncomprehensively evaluate the functional safety of AD. Meanwhile, real-world AD\naccidents are underrepresented in current driving datasets. This scarcity leads\nto inadequate evaluation of AD performance, posing risks to safety validation\nand practical deployment. To address these challenges, we propose Safety2Drive,\na safety-critical scenario library designed to evaluate AD systems.\nSafety2Drive offers three key contributions. (1) Safety2Drive comprehensively\ncovers the test items required by standard regulations and contains 70 AD\nfunction test items. (2) Safety2Drive supports the safety-critical scenario\ngeneralization. It has the ability to inject safety threats such as natural\nenvironment corruptions and adversarial attacks cross camera and LiDAR sensors.\n(3) Safety2Drive supports multi-dimensional evaluation. In addition to the\nevaluation of AD systems, it also supports the evaluation of various perception\ntasks, such as object detection and lane detection. Safety2Drive provides a\nparadigm from scenario construction to validation, establishing a standardized\ntest framework for the safe deployment of AD.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 03:27:06", "updated": "2025-05-20 03:27:06", "pdf_url": "http://arxiv.org/pdf/2505.13872v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13873v1", "title": "Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model", "authors": ["Peisong Niu", "Ziqing Ma", "Tian Zhou", "Weiqi Chen", "Lefei Shen", "Rong Jin", "Liang Sun"], "abstract": "Weather forecasting has long posed a significant challenge for humanity.\nWhile recent AI-based models have surpassed traditional numerical weather\nprediction (NWP) methods in global forecasting tasks, overfitting remains a\ncritical issue due to the limited availability of real-world weather data\nspanning only a few decades. Unlike fields like computer vision or natural\nlanguage processing, where data abundance can mitigate overfitting, weather\nforecasting demands innovative strategies to address this challenge with\nexisting data. In this paper, we explore pre-training methods for weather\nforecasting, finding that selecting an appropriately challenging pre-training\ntask introduces locality bias, effectively mitigating overfitting and enhancing\nperformance. We introduce Baguan, a novel data-driven model for medium-range\nweather forecasting, built on a Siamese Autoencoder pre-trained in a\nself-supervised manner and fine-tuned for different lead times. Experimental\nresults show that Baguan outperforms traditional methods, delivering more\naccurate forecasts. Additionally, the pre-trained Baguan demonstrates robust\noverfitting control and excels in downstream tasks, such as\nsubseasonal-to-seasonal (S2S) modeling and regional forecasting, after\nfine-tuning.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 03:29:23", "updated": "2025-05-20 03:29:23", "pdf_url": "http://arxiv.org/pdf/2505.13873v1", "comment": "KDD2025 research track accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13887v1", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "abstract": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 03:48:19", "updated": "2025-05-20 03:48:19", "pdf_url": "http://arxiv.org/pdf/2505.13887v1", "comment": "17 pages, 7 figures, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:2502.17110", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13898v1", "title": "Do Language Models Use Their Depth Efficiently?", "authors": ["R\u00f3bert Csord\u00e1s", "Christopher D. Manning", "Christopher Potts"], "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,\ncomparing the output of the sublayers to the residual stream reveals that\nlayers in the second half contribute much less than those in the first half,\nwith a clear phase transition between the two halves. Second, skipping layers\nin the second half has a much smaller effect on future computations and output\npredictions. Third, for multihop tasks, we are unable to find evidence that\nmodels are using increased depth to compose subresults in examples involving\nmany hops. Fourth, we seek to directly address whether deeper models are using\ntheir additional layers to perform new kinds of computation. To do this, we\ntrain linear maps from the residual stream of a shallow model to a deeper one.\nWe find that layers with the same relative depth map best to each other,\nsuggesting that the larger model simply spreads the same computations out over\nits many layers. All this evidence suggests that deeper models are not using\ntheir depth to learn new kinds of computation, but only using the greater depth\nto perform more fine-grained adjustments to the residual. This may help explain\nwhy increasing scale leads to diminishing returns for stacked Transformer\narchitectures.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "published": "2025-05-20 04:00:56", "updated": "2025-05-20 04:00:56", "pdf_url": "http://arxiv.org/pdf/2505.13898v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13904v1", "title": "Learning to Insert for Constructive Neural Vehicle Routing Solver", "authors": ["Fu Luo", "Xi Lin", "Mengyuan Zhong", "Fei Liu", "Zhenkun Wang", "Jianyong Sun", "Qingfu Zhang"], "abstract": "Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.", "categories": ["cs.LG", "cs.AI", "cs.RO", "math.OC"], "published": "2025-05-20 04:10:50", "updated": "2025-05-20 04:10:50", "pdf_url": "http://arxiv.org/pdf/2505.13904v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13906v1", "title": "XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data", "authors": ["Soyabul Islam Lincoln", "Mirza Mohd Shahriar Maswood"], "abstract": "A common neurodegenerative disease, Alzheimer's disease requires a precise\ndiagnosis and efficient treatment, particularly in light of escalating\nhealthcare expenses and the expanding use of artificial intelligence in medical\ndiagnostics. Many recent studies shows that the combination of brain Magnetic\nResonance Imaging (MRI) and deep neural networks have achieved promising\nresults for diagnosing AD. Using deep convolutional neural networks, this paper\nintroduces a novel deep learning architecture that incorporates multiresidual\nblocks, specialized spatial attention blocks, grouped query attention, and\nmulti-head attention. The study assessed the model's performance on four\npublicly accessible datasets and concentrated on identifying binary and\nmulticlass issues across various categories. This paper also takes into account\nof the explainability of AD's progression and compared with state-of-the-art\nmethods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster\nScore-CAM, and XGRADCAM. Our methodology consistently outperforms current\napproaches, achieving 99.66\\% accuracy in 4-class classification, 99.63\\% in\n3-class classification, and 100\\% in binary classification using Kaggle\ndatasets. For Open Access Series of Imaging Studies (OASIS) datasets the\naccuracies are 99.92\\%, 99.90\\%, and 99.95\\% respectively. The Alzheimer's\nDisease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in\nthree planes (axial, sagittal, and coronal) and a combination of all planes.\nThe study achieved accuracies of 99.08\\% for axis, 99.85\\% for sagittal, 99.5\\%\nfor coronal, and 99.17\\% for all axis, and 97.79\\% and 8.60\\% respectively for\nADNI-2. The network's ability to retrieve important information from MRI images\nis demonstrated by its excellent accuracy in categorizing AD stages.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-20 04:17:28", "updated": "2025-05-20 04:17:28", "pdf_url": "http://arxiv.org/pdf/2505.13906v1", "comment": "20 pages, 12 figures,", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13909v1", "title": "Efficient Agent Training for Computer Use", "authors": ["Yanheng He", "Jiahe Jin", "Pengfei Liu"], "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 04:20:18", "updated": "2025-05-20 04:20:18", "pdf_url": "http://arxiv.org/pdf/2505.13909v1", "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13911v1", "title": "Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation", "authors": ["Ruijie Zhao", "Zuopeng Tan", "Xiao Xue", "Longfei Zhao", "Bing Li", "Zicheng Liao", "Ying Ming", "Jiaru Wang", "Ran Xiao", "Sirong Piao", "Rui Zhao", "Qiqi Xu", "Wei Song"], "abstract": "Pulmonary segment segmentation is crucial for cancer localization and\nsurgical planning. However, the pixel-wise annotation of pulmonary segments is\nlaborious, as the boundaries between segments are indistinguishable in medical\nimages. To this end, we propose a weakly supervised learning (WSL) method,\ntermed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise\nclinical anatomical definition of pulmonary segments to perform pulmonary\nsegment segmentation. Since pulmonary segments reside within the lobes and are\ndetermined by the bronchovascular tree, i.e., artery, airway and vein, the\ndesign of the loss function is founded on two principles. First, segment-level\nlabels are utilized to directly supervise the output of the pulmonary segments,\nensuring that they accurately encompass the appropriate bronchovascular tree.\nSecond, lobe-level supervision indirectly oversees the pulmonary segment,\nensuring their inclusion within the corresponding lobe. Besides, we introduce a\ntwo-stage segmentation strategy that incorporates bronchovascular priori\ninformation. Furthermore, a consistency loss is proposed to enhance the\nsmoothness of segment boundaries, along with an evaluation metric designed to\nmeasure the smoothness of pulmonary segment boundaries. Visual inspection and\nevaluation metrics from experiments conducted on a private dataset demonstrate\nthe effectiveness of our method.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-20 04:23:12", "updated": "2025-05-20 04:23:12", "pdf_url": "http://arxiv.org/pdf/2505.13911v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13914v1", "title": "Parallel Belief Revision via Order Aggregation", "authors": ["Jake Chandler", "Richard Booth"], "abstract": "Despite efforts to better understand the constraints that operate on\nsingle-step parallel (aka \"package\", \"multiple\") revision, very little work has\nbeen carried out on how to extend the model to the iterated case. A recent\npaper by Delgrande & Jin outlines a range of relevant rationality postulates.\nWhile many of these are plausible, they lack an underlying unifying\nexplanation. We draw on recent work on iterated parallel contraction to offer a\ngeneral method for extending serial iterated belief revision operators to\nhandle parallel change. This method, based on a family of order aggregators\nknown as TeamQueue aggregators, provides a principled way to recover the\nindependently plausible properties that can be found in the literature, without\nyielding the more dubious ones.", "categories": ["cs.AI", "I.2.4"], "published": "2025-05-20 04:26:01", "updated": "2025-05-20 04:26:01", "pdf_url": "http://arxiv.org/pdf/2505.13914v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13921v1", "title": "APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight", "authors": ["Wanjing Huang", "Weixiang Yan", "Zhen Zhang", "Ambuj Singh"], "abstract": "Large Language Models (LLMs) demonstrate strong reasoning and task planning\ncapabilities but remain fundamentally limited in physical interaction modeling.\nExisting approaches integrate perception via Vision-Language Models (VLMs) or\nadaptive decision-making through Reinforcement Learning (RL), but they fail to\ncapture dynamic object interactions or require task-specific training, limiting\ntheir real-world applicability. We introduce APEX (Anticipatory\nPhysics-Enhanced Execution), a framework that equips LLMs with physics-driven\nforesight for real-time task planning. APEX constructs structured graphs to\nidentify and model the most relevant dynamic interactions in the environment,\nproviding LLMs with explicit physical state updates. Simultaneously, APEX\nprovides low-latency forward simulations of physically feasible actions,\nallowing LLMs to select optimal strategies based on predictive outcomes rather\nthan static observations. We evaluate APEX on three benchmarks designed to\nassess perception, prediction, and decision-making: (1) Physics Reasoning\nBenchmark, testing causal inference and object motion prediction; (2) Tetris,\nevaluating whether physics-informed prediction enhances decision-making\nperformance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,\nassessing the immediate integration of perception and action feasibility\nanalysis. APEX significantly outperforms standard LLMs and VLM-based models,\ndemonstrating the necessity of explicit physics reasoning for bridging the gap\nbetween language-based intelligence and real-world task execution. The source\ncode and experiment setup are publicly available at\nhttps://github.com/hwj20/APEX_EXP .", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 04:34:58", "updated": "2025-05-20 04:34:58", "pdf_url": "http://arxiv.org/pdf/2505.13921v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13934v1", "title": "RLVR-World: Training World Models with Reinforcement Learning", "authors": ["Jialong Wu", "Shaofeng Yin", "Ningya Feng", "Mingsheng Long"], "abstract": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 05:02:53", "updated": "2025-05-20 05:02:53", "pdf_url": "http://arxiv.org/pdf/2505.13934v1", "comment": "Code is available at project website:\n  https://thuml.github.io/RLVR-World/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13936v1", "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "abstract": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:04:15", "updated": "2025-05-20 05:04:15", "pdf_url": "http://arxiv.org/pdf/2505.13936v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13938v1", "title": "CLEVER: A Curated Benchmark for Formally Verified Code Generation", "authors": ["Amitayush Thakur", "Jasper Lee", "George Tsoukalas", "Meghana Sistla", "Matthew Zhao", "Stefan Zetzche", "Greg Durrett", "Yisong Yue", "Swarat Chaudhuri"], "abstract": "We introduce ${\\rm C{\\small LEVER}}$, a high-quality, curated benchmark of\n161 problems for end-to-end verified code generation in Lean. Each problem\nconsists of (1) the task of generating a specification that matches a held-out\nground-truth specification, and (2) the task of generating a Lean\nimplementation that provably satisfies this specification. Unlike prior\nbenchmarks, ${\\rm C{\\small LEVER}}$ avoids test-case supervision, LLM-generated\nannotations, and specifications that leak implementation logic or allow vacuous\nsolutions. All outputs are verified post-hoc using Lean's type checker to\nensure machine-checkable correctness. We use ${\\rm C{\\small LEVER}}$ to\nevaluate several few-shot and agentic approaches based on state-of-the-art\nlanguage models. These methods all struggle to achieve full verification,\nestablishing it as a challenging frontier benchmark for program synthesis and\nformal reasoning. Our benchmark can be found on\nGitHub(https://github.com/trishullab/clever) as well as\nHuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our\nevaluation code is also available\nonline(https://github.com/trishullab/clever-prover).", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.PL", "cs.SE"], "published": "2025-05-20 05:15:47", "updated": "2025-05-20 05:15:47", "pdf_url": "http://arxiv.org/pdf/2505.13938v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13940v1", "title": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery", "authors": ["Kun Li", "Zhennan Wu", "Shoupeng Wang", "Wenbin Hu"], "abstract": "In the field of AI4Science, large-scale language models (LLMs) show great\npotential to parse complex scientific semantics, integrate cross-disciplinary\nknowledge, and assist critical task research. However, in the field of drug\ndiscovery, despite the optimization through professional data pre-training,\ncontext window expansion, and internet search, the existing LLMs are still\nfacing challenges such as massive multi-modal and heterogeneous data\nprocessing, domain knowledge dynamic updating delay, and insufficient\nconfidence in predicting the results of complex computational tasks. To address\nthese challenges, we propose the DrugPilot, an LLM-based agent with\nparameterized reasoning for drug discovery. DrugPilot addresses key limitations\nof traditional end-to-end LLM prediction approaches through its parametric\ninference architecture. This agent system supports major phases of the drug\ndiscovery pipeline, facilitating automated planning and execution of\nmulti-stage research tasks. To address the critical challenge of multi-modal\ndrug data analysis (incorporating both public datasets and user-submitted\ndata), we developed an interactive parameterized memory pool. This innovative\ncomponent standardizes real-world drug data into parametric representations,\nsimultaneously enabling efficient knowledge retrieval in multi-turn dialogue\nwhile mitigating the information loss inherent in text-based data transmission.\nAdditionally, we created a drug instruct dataset across 8 essential drug\ndiscovery tasks for model fine-tuning and evaluation. Based on the Berkeley\nfunction calling evaluation framework, DrugPilot demonstrated the most advanced\ntool calling capabilities on our drug discovery tool instruction dataset,\noutperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves\ntask completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and\nmulti-turn tasks, respectively.", "categories": ["cs.AI", "q-bio.BM"], "published": "2025-05-20 05:18:15", "updated": "2025-05-20 05:18:15", "pdf_url": "http://arxiv.org/pdf/2505.13940v1", "comment": "22 pages, 10 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13941v1", "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "authors": ["Haoyang Fang", "Boran Han", "Nick Erickson", "Xiyuan Zhang", "Su Zhou", "Anirudh Dagar", "Jiani Zhang", "Ali Caner Turkmen", "Cuixiong Hu", "Huzefa Rangwala", "Ying Nian Wu", "Bernie Wang", "George Karypis"], "abstract": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions.", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 05:20:53", "updated": "2025-05-20 05:20:53", "pdf_url": "http://arxiv.org/pdf/2505.13941v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13946v1", "title": "Visual Instruction Bottleneck Tuning", "authors": ["Changdae Oh", "Jiatong Li", "Shawn Im", "Yixuan Li"], "abstract": "Despite widespread adoption, multimodal large language models (MLLMs) suffer\nperformance degradation when encountering unfamiliar queries under distribution\nshifts. Existing methods to improve MLLM generalization typically require\neither more instruction data or larger advanced model architectures, both of\nwhich incur non-trivial human labor or computational costs. In this work, we\ntake an alternative approach to enhance the robustness of MLLMs under\ndistribution shifts, from a representation learning perspective. Inspired by\nthe information bottleneck (IB) principle, we derive a variational lower bound\nof the IB for MLLMs and devise a practical implementation, Visual Instruction\nBottleneck Tuning (Vittle). We then provide a theoretical justification of\nVittle by revealing its connection to an information-theoretic robustness\nmetric of MLLM. Empirical validation of three MLLMs on open-ended and\nclosed-form question answering and object hallucination detection tasks over 45\ndatasets, including 30 shift scenarios, demonstrates that Vittle consistently\nimproves the MLLM's robustness under shifts by pursuing the learning of a\nminimal sufficient representation.", "categories": ["cs.AI"], "published": "2025-05-20 05:24:53", "updated": "2025-05-20 05:24:53", "pdf_url": "http://arxiv.org/pdf/2505.13946v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13948v1", "title": "Memory-Centric Embodied Question Answer", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "abstract": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.", "categories": ["cs.CL", "cs.AI", "cs.MM"], "published": "2025-05-20 05:27:57", "updated": "2025-05-20 05:27:57", "pdf_url": "http://arxiv.org/pdf/2505.13948v1", "comment": "14pages, 7 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13949v1", "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "abstract": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:28:21", "updated": "2025-05-20 05:28:21", "pdf_url": "http://arxiv.org/pdf/2505.13949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13965v1", "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 06:05:56", "updated": "2025-05-20 06:05:56", "pdf_url": "http://arxiv.org/pdf/2505.13965v1", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13969v1", "title": "Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World", "authors": ["Junya Nakanishi", "Jun Baba", "Yuichiro Yoshikawa", "Hiroko Kamide", "Hiroshi Ishiguro"], "abstract": "This paper discusses the functional advantages of the Selection-Broadcast\nCycle structure proposed by Global Workspace Theory (GWT), inspired by human\nconsciousness, particularly focusing on its applicability to artificial\nintelligence and robotics in dynamic, real-time scenarios. While previous\nstudies often examined the Selection and Broadcast processes independently,\nthis research emphasizes their combined cyclic structure and the resulting\nbenefits for real-time cognitive systems. Specifically, the paper identifies\nthree primary benefits: Dynamic Thinking Adaptation, Experience-Based\nAdaptation, and Immediate Real-Time Adaptation. This work highlights GWT's\npotential as a cognitive architecture suitable for sophisticated\ndecision-making and adaptive performance in unsupervised, dynamic environments.\nIt suggests new directions for the development and implementation of robust,\ngeneral-purpose AI and robotics systems capable of managing complex, real-world\ntasks.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 06:07:21", "updated": "2025-05-20 06:07:21", "pdf_url": "http://arxiv.org/pdf/2505.13969v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13971v1", "title": "The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition", "authors": ["Ming Gao", "Shilong Wu", "Hang Chen", "Jun Du", "Chin-Hui Lee", "Shinji Watanabe", "Jingdong Chen", "Siniscalchi Sabato Marco", "Odette Scharenborg"], "abstract": "Meetings are a valuable yet challenging scenario for speech applications due\nto complex acoustic conditions. This paper summarizes the outcomes of the MISP\n2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,\nmulti-device meeting transcription by incorporating video modality alongside\naudio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual\nSpeech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).\nWe present the challenge's objectives, tasks, dataset, baseline systems, and\nsolutions proposed by participants. The best-performing systems achieved\nsignificant improvements over the baseline: the top AVSD model achieved a\nDiarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system\nachieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the\nbest AVDR system achieved a concatenated minimum-permutation Character Error\nRate (cpCER) of 11.56%, improving by 72.49%.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 06:11:51", "updated": "2025-05-20 06:11:51", "pdf_url": "http://arxiv.org/pdf/2505.13971v1", "comment": "Accepted by Interspeech 2025. Camera-ready version", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13973v1", "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "abstract": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 06:12:20", "updated": "2025-05-20 06:12:20", "pdf_url": "http://arxiv.org/pdf/2505.13973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13986v1", "title": "Solving Normalized Cut Problem with Constrained Action Space", "authors": ["Qize Jiang", "Linsey Pang", "Alice Gatti", "Mahima Aggarwa", "Giovanna Vantin", "Xiaosong Ma", "Weiwei Sun", "Sanjay Chawla"], "abstract": "Reinforcement Learning (RL) has emerged as an important paradigm to solve\ncombinatorial optimization problems primarily due to its ability to learn\nheuristics that can generalize across problem instances. However, integrating\nexternal knowledge that will steer combinatorial optimization problem solutions\ntowards domain appropriate outcomes remains an extremely challenging task. In\nthis paper, we propose the first RL solution that uses constrained action\nspaces to guide the normalized cut problem towards pre-defined template\ninstances. Using transportation networks as an example domain, we create a\nWedge and Ring Transformer that results in graph partitions that are shaped in\nform of Wedges and Rings and which are likely to be closer to natural optimal\npartitions. However, our approach is general as it is based on principles that\ncan be generalized to other domains.", "categories": ["math.OC", "cs.AI", "cs.LG", "I.2.8"], "published": "2025-05-20 06:33:39", "updated": "2025-05-20 06:33:39", "pdf_url": "http://arxiv.org/pdf/2505.13986v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13989v1", "title": "When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty", "authors": ["Yanzhe Wen", "Xunkai Li", "Qi Zhang", "Zhu Lei", "Guang Zeng", "Rong-Hua Li", "Guoren Wang"], "abstract": "Recently, large language models (LLMs) have significantly advanced\ntext-attributed graph (TAG) learning. However, existing methods inadequately\nhandle data uncertainty in open-world scenarios, especially concerning limited\nlabeling and unknown-class nodes. Prior solutions typically rely on isolated\nsemantic or structural approaches for unknown-class rejection, lacking\neffective annotation pipelines. To address these limitations, we propose\nOpen-world Graph Assistant (OGA), an LLM-based framework that combines adaptive\nlabel traceability, which integrates semantics and topology for unknown-class\nrejection, and a graph label annotator to enable model updates using newly\nannotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and\npracticality.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 06:37:18", "updated": "2025-05-20 06:37:18", "pdf_url": "http://arxiv.org/pdf/2505.13989v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13994v1", "title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning", "authors": ["Ruiyi Yang", "Hao Xue", "Imran Razzak", "Hakim Hacid", "Flora D. Salim"], "abstract": "Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches.", "categories": ["cs.AI", "cs.IR", "cs.MA"], "published": "2025-05-20 06:44:34", "updated": "2025-05-20 06:44:34", "pdf_url": "http://arxiv.org/pdf/2505.13994v1", "comment": "20 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13995v1", "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "abstract": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-20 06:45:17", "updated": "2025-05-20 06:45:17", "pdf_url": "http://arxiv.org/pdf/2505.13995v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14001v1", "title": "VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change", "authors": ["Sterre Lutz", "Matthijs T. J. Spaan", "Anna Lukina"], "abstract": "Autonomous systems operating in the real world encounter a range of\nuncertainties. Probabilistic neural Lyapunov certification is a powerful\napproach to proving safety of nonlinear stochastic dynamical systems. When\nfaced with changes beyond the modeled uncertainties, e.g., unidentified\nobstacles, probabilistic certificates must be transferred to the new system\ndynamics. However, even when the changes are localized in a known part of the\nstate space, state-of-the-art requires complete re-certification, which is\nparticularly costly for neural certificates. We introduce VeRecycle, the first\nframework to formally reclaim guarantees for discrete-time stochastic dynamical\nsystems. VeRecycle efficiently reuses probabilistic certificates when the\nsystem dynamics deviate only in a given subset of states. We present a general\ntheoretical justification and algorithmic implementation. Our experimental\nevaluation shows scenarios where VeRecycle both saves significant computational\neffort and achieves competitive probabilistic guarantees in compositional\nneural control.", "categories": ["cs.AI", "cs.SY", "eess.SY"], "published": "2025-05-20 06:54:19", "updated": "2025-05-20 06:54:19", "pdf_url": "http://arxiv.org/pdf/2505.14001v1", "comment": "accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14005v1", "title": "Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks", "authors": ["Han Zhang", "Yan Wang", "Guanfeng Liu", "Pengfei Ding", "Huaxiong Wang", "Kwok-Yan Lam"], "abstract": "To enhance the reliability and credibility of graph neural networks (GNNs)\nand improve the transparency of their decision logic, a new field of\nexplainability of GNNs (XGNN) has emerged. However, two major limitations\nseverely degrade the performance and hinder the generalizability of existing\nXGNN methods: they (a) fail to capture the complete decision logic of GNNs\nacross diverse distributions in the entire dataset's sample space, and (b)\nimpose strict prerequisites on edge properties and GNN internal accessibility.\nTo address these limitations, we propose OPEN, a novel c\\textbf{O}mprehensive\nand \\textbf{P}rerequisite-free \\textbf{E}xplainer for G\\textbf{N}Ns. OPEN, as\nthe first work in the literature, can infer and partition the entire dataset's\nsample space into multiple environments, each containing graphs that follow a\ndistinct distribution. OPEN further learns the decision logic of GNNs across\ndifferent distributions by sampling subgraphs from each environment and\nanalyzing their predictions, thus eliminating the need for strict\nprerequisites. Experimental results demonstrate that OPEN captures nearly\ncomplete decision logic of GNNs, outperforms state-of-the-art methods in\nfidelity while maintaining similar efficiency, and enhances robustness in\nreal-world scenarios.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 07:01:47", "updated": "2025-05-20 07:01:47", "pdf_url": "http://arxiv.org/pdf/2505.14005v1", "comment": "Accepted by IJCAI 2025 AI4Tech Track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14020v1", "title": "Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning", "authors": ["Hao Dong", "Ziyue Qiao", "Zhiyuan Ning", "Qi Hao", "Yi Du", "Pengyang Wang", "Yuanchun Zhou"], "abstract": "Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs\n(KGs), incorporate the temporal feature to express the transience of knowledge\nby describing when facts occur. TKG extrapolation aims to infer possible future\nfacts based on known history, which has garnered significant attention in\nrecent years. Some existing methods treat TKG as a sequence of independent\nsubgraphs to model temporal evolution patterns, demonstrating impressive\nreasoning performance. However, they still have limitations: 1) In modeling\nsubgraph semantic evolution, they usually neglect the internal structural\ninteractions between subgraphs, which are actually crucial for encoding TKGs.\n2) They overlook the potential smooth features that do not lead to semantic\nchanges, which should be distinguished from the semantic evolution process.\nTherefore, we propose a novel Disentangled Multi-span Evolutionary Network\n(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution\nstrategy that captures local neighbor features while perceiving historical\nneighbor semantic information, thus enabling internal interactions between\nsubgraphs during the evolution process. To maximize the capture of semantic\nchange patterns, we design a disentangle component that adaptively separates\nnodes' active and stable features, used to dynamically control the influence of\nhistorical semantics on future evolution. Extensive experiments conducted on\nfour real-world TKG datasets show that DiMNet demonstrates substantial\nperformance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%\nin MRR.", "categories": ["cs.AI", "cs.IR", "cs.LG"], "published": "2025-05-20 07:22:03", "updated": "2025-05-20 07:22:03", "pdf_url": "http://arxiv.org/pdf/2505.14020v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14024v1", "title": "FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix", "authors": ["Di Wu", "Qian Li", "Heng Yang", "Yong Han"], "abstract": "Federated Learning (FL) enables geographically distributed clients to\ncollaboratively train machine learning models by sharing only their local\nmodels, ensuring data privacy. However, FL is vulnerable to untargeted attacks\nthat aim to degrade the global model's performance on the underlying data\ndistribution. Existing defense mechanisms attempt to improve FL's resilience\nagainst such attacks, but their effectiveness is limited in practical FL\nenvironments due to data heterogeneity. On the contrary, we aim to detect and\nremove the attacks to mitigate their impact. Generalization contribution plays\na crucial role in distinguishing untargeted attacks. Our observations indicate\nthat, with limited data, the divergence between embeddings representing\ndifferent classes provides a better measure of generalization than direct\naccuracy. In light of this, we propose a novel robust aggregation method,\nFedGraM, designed to defend against untargeted attacks in FL. The server\nmaintains an auxiliary dataset containing one sample per class to support\naggregation. This dataset is fed to the local models to extract embeddings.\nThen, the server calculates the norm of the Gram Matrix of the embeddings for\neach local model. The norm serves as an indicator of each model's inter-class\nseparation capability in the embedding space. FedGraM identifies and removes\npotentially malicious models by filtering out those with the largest norms,\nthen averages the remaining local models to form the global model. We conduct\nextensive experiments to evaluate the performance of FedGraM. Our empirical\nresults show that with limited data samples used to construct the auxiliary\ndataset, FedGraM achieves exceptional performance, outperforming\nstate-of-the-art defense methods.", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "published": "2025-05-20 07:26:54", "updated": "2025-05-20 07:26:54", "pdf_url": "http://arxiv.org/pdf/2505.14024v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14027v1", "title": "CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data", "authors": ["Yifan Zeng"], "abstract": "As computer networks proliferate, the gravity of network intrusions has\nescalated, emphasizing the criticality of network intrusion detection systems\nfor safeguarding security. While deep learning models have exhibited promising\nresults in intrusion detection, they face challenges in managing\nhigh-dimensional, complex traffic patterns and imbalanced data categories. This\npaper presents CSAGC-IDS, a network intrusion detection model based on deep\nlearning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced\nconvolutional conditional generative adversarial network that generates\nhigh-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS\nintegrates CSCA-CNN, a convolutional neural network enhanced through cost\nsensitive learning and channel attention mechanism, to extract features from\ncomplex traffic data for precise detection. Experiments conducted on the\nNSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of\n84.52% in five-class classification task, and an accuracy of 91.09% and an F1\nscore of 92.04% in binary classification task.Furthermore, this paper provides\nan interpretability analysis of the proposed model, using SHAP and LIME to\nexplain the decision-making mechanisms of the model.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 07:27:51", "updated": "2025-05-20 07:27:51", "pdf_url": "http://arxiv.org/pdf/2505.14027v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14029v1", "title": "AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards", "authors": ["Laura-Sophia von Hirschhausen", "Jannes S. Magnusson", "Mykyta Kovalenko", "Fredrik Boye", "Tanay Rawat", "Peter Eisert", "Anna Hilsmann", "Sebastian Pretzsch", "Sebastian Bosse"], "abstract": "Deep learning has transformed computer vision for precision agriculture, yet\napple orchard monitoring remains limited by dataset constraints. The lack of\ndiverse, realistic datasets and the difficulty of annotating dense,\nheterogeneous scenes. Existing datasets overlook different growth stages and\nstereo imagery, both essential for realistic 3D modeling of orchards and tasks\nlike fruit localization, yield estimation, and structural analysis. To address\nthese gaps, we present AppleGrowthVision, a large-scale dataset comprising two\nsubsets. The first includes 9,317 high resolution stereo images collected from\na farm in Brandenburg (Germany), covering six agriculturally validated growth\nstages over a full growth cycle. The second subset consists of 1,125 densely\nannotated images from the same farm in Brandenburg and one in Pillnitz\n(Germany), containing a total of 31,084 apple labels. AppleGrowthVision\nprovides stereo-image data with agriculturally validated growth stages,\nenabling precise phenological analysis and 3D reconstructions. Extending\nMinneApple with our data improves YOLOv8 performance by 7.69 % in terms of\nF1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by\n31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy\nusing VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges\nthe gap between agricultural science and computer vision, by enabling the\ndevelopment of robust models for fruit detection, growth modeling, and 3D\nanalysis in precision agriculture. Future work includes improving annotation,\nenhancing 3D reconstruction, and extending multimodal analysis across all\ngrowth stages.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 07:29:22", "updated": "2025-05-20 07:29:22", "pdf_url": "http://arxiv.org/pdf/2505.14029v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14036v1", "title": "Adaptive Cyclic Diffusion for Inference Scaling", "authors": ["Gyubin Lee", "Truong Nhat Nguyen Bao", "Jaesik Yoon", "Dongwoo Lee", "Minsu Kim", "Yoshua Bengio", "Sungjin Ahn"], "abstract": "Diffusion models have demonstrated strong generative capabilities across\ndomains ranging from image synthesis to complex reasoning tasks. However, most\ninference-time scaling methods rely on fixed denoising schedules, limiting\ntheir ability to allocate computation based on instance difficulty or\ntask-specific demands adaptively. We introduce the challenge of adaptive\ninference-time scaling-dynamically adjusting computational effort during\ninference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a\nflexible, search-based inference framework. ABCD refines outputs through\nbi-directional diffusion cycles while adaptively controlling exploration depth\nand termination. It comprises three components: Cyclic Diffusion Search,\nAutomatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.\nExperiments show that ABCD improves performance across diverse tasks while\nmaintaining computational efficiency.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 07:31:38", "updated": "2025-05-20 07:31:38", "pdf_url": "http://arxiv.org/pdf/2505.14036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14038v1", "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data", "authors": ["Xinzhe Zheng", "Sijie Ji", "Jiawei Sun", "Renqi Chen", "Wei Gao", "Mani Srivastava"], "abstract": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 07:36:28", "updated": "2025-05-20 07:36:28", "pdf_url": "http://arxiv.org/pdf/2505.14038v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14045v1", "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 07:43:45", "updated": "2025-05-20 07:43:45", "pdf_url": "http://arxiv.org/pdf/2505.14045v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14057v1", "title": "Field Matters: A lightweight LLM-enhanced Method for CTR Prediction", "authors": ["Yu Cui", "Feng Liu", "Jiawei Chen", "Xingyu Lou", "Changwang Zhang", "Jun Wang", "Yuegang Sun", "Xiaohu Yang", "Can Wang"], "abstract": "Click-through rate (CTR) prediction is a fundamental task in modern\nrecommender systems. In recent years, the integration of large language models\n(LLMs) has been shown to effectively enhance the performance of traditional CTR\nmethods. However, existing LLM-enhanced methods often require extensive\nprocessing of detailed textual descriptions for large-scale instances or\nuser/item entities, leading to substantial computational overhead. To address\nthis challenge, this work introduces LLaCTR, a novel and lightweight\nLLM-enhanced CTR method that employs a field-level enhancement paradigm.\nSpecifically, LLaCTR first utilizes LLMs to distill crucial and lightweight\nsemantic knowledge from small-scale feature fields through self-supervised\nfield-feature fine-tuning. Subsequently, it leverages this field-level semantic\nknowledge to enhance both feature representation and feature interactions. In\nour experiments, we integrate LLaCTR with six representative CTR models across\nfour datasets, demonstrating its superior performance in terms of both\neffectiveness and efficiency compared to existing LLM-enhanced methods. Our\ncode is available at https://anonymous.4open.science/r/LLaCTR-EC46.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-20 08:02:41", "updated": "2025-05-20 08:02:41", "pdf_url": "http://arxiv.org/pdf/2505.14057v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14064v1", "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI", "authors": ["Cosmin I. Bercea", "Jun Li", "Philipp Raffler", "Evamaria O. Riedel", "Lena Schmitzer", "Angela Kurz", "Felix Bitzer", "Paula Ro\u00dfm\u00fcller", "Julian Canisius", "Mirjam L. Beyrle", "Che Liu", "Wenjia Bai", "Bernhard Kainz", "Julia A. Schnabel", "Benedikt Wiestler"], "abstract": "In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously $unknown$ categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present $NOVA$, a challenging, real-life $evaluation-only$\nbenchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\n$extreme$ stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies.", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "published": "2025-05-20 08:10:57", "updated": "2025-05-20 08:10:57", "pdf_url": "http://arxiv.org/pdf/2505.14064v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14072v1", "title": "Personalized Student Knowledge Modeling for Future Learning Resource Prediction", "authors": ["Soroush Hashemifar", "Sherry Sahebi"], "abstract": "Despite advances in deep learning for education, student knowledge tracing\nand behavior modeling face persistent challenges: limited personalization,\ninadequate modeling of diverse learning activities (especially non-assessed\nmaterials), and overlooking the interplay between knowledge acquisition and\nbehavioral patterns. Practical limitations, such as fixed-size sequence\nsegmentation, frequently lead to the loss of contextual information vital for\npersonalized learning. Moreover, reliance on student performance on assessed\nmaterials limits the modeling scope, excluding non-assessed interactions like\nlectures. To overcome these shortcomings, we propose Knowledge Modeling and\nMaterial Prediction (KMaP), a stateful multi-task approach designed for\npersonalized and simultaneous modeling of student knowledge and behavior. KMaP\nemploys clustering-based student profiling to create personalized student\nrepresentations, improving predictions of future learning resource preferences.\nExtensive experiments on two real-world datasets confirm significant behavioral\ndifferences across student clusters and validate the efficacy of the KMaP\nmodel.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-20 08:23:50", "updated": "2025-05-20 08:23:50", "pdf_url": "http://arxiv.org/pdf/2505.14072v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14080v1", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "abstract": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "published": "2025-05-20 08:36:47", "updated": "2025-05-20 08:36:47", "pdf_url": "http://arxiv.org/pdf/2505.14080v1", "comment": null, "doi": "10.1145/3715275.3732112", "journal_ref": "FAccT '25: Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency"}
{"arxiv_id": "2505.14103v1", "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models", "authors": ["Guangke Chen", "Fu Song", "Zhe Zhao", "Xiaojun Jia", "Yang Liu", "Yanchen Qiao", "Weizhe Zhang"], "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak.", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-20 09:10:45", "updated": "2025-05-20 09:10:45", "pdf_url": "http://arxiv.org/pdf/2505.14103v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14106v1", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "abstract": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:13:22", "updated": "2025-05-20 09:13:22", "pdf_url": "http://arxiv.org/pdf/2505.14106v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14107v1", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "abstract": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:14:53", "updated": "2025-05-20 09:14:53", "pdf_url": "http://arxiv.org/pdf/2505.14107v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14117v1", "title": "Collaborative Unlabeled Data Optimization", "authors": ["Xinyi Shang", "Peng Sun", "Fengyuan Liu", "Tao Lin"], "abstract": "This paper pioneers a novel data-centric paradigm to maximize the utility of\nunlabeled data, tackling a critical question: How can we enhance the efficiency\nand sustainability of deep learning training by optimizing the data itself? We\nbegin by identifying three key limitations in existing model-centric\napproaches, all rooted in a shared bottleneck: knowledge extracted from data is\nlocked to model parameters, hindering its reusability and scalability. To this\nend, we propose CoOpt, a highly efficient, parallelized framework for\ncollaborative unlabeled data optimization, thereby effectively encoding\nknowledge into the data itself. By distributing unlabeled data and leveraging\npublicly available task-agnostic models, CoOpt facilitates scalable, reusable,\nand sustainable training pipelines. Extensive experiments across diverse\ndatasets and architectures demonstrate its efficacy and efficiency, achieving\n13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,\nwith training speedups of $1.94 \\times $ and $1.2 \\times$.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:21:40", "updated": "2025-05-20 09:21:40", "pdf_url": "http://arxiv.org/pdf/2505.14117v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14125v1", "title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning", "authors": ["Viet Anh Khoa Tran", "Emre Neftci", "Willem. A. M. Wybo"], "abstract": "Biological brains learn continually from a stream of unlabeled data, while\nintegrating specialized information from sparsely labeled examples without\ncompromising their ability to generalize. Meanwhile, machine learning methods\nare susceptible to catastrophic forgetting in this natural learning setting, as\nsupervised specialist fine-tuning degrades performance on the original task. We\nintroduce task-modulated contrastive learning (TMCL), which takes inspiration\nfrom the biophysical machinery in the neocortex, using predictive coding\nprinciples to integrate top-down information continually and without\nsupervision. We follow the idea that these principles build a view-invariant\nrepresentation space, and that this can be implemented using a contrastive\nloss. Then, whenever labeled samples of a new class occur, new affine\nmodulations are learned that improve separation of the new class from all\nothers, without affecting feedforward weights. By co-opting the view-invariance\nlearning mechanism, we then train feedforward weights to match the unmodulated\nrepresentation of a data sample to its modulated counterparts. This introduces\nmodulation invariance into the representation space, and, by also using past\nmodulations, stabilizes it. Our experiments show improvements in both\nclass-incremental and transfer learning over state-of-the-art unsupervised\napproaches, as well as over comparable supervised approaches, using as few as\n1% of available labels. Taken together, our work suggests that top-down\nmodulations play a crucial role in balancing stability and plasticity.", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "68T05 (primary), 68T07, 68T45 (secondary)", "I.2.6; I.2.10"], "published": "2025-05-20 09:31:57", "updated": "2025-05-20 09:31:57", "pdf_url": "http://arxiv.org/pdf/2505.14125v1", "comment": "33 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14128v1", "title": "A Methodological Framework for Measuring Spatial Labeling Similarity", "authors": ["Yihang Du", "Jiaying Hu", "Suyang Hou", "Yueyang Ding", "Xiaobo Sun"], "abstract": "Spatial labeling assigns labels to specific spatial locations to characterize\ntheir spatial properties and relationships, with broad applications in\nscientific research and practice. Measuring the similarity between two spatial\nlabelings is essential for understanding their differences and the contributing\nfactors, such as changes in location properties or labeling methods. An\nadequate and unbiased measurement of spatial labeling similarity should\nconsider the number of matched labels (label agreement), the topology of\nspatial label distribution, and the heterogeneous impacts of mismatched labels.\nHowever, existing methods often fail to account for all these aspects. To\naddress this gap, we propose a methodological framework to guide the\ndevelopment of methods that meet these requirements. Given two spatial\nlabelings, the framework transforms them into graphs based on location\norganization, labels, and attributes (e.g., location significance). The\ndistributions of their graph attributes are then extracted, enabling an\nefficient computation of distributional discrepancy to reflect the\ndissimilarity level between the two labelings. We further provide a concrete\nimplementation of this framework, termed Spatial Labeling Analogy Metric\n(SLAM), along with an analysis of its theoretical foundation, for evaluating\nspatial labeling results in spatial transcriptomics (ST) \\textit{as per} their\nsimilarity with ground truth labeling. Through a series of carefully designed\nexperimental cases involving both simulated and real ST data, we demonstrate\nthat SLAM provides a comprehensive and accurate reflection of labeling quality\ncompared to other well-established evaluation metrics. Our code is available at\nhttps://github.com/YihDu/SLAM.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:34:03", "updated": "2025-05-20 09:34:03", "pdf_url": "http://arxiv.org/pdf/2505.14128v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14136v1", "title": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging", "authors": ["Ryo Bertolissi", "Jonas H\u00fcbotter", "Ido Hakimi", "Andreas Krause"], "abstract": "Mixture of expert (MoE) models are a promising approach to increasing model\ncapacity without increasing inference cost, and are core components of many\nstate-of-the-art language models. However, current MoE models typically use\nonly few experts due to prohibitive training and inference cost. We propose\nTest-Time Model Merging (TTMM) which scales the MoE paradigm to an order of\nmagnitude more experts and uses model merging to avoid almost any test-time\noverhead. We show that TTMM is an approximation of test-time training (TTT),\nwhich fine-tunes an expert model for each prediction task, i.e., prompt. TTT\nhas recently been shown to significantly improve language models, but is\ncomputationally expensive. We find that performance of TTMM improves with more\nexperts and approaches the performance of TTT. Moreover, we find that with a 1B\nparameter base model, TTMM is more than 100x faster than TTT at test-time by\namortizing the cost of TTT at train-time. Thus, TTMM offers a promising\ncost-effective approach to scale test-time training.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 09:39:54", "updated": "2025-05-20 09:39:54", "pdf_url": "http://arxiv.org/pdf/2505.14136v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14137v1", "title": "Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games", "authors": ["Vojt\u011bch K\u016fr", "V\u00edt Musil", "Vojt\u011bch \u0158eh\u00e1k"], "abstract": "Adversarial Patrolling games form a subclass of Security games where a\nDefender moves between locations, guarding vulnerable targets. The main\nalgorithmic problem is constructing a strategy for the Defender that minimizes\nthe worst damage an Attacker can cause. We focus on the class of finite-memory\n(also known as regular) Defender's strategies that experimentally outperformed\nother competing classes. A finite-memory strategy can be seen as a positional\nstrategy on a finite set of states. Each state consists of a pair of a location\nand a certain integer value--called memory. Existing algorithms improve the\ntransitional probabilities between the states but require that the available\nmemory size itself is assigned at each location manually. Choosing the right\nmemory assignment is a well-known open and hard problem that hinders the\nusability of finite-memory strategies. We solve this issue by developing a\ngeneral method that iteratively changes the memory assignment. Our algorithm\ncan be used in connection with \\emph{any} black-box strategy optimization tool.\nWe evaluate our method on various experiments and show its robustness by\nsolving instances of various patrolling models.", "categories": ["cs.AI"], "published": "2025-05-20 09:40:53", "updated": "2025-05-20 09:40:53", "pdf_url": "http://arxiv.org/pdf/2505.14137v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14139v1", "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning", "authors": ["Marvin Alles", "Nutan Chen", "Patrick van der Smagt", "Botond Cseke"], "abstract": "The use of guidance to steer sampling toward desired outcomes has been widely\nexplored within diffusion models, especially in applications such as image and\ntrajectory generation. However, incorporating guidance during training remains\nrelatively underexplored. In this work, we introduce energy-guided flow\nmatching, a novel approach that enhances the training of flow models and\neliminates the need for guidance at inference time. We learn a conditional\nvelocity field corresponding to the flow policy by approximating an\nenergy-guided probability path as a Gaussian path. Learning guided trajectories\nis appealing for tasks where the target distribution is defined by a\ncombination of data and an energy function, as in reinforcement learning.\nDiffusion-based policies have recently attracted attention for their expressive\npower and ability to capture multi-modal action distributions. Typically, these\npolicies are optimized using weighted objectives or by back-propagating\ngradients through actions sampled by the policy. As an alternative, we propose\nFlowQ, an offline reinforcement learning algorithm based on energy-guided flow\nmatching. Our method achieves competitive performance while the policy training\ntime is constant in the number of flow sampling steps.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-20 09:43:05", "updated": "2025-05-20 09:43:05", "pdf_url": "http://arxiv.org/pdf/2505.14139v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14140v1", "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning", "authors": ["Qianyue Hao", "Sibo Li", "Jian Yuan", "Yong Li"], "abstract": "Despite rapid advancements in large language models (LLMs), the token-level\nautoregressive nature constrains their complex reasoning capabilities. To\nenhance LLM reasoning, inference-time techniques, including\nChain/Tree/Graph-of-Thought(s), successfully improve the performance, as they\nare fairly cost-effective by guiding reasoning through sophisticated logical\nstructures without modifying LLMs' parameters. However, these manually\npredefined, task-agnostic frameworks are applied uniformly across diverse\ntasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),\nwhere we train a lightweight navigator model with reinforcement learning (RL)\nto adaptively enhance LLM reasoning at inference time. Specifically, we design\nfive basic logic blocks from the perspective of human cognition. During the\nreasoning process, the trained RL navigator dynamically selects the suitable\nlogic blocks and combines them into task-specific logical structures according\nto problem characteristics. Experiments across multiple reasoning benchmarks\n(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)\nillustrate that RLoT outperforms established inference-time techniques by up to\n13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to\nmake sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL\nnavigator demonstrates strong transferability: a model trained on one specific\nLLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is\nopen-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for\nreproducibility.", "categories": ["cs.AI"], "published": "2025-05-20 09:43:33", "updated": "2025-05-20 09:43:33", "pdf_url": "http://arxiv.org/pdf/2505.14140v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14141v1", "title": "Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent", "authors": ["Fanglin Mo", "Junzhe Chen", "Haoxuan Zhu", "Xuming Hu"], "abstract": "Mobile GUI agents execute user commands by directly interacting with the\ngraphical user interface (GUI) of mobile devices, demonstrating significant\npotential to enhance user convenience. However, these agents face considerable\nchallenges in task planning, as they must continuously analyze the GUI and\ngenerate operation instructions step by step. This process often leads to\ndifficulties in making accurate task plans, as GUI agents lack a deep\nunderstanding of how to effectively use the target applications, which can\ncause them to become \"lost\" during task execution. To address the task planning\nissue, we propose SPlanner, a plug-and-play planning module to generate\nexecution plans that guide vision language model(VLMs) in executing tasks. The\nproposed planning module utilizes extended finite state machines (EFSMs) to\nmodel the control logits and configurations of mobile applications. It then\ndecomposes a user instruction into a sequence of primary function modeled in\nEFSMs, and generate the execution path by traversing the EFSMs. We further\nrefine the execution path into a natural language plan using an LLM. The final\nplan is concise and actionable, and effectively guides VLMs to generate\ninteractive GUI actions to accomplish user tasks. SPlanner demonstrates strong\nperformance on dynamic benchmarks reflecting real-world mobile usage. On the\nAndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired\nwith Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point\nimprovement compared to using Qwen2.5-VL-72B without planning assistance.", "categories": ["cs.AI", "I.2.11; H.5.2"], "published": "2025-05-20 09:45:55", "updated": "2025-05-20 09:45:55", "pdf_url": "http://arxiv.org/pdf/2505.14141v1", "comment": "10 pages. Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14143v1", "title": "Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition", "authors": ["Shuo Zhang", "Jinsong Zhang", "Zhejun Zhang", "Lei Li"], "abstract": "Multi-task learning (MTL) enables the efficient transfer of extra knowledge\nacquired from other tasks. The high correlation between multimodal sentiment\nanalysis (MSA) and multimodal emotion recognition (MER) supports their joint\ntraining. However, existing methods primarily employ hard parameter sharing,\nignoring parameter conflicts caused by complex task correlations. In this\npaper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture\nof Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts\nto distinctly model common and unique task characteristics, thereby avoiding\nparameter conflicts. Additionally, inspired by low-rank structures in the\nMixture of Experts (MoE) framework, we design low-rank expert networks to\nreduce parameter and computational overhead as the number of experts increases.\nExtensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that\nMMoLRE achieves state-of-the-art performance on the MSA task and competitive\nresults on the MER task.", "categories": ["cs.AI"], "published": "2025-05-20 09:46:56", "updated": "2025-05-20 09:46:56", "pdf_url": "http://arxiv.org/pdf/2505.14143v1", "comment": "Accepted to ICME 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14146v1", "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "abstract": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 09:53:56", "updated": "2025-05-20 09:53:56", "pdf_url": "http://arxiv.org/pdf/2505.14146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14147v1", "title": "SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning", "authors": ["Xiong Jun Wu", "Zhenduo Zhang", "ZuJie Wen", "Zhiqiang Zhang", "Wang Ren", "Lei Shi", "Cai Chen", "Deng Zhao", "Dingnan Jin", "Qing Cui", "Jun Zhou"], "abstract": "Training large reasoning models (LRMs) with reinforcement learning in STEM\ndomains is hindered by the scarcity of high-quality, diverse, and verifiable\nproblem sets. Existing synthesis methods, such as Chain-of-Thought prompting,\noften generate oversimplified or uncheckable data, limiting model advancement\non complex tasks. To address these challenges, we introduce SHARP, a unified\napproach to Synthesizing High-quality Aligned Reasoning Problems for LRMs\nreinforcement learning with verifiable rewards (RLVR). SHARP encompasses a\nstrategic set of self-alignment principles -- targeting graduate and\nOlympiad-level difficulty, rigorous logical consistency, and unambiguous,\nverifiable answers -- and a structured three-phase framework (Alignment,\nInstantiation, Inference) that ensures thematic diversity and fine-grained\ncontrol over problem generation. We implement SHARP by leveraging a\nstate-of-the-art LRM to infer and verify challenging STEM questions, then\nemploy a reinforcement learning loop to refine the model's reasoning through\nverifiable reward signals. Experiments on benchmarks such as GPQA demonstrate\nthat SHARP-augmented training substantially outperforms existing methods,\nmarkedly improving complex reasoning accuracy and pushing LRM performance\ncloser to expert-level proficiency. Our contributions include the SHARP\nstrategy, framework design, end-to-end implementation, and experimental\nevaluation of its effectiveness in elevating LRM reasoning capabilities.", "categories": ["cs.AI"], "published": "2025-05-20 09:54:42", "updated": "2025-05-20 09:54:42", "pdf_url": "http://arxiv.org/pdf/2505.14147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14148v1", "title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem", "authors": ["Fan Liu", "Zherui Yang", "Cancheng Liu", "Tianrui Song", "Xiaofeng Gao", "Hao Liu"], "abstract": "Mathematical modeling is a cornerstone of scientific discovery and\nengineering practice, enabling the translation of real-world problems into\nformal systems across domains such as physics, biology, and economics. Unlike\nmathematical reasoning, which assumes a predefined formulation, modeling\nrequires open-ended problem analysis, abstraction, and principled\nformalization. While Large Language Models (LLMs) have shown strong reasoning\ncapabilities, they fall short in rigorous model construction, limiting their\nutility in real-world problem-solving. To this end, we formalize the task of\nLLM-powered real-world mathematical modeling, where agents must analyze\nproblems, construct domain-appropriate formulations, and generate complete\nend-to-end solutions. We introduce MM-Bench, a curated benchmark of 111\nproblems from the Mathematical Contest in Modeling (MCM/ICM), spanning the\nyears 2000 to 2025 and across ten diverse domains such as physics, biology, and\neconomics. To tackle this task, we propose MM-Agent, an expert-inspired\nframework that decomposes mathematical modeling into four stages: open-ended\nproblem analysis, structured model formulation, computational problem solving,\nand report generation. Experiments on MM-Bench show that MM-Agent significantly\noutperforms baseline agents, achieving an 11.88\\% improvement over human expert\nsolutions while requiring only 15 minutes and \\$0.88 per task using GPT-4o.\nFurthermore, under official MCM/ICM protocols, MM-Agent assisted two\nundergraduate teams in winning the Finalist Award (\\textbf{top 2.0\\% among\n27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a\nmodeling copilot. Our code is available at\nhttps://github.com/usail-hkust/LLM-MM-Agent", "categories": ["cs.AI"], "published": "2025-05-20 09:55:31", "updated": "2025-05-20 09:55:31", "pdf_url": "http://arxiv.org/pdf/2505.14148v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14156v1", "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "abstract": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "categories": ["cs.CV", "cs.AI", "cs.IR", "I.2; H.3.3"], "published": "2025-05-20 10:05:06", "updated": "2025-05-20 10:05:06", "pdf_url": "http://arxiv.org/pdf/2505.14156v1", "comment": null, "doi": "10.1145/3589334.3645574", "journal_ref": null}
{"arxiv_id": "2505.14157v1", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "abstract": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:05:11", "updated": "2025-05-20 10:05:11", "pdf_url": "http://arxiv.org/pdf/2505.14157v1", "comment": "25 pages, 42 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14163v1", "title": "DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation", "authors": ["He Wang", "Alexander Hanbo Li", "Yiqun Hu", "Sheng Zhang", "Hideo Kobayashi", "Jiani Zhang", "Henry Zhu", "Chung-Wei Hang", "Patrick Ng"], "abstract": "Large language model (LLM) agents have shown promising performance in\ngenerating code for solving complex data science problems. Recent studies\nprimarily focus on enhancing in-context learning through improved search,\nsampling, and planning techniques, while overlooking the importance of the\norder in which problems are tackled during inference. In this work, we develop\na novel inference-time optimization framework, referred to as DSMentor, which\nleverages curriculum learning -- a strategy that introduces simpler task first\nand progressively moves to more complex ones as the learner improves -- to\nenhance LLM agent performance in challenging data science tasks. Our\nmentor-guided framework organizes data science tasks in order of increasing\ndifficulty and incorporates a growing long-term memory to retain prior\nexperiences, guiding the agent's learning progression and enabling more\neffective utilization of accumulated knowledge. We evaluate DSMentor through\nextensive experiments on DSEval and QRData benchmarks. Experiments show that\nDSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval\nand QRData compared to baseline agents. Furthermore, DSMentor demonstrates\nstronger causal reasoning ability, improving the pass rate by 8.8% on the\ncausality problems compared to GPT-4 using Program-of-Thoughts prompts. Our\nwork underscores the importance of developing effective strategies for\naccumulating and utilizing knowledge during inference, mirroring the human\nlearning process and opening new avenues for improving LLM performance through\ncurriculum-based inference optimization.", "categories": ["cs.AI"], "published": "2025-05-20 10:16:21", "updated": "2025-05-20 10:16:21", "pdf_url": "http://arxiv.org/pdf/2505.14163v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14178v1", "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "abstract": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:32:30", "updated": "2025-05-20 10:32:30", "pdf_url": "http://arxiv.org/pdf/2505.14178v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14179v1", "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "abstract": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-20 10:34:45", "updated": "2025-05-20 10:34:45", "pdf_url": "http://arxiv.org/pdf/2505.14179v1", "comment": null, "doi": "10.1016/j.eswa.2024.125529", "journal_ref": "Expert Systems with Applications, 2025"}
{"arxiv_id": "2505.14185v1", "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 10:41:49", "updated": "2025-05-20 10:41:49", "pdf_url": "http://arxiv.org/pdf/2505.14185v1", "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14190v1", "title": "$\u03b1$-GAN by R\u00e9nyi Cross Entropy", "authors": ["Ni Ding", "Miao Qiao", "Jiaxing Xu", "Yiping Ke", "Xiaoyu Zhang"], "abstract": "This paper proposes $\\alpha$-GAN, a generative adversarial network using\nR\\'{e}nyi measures. The value function is formulated, by R\\'{e}nyi cross\nentropy, as an expected certainty measure incurred by the discriminator's soft\ndecision as to where the sample is from, true population or the generator. The\ndiscriminator tries to maximize the R\\'{e}nyi certainty about sample source,\nwhile the generator wants to reduce it by injecting fake samples. This forms a\nmin-max problem with the solution parameterized by the R\\'{e}nyi order\n$\\alpha$. This $\\alpha$-GAN reduces to vanilla GAN at $\\alpha = 1$, where the\nvalue function is exactly the binary cross entropy. The optimization of\n$\\alpha$-GAN is over probability (vector) space. It is shown that the gradient\nis exponentially enlarged when R\\'{e}nyi order is in the range $\\alpha \\in\n(0,1)$. This makes convergence faster, which is verified by experimental\nresults. A discussion shows that choosing $\\alpha \\in (0,1)$ may be able to\nsolve some common problems, e.g., vanishing gradient. A following observation\nreveals that this range has not been fully explored in the existing R\\'{e}nyi\nversion GANs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 10:45:11", "updated": "2025-05-20 10:45:11", "pdf_url": "http://arxiv.org/pdf/2505.14190v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14193v1", "title": "Dynamic Replanning for Improved Public Transport Routing", "authors": ["Abdallah Abuaisha", "Bojie Shen", "Daniel Harabor", "Peter Stuckey", "Mark Wallace"], "abstract": "Delays in public transport are common, often impacting users through\nprolonged travel times and missed transfers. Existing solutions for handling\ndelays remain limited; backup plans based on historical data miss opportunities\nfor earlier arrivals, while snapshot planning accounts for current delays but\nnot future ones. With the growing availability of live delay data, users can\nadjust their journeys in real-time. However, the literature lacks a framework\nthat fully exploits this advantage for system-scale dynamic replanning. To\naddress this, we formalise the dynamic replanning problem in public transport\nrouting and propose two solutions: a \"pull\" approach, where users manually\nrequest replanning, and a novel \"push\" approach, where the server proactively\nmonitors and adjusts journeys. Our experiments show that the push approach\noutperforms the pull approach, achieving significant speedups. The results also\nreveal substantial arrival time savings enabled by dynamic replanning.", "categories": ["cs.AI"], "published": "2025-05-20 10:50:58", "updated": "2025-05-20 10:50:58", "pdf_url": "http://arxiv.org/pdf/2505.14193v1", "comment": "Accepted for publication at IJCAI 2025. 8 pages, 4 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14201v1", "title": "FLASH-D: FlashAttention with Hidden Softmax Division", "authors": ["Kosmas Alexandridis", "Vasileios Titopoulos", "Giorgos Dimitrakopoulos"], "abstract": "The transformer's attention mechanism has revolutionized AI and machine\nlearning, with its efficient computation being crucial to its performance.\nHowever, calculating attention involves matrix operations interspersed with\nsoftmax rescaling, which inherently slows down computation and requires\nprocessing the entire input sequence. Building on online softmax computation,\nFlashAttention integrates softmax calculation with matrix arithmetic, enabling\ntiled computation independent of sequence length. While optimized for GPUs,\nFlashAttention's simplicity makes it amenable to direct hardware acceleration.\nThis work re-evaluates the core FlashAttention kernel, presenting FLASH-D a\nmathematically equivalent, yet simplified, formulation that achieves: (a)\nhiding softmax division within other non-linear function evaluations; (b)\ninherently numerically stable computation of exponentials, eliminating the need\nfor maximum value subtraction; and (c) a reduction in computational cost\nwithout introducing numerical approximations to the FlashAttention kernel.\nImportantly, the essential FlashAttention properties that facilitate efficient\ntiled implementation are fully preserved. Hardware implementation results at\n28nm demonstrate that this proposed formulation achieves a 22.8% reduction in\narea and a 20.3% reduction in power, on average, compared to state-of-the-art\nparallel hardware architectures without any performance penalty.", "categories": ["cs.LG", "cs.AI", "cs.AR"], "published": "2025-05-20 11:01:33", "updated": "2025-05-20 11:01:33", "pdf_url": "http://arxiv.org/pdf/2505.14201v1", "comment": "IEEE/ACM International Symposium on Low Power Electronics and Design\n  (ISLPED) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14206v1", "title": "Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data", "authors": ["Flavio Di Martino", "Franca Delmastro"], "abstract": "The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 11:05:06", "updated": "2025-05-20 11:05:06", "pdf_url": "http://arxiv.org/pdf/2505.14206v1", "comment": "Submitted to ACM Transactions on Computing for Healthcare (ACM\n  HEALTH)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14209v1", "title": "Embedded Mean Field Reinforcement Learning for Perimeter-defense Game", "authors": ["Li Wang", "Xin Yu", "Xuxin Lv", "Gangzheng Ai", "Wenjun Wu"], "abstract": "With the rapid advancement of unmanned aerial vehicles (UAVs) and missile\ntechnologies, perimeter-defense game between attackers and defenders for the\nprotection of critical regions have become increasingly complex and\nstrategically significant across a wide range of domains. However, existing\nstudies predominantly focus on small-scale, simplified two-dimensional\nscenarios, often overlooking realistic environmental perturbations, motion\ndynamics, and inherent heterogeneity--factors that pose substantial challenges\nto real-world applicability. To bridge this gap, we investigate large-scale\nheterogeneous perimeter-defense game in a three-dimensional setting,\nincorporating realistic elements such as motion dynamics and wind fields. We\nderive the Nash equilibrium strategies for both attackers and defenders,\ncharacterize the victory regions, and validate our theoretical findings through\nextensive simulations. To tackle large-scale heterogeneous control challenges\nin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)\nframework. EMFAC leverages representation learning to enable high-level action\naggregation in a mean-field manner, supporting scalable coordination among\ndefenders. Furthermore, we introduce a lightweight agent-level attention\nmechanism based on reward representation, which selectively filters\nobservations and mean-field information to enhance decision-making efficiency\nand accelerate convergence in large-scale tasks. Extensive simulations across\nvarying scales demonstrate the effectiveness and adaptability of EMFAC, which\noutperforms established baselines in both convergence speed and overall\nperformance. To further validate practicality, we test EMFAC in small-scale\nreal-world experiments and conduct detailed analyses, offering deeper insights\ninto the framework's effectiveness in complex scenarios.", "categories": ["cs.AI"], "published": "2025-05-20 11:11:46", "updated": "2025-05-20 11:11:46", "pdf_url": "http://arxiv.org/pdf/2505.14209v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14212v1", "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "abstract": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:16:29", "updated": "2025-05-20 11:16:29", "pdf_url": "http://arxiv.org/pdf/2505.14212v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14216v1", "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "abstract": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 11:22:34", "updated": "2025-05-20 11:22:34", "pdf_url": "http://arxiv.org/pdf/2505.14216v1", "comment": "23 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14217v1", "title": "Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned", "authors": ["Jorge Fabila", "Lidia Garrucho", "V\u00edctor M. Campello", "Carlos Mart\u00edn-Isla", "Karim Lekadir"], "abstract": "This study explores the use of Federated Learning (FL) for tuberculosis (TB)\ndiagnosis using chest X-rays in low-resource settings across Africa. FL allows\nhospitals to collaboratively train AI models without sharing raw patient data,\naddressing privacy concerns and data scarcity that hinder traditional\ncentralized models. The research involved hospitals and research centers in\neight African countries. Most sites used local datasets, while Ghana and The\nGambia used public ones. The study compared locally trained models with a\nfederated model built across all institutions to evaluate FL's real-world\nfeasibility. Despite its promise, implementing FL in sub-Saharan Africa faces\nchallenges such as poor infrastructure, unreliable internet, limited digital\nliteracy, and weak AI regulations. Some institutions were also reluctant to\nshare model updates due to data control concerns. In conclusion, FL shows\nstrong potential for enabling AI-driven healthcare in underserved regions, but\nbroader adoption will require improvements in infrastructure, education, and\nregulatory support.", "categories": ["cs.LG", "cs.AI", "eess.IV"], "published": "2025-05-20 11:23:52", "updated": "2025-05-20 11:23:52", "pdf_url": "http://arxiv.org/pdf/2505.14217v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14226v1", "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "abstract": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:35:25", "updated": "2025-05-20 11:35:25", "pdf_url": "http://arxiv.org/pdf/2505.14226v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14227v1", "title": "VoQA: Visual-only Question Answering", "authors": ["Luyang Jiang", "Jianing An", "Jie Luo", "Wenjun Wu", "Lei Huang"], "abstract": "We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 11:37:49", "updated": "2025-05-20 11:37:49", "pdf_url": "http://arxiv.org/pdf/2505.14227v1", "comment": "18 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14233v1", "title": "Mechanistic Fine-tuning for In-context Learning", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "abstract": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:41:21", "updated": "2025-05-20 11:41:21", "pdf_url": "http://arxiv.org/pdf/2505.14233v1", "comment": "28 pages, 31 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14234v1", "title": "Fast and close Shannon entropy approximation", "authors": ["Illia Horenko", "Davide Bassetti", "Luk\u00e1\u0161 Posp\u00ed\u0161il"], "abstract": "Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy\nare key components in many tools used in physics, information theory, machine\nlearning (ML) and quantum computing. Besides of the significant amounts of SE\ncomputations required in these fields, the singularity of the SE gradient is\none of the central mathematical reason inducing the high cost, frequently low\nrobustness and slow convergence of such tools. Here we propose the Fast Entropy\nApproximation (FEA) - a non-singular rational approximation of Shannon entropy\nand its gradient that achieves a mean absolute error of $10^{-3}$, which is\napproximately $20$ times lower than comparable state-of-the-art methods. FEA\nallows around $50\\%$ faster computation, requiring only $5$ to $6$ elementary\ncomputational operations, as compared to tens of elementary operations behind\nthe fastest entropy computation algorithms with table look-ups, bitshifts, or\nseries approximations. On a set of common benchmarks for the feature selection\nproblem in machine learning, we show that the combined effect of fewer\nelementary operations, low approximation error, and a non-singular gradient\nallows significantly better model quality and enables ML feature extraction\nthat is two to three orders of magnitude faster and computationally cheaper\nwhen incorporating FEA into AI tools.", "categories": ["cs.LG", "cs.AI", "68T01 (Primary) 68Q01, 90C99 (Secondary)"], "published": "2025-05-20 11:41:26", "updated": "2025-05-20 11:41:26", "pdf_url": "http://arxiv.org/pdf/2505.14234v1", "comment": "8 pages, 1 figure", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14235v1", "title": "Toward Embodied AGI: A Review of Embodied AI and the Road Ahead", "authors": ["Yequan Wang", "Aixin Sun"], "abstract": "Artificial General Intelligence (AGI) is often envisioned as inherently\nembodied. With recent advances in robotics and foundational AI models, we stand\nat the threshold of a new era-one marked by increasingly generalized embodied\nAI systems. This paper contributes to the discourse by introducing a systematic\ntaxonomy of Embodied AGI spanning five levels (L1-L5). We review existing\nresearch and challenges at the foundational stages (L1-L2) and outline the key\ncomponents required to achieve higher-level capabilities (L3-L5). Building on\nthese insights and existing technologies, we propose a conceptual framework for\nan L3+ robotic brain, offering both a technical outlook and a foundation for\nfuture exploration.", "categories": ["cs.AI"], "published": "2025-05-20 11:42:26", "updated": "2025-05-20 11:42:26", "pdf_url": "http://arxiv.org/pdf/2505.14235v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14238v1", "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "abstract": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:43:25", "updated": "2025-05-20 11:43:25", "pdf_url": "http://arxiv.org/pdf/2505.14238v1", "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14246v1", "title": "Visual Agentic Reinforcement Fine-Tuning", "authors": ["Ziyu Liu", "Yuhang Zang", "Yushan Zou", "Zijian Liang", "Xiaoyi Dong", "Yuhang Cao", "Haodong Duan", "Dahua Lin", "Jiaqi Wang"], "abstract": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 11:59:25", "updated": "2025-05-20 11:59:25", "pdf_url": "http://arxiv.org/pdf/2505.14246v1", "comment": "project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14252v1", "title": "Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks", "authors": ["Mouad Elaarabi", "Domenico Borzacchiello", "Philippe Le Bot", "Nathan Lauzeral", "Sebastien Comas-Cardona"], "abstract": "In this work, we explore the integration of Sequence Encoding for Online\nParameter Identification with Physics-Informed Neural Networks to create a\nmodel that, once trained, can be utilized for real time applications with\nvariable parameters, boundary conditions, and initial conditions. Recently, the\ncombination of PINNs with Sparse Regression has emerged as a method for\nperforming dynamical system identification through supervised learning and\nsparse regression optimization, while also solving the dynamics using PINNs.\nHowever, this approach can be limited by variations in parameters or boundary\nand initial conditions, requiring retraining of the model whenever changes\noccur. In this work, we introduce an architecture that employs Deep Sets or\nSequence Encoders to encode dynamic parameters, boundary conditions, and\ninitial conditions, using these encoded features as inputs for the PINN,\nenabling the model to adapt to changes in parameters, BCs, and ICs. We apply\nthis approach to three different problems. First, we analyze the Rossler ODE\nsystem, demonstrating the robustness of the model with respect to noise and its\nability to generalize. Next, we explore the model's capability in a 2D\nNavier-Stokes PDE problem involving flow past a cylinder with a parametric\nsinusoidal inlet velocity function, showing that the model can encode pressure\ndata from a few points to identify the inlet velocity profile and utilize\nphysics to compute velocity and pressure throughout the domain. Finally, we\naddress a 1D heat monitoring problem using real data from the heating of glass\nfiber and thermoplastic composite plates.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 12:05:17", "updated": "2025-05-20 12:05:17", "pdf_url": "http://arxiv.org/pdf/2505.14252v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14256v1", "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "abstract": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:09:17", "updated": "2025-05-20 12:09:17", "pdf_url": "http://arxiv.org/pdf/2505.14256v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14260v1", "title": "Speculative Decoding Reimagined for Multimodal Large Language Models", "authors": ["Luxi Lin", "Zhihang Lin", "Zhanpeng Zeng", "Rongrong Ji"], "abstract": "This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 12:12:17", "updated": "2025-05-20 12:12:17", "pdf_url": "http://arxiv.org/pdf/2505.14260v1", "comment": "12 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14268v1", "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:19:10", "updated": "2025-05-20 12:19:10", "pdf_url": "http://arxiv.org/pdf/2505.14268v1", "comment": "16 pages, 14 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14273v1", "title": "X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning", "authors": ["Hiroki Shiraishi", "Hisao Ishibuchi", "Masaya Nakata"], "abstract": "Function approximation is a critical task in various fields. However,\nexisting neural network approaches struggle with locally complex or\ndiscontinuous functions due to their reliance on a single global model covering\nthe entire problem space. We propose X-KAN, a novel method that optimizes\nmultiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary\nrule-based machine learning framework called XCSF. X-KAN combines KAN's high\nexpressiveness with XCSF's adaptive partitioning capability by implementing\nlocal KAN models as rule consequents and defining local regions via rule\nantecedents. Our experimental results on artificial test functions and\nreal-world datasets demonstrate that X-KAN significantly outperforms\nconventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms\nof approximation accuracy. Notably, X-KAN effectively handles functions with\nlocally complex or discontinuous structures that are challenging for\nconventional KAN, using a compact set of rules (average 7.2 $\\pm$ 2.3 rules).\nThese results validate the effectiveness of using KAN as a local model in XCSF,\nwhich evaluates the rule fitness based on both accuracy and generality. Our\nX-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "published": "2025-05-20 12:26:03", "updated": "2025-05-20 12:26:03", "pdf_url": "http://arxiv.org/pdf/2505.14273v1", "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "doi": null, "journal_ref": "34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)"}
{"arxiv_id": "2505.14279v1", "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin M\u00fcnch"], "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:30:46", "updated": "2025-05-20 12:30:46", "pdf_url": "http://arxiv.org/pdf/2505.14279v1", "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14285v1", "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis", "authors": ["Eirini Panteli", "Paulo E. Santos", "Nabil Humphrey"], "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-20 12:35:43", "updated": "2025-05-20 12:35:43", "pdf_url": "http://arxiv.org/pdf/2505.14285v1", "comment": "8 pages; 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14289v1", "title": "EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection", "authors": ["Yijie Lu", "Tianjie Ju", "Manman Zhao", "Xinbei Ma", "Yuan Guo", "ZhuoSheng Zhang"], "abstract": "As multimodal agents are increasingly trained to operate graphical user\ninterfaces (GUIs) to complete user tasks, they face a growing threat from\nindirect prompt injection, attacks in which misleading instructions are\nembedded into the agent's visual environment, such as popups or chat messages,\nand misinterpreted as part of the intended task. A typical example is\nenvironmental injection, in which GUI elements are manipulated to influence\nagent behavior without directly modifying the user prompt. To address these\nemerging attacks, we propose EVA, a red teaming framework for indirect prompt\ninjection which transforms the attack into a closed loop optimization by\ncontinuously monitoring an agent's attention distribution over the GUI and\nupdating adversarial cues, keywords, phrasing, and layout, in response.\nCompared with prior one shot methods that generate fixed prompts without regard\nfor how the model allocates visual attention, EVA dynamically adapts to\nemerging attention hotspots, yielding substantially higher attack success rates\nand far greater transferability across diverse GUI scenarios. We evaluate EVA\non six widely used generalist and specialist GUI agents in realistic settings\nsuch as popup manipulation, chat based phishing, payments, and email\ncomposition. Experimental results show that EVA substantially improves success\nrates over static baselines. Under goal agnostic constraints, where the\nattacker does not know the agent's task intent, EVA still discovers effective\npatterns. Notably, we find that injection styles transfer well across models,\nrevealing shared behavioral biases in GUI agents. These results suggest that\nevolving indirect prompt injection is a powerful tool not only for red teaming\nagents, but also for uncovering common vulnerabilities in their multimodal\ndecision making.", "categories": ["cs.AI"], "published": "2025-05-20 12:41:05", "updated": "2025-05-20 12:41:05", "pdf_url": "http://arxiv.org/pdf/2505.14289v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14295v1", "title": "Benchmarking data encoding methods in Quantum Machine Learning", "authors": ["Orlane Zang", "Gr\u00e9goire Barru\u00e9", "Tony Quertier"], "abstract": "Data encoding plays a fundamental and distinctive role in Quantum Machine\nLearning (QML). While classical approaches process data directly as vectors,\nQML may require transforming classical data into quantum states through\nencoding circuits, known as quantum feature maps or quantum embeddings. This\nstep leverages the inherently high-dimensional and non-linear nature of Hilbert\nspace, enabling more efficient data separation in complex feature spaces that\nmay be inaccessible to classical methods. This encoding part significantly\naffects the performance of the QML model, so it is important to choose the\nright encoding method for the dataset to be encoded. However, this choice is\ngenerally arbitrary, since there is no \"universal\" rule for knowing which\nencoding to choose based on a specific set of data. There are currently a\nvariety of encoding methods using different quantum logic gates. We studied the\nmost commonly used types of encoding methods and benchmarked them using\ndifferent datasets.", "categories": ["quant-ph", "cs.AI"], "published": "2025-05-20 12:44:14", "updated": "2025-05-20 12:44:14", "pdf_url": "http://arxiv.org/pdf/2505.14295v1", "comment": "30 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14300v1", "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors", "authors": ["Maheep Chaudhary", "Fazl Barez"], "abstract": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 12:49:58", "updated": "2025-05-20 12:49:58", "pdf_url": "http://arxiv.org/pdf/2505.14300v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14312v1", "title": "MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains", "authors": ["Kyungeun Lee", "Moonjung Eo", "Hye-Seung Cho", "Dongmin Kim", "Ye Seul Sim", "Seoyoon Kim", "Min-Kook Suh", "Woohyung Lim"], "abstract": "Despite the widespread use of tabular data in real-world applications, most\nbenchmarks rely on average-case metrics, which fail to reveal how model\nbehavior varies across diverse data regimes. To address this, we propose\nMultiTab, a benchmark suite and evaluation framework for multi-dimensional,\ndata-aware analysis of tabular learning algorithms. Rather than comparing\nmodels only in aggregate, MultiTab categorizes 196 publicly available datasets\nalong key data characteristics, including sample size, label imbalance, and\nfeature interaction, and evaluates 13 representative models spanning a range of\ninductive biases. Our analysis shows that model performance is highly sensitive\nto such regimes: for example, models using sample-level similarity excel on\ndatasets with large sample sizes or high inter-feature correlation, while\nmodels encoding inter-feature dependencies perform best with weakly correlated\nfeatures. These findings reveal that inductive biases do not always behave as\nintended, and that regime-aware evaluation is essential for understanding and\nimproving model behavior. MultiTab enables more principled model design and\noffers practical guidance for selecting models tailored to specific data\ncharacteristics. All datasets, code, and optimization logs are publicly\navailable at https://huggingface.co/datasets/LGAI-DILab/Multitab.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 13:00:43", "updated": "2025-05-20 13:00:43", "pdf_url": "http://arxiv.org/pdf/2505.14312v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14316v1", "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion", "authors": ["Tiehan Cui", "Yanxu Mao", "Peipei Liu", "Congying Liu", "Datao You"], "abstract": "Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 13:03:15", "updated": "2025-05-20 13:03:15", "pdf_url": "http://arxiv.org/pdf/2505.14316v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14330v1", "title": "Handloom Design Generation Using Generative Networks", "authors": ["Rajat Kanti Bhattacharjee", "Meghali Nandi", "Amrit Jha", "Gunajit Kalita", "Ferdous Ahmed Barbhuiya"], "abstract": "This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-20 13:16:55", "updated": "2025-05-20 13:16:55", "pdf_url": "http://arxiv.org/pdf/2505.14330v1", "comment": null, "doi": "10.1109/ICIP40778.2020.9190925", "journal_ref": null}
{"arxiv_id": "2505.14341v1", "title": "Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image", "authors": ["Sifan Li", "Ming Tao", "Hao Zhao", "Ling Shao", "Hao Tang"], "abstract": "Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 13:27:52", "updated": "2025-05-20 13:27:52", "pdf_url": "http://arxiv.org/pdf/2505.14341v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14345v1", "title": "Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights", "authors": ["Aydin Abedinia", "Shima Tabakhi", "Vahid Seydi"], "abstract": "Recent advancements in semi-supervised deep learning have introduced\neffective strategies for leveraging both labeled and unlabeled data to improve\nclassification performance. This work proposes a semi-supervised framework that\nutilizes a distance-based weighting mechanism to prioritize critical training\nsamples based on their proximity to test data. By focusing on the most\ninformative examples, the method enhances model generalization and robustness,\nparticularly in challenging scenarios with noisy or imbalanced datasets.\nBuilding on techniques such as uncertainty consistency and graph-based\nrepresentations, the approach addresses key challenges of limited labeled data\nwhile maintaining scalability. Experiments on twelve benchmark datasets\ndemonstrate significant improvements across key metrics, including accuracy,\nprecision, and recall, consistently outperforming existing methods. This\nframework provides a robust and practical solution for semi-supervised\nlearning, with potential applications in domains such as healthcare and\nsecurity where data limitations pose significant challenges.", "categories": ["cs.LG", "cs.AI", "68T05, 62H30", "I.2.6; I.5.1; I.5.4"], "published": "2025-05-20 13:29:04", "updated": "2025-05-20 13:29:04", "pdf_url": "http://arxiv.org/pdf/2505.14345v1", "comment": "5 pages, 6 figures. This paper has been accepted for publication and\n  oral presentation at the 2025 10th IEEE International Conference on Machine\n  Learning Technologies (ICMLT 2025). The final authenticated version will be\n  available in IEEE Xplore following the conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14349v1", "title": "Upgrading Democracies with Fairer Voting Methods", "authors": ["Evangelos Pournaras", "Srijoni Majumdar", "Thomas Wellings", "Joshua C. Yang", "Fatemeh B. Heravan", "Regula H\u00e4nggli Fricker", "Dirk Helbing"], "abstract": "Voting methods are instrumental design element of democracies. Citizens use\nthem to express and aggregate their preferences to reach a collective decision.\nHowever, voting outcomes can be as sensitive to voting rules as they are to\npeople's voting choices. Despite the significance and inter-disciplinary\nscientific progress on voting methods, several democracies keep relying on\noutdated voting methods that do not fit modern, pluralistic societies well,\nwhile lacking social innovation. Here, we demonstrate how one can upgrade\nreal-world democracies, namely by using alternative preferential voting methods\nsuch as cumulative voting and the method of equal shares designed for a\nproportional representation of voters' preferences. By rigorously assessing a\nnew participatory budgeting approach applied in the city of Aarau, Switzerland,\nwe unravel the striking voting outcomes of fair voting methods: more winning\nprojects with the same budget and broader geographic and preference\nrepresentation of citizens by the elected projects, in particular for voters\nwho used to be under-represented, while promoting novel project ideas. We\nprovide profound causal evidence showing that citizens prefer proportional\nvoting methods, which possess strong legitimacy without the need of very\ntechnical specialized explanations. We also reveal strong underlying democratic\nvalues exhibited by citizens who support fair voting methods such as altruism\nand compromise. These findings come with a global momentum to unleash a new and\nlong-awaited participation blueprint of how to upgrade democracies.", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "cs.MA"], "published": "2025-05-20 13:31:43", "updated": "2025-05-20 13:31:43", "pdf_url": "http://arxiv.org/pdf/2505.14349v1", "comment": "Includes Supplementary Information", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14351v1", "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 13:35:55", "updated": "2025-05-20 13:35:55", "pdf_url": "http://arxiv.org/pdf/2505.14351v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14366v1", "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds", "authors": ["Joel Currie", "Gioele Migno", "Enrico Piacenti", "Maria Elena Giannaccini", "Patric Bach", "Davide De Tommaso", "Agnieszka Wykowska"], "abstract": "We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.", "categories": ["cs.AI", "cs.RO"], "published": "2025-05-20 13:49:09", "updated": "2025-05-20 13:49:09", "pdf_url": "http://arxiv.org/pdf/2505.14366v1", "comment": "Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late\n  Breaking Report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14377v1", "title": "When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making", "authors": ["Ulrike Kuhl", "Annika Bush"], "abstract": "Although the integration of artificial intelligence (AI) into everyday tasks\nimproves efficiency and objectivity, it also risks transmitting bias to human\ndecision-making. In this study, we conducted a controlled experiment that\nsimulated hiring decisions to examine how biased AI recommendations - augmented\nwith or without counterfactual explanations - influence human judgment over\ntime. Participants, acting as hiring managers, completed 60 decision trials\ndivided into a baseline phase without AI, followed by a phase with biased (X)AI\nrecommendations (favoring either male or female candidates), and a final\npost-interaction phase without AI. Our results indicate that the participants\nfollowed the AI recommendations 70% of the time when the qualifications of the\ngiven candidates were comparable. Yet, only a fraction of participants detected\nthe gender bias (8 out of 294). Crucially, exposure to biased AI altered\nparticipants' inherent preferences: in the post-interaction phase,\nparticipants' independent decisions aligned with the bias when no\ncounterfactual explanations were provided before, but reversed the bias when\nexplanations were given. Reported trust did not differ significantly across\nconditions. Confidence varied throughout the study phases after exposure to\nmale-biased AI, indicating nuanced effects of AI bias on decision certainty.\nOur findings point to the importance of calibrating XAI to avoid unintended\nbehavioral shifts in order to safeguard equitable decision-making and prevent\nthe adoption of algorithmic bias.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-20 14:00:28", "updated": "2025-05-20 14:00:28", "pdf_url": "http://arxiv.org/pdf/2505.14377v1", "comment": "Accepted for XAI2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14381v1", "title": "SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation", "authors": ["Yuyang Dong", "Nobuhiro Ueda", "Kriszti\u00e1n Boros", "Daiki Ito", "Takuya Sera", "Masafumi Oyamada"], "abstract": "With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions.", "categories": ["cs.AI"], "published": "2025-05-20 14:03:24", "updated": "2025-05-20 14:03:24", "pdf_url": "http://arxiv.org/pdf/2505.14381v1", "comment": "v1", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14391v1", "title": "Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning", "authors": ["Zhaohui Yang", "Chenghua He", "Xiaowen Shi", "Linjing Li", "Qiyue Yin", "Shihong Deng", "Daxin Jiang"], "abstract": "Many studies focus on data annotation techniques for training effective PRMs.\nHowever, current methods encounter a significant issue when applied to long CoT\nreasoning processes: they tend to focus solely on the first incorrect step and\nall preceding steps, assuming that all subsequent steps are incorrect. These\nmethods overlook the unique self-correction and reflection mechanisms inherent\nin long CoT, where correct reasoning steps may still occur after initial\nreasoning mistakes. To address this issue, we propose a novel data annotation\nmethod for PRMs specifically designed to score the long CoT reasoning process.\nGiven that under the reflection pattern, correct and incorrect steps often\nalternate, we introduce the concepts of Error Propagation and Error Cessation,\nenhancing PRMs' ability to identify both effective self-correction behaviors\nand reasoning based on erroneous steps. Leveraging an LLM-based judger for\nannotation, we collect 1.7 million data samples to train a 7B PRM and evaluate\nit at both solution and step levels. Experimental results demonstrate that\ncompared to existing open-source PRMs and PRMs trained on open-source datasets,\nour PRM achieves superior performance across various metrics, including search\nguidance, BoN, and F1 scores. Compared to widely used MC-based annotation\nmethods, our annotation approach not only achieves higher data efficiency but\nalso delivers superior performance. Detailed analysis is also conducted to\ndemonstrate the stability and generalizability of our method.", "categories": ["cs.AI"], "published": "2025-05-20 14:12:05", "updated": "2025-05-20 14:12:05", "pdf_url": "http://arxiv.org/pdf/2505.14391v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14394v1", "title": "Knowledge Graph Based Repository-Level Code Generation", "authors": ["Mihir Athale", "Vishal Vaddina"], "abstract": "Recent advancements in Large Language Models (LLMs) have transformed code\ngeneration from natural language queries. However, despite their extensive\nknowledge and ability to produce high-quality code, LLMs often struggle with\ncontextual accuracy, particularly in evolving codebases. Current code search\nand retrieval methods frequently lack robustness in both the quality and\ncontextual relevance of retrieved results, leading to suboptimal code\ngeneration. This paper introduces a novel knowledge graph-based approach to\nimprove code search and retrieval leading to better quality of code generation\nin the context of repository-level tasks. The proposed approach represents code\nrepositories as graphs, capturing structural and relational information for\nenhanced context-aware code generation. Our framework employs a hybrid approach\nfor code retrieval to improve contextual relevance, track inter-file modular\ndependencies, generate more robust code and ensure consistency with the\nexisting codebase. We benchmark the proposed approach on the Evolutionary Code\nBenchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,\nand demonstrate that our method significantly outperforms the baseline\napproach. These findings suggest that knowledge graph based code generation\ncould advance robust, context-sensitive coding assistance tools.", "categories": ["cs.AI"], "published": "2025-05-20 14:13:59", "updated": "2025-05-20 14:13:59", "pdf_url": "http://arxiv.org/pdf/2505.14394v1", "comment": "8 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14395v1", "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "abstract": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:14:00", "updated": "2025-05-20 14:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14395v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "authors": ["Ga\u00ebl Gendron", "Jo\u017ee M. Ro\u017eanec", "Michael Witbrock", "Gillian Dobbie"], "abstract": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"], "published": "2025-05-20 14:14:05", "updated": "2025-05-20 14:14:05", "pdf_url": "http://arxiv.org/pdf/2505.14396v1", "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14398v1", "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "abstract": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 14:14:38", "updated": "2025-05-20 14:14:38", "pdf_url": "http://arxiv.org/pdf/2505.14398v1", "comment": "Data and code are available at https://peterbaile.github.io/lag/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14403v1", "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning", "authors": ["Zhaohui Yang", "Shilei Jiang", "Chen Hu", "Linjing Li", "Shihong Deng", "Daxin Jiang"], "abstract": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.", "categories": ["cs.AI"], "published": "2025-05-20 14:16:49", "updated": "2025-05-20 14:16:49", "pdf_url": "http://arxiv.org/pdf/2505.14403v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14412v1", "title": "PRL: Prompts from Reinforcement Learning", "authors": ["Pawe\u0142 Batorski", "Adrian Kosmala", "Paul Swoboda"], "abstract": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 14:26:19", "updated": "2025-05-20 14:26:19", "pdf_url": "http://arxiv.org/pdf/2505.14412v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14419v1", "title": "SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation", "authors": ["Huimin Xu", "Xin Mao", "Feng-Lin Li", "Xiaobao Wu", "Wang Chen", "Wei Zhang", "Anh Tuan Luu"], "abstract": "Process Reward Models (PRMs) have demonstrated promising results in\nmathematical reasoning, but existing process annotation approaches, whether\nthrough human annotations or Monte Carlo simulations, remain computationally\nexpensive. In this paper, we introduce Step COmpression for Process Estimation\n(SCOPE), a novel compression-based approach that significantly reduces\nannotation costs. We first translate natural language reasoning steps into code\nand normalize them through Abstract Syntax Tree, then merge equivalent steps to\nconstruct a prefix tree. Unlike simulation-based methods that waste numerous\nsamples on estimation, SCOPE leverages a compression-based prefix tree where\neach root-to-leaf path serves as a training sample, reducing the complexity\nfrom $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K\nsamples with only 5% of the computational resources required by previous\nmethods. Empirical results demonstrate that PRMs trained on our dataset\nconsistently outperform existing automated annotation approaches on both\nBest-of-N strategy and ProcessBench.", "categories": ["cs.AI"], "published": "2025-05-20 14:31:15", "updated": "2025-05-20 14:31:15", "pdf_url": "http://arxiv.org/pdf/2505.14419v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14428v1", "title": "Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications", "authors": ["Riccardo D'Elia"], "abstract": "The objective of this proposal is to bridge the gap between Deep Learning\n(DL) and System Dynamics (SD) by developing an interpretable neural system\ndynamics framework. While DL excels at learning complex models and making\naccurate predictions, it lacks interpretability and causal reliability.\nTraditional SD approaches, on the other hand, provide transparency and causal\ninsights but are limited in scalability and require extensive domain knowledge.\nTo overcome these limitations, this project introduces a Neural System Dynamics\npipeline, integrating Concept-Based Interpretability, Mechanistic\nInterpretability, and Causal Machine Learning. This framework combines the\npredictive power of DL with the interpretability of traditional SD models,\nresulting in both causal reliability and scalability. The efficacy of the\nproposed pipeline will be validated through real-world applications of the\nEU-funded AutoMoTIF project, which is focused on autonomous multimodal\ntransportation systems. The long-term goal is to collect actionable insights\nthat support the integration of explainability and safety in autonomous\nsystems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 14:38:39", "updated": "2025-05-20 14:38:39", "pdf_url": "http://arxiv.org/pdf/2505.14428v1", "comment": "To be submitted to CEUR-WS.org for publication in the Doctoral\n  Consortium Proceedings of XAI 2025, The World Conference on Explainable\n  Artificial Intelligence", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14435v1", "title": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI", "authors": ["Annika Bush", "Meltem Aksoy", "Markus Pauly", "Greta Ontrup"], "abstract": "As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making.", "categories": ["cs.CY", "cs.AI"], "published": "2025-05-20 14:41:56", "updated": "2025-05-20 14:41:56", "pdf_url": "http://arxiv.org/pdf/2505.14435v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14436v1", "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:42:03", "updated": "2025-05-20 14:42:03", "pdf_url": "http://arxiv.org/pdf/2505.14436v1", "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14442v1", "title": "Creative Preference Optimization", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:43:41", "updated": "2025-05-20 14:43:41", "pdf_url": "http://arxiv.org/pdf/2505.14442v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14451v1", "title": "RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation", "authors": ["Md Atik Ahamed", "Qiang Ye", "Qiang Cheng"], "abstract": "Missing values in high-dimensional, mixed-type datasets pose significant\nchallenges for data imputation, particularly under Missing Not At Random (MNAR)\nmechanisms. Existing methods struggle to integrate local and global data\ncharacteristics, limiting performance in MNAR and high-dimensional settings. We\npropose an innovative framework, RefiDiff, combining local machine learning\npredictions with a novel Mamba-based denoising network capturing\ninterrelationships among distant features and samples. Our approach leverages\npre-refinement for initial warm-up imputations and post-refinement to polish\nresults, enhancing stability and accuracy. By encoding mixed-type data into\nunified tokens, RefiDiff enables robust imputation without architectural or\nhyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods\nacross missing-value settings, excelling in MNAR with a 4x faster training time\nthan SOTA DDPM-based approaches. Extensive evaluations on nine real-world\ndatasets demonstrate its robustness, scalability, and effectiveness in handling\ncomplex missingness patterns.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 14:51:07", "updated": "2025-05-20 14:51:07", "pdf_url": "http://arxiv.org/pdf/2505.14451v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14452v1", "title": "How Managers Perceive AI-Assisted Conversational Training for Workplace Communication", "authors": ["Lance T Wilhelm", "Xiaohan Ding", "Kirk McInnis Knutsen", "Buse Carik", "Eugenia H Rho"], "abstract": "Effective workplace communication is essential for managerial success, yet\nmany managers lack access to tailored and sustained training. Although\nAI-assisted communication systems may offer scalable training solutions, little\nis known about how managers envision the role of AI in helping them improve\ntheir communication skills. To investigate this, we designed a conversational\nrole-play system, CommCoach, as a functional probe to understand how managers\nanticipate using AI to practice their communication skills. Through\nsemi-structured interviews, participants emphasized the value of adaptive,\nlow-risk simulations for practicing difficult workplace conversations. They\nalso highlighted opportunities, including human-AI teaming, transparent and\ncontext-aware feedback, and greater control over AI-generated personas.\nAI-assisted communication training should balance personalization, structured\nlearning objectives, and adaptability to different user styles and contexts.\nHowever, achieving this requires carefully navigating tensions between adaptive\nand consistent AI feedback, realism and potential bias, and the open-ended\nnature of AI conversations versus structured workplace discourse.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-20 14:51:27", "updated": "2025-05-20 14:51:27", "pdf_url": "http://arxiv.org/pdf/2505.14452v1", "comment": "accepted to CUI '25", "doi": "10.1145/3719160.3736639", "journal_ref": null}
{"arxiv_id": "2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "authors": ["Chihan Huang", "Hao Tang"], "abstract": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:52:41", "updated": "2025-05-20 14:52:41", "pdf_url": "http://arxiv.org/pdf/2505.14455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14469v1", "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "abstract": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:05:03", "updated": "2025-05-20 15:05:03", "pdf_url": "http://arxiv.org/pdf/2505.14469v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14479v1", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "abstract": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:13:32", "updated": "2025-05-20 15:13:32", "pdf_url": "http://arxiv.org/pdf/2505.14479v1", "comment": "long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14489v1", "title": "Reasoning Models Better Express Their Confidence", "authors": ["Dongkeun Yoon", "Seungone Kim", "Sohee Yang", "Sunkyoung Kim", "Soyeon Kim", "Yongil Kim", "Eunbi Choi", "Yireun Kim", "Minjoon Seo"], "abstract": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:19:00", "updated": "2025-05-20 15:19:00", "pdf_url": "http://arxiv.org/pdf/2505.14489v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14499v1", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "abstract": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:28:26", "updated": "2025-05-20 15:28:26", "pdf_url": "http://arxiv.org/pdf/2505.14499v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14505v1", "title": "ModRWKV: Transformer Multimodality in Linear Time", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "abstract": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:34:36", "updated": "2025-05-20 15:34:36", "pdf_url": "http://arxiv.org/pdf/2505.14505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14510v1", "title": "BACON: A fully explainable AI model with graded logic for decision making problems", "authors": ["Haishi Bai", "Jozo Dujmovic", "Jianwu Wang"], "abstract": "As machine learning models and autonomous agents are increasingly deployed in\nhigh-stakes, real-world domains such as healthcare, security, finance, and\nrobotics, the need for transparent and trustworthy explanations has become\ncritical. To ensure end-to-end transparency of AI decisions, we need models\nthat are not only accurate but also fully explainable and human-tunable. We\nintroduce BACON, a novel framework for automatically training explainable AI\nmodels for decision making problems using graded logic. BACON achieves high\npredictive accuracy while offering full structural transparency and precise,\nlogic-based symbolic explanations, enabling effective human-AI collaboration\nand expert-guided refinement. We evaluate BACON with a diverse set of\nscenarios: classic Boolean approximation, Iris flower classification, house\npurchasing decisions and breast cancer diagnosis. In each case, BACON provides\nhigh-performance models while producing compact, human-verifiable decision\nlogic. These results demonstrate BACON's potential as a practical and\nprincipled approach for delivering crisp, trustworthy explainable AI.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-20 15:39:05", "updated": "2025-05-20 15:39:05", "pdf_url": "http://arxiv.org/pdf/2505.14510v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14513v1", "title": "Latent Flow Transformer", "authors": ["Yen-Chen Wu", "Feng-Ting Liao", "Meng-Hsi Chen", "Pei-Chen Ho", "Farhang Nabiei", "Da-shan Shiu"], "abstract": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 15:41:05", "updated": "2025-05-20 15:41:05", "pdf_url": "http://arxiv.org/pdf/2505.14513v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14523v1", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "abstract": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-20 15:46:44", "updated": "2025-05-20 15:46:44", "pdf_url": "http://arxiv.org/pdf/2505.14523v1", "comment": "To be published in ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14524v1", "title": "Guarded Query Routing for Large Language Models", "authors": ["Richard \u0160l\u00e9her", "William Brach", "Tibor Sloboda", "Kristi\u00e1n Ko\u0161\u0165\u00e1l", "Lukas Galke"], "abstract": "Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}.", "categories": ["cs.AI"], "published": "2025-05-20 15:46:59", "updated": "2025-05-20 15:46:59", "pdf_url": "http://arxiv.org/pdf/2505.14524v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14526v1", "title": "NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation", "authors": ["Matteo El-Hariry", "Antoine Richard", "Ricard M. Castan", "Luis F. W. Batista", "Matthieu Geist", "Cedric Pradalier", "Miguel Olivares-Mendez"], "abstract": "Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-20 15:48:23", "updated": "2025-05-20 15:48:23", "pdf_url": "http://arxiv.org/pdf/2505.14526v1", "comment": "Submitted for publication. Under review (2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14533v1", "title": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers", "authors": ["Mohammad Irfan Uddin", "Nishad Tasnim", "Md Omor Faruk", "Zejian Zhou"], "abstract": "Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 15:52:43", "updated": "2025-05-20 15:52:43", "pdf_url": "http://arxiv.org/pdf/2505.14533v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14539v1", "title": "A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)", "authors": ["Gaia Belardinelli", "Thomas Bolander", "Sebastian Watzl"], "abstract": "In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.", "categories": ["cs.AI"], "published": "2025-05-20 15:56:34", "updated": "2025-05-20 15:56:34", "pdf_url": "http://arxiv.org/pdf/2505.14539v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14544v1", "title": "Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study", "authors": ["Saahil Mahato"], "abstract": "Urban traffic congestion, particularly at intersections, significantly\nimpacts travel time, fuel consumption, and emissions. Traditional fixed-time\nsignal control systems often lack the adaptability to manage dynamic traffic\npatterns effectively. This study explores the application of multi-agent\nreinforcement learning (MARL) to optimize traffic signal coordination across\nmultiple intersections within a simulated environment. Utilizing Pygame, a\nsimulation was developed to model a network of interconnected intersections\nwith randomly generated vehicle flows to reflect realistic traffic variability.\nA decentralized MARL controller was implemented, in which each traffic signal\noperates as an autonomous agent, making decisions based on local observations\nand information from neighboring agents. Performance was evaluated against a\nbaseline fixed-time controller using metrics such as average vehicle wait time\nand overall throughput. The MARL approach demonstrated statistically\nsignificant improvements, including reduced average waiting times and improved\nthroughput. These findings suggest that MARL-based dynamic control strategies\nhold substantial promise for improving urban traffic management efficiency.\nMore research is recommended to address scalability and real-world\nimplementation challenges.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-20 15:59:44", "updated": "2025-05-20 15:59:44", "pdf_url": "http://arxiv.org/pdf/2505.14544v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14549v1", "title": "Can Large Language Models Really Recognize Your Name?", "authors": ["Dzung Pham", "Peter Kairouz", "Niloofar Mireshghallah", "Eugene Bagdasarian", "Chau Minh Pham", "Amir Houmansadr"], "abstract": "Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-20 16:05:05", "updated": "2025-05-20 16:05:05", "pdf_url": "http://arxiv.org/pdf/2505.14549v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14551v1", "title": "Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains", "authors": ["Petros Drineas", "Rohit Nema", "Rafail Ostrovsky", "Vassilis Zikas"], "abstract": "Reputation systems play an essential role in the Internet era, as they enable\npeople to decide whom to trust, by collecting and aggregating data about users'\nbehavior. Recently, several works proposed the use of reputation for the design\nand scalability improvement of decentralized (blockchain) ledgers; however,\nsuch systems are prone to manipulation and to our knowledge no game-theoretic\ntreatment exists that can support their economic robustness.\n  In this work we put forth a new model for the design of what we call, {\\em\ntrustworthy reputation systems}. Concretely, we describe a class of games,\nwhich we term {\\em trustworthy reputation games}, that enable a set of users to\nreport a function of their beliefs about the trustworthiness of each server in\na set -- i.e., their estimate of the probability that this server will behave\naccording to its specified strategy -- in a way that satisfies the following\nproperties:\n  1. It is $(\\epsilon$-)best response for any rational user in the game to play\na prescribed (truthful) strategy according to their true belief.\n  2. Assuming that the users' beliefs are not too far from the {\\em true}\ntrustworthiness of the servers, playing the above ($\\epsilon-$)Nash equilibrium\nallows anyone who observes the users' strategies to estimate the relative\ntrustworthiness of any two servers.\n  Our utilities and decoding function build on a connection between the well\nknown PageRank algorithm and the problem of trustworthiness discovery, which\ncan be of independent interest. Finally, we show how the above games are\nmotivated by and can be leveraged in proof-of-reputation (PoR) blockchains.", "categories": ["cs.GT", "cs.AI", "cs.CR"], "published": "2025-05-20 16:06:25", "updated": "2025-05-20 16:06:25", "pdf_url": "http://arxiv.org/pdf/2505.14551v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14552v1", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "abstract": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:06:32", "updated": "2025-05-20 16:06:32", "pdf_url": "http://arxiv.org/pdf/2505.14552v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14555v1", "title": "Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting", "authors": ["Yingtao Luo", "Shikai Fang", "Binqing Wu", "Qingsong Wen", "Liang Sun"], "abstract": "Weather forecasting is essential but remains computationally intensive and\nphysically incomplete in traditional numerical weather prediction (NWP)\nmethods. Deep learning (DL) models offer efficiency and accuracy but often\nignore physical laws, limiting interpretability and generalization. We propose\nPhyDL-NWP, a physics-guided deep learning framework that integrates physical\nequations with latent force parameterization into data-driven models. It\npredicts weather variables from arbitrary spatiotemporal coordinates, computes\nphysical terms via automatic differentiation, and uses a physics-informed loss\nto align predictions with governing dynamics. PhyDL-NWP enables resolution-free\ndownscaling by modeling weather as a continuous function and fine-tunes\npre-trained models with minimal overhead, achieving up to 170x faster inference\nwith only 55K parameters. Experiments show that PhyDL-NWP improves both\nforecasting performance and physical consistency.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:13:20", "updated": "2025-05-20 16:13:20", "pdf_url": "http://arxiv.org/pdf/2505.14555v1", "comment": "Published/Accepted in KDD 2025 (February Cycle)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14561v1", "title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification", "authors": ["Theo Lepage", "Reda Dehak"], "abstract": "Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS.", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "published": "2025-05-20 16:19:34", "updated": "2025-05-20 16:19:34", "pdf_url": "http://arxiv.org/pdf/2505.14561v1", "comment": "accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14564v1", "title": "Bellman operator convergence enhancements in reinforcement learning algorithms", "authors": ["David Krame Kadurha", "Domini Jocema Leko Moutouo", "Yae Ulrich Gaba"], "abstract": "This paper reviews the topological groundwork for the study of reinforcement\nlearning (RL) by focusing on the structure of state, action, and policy spaces.\nWe begin by recalling key mathematical concepts such as complete metric spaces,\nwhich form the foundation for expressing RL problems. By leveraging the Banach\ncontraction principle, we illustrate how the Banach fixed-point theorem\nexplains the convergence of RL algorithms and how Bellman operators, expressed\nas operators on Banach spaces, ensure this convergence. The work serves as a\nbridge between theoretical mathematics and practical algorithm design, offering\nnew approaches to enhance the efficiency of RL. In particular, we investigate\nalternative formulations of Bellman operators and demonstrate their impact on\nimproving convergence rates and performance in standard RL environments such as\nMountainCar, CartPole, and Acrobot. Our findings highlight how a deeper\nmathematical understanding of RL can lead to more effective algorithms for\ndecision-making problems.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:24:42", "updated": "2025-05-20 16:24:42", "pdf_url": "http://arxiv.org/pdf/2505.14564v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14566v1", "title": "KIPPO: Koopman-Inspired Proximal Policy Optimization", "authors": ["Andrei Cozma", "Landon Harris", "Hairong Qi"], "abstract": "Reinforcement Learning (RL) has made significant strides in various domains,\nand policy gradient methods like Proximal Policy Optimization (PPO) have gained\npopularity due to their balance in performance, training stability, and\ncomputational efficiency. These methods directly optimize policies through\ngradient-based updates. However, developing effective control policies for\nenvironments with complex and non-linear dynamics remains a challenge. High\nvariance in gradient estimates and non-convex optimization landscapes often\nlead to unstable learning trajectories. Koopman Operator Theory has emerged as\na powerful framework for studying non-linear systems through an\ninfinite-dimensional linear operator that acts on a higher-dimensional space of\nmeasurement functions. In contrast with their non-linear counterparts, linear\nsystems are simpler, more predictable, and easier to analyze. In this paper, we\npresent Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an\napproximately linear latent-space representation of the underlying system's\ndynamics while retaining essential features for effective policy learning. This\nis achieved through a Koopman-approximation auxiliary network that can be added\nto the baseline policy optimization algorithms without altering the\narchitecture of the core policy or value function. Extensive experimental\nresults demonstrate consistent improvements over the PPO baseline with 6-60%\nincreased performance while reducing variability by up to 91% when evaluated on\nvarious continuous control tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 16:25:41", "updated": "2025-05-20 16:25:41", "pdf_url": "http://arxiv.org/pdf/2505.14566v1", "comment": "Accepted for IJCAI 2025. This arXiv submission is the full version of\n  the conference paper, including the appendix and supplementary material\n  omitted from the IJCAI proceedings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "abstract": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 16:28:08", "updated": "2025-05-20 16:28:08", "pdf_url": "http://arxiv.org/pdf/2505.14569v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14599v1", "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "abstract": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 16:49:40", "updated": "2025-05-20 16:49:40", "pdf_url": "http://arxiv.org/pdf/2505.14599v1", "comment": "Accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14603v1", "title": "Towards a Foundation Model for Communication Systems", "authors": ["Davide Buffelli", "Sowmen Das", "Yu-Wei Lin", "Sattar Vakili", "Chien-Yi Wang", "Masoud Attarifar", "Pritthijit Nath", "Da-shan Shiu"], "abstract": "Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.", "categories": ["cs.AI", "cs.LG", "eess.SP"], "published": "2025-05-20 16:52:11", "updated": "2025-05-20 16:52:11", "pdf_url": "http://arxiv.org/pdf/2505.14603v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14604v1", "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning", "authors": ["Haoran Zhao", "Yuchen Yan", "Yongliang Shen", "Haolei Xu", "Wenqi Zhang", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.", "categories": ["cs.AI"], "published": "2025-05-20 16:53:40", "updated": "2025-05-20 16:53:40", "pdf_url": "http://arxiv.org/pdf/2505.14604v1", "comment": "Github:https://github.com/CCAI-Lab/Self-Braking-Tuning; Project:\n  https://CCAI-Lab.github.io/SBT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14608v1", "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "abstract": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:55:44", "updated": "2025-05-20 16:55:44", "pdf_url": "http://arxiv.org/pdf/2505.14608v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14615v1", "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", "authors": ["Anjiang Wei", "Yuheng Wu", "Yingjia Wan", "Tarun Suresh", "Huanmi Tan", "Zhanke Zhou", "Sanmi Koyejo", "Ke Wang", "Alex Aiken"], "abstract": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "published": "2025-05-20 17:00:22", "updated": "2025-05-20 17:00:22", "pdf_url": "http://arxiv.org/pdf/2505.14615v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14625v1", "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 17:16:44", "updated": "2025-05-20 17:16:44", "pdf_url": "http://arxiv.org/pdf/2505.14625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14627v1", "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "authors": ["Ashutosh Adhikari", "Mirella Lapata"], "abstract": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:18:17", "updated": "2025-05-20 17:18:17", "pdf_url": "http://arxiv.org/pdf/2505.14627v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14629v1", "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "abstract": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-20 17:19:57", "updated": "2025-05-20 17:19:57", "pdf_url": "http://arxiv.org/pdf/2505.14629v1", "comment": "Accepted at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "published": "2025-05-20 17:24:09", "updated": "2025-05-20 17:24:09", "pdf_url": "http://arxiv.org/pdf/2505.14633v1", "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14646v1", "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation", "authors": ["Anna C. Doris", "Md Ferdous Alam", "Amin Heyrani Nobari", "Faez Ahmed"], "abstract": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-20 17:34:44", "updated": "2025-05-20 17:34:44", "pdf_url": "http://arxiv.org/pdf/2505.14646v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "abstract": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-20 17:42:34", "updated": "2025-05-20 17:42:34", "pdf_url": "http://arxiv.org/pdf/2505.14654v1", "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14656v1", "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning", "authors": ["Zihao Zhang", "Fei Liu"], "abstract": "While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch.", "categories": ["cs.AI"], "published": "2025-05-20 17:43:33", "updated": "2025-05-20 17:43:33", "pdf_url": "http://arxiv.org/pdf/2505.14656v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14659v1", "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks", "authors": ["Navneet Kaur", "Lav Gupta"], "abstract": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-20 17:46:09", "updated": "2025-05-20 17:46:09", "pdf_url": "http://arxiv.org/pdf/2505.14659v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14660v1", "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "authors": ["Ronald Seoh", "Dan Goldwasser"], "abstract": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 17:47:04", "updated": "2025-05-20 17:47:04", "pdf_url": "http://arxiv.org/pdf/2505.14660v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14661v1", "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems", "authors": ["Matthew Russo", "Sivaprasad Sudhir", "Gerardo Vitagliano", "Chunwei Liu", "Tim Kraska", "Samuel Madden", "Michael Cafarella"], "abstract": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.", "categories": ["cs.DB", "cs.AI", "H.2.4; I.2.5"], "published": "2025-05-20 17:49:46", "updated": "2025-05-20 17:49:46", "pdf_url": "http://arxiv.org/pdf/2505.14661v1", "comment": "16 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14664v1", "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings", "authors": ["Yilin Ye", "Junchao Huang", "Xingchen Zeng", "Jiazhi Xia", "Wei Zeng"], "abstract": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "published": "2025-05-20 17:52:03", "updated": "2025-05-20 17:52:03", "pdf_url": "http://arxiv.org/pdf/2505.14664v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:54:54", "updated": "2025-05-20 17:54:54", "pdf_url": "http://arxiv.org/pdf/2505.14667v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "categories": ["cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:55:25", "updated": "2025-05-20 17:55:25", "pdf_url": "http://arxiv.org/pdf/2505.14668v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14673v1", "title": "Training-Free Watermarking for Autoregressive Image Generation", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "abstract": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.", "categories": ["cs.CV", "cs.AI", "cs.CR"], "published": "2025-05-20 17:58:02", "updated": "2025-05-20 17:58:02", "pdf_url": "http://arxiv.org/pdf/2505.14673v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "abstract": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:59:13", "updated": "2025-05-20 17:59:13", "pdf_url": "http://arxiv.org/pdf/2505.14680v1", "comment": "SIGIR 2025 Perspective Paper", "doi": "10.1145/3726302.3730353", "journal_ref": null}
{"arxiv_id": "2505.14681v1", "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "published": "2025-05-20 17:59:16", "updated": "2025-05-20 17:59:16", "pdf_url": "http://arxiv.org/pdf/2505.14681v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14684v1", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 17:59:31", "updated": "2025-05-20 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.14684v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13792v1", "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "abstract": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 00:49:19", "updated": "2025-05-20 00:49:19", "pdf_url": "http://arxiv.org/pdf/2505.13792v1", "comment": "10 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13820v1", "title": "Structured Agent Distillation for Large Language Model", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 02:01:55", "updated": "2025-05-20 02:01:55", "pdf_url": "http://arxiv.org/pdf/2505.13820v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "abstract": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 02:27:08", "updated": "2025-05-20 02:27:08", "pdf_url": "http://arxiv.org/pdf/2505.13840v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13844v1", "title": "Improve Language Model and Brain Alignment via Associative Memory", "authors": ["Congchi Yin", "Yongpeng Zhang", "Xuyun Wen", "Piji Li"], "abstract": "Associative memory engages in the integration of relevant information for\ncomprehension in the human cognition system. In this work, we seek to improve\nalignment between language models and human brain while processing speech\ninformation by integrating associative memory. After verifying the alignment\nbetween language model and brain by mapping language model activations to brain\nactivity, the original text stimuli expanded with simulated associative memory\nare regarded as input to computational language models. We find the alignment\nbetween language model and brain is improved in brain regions closely related\nto associative memory processing. We also demonstrate large language models\nafter specific supervised fine-tuning better align with brain response, by\nbuilding the \\textit{Association} dataset containing 1000 samples of stories,\nwith instructions encouraging associative memory as input and associated\ncontent as output.", "categories": ["cs.CL"], "published": "2025-05-20 02:39:09", "updated": "2025-05-20 02:39:09", "pdf_url": "http://arxiv.org/pdf/2505.13844v1", "comment": "Accepted by Findings of ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13847v1", "title": "Forensic deepfake audio detection using segmental speech features", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "abstract": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 02:42:46", "updated": "2025-05-20 02:42:46", "pdf_url": "http://arxiv.org/pdf/2505.13847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13855v1", "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "abstract": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 03:02:05", "updated": "2025-05-20 03:02:05", "pdf_url": "http://arxiv.org/pdf/2505.13855v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13862v1", "title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks", "authors": ["Guobin Shen", "Dongcheng Zhao", "Linghao Feng", "Xiang He", "Jihang Wang", "Sicheng Shen", "Haibo Tong", "Yiting Dong", "Jindong Li", "Xiang Zheng", "Yi Zeng"], "abstract": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 03:14:57", "updated": "2025-05-20 03:14:57", "pdf_url": "http://arxiv.org/pdf/2505.13862v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13866v1", "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning", "authors": ["Jiwon Song", "Dongwon Jo", "Yulhwa Kim", "Jae-Joon Kim"], "abstract": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.", "categories": ["cs.CL"], "published": "2025-05-20 03:21:52", "updated": "2025-05-20 03:21:52", "pdf_url": "http://arxiv.org/pdf/2505.13866v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13878v1", "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models", "authors": ["Yanggan Gu", "Zhaoyi Yan", "Yuanyi Wang", "Yiming Zhang", "Qi Zhou", "Fei Wu", "Hongxia Yang"], "abstract": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 03:32:37", "updated": "2025-05-20 03:32:37", "pdf_url": "http://arxiv.org/pdf/2505.13878v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13886v1", "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning", "authors": ["Jingqi Tong", "Jixin Tang", "Hangcheng Li", "Yurong Mou", "Ming Zhang", "Jun Zhao", "Yanbo Wen", "Fan Song", "Jiahao Zhan", "Yuyang Lu", "Chaoran Tao", "Zhiyuan Guo", "Jizhou Yu", "Tianhao Cheng", "Changhao Jiang", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Weifeng Ge", "Guanhua Chen", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "abstract": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable to produce, challenging for state-of-the-art\nmodels, and diverse with 30 games and 158 tasks. Surprisingly, despite training\nsolely on game data, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33\\% across 7 diverse\nvision-language benchmarks. Our code and dataset are available at\nhttps://github.com/tongjingqi/Code2Logic.", "categories": ["cs.CL", "I.2.7; I.2.10"], "published": "2025-05-20 03:47:44", "updated": "2025-05-20 03:47:44", "pdf_url": "http://arxiv.org/pdf/2505.13886v1", "comment": "49 pages, 19 figures, submitted to NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13887v1", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "abstract": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 03:48:19", "updated": "2025-05-20 03:48:19", "pdf_url": "http://arxiv.org/pdf/2505.13887v1", "comment": "17 pages, 7 figures, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:2502.17110", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13890v1", "title": "Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM", "authors": ["Zhen Xiong", "Yujun Cai", "Zhecheng Li", "Yiwei Wang"], "abstract": "Recent advances in test-time scaling have enabled Large Language Models\n(LLMs) to display sophisticated reasoning abilities via extended\nChain-of-Thought (CoT) generation. Despite their potential, these Reasoning\nLLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as\nperformance degradation under few-shot prompting, that challenge our current\nunderstanding of RLMs. In this work, we introduce a unified graph-based\nanalytical framework for better modeling the reasoning processes of RLMs. Our\nmethod first clusters long, verbose CoT outputs into semantically coherent\nreasoning steps, then constructs directed reasoning graphs to capture\ncontextual and logical dependencies among these steps. Through comprehensive\nanalysis across models and prompting regimes, we reveal that structural\nproperties, such as exploration density, branching, and convergence ratios,\nstrongly correlate with reasoning accuracy. Our findings demonstrate how\nprompting strategies substantially reshape the internal reasoning structure of\nRLMs, directly affecting task outcomes. The proposed framework not only enables\nquantitative evaluation of reasoning quality beyond conventional metrics but\nalso provides practical insights for prompt engineering and the cognitive\nanalysis of LLMs. Code and resources will be released to facilitate future\nresearch in this direction.", "categories": ["cs.CL"], "published": "2025-05-20 03:54:57", "updated": "2025-05-20 03:54:57", "pdf_url": "http://arxiv.org/pdf/2505.13890v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13893v1", "title": "InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion", "authors": ["Yuanyi Wang", "Zhaoyi Yan", "Yiming Zhang", "Qi Zhou", "Yanggan Gu", "Fei Wu", "Hongxia Yang"], "abstract": "Recent advances in large language models (LLMs) have intensified efforts to\nfuse heterogeneous open-source models into a unified system that inherits their\ncomplementary strengths. Existing logit-based fusion methods maintain inference\nefficiency but treat vocabulary dimensions independently, overlooking semantic\ndependencies encoded by cross-dimension interactions. These dependencies\nreflect how token types interact under a model's internal reasoning and are\nessential for aligning models with diverse generation behaviors. To explicitly\nmodel these dependencies, we propose \\textbf{InfiGFusion}, the first\nstructure-aware fusion framework with a novel \\textit{Graph-on-Logits\nDistillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output\nand aggregate their outer products across sequence positions to form a global\nco-activation graph, where nodes represent vocabulary channels and edges\nquantify their joint activations. To ensure scalability and efficiency, we\ndesign a sorting-based closed-form approximation that reduces the original\n$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \\log n)$, with provable\napproximation guarantees. Experiments across multiple fusion settings show that\nGLD consistently improves fusion quality and stability. InfiGFusion outperforms\nSOTA models and fusion baselines across 11 benchmarks spanning reasoning,\ncoding, and mathematics. It shows particular strength in complex reasoning\ntasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal\nJudgement over SFT, demonstrating superior multi-step and relational inference.", "categories": ["cs.CL"], "published": "2025-05-20 03:55:35", "updated": "2025-05-20 03:55:35", "pdf_url": "http://arxiv.org/pdf/2505.13893v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13903v1", "title": "Let's Verify Math Questions Step by Step", "authors": ["Chengyu Shen", "Zhen Hao Wong", "Runming He", "Hao Liang", "Meiyi Qiang", "Zimo Meng", "Zhengyang Zhao", "Bohan Zeng", "Zhengzhou Zhu", "Bin Cui", "Wentao Zhang"], "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress in\nmathematical reasoning. To enable such capabilities, many existing works\ndistill strong reasoning models into long chains of thought or design\nalgorithms to construct high-quality math QA data for training. However, these\nefforts primarily focus on generating correct reasoning paths and answers,\nwhile largely overlooking the validity of the questions themselves. In this\nwork, we propose Math Question Verification (MathQ-Verify), a novel five-stage\npipeline designed to rigorously filter ill-posed or under-specified math\nproblems. MathQ-Verify first performs format-level validation to remove\nredundant instructions and ensure that each question is syntactically\nwell-formed. It then formalizes each question, decomposes it into atomic\nconditions, and verifies them against mathematical definitions. Next, it\ndetects logical contradictions among these conditions, followed by a\ngoal-oriented completeness check to ensure the question provides sufficient\ninformation for solving. To evaluate this task, we use existing benchmarks\nalong with an additional dataset we construct, containing 2,147 math questions\nwith diverse error types, each manually double-validated. Experiments show that\nMathQ-Verify achieves state-of-the-art performance across multiple benchmarks,\nimproving the F1 score by up to 25 percentage points over the direct\nverification baseline. It further attains approximately 90% precision and 63%\nrecall through a lightweight model voting scheme. MathQ-Verify offers a\nscalable and accurate solution for curating reliable mathematical datasets,\nreducing label noise and avoiding unnecessary computation on invalid questions.\nOur code and data are available at https://github.com/scuuy/MathQ-Verify.", "categories": ["cs.CL"], "published": "2025-05-20 04:07:29", "updated": "2025-05-20 04:07:29", "pdf_url": "http://arxiv.org/pdf/2505.13903v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13908v1", "title": "Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology", "authors": ["Ajitesh Bankula", "Praney Bankula"], "abstract": "Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it\nallows for models trained on resource-rich languages to be applied to\nlow-resource languages more effectively. Recently massively multilingual\npre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot\ntransfer capabilities[14] [13]. This paper investigates cross-linguistic\ntransfer through the lens of language families and morphology. Investigating\nhow language family proximity and morphological similarity affect performance\nacross NLP tasks. We further discuss our results and how it relates to findings\nfrom recent literature. Overall, we compare multilingual model performance and\nreview how linguistic distance metrics correlate with transfer outcomes. We\nalso look into emerging approaches that integrate typological and morphological\ninformation into model pre-training to improve transfer to diverse\nlanguages[18] [19].", "categories": ["cs.CL"], "published": "2025-05-20 04:19:34", "updated": "2025-05-20 04:19:34", "pdf_url": "http://arxiv.org/pdf/2505.13908v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13909v1", "title": "Efficient Agent Training for Computer Use", "authors": ["Yanheng He", "Jiahe Jin", "Pengfei Liu"], "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 04:20:18", "updated": "2025-05-20 04:20:18", "pdf_url": "http://arxiv.org/pdf/2505.13909v1", "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13913v1", "title": "Word length predicts word order: \"Min-max\"-ing drives language evolution", "authors": ["Hiram Ring"], "abstract": "Current theories of language propose an innate (Baker 2001; Chomsky 1981) or\na functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface\nstructures (i.e. word order) that we observe in languages of the world, while\nevolutionary modeling (Dunn et al. 2011) suggests that descent is the primary\nfactor influencing such patterns. Although there are hypotheses for word order\nchange from both innate and usage-based perspectives for specific languages and\nfamilies, there are key disagreements between the two major proposals for\nmechanisms that drive the evolution of language more broadly (Wasow 2002; Levy\n2008). This paper proposes a universal underlying mechanism for word order\nchange based on a large tagged parallel dataset of over 1,500 languages\nrepresenting 133 language families and 111 isolates. Results indicate that word\nclass length is significantly correlated with word order crosslinguistically,\nbut not in a straightforward manner, partially supporting opposing theories of\nprocessing, while at the same time predicting historical word order change in\ntwo different phylogenetic lines and explaining more variance than descent or\nlanguage area in regression models. Such findings suggest an integrated\n\"Min-Max\" theory of language evolution driven by competing pressures of\nprocessing and information structure, aligning with recent efficiency-oriented\n(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et\nal. 2025).", "categories": ["cs.CL"], "published": "2025-05-20 04:25:55", "updated": "2025-05-20 04:25:55", "pdf_url": "http://arxiv.org/pdf/2505.13913v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13936v1", "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "abstract": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:04:15", "updated": "2025-05-20 05:04:15", "pdf_url": "http://arxiv.org/pdf/2505.13936v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13941v1", "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "authors": ["Haoyang Fang", "Boran Han", "Nick Erickson", "Xiyuan Zhang", "Su Zhou", "Anirudh Dagar", "Jiani Zhang", "Ali Caner Turkmen", "Cuixiong Hu", "Huzefa Rangwala", "Ying Nian Wu", "Bernie Wang", "George Karypis"], "abstract": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions.", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 05:20:53", "updated": "2025-05-20 05:20:53", "pdf_url": "http://arxiv.org/pdf/2505.13941v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13944v1", "title": "Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Nam Le", "Linh Ngo Van"], "abstract": "Memory-based approaches have shown strong performance in Continual Relation\nExtraction (CRE). However, storing examples from previous tasks increases\nmemory usage and raises privacy concerns. Recently, prompt-based methods have\nemerged as a promising alternative, as they do not rely on storing past\nsamples. Despite this progress, current prompt-based techniques face several\ncore challenges in CRE, particularly in accurately identifying task identities\nand mitigating catastrophic forgetting. Existing prompt selection strategies\noften suffer from inaccuracies, lack robust mechanisms to prevent forgetting in\nshared parameters, and struggle to handle both cross-task and within-task\nvariations. In this paper, we propose WAVE++, a novel approach inspired by the\nconnection between prefix-tuning and mixture of experts. Specifically, we\nintroduce task-specific prompt pools that enhance flexibility and adaptability\nacross diverse tasks while avoiding boundary-spanning risks; this design more\neffectively captures variations within each task and across tasks. To further\nrefine relation classification, we incorporate label descriptions that provide\nricher, more global context, enabling the model to better distinguish among\ndifferent relations. We also propose a training-free mechanism to improve task\nprediction during inference. Moreover, we integrate a generative model to\nconsolidate prior knowledge within the shared parameters, thereby removing the\nneed for explicit data storage. Extensive experiments demonstrate that WAVE++\noutperforms state-of-the-art prompt-based and rehearsal-based methods, offering\na more robust solution for continual relation extraction. Our code is publicly\navailable at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.", "categories": ["cs.CL"], "published": "2025-05-20 05:22:17", "updated": "2025-05-20 05:22:17", "pdf_url": "http://arxiv.org/pdf/2505.13944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13948v1", "title": "Memory-Centric Embodied Question Answer", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "abstract": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.", "categories": ["cs.CL", "cs.AI", "cs.MM"], "published": "2025-05-20 05:27:57", "updated": "2025-05-20 05:27:57", "pdf_url": "http://arxiv.org/pdf/2505.13948v1", "comment": "14pages, 7 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13949v1", "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "abstract": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 05:28:21", "updated": "2025-05-20 05:28:21", "pdf_url": "http://arxiv.org/pdf/2505.13949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13957v1", "title": "Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation", "authors": ["Jiankun Zhang", "Shenglai Zeng", "Jie Ren", "Tianqi Zheng", "Hui Liu", "Xianfeng Tang", "Hui Liu", "Yi Chang"], "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by\nintegrating external multimodal databases, but introduce unexplored privacy\nvulnerabilities. While text-based RAG privacy risks have been studied,\nmultimodal data presents unique challenges. We provide the first systematic\nanalysis of MRAG privacy vulnerabilities across vision-language and\nspeech-language modalities. Using a novel compositional structured prompt\nattack in a black-box setting, we demonstrate how attackers can extract private\ninformation by manipulating queries. Our experiments reveal that LMMs can both\ndirectly generate outputs resembling retrieved content and produce descriptions\nthat indirectly expose sensitive information, highlighting the urgent need for\nrobust privacy-preserving MRAG techniques.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 05:37:22", "updated": "2025-05-20 05:37:22", "pdf_url": "http://arxiv.org/pdf/2505.13957v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13963v1", "title": "Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability", "authors": ["Qianli Wang", "Mingyang Wang", "Nils Feldhus", "Simon Ostermann", "Yuan Cao", "Hinrich Sch\u00fctze", "Sebastian M\u00f6ller", "Vera Schmitt"], "abstract": "Quantization methods are widely used to accelerate inference and streamline\nthe deployment of large language models (LLMs). While prior research has\nextensively investigated the degradation of various LLM capabilities due to\nquantization, its effects on model explainability and interpretability, which\nare crucial for understanding decision-making processes, remain unexplored. To\naddress this gap, we conduct comprehensive experiments using three common\nquantization techniques at distinct bit widths, in conjunction with two\nexplainability methods, counterfactual examples and natural language\nexplanations, as well as two interpretability approaches, knowledge\nmemorization analysis and latent multi-hop reasoning analysis. We complement\nour analysis with a thorough user study, evaluating selected explainability\nmethods. Our findings reveal that, depending on the configuration, quantization\ncan significantly impact model explainability and interpretability. Notably,\nthe direction of this effect is not consistent, as it strongly depends on (1)\nthe quantization method, (2) the explainability or interpretability approach,\nand (3) the evaluation protocol. In some settings, human evaluation shows that\nquantization degrades explainability, while in others, it even leads to\nimprovements. Our work serves as a cautionary tale, demonstrating that\nquantization can unpredictably affect model transparency. This insight has\nimportant implications for deploying LLMs in applications where transparency is\na critical requirement.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 06:01:09", "updated": "2025-05-20 06:01:09", "pdf_url": "http://arxiv.org/pdf/2505.13963v1", "comment": "In submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13965v1", "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 06:05:56", "updated": "2025-05-20 06:05:56", "pdf_url": "http://arxiv.org/pdf/2505.13965v1", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13972v1", "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals", "authors": ["Qianli Wang", "Van Bach Nguyen", "Nils Feldhus", "Luis Felipe Villa-Arenas", "Christin Seifert", "Sebastian M\u00f6ller", "Vera Schmitt"], "abstract": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models. Through extensive experiments involving two state-of-the-art\nLLM-based methods, three datasets, five generator models, and 15 judge models,\ncomplemented by a user study (n = 90), we demonstrate that judge models with an\nindependent, non-fine-tuned relationship to the generator model provide the\nmost reliable label flipping evaluations. Relationships between the generator\nand judge models, which are closely aligned with the user study for CDA, result\nin better model performance and robustness. Nevertheless, we find that the gap\nbetween the most effective judge models and the results obtained from the user\nstudy remains considerably large. This suggests that a fully automated pipeline\nfor CDA may be inadequate and requires human intervention.", "categories": ["cs.CL"], "published": "2025-05-20 06:12:17", "updated": "2025-05-20 06:12:17", "pdf_url": "http://arxiv.org/pdf/2505.13972v1", "comment": "in submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13973v1", "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "abstract": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 06:12:20", "updated": "2025-05-20 06:12:20", "pdf_url": "http://arxiv.org/pdf/2505.13973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13975v1", "title": "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models", "authors": ["Yuxuan Jiang", "Dawei Li", "Frank Ferraro"], "abstract": "While Large Reasoning Models (LRMs) have demonstrated success in complex\nreasoning tasks through long chain-of-thought (CoT) reasoning, their inference\noften involves excessively verbose reasoning traces, resulting in substantial\ninefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a\nhybrid framework that combines inference-time pruning with tuning-based\ndistillation, two widely used strategies for efficient reasoning. DRP uses a\nteacher model to perform skill-aware step decomposition and content pruning,\nand then distills the pruned reasoning paths into a student model, enabling it\nto reason both efficiently and accurately. Across several challenging\nmathematical reasoning datasets, we find that models trained with DRP achieve\nsubstantial improvements in token efficiency without sacrificing accuracy.\nSpecifically, DRP reduces average token usage on GSM8K from 917 to 328 while\nimproving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on\nAIME with no performance drop. Further analysis shows that aligning the\nreasoning structure of training CoTs with the student's reasoning capacity is\ncritical for effective knowledge transfer and performance gains.", "categories": ["cs.CL"], "published": "2025-05-20 06:15:15", "updated": "2025-05-20 06:15:15", "pdf_url": "http://arxiv.org/pdf/2505.13975v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13979v1", "title": "Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection", "authors": ["Maya Srikanth", "Run Chen", "Julia Hirschberg"], "abstract": "Multimodal models play a key role in empathy detection, but their performance\ncan suffer when modalities provide conflicting cues. To understand these\nfailures, we examine cases where unimodal and multimodal predictions diverge.\nUsing fine-tuned models for text, audio, and video, along with a gated fusion\nmodel, we find that such disagreements often reflect underlying ambiguity, as\nevidenced by annotator uncertainty. Our analysis shows that dominant signals in\none modality can mislead fusion when unsupported by others. We also observe\nthat humans, like models, do not consistently benefit from multimodal input.\nThese insights position disagreement as a useful diagnostic signal for\nidentifying challenging examples and improving empathy system robustness.", "categories": ["cs.CL"], "published": "2025-05-20 06:25:02", "updated": "2025-05-20 06:25:02", "pdf_url": "http://arxiv.org/pdf/2505.13979v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13988v1", "title": "The Hallucination Tax of Reinforcement Finetuning", "authors": ["Linxin Song", "Taiwei Shi", "Jieyu Zhao"], "abstract": "Reinforcement finetuning (RFT) has become a standard approach for enhancing\nthe reasoning capabilities of large language models (LLMs). However, its impact\non model trustworthiness remains underexplored. In this work, we identify and\nsystematically study a critical side effect of RFT, which we term the\nhallucination tax: a degradation in refusal behavior causing models to produce\nhallucinated answers to unanswerable questions confidently. To investigate\nthis, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of\nunanswerable math problems designed to probe models' ability to recognize an\nunanswerable question by reasoning from the insufficient or ambiguous\ninformation. Our results show that standard RFT training could reduce model\nrefusal rates by more than 80%, which significantly increases model's tendency\nto hallucinate. We further demonstrate that incorporating just 10% SUM during\nRFT substantially restores appropriate refusal behavior, with minimal accuracy\ntrade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage\ninference-time compute to reason about their own uncertainty and knowledge\nboundaries, improving generalization not only to out-of-domain math problems\nbut also to factual question answering tasks.", "categories": ["cs.CL"], "published": "2025-05-20 06:36:45", "updated": "2025-05-20 06:36:45", "pdf_url": "http://arxiv.org/pdf/2505.13988v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13990v1", "title": "DecIF: Improving Instruction-Following through Meta-Decomposition", "authors": ["Tingfeng Hui", "Pengyu Zhu", "Bowen Ping", "Ling Tang", "Yaqi Zhang", "Sen Su"], "abstract": "Instruction-following has emerged as a crucial capability for large language\nmodels (LLMs). However, existing approaches often rely on pre-existing\ndocuments or external resources to synthesize instruction-following data, which\nlimits their flexibility and generalizability. In this paper, we introduce\nDecIF, a fully autonomous, meta-decomposition guided framework that generates\ndiverse and high-quality instruction-following data using only LLMs. DecIF is\ngrounded in the principle of decomposition. For instruction generation, we\nguide LLMs to iteratively produce various types of meta-information, which are\nthen combined with response constraints to form well-structured and\nsemantically rich instructions. We further utilize LLMs to detect and resolve\npotential inconsistencies within the generated instructions. Regarding response\ngeneration, we decompose each instruction into atomic-level evaluation\ncriteria, enabling rigorous validation and the elimination of inaccurate\ninstruction-response pairs. Extensive experiments across a wide range of\nscenarios and settings demonstrate DecIF's superior performance on\ninstruction-following tasks. Further analysis highlights its strong\nflexibility, scalability, and generalizability in automatically synthesizing\nhigh-quality instruction data.", "categories": ["cs.CL"], "published": "2025-05-20 06:38:28", "updated": "2025-05-20 06:38:28", "pdf_url": "http://arxiv.org/pdf/2505.13990v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.13995v1", "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "abstract": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-20 06:45:17", "updated": "2025-05-20 06:45:17", "pdf_url": "http://arxiv.org/pdf/2505.13995v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14009v1", "title": "Activation-Guided Consensus Merging for Large Language Models", "authors": ["Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Qintong Li", "Mingyang Liu", "Xiongwei Han", "Zhijiang Guo", "Han Wu", "Linqi Song"], "abstract": "Recent research has increasingly focused on reconciling the reasoning\ncapabilities of System 2 with the efficiency of System 1. While existing\ntraining-based and prompt-based approaches face significant challenges in terms\nof efficiency and stability, model merging emerges as a promising strategy to\nintegrate the diverse capabilities of different Large Language Models (LLMs)\ninto a unified model. However, conventional model merging methods often assume\nuniform importance across layers, overlooking the functional heterogeneity\ninherent in neural components. To address this limitation, we propose\n\\textbf{A}ctivation-Guided \\textbf{C}onsensus \\textbf{M}erging (\\textbf{ACM}),\na plug-and-play merging framework that determines layer-specific merging\ncoefficients based on mutual information between activations of pre-trained and\nfine-tuned models. ACM effectively preserves task-specific capabilities without\nrequiring gradient computations or additional training. Extensive experiments\non Long-to-Short (L2S) and general merging tasks demonstrate that ACM\nconsistently outperforms all baseline methods. For instance, in the case of\nQwen-7B models, TIES-Merging equipped with ACM achieves a \\textbf{55.3\\%}\nreduction in response length while simultaneously improving reasoning accuracy\nby \\textbf{1.3} points. We submit the code with the paper for reproducibility,\nand it will be publicly available.", "categories": ["cs.CL"], "published": "2025-05-20 07:04:01", "updated": "2025-05-20 07:04:01", "pdf_url": "http://arxiv.org/pdf/2505.14009v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14015v1", "title": "AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation", "authors": ["Tai D. Nguyen", "Long H. Pham", "Jun Sun"], "abstract": "The rapid advancement of domain-specific large language models (LLMs) in\nfields like law necessitates frameworks that account for nuanced regional legal\ndistinctions, which are critical for ensuring compliance and trustworthiness.\nExisting legal evaluation benchmarks often lack adaptability and fail to\naddress diverse local contexts, limiting their utility in dynamically evolving\nregulatory landscapes. To address these gaps, we propose AutoLaw, a novel\nviolation detection framework that combines adversarial data generation with a\njury-inspired deliberation process to enhance legal compliance of LLMs. Unlike\nstatic approaches, AutoLaw dynamically synthesizes case law to reflect local\nregulations and employs a pool of LLM-based \"jurors\" to simulate judicial\ndecision-making. Jurors are ranked and selected based on synthesized legal\nexpertise, enabling a deliberation process that minimizes bias and improves\ndetection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG\n(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:\nadversarial data generation improves LLM discrimination, while the jury-based\nvoting strategy significantly boosts violation detection rates. Our results\nhighlight the framework's ability to adaptively probe legal misalignments and\ndeliver reliable, context-aware judgments, offering a scalable solution for\nevaluating and enhancing LLMs in legally sensitive applications.", "categories": ["cs.CL"], "published": "2025-05-20 07:09:13", "updated": "2025-05-20 07:09:13", "pdf_url": "http://arxiv.org/pdf/2505.14015v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14038v1", "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data", "authors": ["Xinzhe Zheng", "Sijie Ji", "Jiawei Sun", "Renqi Chen", "Wei Gao", "Mani Srivastava"], "abstract": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 07:36:28", "updated": "2025-05-20 07:36:28", "pdf_url": "http://arxiv.org/pdf/2505.14038v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14045v1", "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 07:43:45", "updated": "2025-05-20 07:43:45", "pdf_url": "http://arxiv.org/pdf/2505.14045v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14052v1", "title": "Improved Methods for Model Pruning and Knowledge Distillation", "authors": ["Wei Jiang", "Anying Fu", "Youling Zhang"], "abstract": "Model pruning is a performance optimization technique for large language\nmodels like R1 or o3-mini. However, existing pruning methods often lead to\nsignificant performance degradation or require extensive retraining and\nfine-tuning. This technique aims to identify and remove neurons, connections\nunlikely leading to the contribution during the human-computer interaction\nphase. Our goal is to obtain a much smaller and faster knowledge distilled\nmodel that can quickly generate content almost as good as those of the unpruned\nones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an\nimproved pruning method that effectively reduces model size and computational\ncomplexity while maintaining performance comparable to the original unpruned\nmodel even at extreme pruned levels. The improved method is based on weights,\nbias fixed in the pre-training phase and GRPO rewards verified during the\npost-training phase as our novel pruning indicators. Preliminary experimental\nresults show that our method outperforms and be comparable to state-of-the-art\nmethods across various pruning levels and different downstream computational\nlinguistics tasks.", "categories": ["cs.CL", "cs.CE"], "published": "2025-05-20 07:53:40", "updated": "2025-05-20 07:53:40", "pdf_url": "http://arxiv.org/pdf/2505.14052v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14070v1", "title": "Enhancing LLMs via High-Knowledge Data Selection", "authors": ["Feiyu Duan", "Xuemiao Zhang", "Sirui Wang", "Haoran Que", "Yuqi Liu", "Wenge Rong", "Xunliang Cai"], "abstract": "The performance of Large Language Models (LLMs) is intrinsically linked to\nthe quality of its training data. Although several studies have proposed\nmethods for high-quality data selection, they do not consider the importance of\nknowledge richness in text corpora. In this paper, we propose a novel and\ngradient-free High-Knowledge Scorer (HKS) to select high-quality data from the\ndimension of knowledge, to alleviate the problem of knowledge scarcity in the\npre-trained corpus. We propose a comprehensive multi-domain knowledge element\npool and introduce knowledge density and coverage as metrics to assess the\nknowledge content of the text. Based on this, we propose a comprehensive\nknowledge scorer to select data with intensive knowledge, which can also be\nutilized for domain-specific high-knowledge data selection by restricting\nknowledge elements to the specific domain. We train models on a high-knowledge\nbilingual dataset, and experimental results demonstrate that our scorer\nimproves the model's performance in knowledge-intensive and general\ncomprehension tasks, and is effective in enhancing both the generic and\ndomain-specific capabilities of the model.", "categories": ["cs.CL"], "published": "2025-05-20 08:21:37", "updated": "2025-05-20 08:21:37", "pdf_url": "http://arxiv.org/pdf/2505.14070v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14071v1", "title": "Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models", "authors": ["Woody Haosheng Gan", "Deqing Fu", "Julian Asilis", "Ollie Liu", "Dani Yogatama", "Vatsal Sharan", "Robin Jia", "Willie Neiswanger"], "abstract": "Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-20 08:23:08", "updated": "2025-05-20 08:23:08", "pdf_url": "http://arxiv.org/pdf/2505.14071v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14079v1", "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "abstract": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules.", "categories": ["cs.CL"], "published": "2025-05-20 08:35:35", "updated": "2025-05-20 08:35:35", "pdf_url": "http://arxiv.org/pdf/2505.14079v1", "comment": null, "doi": null, "journal_ref": "ACL 2025"}
{"arxiv_id": "2505.14080v1", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "abstract": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "published": "2025-05-20 08:36:47", "updated": "2025-05-20 08:36:47", "pdf_url": "http://arxiv.org/pdf/2505.14080v1", "comment": null, "doi": "10.1145/3715275.3732112", "journal_ref": "FAccT '25: Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency"}
{"arxiv_id": "2505.14099v1", "title": "Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering", "authors": ["Yihua Zhu", "Qianying Liu", "Akiko Aizawa", "Hidetoshi Shimodaira"], "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions using structured knowledge from KBs. While LLM-only approaches offer\ngeneralization, they suffer from outdated knowledge, hallucinations, and lack\nof transparency. Chain-based KG-RAG methods address these issues by\nincorporating external KBs, but are limited to simple chain-structured\nquestions due to the absence of planning and logical structuring. Inspired by\nsemantic parsing methods, we propose PDRR: a four-stage framework consisting of\nPredict, Decompose, Retrieve, and Reason. Our method first predicts the\nquestion type and decomposes the question into structured triples. Then\nretrieves relevant information from KBs and guides the LLM as an agent to\nreason over and complete the decomposed triples. Experimental results\ndemonstrate that PDRR consistently outperforms existing methods across various\nLLM backbones and achieves superior performance on both chain-structured and\nnon-chain complex questions.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-20 09:01:52", "updated": "2025-05-20 09:01:52", "pdf_url": "http://arxiv.org/pdf/2505.14099v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14101v1", "title": "MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations", "authors": ["Ernests Lavrinovics", "Russa Biswas", "Katja Hose", "Johannes Bjerva"], "abstract": "Large Language Models (LLMs) have inherent limitations of faithfulness and\nfactuality, commonly referred to as hallucinations. Several benchmarks have\nbeen developed that provide a test bed for factuality evaluation within the\ncontext of English-centric datasets, while relying on supplementary informative\ncontext like web links or text passages but ignoring the available structured\nfactual resources. To this end, Knowledge Graphs (KGs) have been identified as\na useful aid for hallucination mitigation, as they provide a structured way to\nrepresent the facts about entities and their relations with minimal linguistic\noverhead. We bridge the lack of KG paths and multilinguality for factual\nlanguage modeling within the existing hallucination evaluation benchmarks and\npropose a KG-based multilingual, multihop benchmark called \\textbf{MultiHal}\nframed for generative text evaluation. As part of our data collection pipeline,\nwe mined 140k KG-paths from open-domain KGs, from which we pruned noisy\nKG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation\nshows an absolute scale increase by approximately 0.12 to 0.36 points for the\nsemantic similarity score in KG-RAG over vanilla QA across multiple languages\nand multiple models, demonstrating the potential of KG integration. We\nanticipate MultiHal will foster future research towards several graph-based\nhallucination mitigation and fact-checking tasks.", "categories": ["cs.CL"], "published": "2025-05-20 09:03:35", "updated": "2025-05-20 09:03:35", "pdf_url": "http://arxiv.org/pdf/2505.14101v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14104v1", "title": "Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents", "authors": ["Wei Fan", "Tianshi Zheng", "Yiran Hu", "Zheye Deng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Haoran Li", "Weixing Shen", "Yangqiu Song"], "abstract": "Legal rules encompass not only codified statutes but also implicit\nadjudicatory principles derived from precedents that contain discretionary\nnorms, social morality, and policy. While computational legal research has\nadvanced in applying established rules to cases, inducing legal rules from\njudicial decisions remains understudied, constrained by limitations in model\ninference efficacy and symbolic reasoning capability. The advent of Large\nLanguage Models (LLMs) offers unprecedented opportunities for automating the\nextraction of such latent principles, yet progress is stymied by the absence of\nformal task definitions, benchmark datasets, and methodologies. To address this\ngap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,\ngeneralizable doctrinal rules from sets of analogous precedents, distilling\ntheir shared preconditions, normative behaviors, and legal consequences. We\nintroduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese\ncases in total) for model tuning and 216 expert-annotated gold test sets.\nExperimental results reveal that: 1) State-of-the-art LLMs struggle with\nover-generalization and hallucination; 2) Training on our dataset markedly\nenhances LLMs capabilities in capturing nuanced rule patterns across similar\ncases.", "categories": ["cs.CL"], "published": "2025-05-20 09:10:52", "updated": "2025-05-20 09:10:52", "pdf_url": "http://arxiv.org/pdf/2505.14104v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14106v1", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "abstract": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:13:22", "updated": "2025-05-20 09:13:22", "pdf_url": "http://arxiv.org/pdf/2505.14106v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14107v1", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "abstract": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 09:14:53", "updated": "2025-05-20 09:14:53", "pdf_url": "http://arxiv.org/pdf/2505.14107v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14112v1", "title": "Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking", "authors": ["Tianle Gu", "Zongqi Wang", "Kexin Huang", "Yuanqi Yao", "Xiangliang Zhang", "Yujiu Yang", "Xiuying Chen"], "abstract": "Logit-based LLM watermarking traces and verifies AI-generated content by\nmaintaining green and red token lists and increasing the likelihood of green\ntokens during generation. However, it fails in low-entropy scenarios, where\npredictable outputs make green token selection difficult without disrupting\nnatural text flow. Existing approaches address this by assuming access to the\noriginal LLM to calculate entropy and selectively watermark high-entropy\ntokens. However, these methods face two major challenges: (1) high\ncomputational costs and detection delays due to reliance on the original LLM,\nand (2) potential risks of model leakage. To address these limitations, we\npropose Invisible Entropy (IE), a watermarking paradigm designed to enhance\nboth safety and efficiency. Instead of relying on the original LLM, IE\nintroduces a lightweight feature extractor and an entropy tagger to predict\nwhether the entropy of the next token is high or low. Furthermore, based on\ntheoretical analysis, we develop a threshold navigator that adaptively sets\nentropy thresholds. It identifies a threshold where the watermark ratio\ndecreases as the green token count increases, enhancing the naturalness of the\nwatermarked text and improving detection robustness. Experiments on HumanEval\nand MBPP datasets demonstrate that IE reduces parameter size by 99\\% while\nachieving performance on par with state-of-the-art methods. Our work introduces\na safe and efficient paradigm for low-entropy watermarking.\nhttps://github.com/Carol-gutianle/IE\nhttps://huggingface.co/datasets/Carol0110/IE-Tagger", "categories": ["cs.CL", "cs.CR"], "published": "2025-05-20 09:19:06", "updated": "2025-05-20 09:19:06", "pdf_url": "http://arxiv.org/pdf/2505.14112v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14116v1", "title": "Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst", "authors": ["Hongru Wang", "Deng Cai", "Wanjun Zhong", "Shijue Huang", "Jeff Z. Pan", "Zeming Liu", "Kam-Fai Wong"], "abstract": "Inference-time scaling has attracted much attention which significantly\nenhance the performance of Large Language Models (LLMs) in complex reasoning\ntasks by increasing the length of Chain-of-Thought. These longer intermediate\nreasoning rationales embody various meta-reasoning skills in human cognition,\nsuch as reflection and decomposition, being difficult to create and acquire. In\nthis work, we introduce \\textit{Self-Reasoning Language Model} (SRLM), where\nthe model itself can synthesize longer CoT data and iteratively improve\nperformance through self-training. By incorporating a few demonstration\nexamples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from\nexisting responses, which act as a reasoning catalyst, we demonstrate that SRLM\nnot only enhances the model's initial performance but also ensures more stable\nand consistent improvements in subsequent iterations. Our proposed SRLM\nachieves an average absolute improvement of more than $+2.5$ points across five\nreasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.\nMoreover, it brings more improvements with more times of sampling during\ninference, such as absolute $+7.89$ average improvement with $64$ sampling\ntimes, revealing the in-depth, diverse and creative reasoning paths in SRLM\nagainst the strong baseline.", "categories": ["cs.CL"], "published": "2025-05-20 09:21:26", "updated": "2025-05-20 09:21:26", "pdf_url": "http://arxiv.org/pdf/2505.14116v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14130v1", "title": "Probing BERT for German Compound Semantics", "authors": ["Filip Mileti\u0107", "Aaron Schmid", "Sabine Schulte im Walde"], "abstract": "This paper investigates the extent to which pretrained German BERT encodes\nknowledge of noun compound semantics. We comprehensively vary combinations of\ntarget tokens, layers, and cased vs. uncased models, and evaluate them by\npredicting the compositionality of 868 gold standard compounds. Looking at\nrepresentational patterns within the transformer architecture, we observe\ntrends comparable to equivalent prior work on English, with compositionality\ninformation most easily recoverable in the early layers. However, our strongest\nresults clearly lag behind those reported for English, suggesting an inherently\nmore difficult task in German. This may be due to the higher productivity of\ncompounding in German than in English and the associated increase in\nconstituent-level ambiguity, including in our target compound set.", "categories": ["cs.CL"], "published": "2025-05-20 09:36:17", "updated": "2025-05-20 09:36:17", "pdf_url": "http://arxiv.org/pdf/2505.14130v1", "comment": "Accepted to SwissText 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14131v1", "title": "Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "abstract": "In table question answering (TQA), tables are encoded as either texts or\nimages. Prior work suggests that passing images of tables to multi-modal large\nlanguage models (MLLMs) performs comparably to or even better than using\ntextual input with large language models (LLMs). However, the lack of\ncontrolled setups limits fine-grained distinctions between these approaches. In\nthis paper, we conduct the first controlled study on the effectiveness of\nseveral combinations of table representations and models from two perspectives:\nquestion complexity and table size. We build a new benchmark based on existing\nTQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we\nfind that the best combination of table representation and model varies across\nsetups. We propose FRES, a method selecting table representations dynamically,\nand observe a 10% average performance improvement compared to using both\nrepresentations indiscriminately.", "categories": ["cs.CL"], "published": "2025-05-20 09:36:17", "updated": "2025-05-20 09:36:17", "pdf_url": "http://arxiv.org/pdf/2505.14131v1", "comment": "Accepted at ACL25 (Findings)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14146v1", "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "abstract": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 09:53:56", "updated": "2025-05-20 09:53:56", "pdf_url": "http://arxiv.org/pdf/2505.14146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14149v1", "title": "Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information", "authors": ["Chengzhi Zhang", "Xinyi Yan", "Lei Zhao", "Yingyi Zhang"], "abstract": "The exponential increase in academic papers has significantly increased the\ntime required for researchers to access relevant literature. Keyphrase\nExtraction (KPE) offers a solution to this situation by enabling researchers to\nefficiently retrieve relevant literature. The current study on KPE from\nacademic articles aims to improve the performance of extraction models through\ninnovative approaches using Title and Abstract as input corpora. However, the\nsemantic richness of keywords is significantly constrained by the length of the\nabstract. While full-text-based KPE can address this issue, it simultaneously\nintroduces noise, which significantly diminishes KPE performance. To address\nthis issue, this paper utilized the structural features and section texts\nobtained from the section structure information of academic articles to extract\nkeyphrase from academic papers. The approach consists of two main parts: (1)\nexploring the effect of seven structural features on KPE models, and (2)\nintegrating the extraction results from all section texts used as input corpora\nfor KPE models via a keyphrase integration algorithm to obtain the keyphrase\nintegration result. Furthermore, this paper also examined the effect of the\nclassification quality of section structure on the KPE performance. The results\nshow that incorporating structural features improves KPE performance, though\ndifferent features have varying effects on model efficacy. The keyphrase\nintegration approach yields the best performance, and the classification\nquality of section structure can affect KPE performance. These findings\nindicate that using the section structure information of academic articles\ncontributes to effective KPE from academic articles. The code and dataset\nsupporting this study are available at https://github.com/yan-xinyi/SSB_KPE.", "categories": ["cs.CL", "cs.DL", "cs.IR"], "published": "2025-05-20 09:57:34", "updated": "2025-05-20 09:57:34", "pdf_url": "http://arxiv.org/pdf/2505.14149v1", "comment": null, "doi": "10.1007/s11192-025-05286-2", "journal_ref": "Scientometrics, 2025"}
{"arxiv_id": "2505.14157v1", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "abstract": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:05:11", "updated": "2025-05-20 10:05:11", "pdf_url": "http://arxiv.org/pdf/2505.14157v1", "comment": "25 pages, 42 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14158v1", "title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering", "authors": ["Sanjay Govindan", "Maurice Pagnucco", "Yang Song"], "abstract": "Large Language Models (LLMs) are trained on diverse and often conflicting\nknowledge spanning multiple domains and time periods. Some of this knowledge is\nonly valid within specific temporal contexts, such as answering the question,\n\"Who is the President of the United States in 2022?\" Ensuring LLMs generate\ntime appropriate responses is crucial for maintaining relevance and accuracy.\nIn this work we explore activation engineering as a method for temporally\naligning LLMs to improve factual recall without any training or dataset\ncreation. In this research we explore an activation engineering technique to\nground three versions of LLaMA 2 to specific points in time and examine the\neffects of varying injection layers and prompting strategies. Our experiments\ndemonstrate up to a 44% and 16% improvement in relative and explicit prompting\nrespectively, achieving comparable performance to the fine-tuning method\nproposed by Zhao et al. (2024) . Notably, our approach achieves similar results\nto the fine-tuning baseline while being significantly more computationally\nefficient and requiring no pre-aligned datasets.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:09:40", "updated": "2025-05-20 10:09:40", "pdf_url": "http://arxiv.org/pdf/2505.14158v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14160v1", "title": "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models", "authors": ["Zahraa Al Sahili", "Ioannis Patras", "Matthew Purver"], "abstract": "Multilingual vision-language models promise universal image-text retrieval,\nyet their social biases remain under-explored. We present the first systematic\naudit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and\nCAPIVARA-CLIP -- across ten languages that vary in resource availability and\ngrammatical gender. Using balanced subsets of \\textsc{FairFace} and the\n\\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and\ngender bias and measure stereotype amplification. Contrary to the assumption\nthat multilinguality mitigates bias, every model exhibits stronger gender bias\nthan its English-only baseline. CAPIVARA-CLIP shows its largest biases\nprecisely in the low-resource languages it targets, while the shared\ncross-lingual encoder of NLLB-CLIP transports English gender stereotypes into\ngender-neutral languages; loosely coupled encoders largely avoid this transfer.\nHighly gendered languages consistently magnify all measured bias types, but\neven gender-neutral languages remain vulnerable when cross-lingual weight\nsharing imports foreign stereotypes. Aggregated metrics conceal\nlanguage-specific ``hot spots,'' underscoring the need for fine-grained,\nlanguage-aware bias evaluation in future multilingual vision-language research.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:14:00", "updated": "2025-05-20 10:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14160v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14165v1", "title": "PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore", "authors": ["Zhenkai Qin", "Jiajing He", "Qiao Fang"], "abstract": "Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity\ntoward specific aspects within a text, enabling more precise opinion mining in\ndomains such as product reviews and social media. However, traditional FGSA\napproaches often require task-specific architectures and extensive annotated\ndata, limiting their generalization and scalability. To address these\nchallenges, we propose PL-FGSA, a unified prompt learning-based framework\nimplemented using the MindSpore platform, which integrates prompt design with a\nlightweight TextCNN backbone. Our method reformulates FGSA as a multi-task\nprompt-augmented generation problem, jointly tackling aspect extraction,\nsentiment classification, and causal explanation in a unified paradigm. By\nleveraging prompt-based guidance, PL-FGSA enhances interpretability and\nachieves strong performance under both full-data and low-resource conditions.\nExperiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and\nMAMS-demonstrate that our model consistently outperforms traditional\nfine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,\nrespectively. These results validate the effectiveness of prompt-based\ngeneralization and highlight the practical value of PL-FGSA for real-world\nsentiment analysis tasks.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:18:10", "updated": "2025-05-20 10:18:10", "pdf_url": "http://arxiv.org/pdf/2505.14165v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14172v1", "title": "The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models", "authors": ["Adrian Cosma", "Stefan Ruseti", "Emilian Radoi", "Mihai Dascalu"], "abstract": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge slowly, suddenly, and only late in training. We\nfurther show that percolation-based models of concept emergence explain these\npatterns, suggesting that learning character composition is not fundamentally\ndifferent from learning commonsense knowledge. To address this bottleneck, we\npropose a lightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available.", "categories": ["cs.CL"], "published": "2025-05-20 10:25:17", "updated": "2025-05-20 10:25:17", "pdf_url": "http://arxiv.org/pdf/2505.14172v1", "comment": "1 Table, 8 Figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14173v1", "title": "THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jie Zhou"], "abstract": "The sparse Mixture-of-Experts (MoE) has achieved significant progress for\nneural machine translation (NMT). However, there exist two limitations in\ncurrent MoE solutions which may lead to sub-optimal performance: 1) they\ndirectly use the task knowledge of NMT into MoE (\\emph{e.g.},\ndomain/linguistics-specific knowledge), which are generally unavailable at\npractical application and neglect the naturally grouped domain/linguistic\nproperties; 2) the expert selection only depends on the localized token\nrepresentation without considering the context, which fully grasps the state of\neach token in a global view. To address the above limitations, we propose\nTHOR-MoE via arming the MoE with hierarchical task-guided and\ncontext-responsive routing policies. Specifically, it 1) firstly predicts the\ndomain/language label and then extracts mixed domain/language representation to\nallocate task-level experts in a hierarchical manner; 2) injects the context\ninformation to enhance the token routing from the pre-selected task-level\nexperts set, which can help each token to be accurately routed to more\nspecialized and suitable experts. Extensive experiments on multi-domain\ntranslation and multilingual translation benchmarks with different\narchitectures consistently demonstrate the superior performance of THOR-MoE.\nAdditionally, the THOR-MoE operates as a plug-and-play module compatible with\nexisting Top-$k$~\\cite{shazeer2017} and Top-$p$~\\cite{huang-etal-2024-harder}\nrouting schemes, ensuring broad applicability across diverse MoE architectures.\nFor instance, compared with vanilla Top-$p$~\\cite{huang-etal-2024-harder}\nrouting, the context-aware manner can achieve an average improvement of 0.75\nBLEU with less than 22\\% activated parameters on multi-domain translation\ntasks.", "categories": ["cs.CL"], "published": "2025-05-20 10:27:19", "updated": "2025-05-20 10:27:19", "pdf_url": "http://arxiv.org/pdf/2505.14173v1", "comment": "Accepted to ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14174v1", "title": "Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning", "authors": ["Yusuf Denizay D\u00f6nder", "Derek Hommel", "Andrea W Wen-Yi", "David Mimno", "Unso Eun Seo Jo"], "abstract": "LLMs are effective at code generation tasks like text-to-SQL, but is it worth\nthe cost? Many state-of-the-art approaches use non-task-specific LLM techniques\nincluding Chain-of-Thought (CoT), self-consistency, and fine-tuning. These\nmethods can be costly at inference time, sometimes requiring over a hundred LLM\ncalls with reasoning, incurring average costs of up to \\$0.46 per query, while\nfine-tuning models can cost thousands of dollars. We introduce \"N-rep\"\nconsistency, a more cost-efficient text-to-SQL approach that achieves similar\nBIRD benchmark scores as other more expensive methods, at only \\$0.039 per\nquery. N-rep leverages multiple representations of the same schema input to\nmitigate weaknesses in any single representation, making the solution more\nrobust and allowing the use of smaller and cheaper models without any reasoning\nor fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL\napproach in its cost range.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 10:28:46", "updated": "2025-05-20 10:28:46", "pdf_url": "http://arxiv.org/pdf/2505.14174v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14178v1", "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "abstract": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 10:32:30", "updated": "2025-05-20 10:32:30", "pdf_url": "http://arxiv.org/pdf/2505.14178v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14179v1", "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "abstract": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-20 10:34:45", "updated": "2025-05-20 10:34:45", "pdf_url": "http://arxiv.org/pdf/2505.14179v1", "comment": null, "doi": "10.1016/j.eswa.2024.125529", "journal_ref": "Expert Systems with Applications, 2025"}
{"arxiv_id": "2505.14181v1", "title": "SlangDIT: Benchmarking LLMs in Interpretative Slang Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jiaan Wang", "Jie Zhou"], "abstract": "The challenge of slang translation lies in capturing context-dependent\nsemantic extensions, as slang terms often convey meanings beyond their literal\ninterpretation. While slang detection, explanation, and translation have been\nstudied as isolated tasks in the era of large language models (LLMs), their\nintrinsic interdependence remains underexplored. The main reason is lacking of\na benchmark where the two tasks can be a prerequisite for the third one, which\ncan facilitate idiomatic translation. In this paper, we introduce the\ninterpretative slang translation task (named SlangDIT) consisting of three\nsub-tasks: slang detection, cross-lingual slang explanation, and slang\ntranslation within the current context, aiming to generate more accurate\ntranslation with the help of slang detection and slang explanation. To this\nend, we construct a SlangDIT dataset, containing over 25k English-Chinese\nsentence pairs. Each source sentence mentions at least one slang term and is\nlabeled with corresponding cross-lingual slang explanation. Based on the\nbenchmark, we propose a deep thinking model, named SlangOWL. It firstly\nidentifies whether the sentence contains a slang, and then judges whether the\nslang is polysemous and analyze its possible meaning. Further, the SlangOWL\nprovides the best explanation of the slang term targeting on the current\ncontext. Finally, according to the whole thought, the SlangOWL offers a\nsuitable translation. Our experiments on LLMs (\\emph{e.g.}, Qwen2.5 and\nLLama-3.1), show that our deep thinking approach indeed enhances the\nperformance of LLMs where the proposed SLangOWL significantly surpasses the\nvanilla models and supervised fine-tuned models without thinking.", "categories": ["cs.CL"], "published": "2025-05-20 10:37:34", "updated": "2025-05-20 10:37:34", "pdf_url": "http://arxiv.org/pdf/2505.14181v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14183v1", "title": "ThinkSwitcher: When to Think Hard, When to Think Fast", "authors": ["Guosheng Liang", "Longguang Zhong", "Ziyi Yang", "Xiaojun Quan"], "abstract": "Large reasoning models (LRMs) excel at solving complex tasks by leveraging\nlong chain-of-thought (CoT) reasoning. However, this often leads to\noverthinking on simple tasks, resulting in unnecessary computational overhead.\nWe observe that LRMs inherently possess the capability for efficient short CoT\nreasoning, which can be reliably elicited through prompt design. To leverage\nthis capability, we propose ThinkSwitcher, a framework that enables a single\nLRM to dynamically switch between short and long CoT modes based on task\ncomplexity. ThinkSwitcher introduces a lightweight switching module trained\nwith supervision signals derived from the relative performance of each\nreasoning mode across tasks. Experiments on multiple reasoning benchmarks show\nthat ThinkSwitcher reduces computational cost by 20-30% while maintaining high\naccuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher\nas a scalable and efficient solution for unified LRM deployment.", "categories": ["cs.CL"], "published": "2025-05-20 10:40:41", "updated": "2025-05-20 10:40:41", "pdf_url": "http://arxiv.org/pdf/2505.14183v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14185v1", "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 10:41:49", "updated": "2025-05-20 10:41:49", "pdf_url": "http://arxiv.org/pdf/2505.14185v1", "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14195v1", "title": "Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification", "authors": ["Tuc Nguyen", "Yifan Hu", "Thai Le"], "abstract": "Recent advancements in large language models (LLMs) have been fueled by large\nscale training corpora drawn from diverse sources such as websites, news\narticles, and books. These datasets often contain explicit user information,\nsuch as person names and addresses, that LLMs may unintentionally reproduce in\ntheir generated outputs. Beyond such explicit content, LLMs can also leak\nidentity revealing cues through implicit signals such as distinctive writing\nstyles, raising significant concerns about authorship privacy. There are three\nmajor automated tasks in authorship privacy, namely authorship obfuscation\n(AO), authorship mimicking (AM), and authorship verification (AV). Prior\nresearch has studied AO, AM, and AV independently. However, their interplays\nremain under explored, which leaves a major research gap, especially in the era\nof LLMs, where they are profoundly shaping how we curate and share user\ngenerated content, and the distinction between machine generated and human\nauthored text is also increasingly blurred. This work then presents the first\nunified framework for analyzing the dynamic relationships among LLM enabled AO,\nAM, and AV in the context of authorship privacy. We quantify how they interact\nwith each other to transform human authored text, examining effects at a single\npoint in time and iteratively over time. We also examine the role of\ndemographic metadata, such as gender, academic background, in modulating their\nperformances, inter-task dynamics, and privacy risks. All source code will be\npublicly available.", "categories": ["cs.CL"], "published": "2025-05-20 10:52:12", "updated": "2025-05-20 10:52:12", "pdf_url": "http://arxiv.org/pdf/2505.14195v1", "comment": "17 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14212v1", "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "abstract": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:16:29", "updated": "2025-05-20 11:16:29", "pdf_url": "http://arxiv.org/pdf/2505.14212v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14216v1", "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "abstract": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 11:22:34", "updated": "2025-05-20 11:22:34", "pdf_url": "http://arxiv.org/pdf/2505.14216v1", "comment": "23 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14226v1", "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "abstract": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 11:35:25", "updated": "2025-05-20 11:35:25", "pdf_url": "http://arxiv.org/pdf/2505.14226v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14233v1", "title": "Mechanistic Fine-tuning for In-context Learning", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "abstract": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:41:21", "updated": "2025-05-20 11:41:21", "pdf_url": "http://arxiv.org/pdf/2505.14233v1", "comment": "28 pages, 31 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14238v1", "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "abstract": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 11:43:25", "updated": "2025-05-20 11:43:25", "pdf_url": "http://arxiv.org/pdf/2505.14238v1", "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14242v1", "title": "Technical Report on classification of literature related to children speech disorder", "authors": ["Ziang Wang", "Amir Aryani"], "abstract": "This technical report presents a natural language processing (NLP)-based\napproach for systematically classifying scientific literature on childhood\nspeech disorders. We retrieved and filtered 4,804 relevant articles published\nafter 2015 from the PubMed database using domain-specific keywords. After\ncleaning and pre-processing the abstracts, we applied two topic modeling\ntechniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify\nlatent thematic structures in the corpus. Our models uncovered 14 clinically\nmeaningful clusters, such as infantile hyperactivity and abnormal epileptic\nbehavior. To improve relevance and precision, we incorporated a custom stop\nword list tailored to speech pathology. Evaluation results showed that the LDA\nmodel achieved a coherence score of 0.42 and a perplexity of -7.5, indicating\nstrong topic coherence and predictive performance. The BERTopic model exhibited\na low proportion of outlier topics (less than 20%), demonstrating its capacity\nto classify heterogeneous literature effectively. These results provide a\nfoundation for automating literature reviews in speech-language pathology.", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-20 11:52:17", "updated": "2025-05-20 11:52:17", "pdf_url": "http://arxiv.org/pdf/2505.14242v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14244v1", "title": "TransBench: Benchmarking Machine Translation for Industrial-Scale Applications", "authors": ["Haijun Li", "Tianqi Shi", "Zifu Shang", "Yuxuan Han", "Xueyu Zhao", "Hao Wang", "Yu Qian", "Zhiqiang Qian", "Linlong Xu", "Minghao Wu", "Chenyang Lyu", "Longyue Wang", "Gongbo Tang", "Weihua Luo", "Zhao Xu", "Kaifu Zhang"], "abstract": "Machine translation (MT) has become indispensable for cross-border\ncommunication in globalized industries like e-commerce, finance, and legal\nservices, with recent advancements in large language models (LLMs)\nsignificantly enhancing translation quality. However, applying general-purpose\nMT models to industrial scenarios reveals critical limitations due to\ndomain-specific terminology, cultural nuances, and stylistic conventions absent\nin generic benchmarks. Existing evaluation frameworks inadequately assess\nperformance in specialized contexts, creating a gap between academic benchmarks\nand real-world efficacy. To address this, we propose a three-level translation\ncapability framework: (1) Basic Linguistic Competence, (2) Domain-Specific\nProficiency, and (3) Cultural Adaptation, emphasizing the need for holistic\nevaluation across these dimensions. We introduce TransBench, a benchmark\ntailored for industrial MT, initially targeting international e-commerce with\n17,000 professionally translated sentences spanning 4 main scenarios and 33\nlanguage pairs. TransBench integrates traditional metrics (BLEU, TER) with\nMarco-MOS, a domain-specific evaluation model, and provides guidelines for\nreproducible benchmark construction. Our contributions include: (1) a\nstructured framework for industrial MT evaluation, (2) the first publicly\navailable benchmark for e-commerce translation, (3) novel metrics probing\nmulti-level translation quality, and (4) open-sourced evaluation tools. This\nwork bridges the evaluation gap, enabling researchers and practitioners to\nsystematically assess and enhance MT systems for industry-specific needs.", "categories": ["cs.CL"], "published": "2025-05-20 11:54:58", "updated": "2025-05-20 11:54:58", "pdf_url": "http://arxiv.org/pdf/2505.14244v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14256v1", "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "abstract": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:09:17", "updated": "2025-05-20 12:09:17", "pdf_url": "http://arxiv.org/pdf/2505.14256v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14264v1", "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum", "authors": ["Jian Xiong", "Jingbo Zhou", "Jingyong Ye", "Dejing Dou"], "abstract": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 12:13:44", "updated": "2025-05-20 12:13:44", "pdf_url": "http://arxiv.org/pdf/2505.14264v1", "comment": "14 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14268v1", "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:19:10", "updated": "2025-05-20 12:19:10", "pdf_url": "http://arxiv.org/pdf/2505.14268v1", "comment": "16 pages, 14 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14271v1", "title": "FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning", "authors": ["Minh Ngoc Ta", "Dong Cao Van", "Duc-Anh Hoang", "Minh Le-Anh", "Truong Nguyen", "My Anh Tran Nguyen", "Yuxia Wang", "Preslav Nakov", "Sang Dinh"], "abstract": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nAI-generated, and human-AI collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, meanwhile identifying the underlying AI model family. Unlike\nexisting binary classifiers, FAID is built to capture both authorship and\nmodel-specific characteristics. Our method combines multi-level contrastive\nlearning with multi-task auxiliary classification to learn subtle stylistic\ncues. By modeling AI families as distinct stylistic entities, FAID offers\nimproved interpretability. We incorporate an adaptation to address\ndistributional shifts without retraining for unseen data. Experimental results\ndemonstrate that FAID outperforms several baseline approaches, particularly\nenhancing the generalization accuracy on unseen domains and new AI models. It\nprovide a potential solution for improving transparency and accountability in\nAI-assisted writing.", "categories": ["cs.CL"], "published": "2025-05-20 12:23:31", "updated": "2025-05-20 12:23:31", "pdf_url": "http://arxiv.org/pdf/2505.14271v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14272v1", "title": "Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "abstract": "Considering the importance of detecting hateful language, labeled hate speech\ndata is expensive and time-consuming to collect, particularly for low-resource\nlanguages. Prior work has demonstrated the effectiveness of cross-lingual\ntransfer learning and data augmentation in improving performance on tasks with\nlimited labeled data. To develop an efficient and scalable cross-lingual\ntransfer learning approach, we leverage nearest-neighbor retrieval to augment\nminimal labeled data in the target language, thereby enhancing detection\nperformance. Specifically, we assume access to a small set of labeled training\ninstances in the target language and use these to retrieve the most relevant\nlabeled examples from a large multilingual hate speech detection pool. We\nevaluate our approach on eight languages and demonstrate that it consistently\noutperforms models trained solely on the target language data. Furthermore, in\nmost cases, our method surpasses the current state-of-the-art. Notably, our\napproach is highly data-efficient, retrieving as small as 200 instances in some\ncases while maintaining superior performance. Moreover, it is scalable, as the\nretrieval pool can be easily expanded, and the method can be readily adapted to\nnew languages and tasks. We also apply maximum marginal relevance to mitigate\nredundancy and filter out highly similar retrieved instances, resulting in\nimprovements in some languages.", "categories": ["cs.CL", "cs.CY", "cs.MM"], "published": "2025-05-20 12:25:33", "updated": "2025-05-20 12:25:33", "pdf_url": "http://arxiv.org/pdf/2505.14272v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14279v1", "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin M\u00fcnch"], "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 12:30:46", "updated": "2025-05-20 12:30:46", "pdf_url": "http://arxiv.org/pdf/2505.14279v1", "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14286v1", "title": "Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs", "authors": ["Rao Ma", "Mengjie Qian", "Vyas Raina", "Mark Gales", "Kate Knill"], "abstract": "The combination of pre-trained speech encoders with large language models has\nenabled the development of speech LLMs that can handle a wide range of spoken\nlanguage processing tasks. While these models are powerful and flexible, this\nvery flexibility may make them more vulnerable to adversarial attacks. To\nexamine the extent of this problem, in this work we investigate universal\nacoustic adversarial attacks on speech LLMs. Here a fixed, universal,\nadversarial audio segment is prepended to the original input audio. We\ninitially investigate attacks that cause the model to either produce no output\nor to perform a modified task overriding the original prompt. We then extend\nthe nature of the attack to be selective so that it activates only when\nspecific input attributes, such as a speaker gender or spoken language, are\npresent. Inputs without the targeted attribute should be unaffected, allowing\nfine-grained control over the model outputs. Our findings reveal critical\nvulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar\nspeech LLMs may be susceptible to universal adversarial attacks. This\nhighlights the need for more robust training strategies and improved resistance\nto adversarial attacks.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-20 12:35:59", "updated": "2025-05-20 12:35:59", "pdf_url": "http://arxiv.org/pdf/2505.14286v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14297v1", "title": "Cross-Lingual Optimization for Language Transfer in Large Language Models", "authors": ["Jungseob Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "abstract": "Adapting large language models to other languages typically employs\nsupervised fine-tuning (SFT) as a standard approach. However, it often suffers\nfrom an overemphasis on English performance, a phenomenon that is especially\npronounced in data-constrained environments. To overcome these challenges, we\npropose \\textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an\nEnglish-centric LLM to a target language while preserving its English\ncapabilities. CLO utilizes publicly available English SFT data and a\ntranslation model to enable cross-lingual transfer. We conduct experiments\nusing five models on six languages, each possessing varying levels of resource.\nOur results show that CLO consistently outperforms SFT in both acquiring target\nlanguage proficiency and maintaining English performance. Remarkably, in\nlow-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400\nsamples, demonstrating that CLO can achieve better performance with less data.\nFurthermore, we find that SFT is particularly sensitive to data quantity in\nmedium and low-resource languages, whereas CLO remains robust. Our\ncomprehensive analysis emphasizes the limitations of SFT and incorporates\nadditional training strategies in CLO to enhance efficiency.", "categories": ["cs.CL"], "published": "2025-05-20 12:45:09", "updated": "2025-05-20 12:45:09", "pdf_url": "http://arxiv.org/pdf/2505.14297v1", "comment": "Accepted for publication at ACL 2025. Jungseob Lee and Seongtae Hong\n  contributed equally to this work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14300v1", "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors", "authors": ["Maheep Chaudhary", "Fazl Barez"], "abstract": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 12:49:58", "updated": "2025-05-20 12:49:58", "pdf_url": "http://arxiv.org/pdf/2505.14300v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14302v1", "title": "Scaling Law for Quantization-Aware Training", "authors": ["Mengzhao Chen", "Chaoyi Zhang", "Jing Liu", "Yutao Zeng", "Zeyue Xue", "Zhiheng Liu", "Yunshui Li", "Jin Ma", "Jie Huang", "Xun Zhou", "Ping Luo"], "abstract": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 12:54:43", "updated": "2025-05-20 12:54:43", "pdf_url": "http://arxiv.org/pdf/2505.14302v1", "comment": "A unified scaling law for QAT that models quantization error as a\n  function of model size, training data volume, and quantization group size", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14305v1", "title": "JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling", "authors": ["Jinwang Song", "Hongying Zan", "Kunli Zhang", "Lingling Mu", "Yingjie Han", "Haobo Hua", "Min Peng"], "abstract": "Text-to-SQL, which maps natural language to SQL queries, has benefited\ngreatly from recent advances in Large Language Models (LLMs). While LLMs offer\nvarious paradigms for this task, including prompting and supervised fine-tuning\n(SFT), SFT approaches still face challenges such as complex multi-stage\npipelines and poor robustness to noisy schema information. To address these\nlimitations, we present JOLT-SQL, a streamlined single-stage SFT framework that\njointly optimizes schema linking and SQL generation via a unified loss.\nJOLT-SQL employs discriminative schema linking, enhanced by local bidirectional\nattention, alongside a confusion-aware noisy schema sampling strategy with\nselective attention to improve robustness under noisy schema conditions.\nExperiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL\nachieves state-of-the-art execution accuracy among comparable-size open-source\nmodels, while significantly improving both training and inference efficiency.", "categories": ["cs.CL"], "published": "2025-05-20 12:55:10", "updated": "2025-05-20 12:55:10", "pdf_url": "http://arxiv.org/pdf/2505.14305v1", "comment": "Work in progress. 13 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14309v1", "title": "Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency", "authors": ["Ehsan Doostmohammadi", "Marco Kuhlmann"], "abstract": "Retrieval-augmented language models have demonstrated performance comparable\nto much larger models while requiring fewer computational resources. The\neffectiveness of these models crucially depends on the overlap between query\nand retrieved context, but the optimal degree of this overlap remains\nunexplored. In this paper, we systematically investigate how varying levels of\nquery--context overlap affect model performance during both training and\ninference. Our experiments reveal that increased overlap initially has minimal\neffect, but substantially improves test-time perplexity and accelerates model\nlearning above a critical threshold. Building on these findings, we demonstrate\nthat deliberately increasing overlap through synthetic context can enhance data\nefficiency and reduce training time by approximately 40\\% without compromising\nperformance. We specifically generate synthetic context through paraphrasing\nqueries. We validate our perplexity-based findings on question-answering tasks,\nconfirming that the benefits of retrieval-augmented language modeling extend to\npractical applications. Our results provide empirical evidence of significant\noptimization potential for retrieval mechanisms in language model pretraining.", "categories": ["cs.CL"], "published": "2025-05-20 12:58:07", "updated": "2025-05-20 12:58:07", "pdf_url": "http://arxiv.org/pdf/2505.14309v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14311v1", "title": "HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing", "authors": ["Shamsuddeen Hassan Muhammad", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Falalu Ibrahim Lawan", "Babangida Sani", "Sukairaj Hafiz Imam", "Yusuf Aliyu", "Sani Abdullahi Sani", "Ali Usman Umar", "Kenneth Church", "Vukosi Marivate"], "abstract": "Hausa Natural Language Processing (NLP) has gained increasing attention in\nrecent years, yet remains understudied as a low-resource language despite\nhaving over 120 million first-language (L1) and 80 million second-language (L2)\nspeakers worldwide. While significant advances have been made in high-resource\nlanguages, Hausa NLP faces persistent challenges, including limited open-source\ndatasets and inadequate model representation. This paper presents an overview\nof the current state of Hausa NLP, systematically examining existing resources,\nresearch contributions, and gaps across fundamental NLP tasks: text\nclassification, machine translation, named entity recognition, speech\nrecognition, and question answering. We introduce HausaNLP\n(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\ntools, and research works to enhance accessibility and drive further\ndevelopment. Furthermore, we discuss challenges in integrating Hausa into large\nlanguage models (LLMs), addressing issues of suboptimal tokenization and\ndialectal variation. Finally, we propose strategic research directions\nemphasizing dataset expansion, improved language modeling approaches, and\nstrengthened community collaboration to advance Hausa NLP. Our work provides\nboth a foundation for accelerating Hausa NLP progress and valuable insights for\nbroader multilingual NLP research.", "categories": ["cs.CL"], "published": "2025-05-20 12:59:55", "updated": "2025-05-20 12:59:55", "pdf_url": "http://arxiv.org/pdf/2505.14311v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14313v1", "title": "A MIND for Reasoning: Meta-learning for In-context Deduction", "authors": ["Leonardo Bertolazzi", "Manuel Vargas Guzm\u00e1n", "Raffaella Bernardi", "Maciej Malicki", "Jakub Szymanik"], "abstract": "Large language models (LLMs) are increasingly evaluated on formal tasks,\nwhere strong reasoning abilities define the state of the art. However, their\nability to generalize to out-of-distribution problems remains limited. In this\npaper, we investigate how LLMs can achieve a systematic understanding of\ndeductive rules. Our focus is on the task of identifying the appropriate subset\nof premises within a knowledge base needed to derive a given hypothesis. To\ntackle this challenge, we propose Meta-learning for In-context Deduction\n(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND\nis to enable models to generalize more effectively to unseen knowledge bases\nand to systematically apply inference rules. Our results show that MIND\nsignificantly improves generalization in small LMs ranging from 1.5B to 7B\nparameters. The benefits are especially pronounced in smaller models and\nlow-data settings. Remarkably, small models fine-tuned with MIND outperform\nstate-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.", "categories": ["cs.CL"], "published": "2025-05-20 13:00:48", "updated": "2025-05-20 13:00:48", "pdf_url": "http://arxiv.org/pdf/2505.14313v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14318v1", "title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection", "authors": ["Wenjun Hou", "Yi Cheng", "Kaishuai Xu", "Heng Li", "Yan Hu", "Wenjie Li", "Jiang Liu"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration and inefficient\nutilization of learned representations. To address this limitation, we propose\nRADAR, a framework for enhancing radiology report generation with supplementary\nknowledge injection. RADAR improves report generation by systematically\nleveraging both the internal knowledge of an LLM and externally retrieved\ninformation. Specifically, it first extracts the model's acquired knowledge\nthat aligns with expert image-based classification outputs. It then retrieves\nrelevant supplementary knowledge to further enrich this information. Finally,\nby aggregating both sources, RADAR generates more accurate and informative\nradiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU\nX-ray demonstrate that our model outperforms state-of-the-art LLMs in both\nlanguage quality and clinical accuracy", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-20 13:05:41", "updated": "2025-05-20 13:05:41", "pdf_url": "http://arxiv.org/pdf/2505.14318v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14347v1", "title": "QA-prompting: Improving Summarization with Large Language Models using Question-Answering", "authors": ["Neelabh Sinha"], "abstract": "Language Models (LMs) have revolutionized natural language processing,\nenabling high-quality text generation through prompting and in-context\nlearning. However, models often struggle with long-context summarization due to\npositional biases, leading to suboptimal extraction of critical information.\nThere are techniques to improve this with fine-tuning, pipelining, or using\ncomplex techniques, which have their own challenges. To solve these challenges,\nwe propose QA-prompting - a simple prompting method for summarization that\nutilizes question-answering as an intermediate step prior to summary\ngeneration. Our method extracts key information and enriches the context of\ntext to mitigate positional biases and improve summarization in a single LM\ncall per task without requiring fine-tuning or pipelining. Experiments on\nmultiple datasets belonging to different domains using ten state-of-the-art\npre-trained models demonstrate that QA-prompting outperforms baseline and other\nstate-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This\nprovides an effective and scalable solution for summarization and highlights\nthe importance of domain-specific question selection for optimal performance.", "categories": ["cs.CL"], "published": "2025-05-20 13:29:36", "updated": "2025-05-20 13:29:36", "pdf_url": "http://arxiv.org/pdf/2505.14347v1", "comment": "Submitted to ARR", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14350v1", "title": "OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "abstract": "Fine-tuning Large Language Models (LLMs) has become increasingly challenging\ndue to their massive scale and associated computational costs.\nParameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as\ncomputational alternatives; however, their implementations still require\nsignificant resources. In this paper, we present OSoRA (Output-Dimension and\nSingular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.\nOSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value\nDecomposition (SVD) with learnable scaling vectors in a unified framework. It\nfirst performs an SVD of pre-trained weight matrices, then optimizes an\noutput-dimension vector during training, while keeping the corresponding\nsingular vector matrices frozen. OSoRA substantially reduces computational\nresource requirements by minimizing the number of trainable parameters during\nfine-tuning. Comprehensive evaluations across mathematical reasoning, common\nsense reasoning, and other benchmarks demonstrate that OSoRA achieves\ncomparable or superior performance to state-of-the-art methods like LoRA and\nVeRA, while maintaining a linear parameter scaling even as the rank increases\nto higher dimensions. Our ablation studies further confirm that jointly\ntraining both the singular values and the output-dimension vector is critical\nfor optimal performance.", "categories": ["cs.CL"], "published": "2025-05-20 13:34:06", "updated": "2025-05-20 13:34:06", "pdf_url": "http://arxiv.org/pdf/2505.14350v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14351v1", "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-20 13:35:55", "updated": "2025-05-20 13:35:55", "pdf_url": "http://arxiv.org/pdf/2505.14351v1", "comment": "13 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14354v1", "title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications", "authors": ["Xin Li", "Mengbing Liu", "Li Wei", "Jiancheng An", "M\u00e9rouane Debbah", "Chau Yuen"], "abstract": "Large Language Models (LLMs) have achieved impressive results across a broad\narray of tasks, yet their capacity for complex, domain-specific mathematical\nreasoning-particularly in wireless communications-remains underexplored. In\nthis work, we introduce WirelessMathBench, a novel benchmark specifically\ndesigned to evaluate LLMs on mathematical modeling challenges to wireless\ncommunications engineering. Our benchmark consists of 587 meticulously curated\nquestions sourced from 40 state-of-the-art research papers, encompassing a\ndiverse spectrum of tasks ranging from basic multiple-choice questions to\ncomplex equation completion tasks, including both partial and full completions,\nall of which rigorously adhere to physical and dimensional constraints. Through\nextensive experimentation with leading LLMs, we observe that while many models\nexcel in basic recall tasks, their performance degrades significantly when\nreconstructing partially or fully obscured equations, exposing fundamental\nlimitations in current LLMs. Even DeepSeek-R1, the best performer on our\nbenchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%\nsuccess rate in full equation completion. By publicly releasing\nWirelessMathBench along with the evaluation toolkit, we aim to advance the\ndevelopment of more robust, domain-aware LLMs for wireless system analysis and\nbroader engineering applications.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 13:38:10", "updated": "2025-05-20 13:38:10", "pdf_url": "http://arxiv.org/pdf/2505.14354v1", "comment": "Accepted to ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14356v1", "title": "PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs", "authors": ["Sho Inoue", "Shai Wang", "Haizhou Li"], "abstract": "Despite significant progress in neural spoken dialog systems,\npersonality-aware conversation agents -- capable of adapting behavior based on\npersonalities -- remain underexplored due to the absence of personality\nannotations in speech datasets. We propose a pipeline that preprocesses raw\naudio recordings to create a dialogue dataset annotated with timestamps,\nresponse types, and emotion/sentiment labels. We employ an automatic speech\nrecognition (ASR) system to extract transcripts and timestamps, then generate\nconversation-level annotations. Leveraging these annotations, we design a\nsystem that employs large language models to predict conversational\npersonality. Human evaluators were engaged to identify conversational\ncharacteristics and assign personality labels. Our analysis demonstrates that\nthe proposed system achieves stronger alignment with human judgments compared\nto existing approaches.", "categories": ["cs.SD", "cs.CL", "eess.AS"], "published": "2025-05-20 13:41:32", "updated": "2025-05-20 13:41:32", "pdf_url": "http://arxiv.org/pdf/2505.14356v1", "comment": "This is accepted to Interspeech 2025; Added an extra page for\n  supplementary figures; Project page:\n  https://github.com/shinshoji01/Personality-Prediction-for-Conversation-Agents", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14367v1", "title": "Dual Decomposition of Weights and Singular Value Low Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for\nadapting Large Language Models (LLMs) to downstream tasks, among which Low-rank\nAdaptation (LoRA) represents one of the most widely adopted methodologies.\nHowever, existing LoRA-based approaches exhibit two fundamental limitations:\nunstable training dynamics and inefficient knowledge transfer from pre-trained\nmodels, both stemming from random initialization of adapter parameters. To\novercome these challenges, we propose DuDe, a novel approach that decomposes\nweight matrices into magnitude and direction components, employing Singular\nValue Decomposition (SVD) for principled initialization. Our comprehensive\nevaluation demonstrates DuDe's superior performance and robustness, achieving\nup to 48.35\\% accuracy on MMLU and 62.53\\% ($\\pm$ 1.59) accuracy on GSM8K. Our\ntheoretical analysis and empirical validation collectively demonstrate that\nDuDe's decomposition strategy enhances optimization stability and better\npreserves pre-trained representations, particularly for domain-specific tasks\nrequiring specialized knowledge. The combination of robust empirical\nperformance and rigorous theoretical foundations establishes DuDe as a\nsignificant contribution to PEFT methodologies for LLMs.", "categories": ["cs.CL"], "published": "2025-05-20 13:49:15", "updated": "2025-05-20 13:49:15", "pdf_url": "http://arxiv.org/pdf/2505.14367v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14368v1", "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs", "authors": ["Jiawen Wang", "Pritha Gupta", "Ivan Habernal", "Eyke H\u00fcllermeier"], "abstract": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-20 13:50:43", "updated": "2025-05-20 13:50:43", "pdf_url": "http://arxiv.org/pdf/2505.14368v1", "comment": "8 pages, 3 figures, EMNLP 2025 under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14376v1", "title": "AutoRev: Automatic Peer Review System for Academic Research Papers", "authors": ["Maitreya Prafulla Chitale", "Ketaki Mangesh Shetye", "Harshit Gupta", "Manav Chaudhary", "Vasudeva Varma"], "abstract": "Generating a review for an academic research paper is a complex task that\nrequires a deep understanding of the document's content and the\ninterdependencies between its sections. It demands not only insight into\ntechnical details but also an appreciation of the paper's overall coherence and\nstructure. Recent methods have predominantly focused on fine-tuning large\nlanguage models (LLMs) to address this challenge. However, they often overlook\nthe computational and performance limitations imposed by long input token\nlengths. To address this, we introduce AutoRev, an Automatic Peer Review System\nfor Academic Research Papers. Our novel framework represents an academic\ndocument as a graph, enabling the extraction of the most critical passages that\ncontribute significantly to the review. This graph-based approach demonstrates\neffectiveness for review generation and is potentially adaptable to various\ndownstream tasks, such as question answering, summarization, and document\nrepresentation. When applied to review generation, our method outperforms SOTA\nbaselines by an average of 58.72% across all evaluation metrics. We hope that\nour work will stimulate further research in applying graph-based extraction\ntechniques to other downstream tasks in NLP. We plan to make our code public\nupon acceptance.", "categories": ["cs.CL"], "published": "2025-05-20 13:59:58", "updated": "2025-05-20 13:59:58", "pdf_url": "http://arxiv.org/pdf/2505.14376v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14393v1", "title": "Editing Across Languages: A Survey of Multilingual Knowledge Editing", "authors": ["Nadir Durrani", "Basel Mousi", "Fahim Dalvi"], "abstract": "While Knowledge Editing has been extensively studied in monolingual settings,\nit remains underexplored in multilingual contexts. This survey systematizes\nrecent research on Multilingual Knowledge Editing (MKE), a growing subdomain of\nmodel editing focused on ensuring factual edits generalize reliably across\nlanguages. We present a comprehensive taxonomy of MKE methods, covering\nparameter-based, memory-based, fine-tuning, and hypernetwork approaches. We\nsurvey available benchmarks,summarize key findings on method effectiveness and\ntransfer patterns, identify challenges in cross-lingual propagation, and\nhighlight open problems related to language anisotropy, evaluation coverage,\nand edit scalability. Our analysis consolidates a rapidly evolving area and\nlays the groundwork for future progress in editable language-aware LLMs.", "categories": ["cs.CL"], "published": "2025-05-20 14:13:04", "updated": "2025-05-20 14:13:04", "pdf_url": "http://arxiv.org/pdf/2505.14393v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14395v1", "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "abstract": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:14:00", "updated": "2025-05-20 14:14:00", "pdf_url": "http://arxiv.org/pdf/2505.14395v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "authors": ["Ga\u00ebl Gendron", "Jo\u017ee M. Ro\u017eanec", "Michael Witbrock", "Gillian Dobbie"], "abstract": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"], "published": "2025-05-20 14:14:05", "updated": "2025-05-20 14:14:05", "pdf_url": "http://arxiv.org/pdf/2505.14396v1", "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14398v1", "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "abstract": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 14:14:38", "updated": "2025-05-20 14:14:38", "pdf_url": "http://arxiv.org/pdf/2505.14398v1", "comment": "Data and code are available at https://peterbaile.github.io/lag/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14402v1", "title": "OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking", "authors": ["Heng Yang", "Jack Cole", "Yuan Li", "Renzhi Chen", "Geyong Min", "Ke Li"], "abstract": "The code of nature, embedded in DNA and RNA genomes since the origin of life,\nholds immense potential to impact both humans and ecosystems through genome\nmodeling. Genomic Foundation Models (GFMs) have emerged as a transformative\napproach to decoding the genome. As GFMs scale up and reshape the landscape of\nAI-driven genomics, the field faces an urgent need for rigorous and\nreproducible evaluation. We present OmniGenBench, a modular benchmarking\nplatform designed to unify the data, model, benchmarking, and interpretability\nlayers across GFMs. OmniGenBench enables standardized, one-command evaluation\nof any GFM across five benchmark suites, with seamless integration of over 31\nopen-source models. Through automated pipelines and community-extensible\nfeatures, the platform addresses critical reproducibility challenges, including\ndata transparency, model interoperability, benchmark fragmentation, and\nblack-box interpretability. OmniGenBench aims to serve as foundational\ninfrastructure for reproducible genomic AI research, accelerating trustworthy\ndiscovery and collaborative innovation in the era of genome-scale modeling.", "categories": ["q-bio.GN", "cs.CL"], "published": "2025-05-20 14:16:25", "updated": "2025-05-20 14:16:25", "pdf_url": "http://arxiv.org/pdf/2505.14402v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14406v1", "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis", "authors": ["Haoming Huang", "Yibo Yan", "Jiahao Huo", "Xin Zou", "Xinfeng Li", "Kun Wang", "Xuming Hu"], "abstract": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the internal workings of attention heads,\ntracing how competing knowledge pathways contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation.", "categories": ["cs.CL"], "published": "2025-05-20 14:20:30", "updated": "2025-05-20 14:20:30", "pdf_url": "http://arxiv.org/pdf/2505.14406v1", "comment": "18 pages, 6 figures, EMNLP under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14410v1", "title": "Pairwise Evaluation of Accent Similarity in Speech Synthesis", "authors": ["Jinzuomu Zhong", "Suyuan Liu", "Dan Wells", "Korin Richmond"], "abstract": "Despite growing interest in generating high-fidelity accents, evaluating\naccent similarity in speech synthesis has been underexplored. We aim to enhance\nboth subjective and objective evaluation methods for accent similarity.\nSubjectively, we refine the XAB listening test by adding components that\nachieve higher statistical significance with fewer listeners and lower costs.\nOur method involves providing listeners with transcriptions, having them\nhighlight perceived accent differences, and implementing meticulous screening\nfor reliability. Objectively, we utilise pronunciation-related metrics, based\non distances between vowel formants and phonetic posteriorgrams, to evaluate\naccent generation. Comparative experiments reveal that these metrics, alongside\naccent similarity, speaker similarity, and Mel Cepstral Distortion, can be\nused. Moreover, our findings underscore significant limitations of common\nmetrics like Word Error Rate in assessing underrepresented accents.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 14:23:50", "updated": "2025-05-20 14:23:50", "pdf_url": "http://arxiv.org/pdf/2505.14410v1", "comment": "Accepted by INTERSPEECH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14412v1", "title": "PRL: Prompts from Reinforcement Learning", "authors": ["Pawe\u0142 Batorski", "Adrian Kosmala", "Paul Swoboda"], "abstract": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 14:26:19", "updated": "2025-05-20 14:26:19", "pdf_url": "http://arxiv.org/pdf/2505.14412v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14418v1", "title": "Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents", "authors": ["Pengzhou Cheng", "Haowen Hu", "Zheng Wu", "Zongru Wu", "Tianjie Ju", "Daizong Ding", "Zhuosheng Zhang", "Gongshen Liu"], "abstract": "Graphical user interface (GUI) agents powered by multimodal large language\nmodels (MLLMs) have shown greater promise for human-interaction. However, due\nto the high fine-tuning cost, users often rely on open-source GUI agents or\nAPIs offered by AI providers, which introduces a critical but underexplored\nsupply chain threat: backdoor attacks. In this work, we first unveil that\nMLLM-powered GUI agents naturally expose multiple interaction-level triggers,\nsuch as historical steps, environment states, and task progress. Based on this\nobservation, we introduce AgentGhost, an effective and stealthy framework for\nred-teaming backdoor attacks. Specifically, we first construct composite\ntriggers by combining goal and interaction levels, allowing GUI agents to\nunintentionally activate backdoors while ensuring task utility. Then, we\nformulate backdoor injection as a Min-Max optimization problem that uses\nsupervised contrastive learning to maximize the feature difference across\nsample classes at the representation space, improving flexibility of the\nbackdoor. Meanwhile, it adopts supervised fine-tuning to minimize the\ndiscrepancy between backdoor and clean behavior generation, enhancing\neffectiveness and utility. Extensive evaluations of various agent models in two\nestablished mobile benchmarks show that AgentGhost is effective and generic,\nwith attack accuracy that reaches 99.7\\% on three attack objectives, and shows\nstealthiness with only 1\\% utility degradation. Furthermore, we tailor a\ndefense method against AgentGhost that reduces the attack accuracy to 22.1\\%.\nOur code is available at \\texttt{anonymous}.", "categories": ["cs.CL"], "published": "2025-05-20 14:29:18", "updated": "2025-05-20 14:29:18", "pdf_url": "http://arxiv.org/pdf/2505.14418v1", "comment": "25 pages, 10 figures, 12 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14420v1", "title": "SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection", "authors": ["Huopu Zhang", "Yanguang Liu", "Mengnan Du"], "abstract": "Predicting earnings surprises through the analysis of earnings conference\ncall transcripts has attracted increasing attention from the financial research\ncommunity. Conference calls serve as critical communication channels between\ncompany executives, analysts, and shareholders, offering valuable\nforward-looking information. However, these transcripts present significant\nanalytical challenges, typically containing over 5,000 words with substantial\nredundancy and industry-specific terminology that creates obstacles for\nlanguage models. In this work, we propose the Sparse Autoencoder for Financial\nRepresentation Enhancement (SAE-FiRE) framework to address these limitations by\nextracting key information while eliminating redundancy. SAE-FiRE employs\nSparse Autoencoders (SAEs) to efficiently identify patterns and filter out\nnoises, and focusing specifically on capturing nuanced financial signals that\nhave predictive power for earnings surprises. Experimental results indicate\nthat the proposed method can significantly outperform comparing baselines.", "categories": ["q-fin.CP", "cs.CL", "cs.LG"], "published": "2025-05-20 14:31:23", "updated": "2025-05-20 14:31:23", "pdf_url": "http://arxiv.org/pdf/2505.14420v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14423v1", "title": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs", "authors": ["Ona de Gibert", "Joseph Attieh", "Teemu Vahtola", "Mikko Aulamo", "Zihao Li", "Ra\u00fal V\u00e1zquez", "Tiancheng Hu", "J\u00f6rg Tiedemann"], "abstract": "We investigate the potential of LLM-generated synthetic data for improving\nlow-resource machine translation (MT). Focusing on seven diverse target\nlanguages, we construct a document-level synthetic corpus from English\nEuroparl, and extend it via pivoting to 147 additional language pairs.\nAutomatic and human evaluation confirm its high overall quality. We study its\npractical application by (i) identifying effective training regimes, (ii)\ncomparing our data with the HPLT dataset, and (iii) testing its utility beyond\nEnglish-centric MT. Finally, we introduce SynOPUS, a public repository for\nsynthetic parallel datasets. Our findings show that LLM-generated synthetic\ndata, even when noisy, can substantially improve MT performance for\nlow-resource languages.", "categories": ["cs.CL"], "published": "2025-05-20 14:31:54", "updated": "2025-05-20 14:31:54", "pdf_url": "http://arxiv.org/pdf/2505.14423v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14425v1", "title": "From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "abstract": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization.", "categories": ["cs.CL"], "published": "2025-05-20 14:33:29", "updated": "2025-05-20 14:33:29", "pdf_url": "http://arxiv.org/pdf/2505.14425v1", "comment": "4 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14432v1", "title": "Rank-K: Test-Time Reasoning for Listwise Reranking", "authors": ["Eugene Yang", "Andrew Yates", "Kathryn Ricci", "Orion Weller", "Vivek Chari", "Benjamin Van Durme", "Dawn Lawrie"], "abstract": "Retrieve-and-rerank is a popular retrieval pipeline because of its ability to\nmake slow but effective rerankers efficient enough at query time by reducing\nthe number of comparisons. Recent works in neural rerankers take advantage of\nlarge language models for their capability in reasoning between queries and\npassages and have achieved state-of-the-art retrieval effectiveness. However,\nsuch rerankers are resource-intensive, even after heavy optimization. In this\nwork, we introduce Rank-K, a listwise passage reranking model that leverages\nthe reasoning capability of the reasoning language model at query time that\nprovides test time scalability to serve hard queries. We show that Rank-K\nimproves retrieval effectiveness by 23\\% over the RankZephyr, the\nstate-of-the-art listwise reranker, when reranking a BM25 initial ranked list\nand 19\\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is\ninherently a multilingual model, we found that it ranks passages based on\nqueries in different languages as effectively as it does in monolingual\nretrieval.", "categories": ["cs.IR", "cs.CL"], "published": "2025-05-20 14:39:34", "updated": "2025-05-20 14:39:34", "pdf_url": "http://arxiv.org/pdf/2505.14432v1", "comment": "15 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14436v1", "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:42:03", "updated": "2025-05-20 14:42:03", "pdf_url": "http://arxiv.org/pdf/2505.14436v1", "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14438v1", "title": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models", "authors": ["Yuanbo Fang", "Haoze Sun", "Jun Liu", "Tao Zhang", "Zenan Zhou", "Weipeng Chen", "Xiaofen Xing", "Xiangmin Xu"], "abstract": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench.", "categories": ["cs.SD", "cs.CL", "eess.AS"], "published": "2025-05-20 14:42:20", "updated": "2025-05-20 14:42:20", "pdf_url": "http://arxiv.org/pdf/2505.14438v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14442v1", "title": "Creative Preference Optimization", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:43:41", "updated": "2025-05-20 14:43:41", "pdf_url": "http://arxiv.org/pdf/2505.14442v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14449v1", "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "abstract": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 14:50:44", "updated": "2025-05-20 14:50:44", "pdf_url": "http://arxiv.org/pdf/2505.14449v1", "comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "authors": ["Chihan Huang", "Hao Tang"], "abstract": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 14:52:41", "updated": "2025-05-20 14:52:41", "pdf_url": "http://arxiv.org/pdf/2505.14455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14462v1", "title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding", "authors": ["Jiaang Li", "Yifei Yuan", "Wenyan Li", "Mohammad Aliannejadi", "Daniel Hershcovich", "Anders S\u00f8gaard", "Ivan Vuli\u0107", "Wenxuan Zhang", "Paul Pu Liang", "Yang Deng", "Serge Belongie"], "abstract": "As vision-language models (VLMs) become increasingly integrated into daily\nlife, the need for accurate visual culture understanding is becoming critical.\nYet, these models frequently fall short in interpreting cultural nuances\neffectively. Prior work has demonstrated the effectiveness of\nretrieval-augmented generation (RAG) in enhancing cultural understanding in\ntext-only settings, while its application in multimodal scenarios remains\nunderexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented\nVisual culturE uNdErstAnding), a new benchmark designed to advance visual\nculture understanding through retrieval, focusing on two tasks: culture-focused\nvisual question answering (cVQA) and culture-informed image captioning (cIC).\nRAVENEA extends existing datasets by integrating over 10,000 Wikipedia\ndocuments curated and ranked by human annotators. With RAVENEA, we train and\nevaluate seven multimodal retrievers for each image query, and measure the\ndownstream impact of retrieval-augmented inputs across fourteen\nstate-of-the-art VLMs. Our results show that lightweight VLMs, when augmented\nwith culture-aware retrieval, outperform their non-augmented counterparts (by\nat least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the\nvalue of retrieval-augmented methods and culturally inclusive benchmarks for\nmultimodal understanding.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-20 14:57:16", "updated": "2025-05-20 14:57:16", "pdf_url": "http://arxiv.org/pdf/2505.14462v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14464v1", "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters", "authors": ["Xiaoyu Tian", "Yunjie Ji", "Haotian Wang", "Shuaiting Chen", "Sitong Zhao", "Yiping Peng", "Han Zhao", "Xiangang Li"], "abstract": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging Face\\footnote{Datasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},\n\\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.", "categories": ["cs.CL"], "published": "2025-05-20 15:00:51", "updated": "2025-05-20 15:00:51", "pdf_url": "http://arxiv.org/pdf/2505.14464v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14467v1", "title": "Void in Language Models", "authors": ["Mani Shemiranifar"], "abstract": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks.", "categories": ["cs.CL"], "published": "2025-05-20 15:01:56", "updated": "2025-05-20 15:01:56", "pdf_url": "http://arxiv.org/pdf/2505.14467v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14469v1", "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "abstract": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:05:03", "updated": "2025-05-20 15:05:03", "pdf_url": "http://arxiv.org/pdf/2505.14469v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14470v1", "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "authors": ["Nadav Har-Tuv", "Or Tal", "Yossi Adi"], "abstract": "We present PAST, a novel end-to-end framework that jointly models phonetic\ninformation alongside signal reconstruction, eliminating the need for external\npretrained models. Unlike previous approaches that rely on pretrained\nself-supervised models, PAST employs supervised phonetic data, directly\nintegrating domain knowledge into the tokenization process via auxiliary tasks.\nAdditionally, we introduce a streamable, causal variant of PAST, enabling\nreal-time speech applications. Results demonstrate that PAST surpasses existing\nevaluated baseline tokenizers across common evaluation metrics, including\nphonetic representation and speech reconstruction. Notably, PAST also achieves\nsuperior performance when serving as a speech representation for speech\nlanguage models, further highlighting its effectiveness as a foundation for\nspoken language generation. To foster further research, we release the full\nimplementation. For code, model checkpoints, and samples see:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/PAST", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "published": "2025-05-20 15:05:14", "updated": "2025-05-20 15:05:14", "pdf_url": "http://arxiv.org/pdf/2505.14470v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14471v1", "title": "Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning", "authors": ["Tong Li", "Jiachuan Wang", "Yongqi Zhang", "Shuangyin Li", "Lei Chen"], "abstract": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss", "categories": ["cs.CL"], "published": "2025-05-20 15:05:27", "updated": "2025-05-20 15:05:27", "pdf_url": "http://arxiv.org/pdf/2505.14471v1", "comment": "Manuscripts, accepted to KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14479v1", "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "abstract": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:13:32", "updated": "2025-05-20 15:13:32", "pdf_url": "http://arxiv.org/pdf/2505.14479v1", "comment": "long paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14481v1", "title": "PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models", "authors": ["He Zhu", "Junyou Su", "Minxi Chen", "Wen Wang", "Yijie Deng", "Guanhua Chen", "Wenjia Zhang"], "abstract": "In the field of urban planning, existing Vision-Language Models (VLMs)\nfrequently fail to effectively analyze and evaluate planning maps, despite the\ncritical importance of these visual elements for urban planners and related\neducational contexts. Planning maps, which visualize land use, infrastructure\nlayouts, and functional zoning, require specialized understanding of spatial\nconfigurations, regulatory requirements, and multi-scale analysis. To address\nthis challenge, we introduce PlanGPT-VL, the first domain-specific\nVision-Language Model tailored specifically for urban planning maps. PlanGPT-VL\nemploys three innovative approaches: (1) PlanAnno-V framework for high-quality\nVQA data synthesis, (2) Critical Point Thinking to reduce hallucinations\nthrough structured verification, and (3) comprehensive training methodology\ncombining Supervised Fine-Tuning with frozen vision encoder parameters. Through\nsystematic evaluation on our proposed PlanBench-V benchmark, we demonstrate\nthat PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs\nin specialized planning map interpretation tasks, offering urban planning\nprofessionals a reliable tool for map analysis, assessment, and educational\napplications while maintaining high factual accuracy. Our lightweight 7B\nparameter model achieves comparable performance to models exceeding 72B\nparameters, demonstrating efficient domain specialization without sacrificing\nperformance.", "categories": ["cs.CL"], "published": "2025-05-20 15:14:47", "updated": "2025-05-20 15:14:47", "pdf_url": "http://arxiv.org/pdf/2505.14481v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14483v1", "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance", "authors": ["Agam Goyal", "Xianyang Zhan", "Yilun Chen", "Koustuv Saha", "Eshwar Chandrasekharan"], "abstract": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities.", "categories": ["cs.CL"], "published": "2025-05-20 15:16:06", "updated": "2025-05-20 15:16:06", "pdf_url": "http://arxiv.org/pdf/2505.14483v1", "comment": "Preprint: 15 pages, 4 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14489v1", "title": "Reasoning Models Better Express Their Confidence", "authors": ["Dongkeun Yoon", "Seungone Kim", "Sohee Yang", "Sunkyoung Kim", "Soyeon Kim", "Yongil Kim", "Eunbi Choi", "Yireun Kim", "Minjoon Seo"], "abstract": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 15:19:00", "updated": "2025-05-20 15:19:00", "pdf_url": "http://arxiv.org/pdf/2505.14489v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14499v1", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "abstract": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:28:26", "updated": "2025-05-20 15:28:26", "pdf_url": "http://arxiv.org/pdf/2505.14499v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14505v1", "title": "ModRWKV: Transformer Multimodality in Linear Time", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "abstract": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 15:34:36", "updated": "2025-05-20 15:34:36", "pdf_url": "http://arxiv.org/pdf/2505.14505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14518v1", "title": "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "abstract": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.", "categories": ["eess.AS", "cs.CL", "cs.SD"], "published": "2025-05-20 15:44:01", "updated": "2025-05-20 15:44:01", "pdf_url": "http://arxiv.org/pdf/2505.14518v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14523v1", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "abstract": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-20 15:46:44", "updated": "2025-05-20 15:46:44", "pdf_url": "http://arxiv.org/pdf/2505.14523v1", "comment": "To be published in ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14530v1", "title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs", "authors": ["Zhipeng Yang", "Junzhuo Li", "Siyu Xia", "Xuming Hu"], "abstract": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-20 15:49:15", "updated": "2025-05-20 15:49:15", "pdf_url": "http://arxiv.org/pdf/2505.14530v1", "comment": "27 pages, 17 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14536v1", "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders", "authors": ["Agam Goyal", "Vedant Rathi", "William Yeh", "Yian Wang", "Yuen Chen", "Hari Sundaram"], "abstract": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment.", "categories": ["cs.CL"], "published": "2025-05-20 15:55:31", "updated": "2025-05-20 15:55:31", "pdf_url": "http://arxiv.org/pdf/2505.14536v1", "comment": "Preprint: 19 pages, 7 figures, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14552v1", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "abstract": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:06:32", "updated": "2025-05-20 16:06:32", "pdf_url": "http://arxiv.org/pdf/2505.14552v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14553v1", "title": "Pivot Language for Low-Resource Machine Translation", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "abstract": "Certain pairs of languages suffer from lack of a parallel corpus which is\nlarge in size and diverse in domain. One of the ways this is overcome is via\nuse of a pivot language. In this paper we use Hindi as a pivot language to\ntranslate Nepali into English. We describe what makes Hindi a good candidate\nfor the pivot. We discuss ways in which a pivot language can be used, and use\ntwo such approaches - the Transfer Method (fully supervised) and\nBacktranslation (semi-supervised) - to translate Nepali into English. Using the\nformer, we are able to achieve a devtest Set SacreBLEU score of 14.2, which\nimproves the baseline fully supervised score reported by (Guzman et al., 2019)\nby 6.6 points. While we are slightly below the semi-supervised baseline score\nof 15.1, we discuss what may have caused this under-performance, and suggest\nscope for future work.", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "published": "2025-05-20 16:10:10", "updated": "2025-05-20 16:10:10", "pdf_url": "http://arxiv.org/pdf/2505.14553v1", "comment": "7 pages, 3 figures, paper dated May 13, 2019", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "abstract": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-20 16:28:08", "updated": "2025-05-20 16:28:08", "pdf_url": "http://arxiv.org/pdf/2505.14569v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14577v1", "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring", "authors": ["Sohaila Eltanbouly", "Salam Albatarni", "Tamer Elsayed"], "abstract": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant.", "categories": ["cs.CL"], "published": "2025-05-20 16:34:37", "updated": "2025-05-20 16:34:37", "pdf_url": "http://arxiv.org/pdf/2505.14577v1", "comment": "Accepted at ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14582v1", "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning", "authors": ["Shangziqi Zhao", "Jiahao Yuan", "Guisong Yang", "Usman Naseem"], "abstract": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity.", "categories": ["cs.CL"], "published": "2025-05-20 16:38:32", "updated": "2025-05-20 16:38:32", "pdf_url": "http://arxiv.org/pdf/2505.14582v1", "comment": "17 pages,4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14585v1", "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "abstract": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "categories": ["cs.CL"], "published": "2025-05-20 16:40:09", "updated": "2025-05-20 16:40:09", "pdf_url": "http://arxiv.org/pdf/2505.14585v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14590v1", "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "abstract": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.", "categories": ["cs.CL"], "published": "2025-05-20 16:41:45", "updated": "2025-05-20 16:41:45", "pdf_url": "http://arxiv.org/pdf/2505.14590v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14597v1", "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals", "authors": ["Xianzhen Luo", "Qingfu Zhu", "Zhiming Zhang", "Mingzheng Xu", "Tianhao Cheng", "Yixuan Wang", "Zheng Chu", "Shijie Xuyang", "Zhiyuan Ma", "YuanTao Fan", "Wanxiang Che"], "abstract": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance.", "categories": ["cs.CL"], "published": "2025-05-20 16:48:57", "updated": "2025-05-20 16:48:57", "pdf_url": "http://arxiv.org/pdf/2505.14597v1", "comment": "Code & Model is https://github.com/Luowaterbi/CTF-Instruct", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14599v1", "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "abstract": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 16:49:40", "updated": "2025-05-20 16:49:40", "pdf_url": "http://arxiv.org/pdf/2505.14599v1", "comment": "Accepted to IJCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14607v1", "title": "sudoLLM : On Multi-role Alignment of Language Models", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Joy Mahapatra", "Utpal Garain"], "abstract": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.", "categories": ["cs.CL", "cs.CR", "I.2.7"], "published": "2025-05-20 16:54:34", "updated": "2025-05-20 16:54:34", "pdf_url": "http://arxiv.org/pdf/2505.14607v1", "comment": "Under review. Code and data to be released later", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14608v1", "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "abstract": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-20 16:55:44", "updated": "2025-05-20 16:55:44", "pdf_url": "http://arxiv.org/pdf/2505.14608v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14615v1", "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", "authors": ["Anjiang Wei", "Yuheng Wu", "Yingjia Wan", "Tarun Suresh", "Huanmi Tan", "Zhanke Zhou", "Sanmi Koyejo", "Ke Wang", "Alex Aiken"], "abstract": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "published": "2025-05-20 17:00:22", "updated": "2025-05-20 17:00:22", "pdf_url": "http://arxiv.org/pdf/2505.14615v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14617v1", "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models", "authors": ["Sahar Abdelnabi", "Ahmed Salem"], "abstract": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation.", "categories": ["cs.CL", "cs.CY"], "published": "2025-05-20 17:03:12", "updated": "2025-05-20 17:03:12", "pdf_url": "http://arxiv.org/pdf/2505.14617v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14620v1", "title": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs", "authors": ["Morgan Lindsay Heisler", "Linzi Xing", "Ge Shi", "Hanieh Sadri", "Gursimran Singh", "Weiwei Zhang", "Tao Ye", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "abstract": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-20 17:11:18", "updated": "2025-05-20 17:11:18", "pdf_url": "http://arxiv.org/pdf/2505.14620v1", "comment": "Accepted at ACM KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14625v1", "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-20 17:16:44", "updated": "2025-05-20 17:16:44", "pdf_url": "http://arxiv.org/pdf/2505.14625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14627v1", "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "authors": ["Ashutosh Adhikari", "Mirella Lapata"], "abstract": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:18:17", "updated": "2025-05-20 17:18:17", "pdf_url": "http://arxiv.org/pdf/2505.14627v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14629v1", "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "abstract": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-20 17:19:57", "updated": "2025-05-20 17:19:57", "pdf_url": "http://arxiv.org/pdf/2505.14629v1", "comment": "Accepted at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14631v1", "title": "Think Only When You Need with Large Hybrid-Reasoning Models", "authors": ["Lingjie Jiang", "Xun Wu", "Shaohan Huang", "Qingxiu Dong", "Zewen Chi", "Li Dong", "Xingxing Zhang", "Tengchao Lv", "Lei Cui", "Furu Wei"], "abstract": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.", "categories": ["cs.CL"], "published": "2025-05-20 17:23:25", "updated": "2025-05-20 17:23:25", "pdf_url": "http://arxiv.org/pdf/2505.14631v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "published": "2025-05-20 17:24:09", "updated": "2025-05-20 17:24:09", "pdf_url": "http://arxiv.org/pdf/2505.14633v1", "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14638v1", "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference", "authors": ["Tomer Gafni", "Asaf Karnieli", "Yair Hanani"], "abstract": "Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model.", "categories": ["cs.CV", "cs.CL", "cs.LG"], "published": "2025-05-20 17:26:12", "updated": "2025-05-20 17:26:12", "pdf_url": "http://arxiv.org/pdf/2505.14638v1", "comment": "Accepted at eLVM Workshop, CVPR, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14652v1", "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "abstract": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.", "categories": ["cs.CL"], "published": "2025-05-20 17:41:33", "updated": "2025-05-20 17:41:33", "pdf_url": "http://arxiv.org/pdf/2505.14652v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "abstract": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-20 17:42:34", "updated": "2025-05-20 17:42:34", "pdf_url": "http://arxiv.org/pdf/2505.14654v1", "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14660v1", "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "authors": ["Ronald Seoh", "Dan Goldwasser"], "abstract": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "published": "2025-05-20 17:47:04", "updated": "2025-05-20 17:47:04", "pdf_url": "http://arxiv.org/pdf/2505.14660v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-20 17:54:54", "updated": "2025-05-20 17:54:54", "pdf_url": "http://arxiv.org/pdf/2505.14667v1", "comment": "22 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "categories": ["cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:55:25", "updated": "2025-05-20 17:55:25", "pdf_url": "http://arxiv.org/pdf/2505.14668v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14674v1", "title": "Reward Reasoning Model", "authors": ["Jiaxin Guo", "Zewen Chi", "Li Dong", "Qingxiu Dong", "Xun Wu", "Shaohan Huang", "Furu Wei"], "abstract": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.", "categories": ["cs.CL"], "published": "2025-05-20 17:58:03", "updated": "2025-05-20 17:58:03", "pdf_url": "http://arxiv.org/pdf/2505.14674v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14679v1", "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "abstract": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.", "categories": ["cs.CL"], "published": "2025-05-20 17:59:04", "updated": "2025-05-20 17:59:04", "pdf_url": "http://arxiv.org/pdf/2505.14679v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "abstract": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "published": "2025-05-20 17:59:13", "updated": "2025-05-20 17:59:13", "pdf_url": "http://arxiv.org/pdf/2505.14680v1", "comment": "SIGIR 2025 Perspective Paper", "doi": "10.1145/3726302.3730353", "journal_ref": null}
{"arxiv_id": "2505.14681v1", "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "published": "2025-05-20 17:59:16", "updated": "2025-05-20 17:59:16", "pdf_url": "http://arxiv.org/pdf/2505.14681v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14684v1", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "abstract": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-20 17:59:31", "updated": "2025-05-20 17:59:31", "pdf_url": "http://arxiv.org/pdf/2505.14684v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.14685v1", "title": "Language Models use Lookbacks to Track Beliefs", "authors": ["Nikhil Prakash", "Natalie Shapira", "Arnab Sen Sharma", "Christoph Riedl", "Yonatan Belinkov", "Tamar Rott Shaham", "David Bau", "Atticus Geiger"], "abstract": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs.", "categories": ["cs.CL"], "published": "2025-05-20 17:59:45", "updated": "2025-05-20 17:59:45", "pdf_url": "http://arxiv.org/pdf/2505.14685v1", "comment": "32 pages, 32 figures. Code and data at https://belief.baulab.info/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16086v1", "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "abstract": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 00:00:27", "updated": "2025-05-22 00:00:27", "pdf_url": "http://arxiv.org/pdf/2505.16086v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16088v1", "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "abstract": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday).", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 00:06:29", "updated": "2025-05-22 00:06:29", "pdf_url": "http://arxiv.org/pdf/2505.16088v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16090v1", "title": "Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance", "authors": ["Dominick Kubica", "Dylan T. Gordon", "Nanami Emura", "Derleen Saini", "Charlie Goldenberg"], "abstract": "As of 2025, Generative Artificial Intelligence (GenAI) has become a central\ntool for productivity across industries. Beyond text generation, GenAI now\nplays a critical role in coding, data analysis, and research workflows. As\nlarge language models (LLMs) continue to evolve, it is essential to assess the\nreliability and accuracy of their outputs, especially in specialized,\nhigh-stakes domains like finance. Most modern LLMs transform text into\nnumerical vectors, which are used in operations such as cosine similarity\nsearches to generate responses. However, this abstraction process can lead to\nmisinterpretation of emotional tone, particularly in nuanced financial\ncontexts. While LLMs generally excel at identifying sentiment in everyday\nlanguage, these models often struggle with the nuanced, strategically ambiguous\nlanguage found in earnings call transcripts. Financial disclosures frequently\nembed sentiment in hedged statements, forward-looking language, and\nindustry-specific jargon, making it difficult even for human analysts to\ninterpret consistently, let alone AI models. This paper presents findings from\nthe Santa Clara Microsoft Practicum Project, led by Professor Charlie\nGoldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's\nChatGPT, Google's Gemini, and traditional machine learning models for sentiment\nanalysis of financial text. Using Microsoft earnings call transcripts, the\nanalysis assesses how well LLM-derived sentiment correlates with market\nsentiment and stock movements and evaluates the accuracy of model outputs.\nPrompt engineering techniques are also examined to improve sentiment analysis\nresults. Visualizations of sentiment consistency are developed to evaluate\nalignment between tone and stock performance, with sentiment trends analyzed\nacross Microsoft's lines of business to determine which segments exert the\ngreatest influence.", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.7"], "published": "2025-05-22 00:09:11", "updated": "2025-05-22 00:09:11", "pdf_url": "http://arxiv.org/pdf/2505.16090v1", "comment": "6 pages, 4 figures. Research conducted as part of a\n  Microsoft-sponsored Capstone Project at Santa Clara University", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16097v1", "title": "TrialPanorama: Database and Benchmark for Systematic Review and Design of Clinical Trials", "authors": ["Zifeng Wang", "Qiao Jin", "Jiacheng Lin", "Junyi Gao", "Jathurshan Pradeepkumar", "Pengcheng Jiang", "Benjamin Danek", "Zhiyong Lu", "Jimeng Sun"], "abstract": "Developing artificial intelligence (AI) for vertical domains requires a solid\ndata foundation for both training and evaluation. In this work, we introduce\nTrialPanorama, a large-scale, structured database comprising 1,657,476 clinical\ntrial records aggregated from 15 global sources. The database captures key\naspects of trial design and execution, including trial setups, interventions,\nconditions, biomarkers, and outcomes, and links them to standard biomedical\nontologies such as DrugBank and MedDRA. This structured and ontology-grounded\ndesign enables TrialPanorama to serve as a unified, extensible resource for a\nwide range of clinical trial tasks, including trial planning, design, and\nsummarization. To demonstrate its utility, we derive a suite of benchmark tasks\ndirectly from the TrialPanorama database. The benchmark spans eight tasks\nacross two categories: three for systematic review (study search, study\nscreening, and evidence summarization) and five for trial design (arm design,\neligibility criteria, endpoint selection, sample size estimation, and trial\ncompletion assessment). The experiments using five state-of-the-art large\nlanguage models (LLMs) show that while general-purpose LLMs exhibit some\nzero-shot capability, their performance is still inadequate for high-stakes\nclinical trial workflows. We release TrialPanorama database and the benchmark\nto facilitate further research on AI for clinical trials.", "categories": ["cs.AI"], "published": "2025-05-22 00:58:43", "updated": "2025-05-22 00:58:43", "pdf_url": "http://arxiv.org/pdf/2505.16097v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16100v1", "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "abstract": "Validating scientific hypotheses is a central challenge in biomedical\nresearch, and remains difficult for artificial intelligence (AI) agents due to\nthe complexity of real-world data analysis and evidence interpretation. In this\nwork, we present BioDSA-1K, a benchmark designed to evaluate AI agents on\nrealistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K\nconsists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,\ncurated from over 300 published biomedical studies to reflect the structure and\nreasoning found in authentic research workflows. Each task includes a\nstructured hypothesis derived from the original study's conclusions, expressed\nin the affirmative to reflect the language of scientific reporting, and one or\nmore pieces of supporting evidence grounded in empirical data tables. While\nthese hypotheses mirror published claims, they remain testable using standard\nstatistical or machine learning methods. The benchmark enables evaluation along\nfour axes: (1) hypothesis decision accuracy, (2) alignment between evidence and\nconclusion, (3) correctness of the reasoning process, and (4) executability of\nthe AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable\nhypotheses: cases where the available data are insufficient to support or\nrefute a claim, reflecting a common yet underexplored scenario in real-world\nscience. We propose BioDSA-1K as a foundation for building and evaluating\ngeneralizable, trustworthy AI agents for biomedical discovery.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 01:02:21", "updated": "2025-05-22 01:02:21", "pdf_url": "http://arxiv.org/pdf/2505.16100v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16103v1", "title": "Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI", "authors": ["Monirul Islam Mahmud"], "abstract": "Keylogger detection involves monitoring for unusual system behaviors such as\ndelays between typing and character display, analyzing network traffic patterns\nfor data exfiltration. In this study, we provide a comprehensive analysis for\nkeylogger detection with traditional machine learning models - SVC, Random\nForest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes\nand advanced ensemble methods including Stacking, Blending and Voting.\nMoreover, feature selection approaches such as Information gain, Lasso L1 and\nFisher Score are thoroughly assessed to improve predictive performance and\nlower computational complexity. The Keylogger Detection dataset from publicly\navailable Kaggle website is used in this project. In addition to accuracy-based\nclassification, this study implements the approach for model interpretation\nusing Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to\ndeliver finer explanations for how much each feature contributes in assisting\nor hindering the detection process. To evaluate the models result, we have used\nAUC score, sensitivity, Specificity, Accuracy and F1 score. The best\nperformance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,\n100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is\nnear-perfect classification with Fisher Score.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 01:04:13", "updated": "2025-05-22 01:04:13", "pdf_url": "http://arxiv.org/pdf/2505.16103v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16114v1", "title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language", "authors": ["Naiqi Li", "Peiyuan Liu", "Zheng Liu", "Tao Dai", "Yong Jiang", "Shu-Tao Xia"], "abstract": "Solving puzzles in natural language poses a long-standing challenge in AI.\nWhile large language models (LLMs) have recently shown impressive capabilities\nin a variety of tasks, they continue to struggle with complex puzzles that\ndemand precise reasoning and exhaustive search. In this paper, we propose\nLogic-of-Thought (Logot), a novel framework that bridges LLMs with logic\nprogramming to address this problem. Our method leverages LLMs to translate\npuzzle rules and states into answer set programs (ASPs), the solution of which\nare then accurately and efficiently inferred by an ASP interpreter. This hybrid\napproach combines the natural language understanding of LLMs with the precise\nreasoning capabilities of logic programs. We evaluate our method on various\ngrid puzzles and dynamic puzzles involving actions, demonstrating near-perfect\naccuracy across all tasks. Our code and data are available at:\nhttps://github.com/naiqili/Logic-of-Thought.", "categories": ["cs.AI"], "published": "2025-05-22 01:37:40", "updated": "2025-05-22 01:37:40", "pdf_url": "http://arxiv.org/pdf/2505.16114v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16120v1", "title": "LLM-Powered AI Agent Systems and Their Applications in Industry", "authors": ["Guannan Liang", "Qianqian Tong"], "abstract": "The emergence of Large Language Models (LLMs) has reshaped agent systems.\nUnlike traditional rule-based agents with limited task scope, LLM-powered\nagents offer greater flexibility, cross-domain reasoning, and natural language\ninteraction. Moreover, with the integration of multi-modal LLMs, current agent\nsystems are highly capable of processing diverse data modalities, including\ntext, images, audio, and structured tabular data, enabling richer and more\nadaptive real-world behavior. This paper comprehensively examines the evolution\nof agent systems from the pre-LLM era to current LLM-powered architectures. We\ncategorize agent systems into software-based, physical, and adaptive hybrid\nsystems, highlighting applications across customer service, software\ndevelopment, manufacturing automation, personalized education, financial\ntrading, and healthcare. We further discuss the primary challenges posed by\nLLM-powered agents, including high inference latency, output uncertainty, lack\nof evaluation metrics, and security vulnerabilities, and propose potential\nsolutions to mitigate these concerns.", "categories": ["cs.AI"], "published": "2025-05-22 01:52:15", "updated": "2025-05-22 01:52:15", "pdf_url": "http://arxiv.org/pdf/2505.16120v1", "comment": "This is the author's accepted version of the paper accepted to appear\n  at IEEE AIIoT 2025. The final version will be available via IEEE Xplore.\n  \\c{opyright}2025 IEEE. Personal use of this material is permitted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16130v1", "title": "Scalable Graph Generative Modeling via Substructure Sequences", "authors": ["Zehong Wang", "Zheyuan Zhang", "Tianyi Ma", "Chuxu Zhang", "Yanfang Ye"], "abstract": "Graph neural networks (GNNs) has been predominantly driven by\nmessage-passing, where node representations are iteratively updated via local\nneighborhood aggregation. Despite their success, message-passing suffers from\nfundamental limitations -- including constrained expressiveness,\nover-smoothing, over-squashing, and limited capacity to model long-range\ndependencies. These issues hinder scalability: increasing data size or model\nsize often fails to yield improved performance, limiting the viability of GNNs\nas backbones for graph foundation models. In this work, we explore pathways\nbeyond message-passing and introduce Generative Graph Pattern Machine\n(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM\nrepresents graph instances (nodes, edges, or entire graphs) as sequences of\nsubstructures, and employs generative pre-training over the sequences to learn\ngeneralizable, transferable representations. Empirically, G$^2$PM demonstrates\nstrong scalability: on the ogbn-arxiv benchmark, it continues to improve with\nmodel sizes up to 60M parameters, outperforming prior generative approaches\nthat plateau at significantly smaller scales (e.g., 3M). In addition, we\nsystematically analyze the model design space, highlighting key architectural\nchoices that contribute to its scalability and generalization. Across diverse\ntasks -- including node classification, graph classification, and transfer\nlearning -- G$^2$PM consistently outperforms strong baselines, establishing a\ncompelling foundation for scalable graph learning. The code and dataset are\navailable at https://github.com/Zehong-Wang/G2PM.", "categories": ["cs.LG", "cs.AI", "cs.SI"], "published": "2025-05-22 02:16:34", "updated": "2025-05-22 02:16:34", "pdf_url": "http://arxiv.org/pdf/2505.16130v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16135v1", "title": "Sudoku-Bench: Evaluating creative reasoning with Sudoku variants", "authors": ["Jeffrey Seely", "Yuki Imajuku", "Tianyu Zhao", "Edoardo Cetin", "Llion Jones"], "abstract": "Existing reasoning benchmarks for large language models (LLMs) frequently\nfail to capture authentic creativity, often rewarding memorization of\npreviously observed patterns. We address this shortcoming with Sudoku-Bench, a\ncurated benchmark of challenging and unconventional Sudoku variants\nspecifically selected to evaluate creative, multi-step logical reasoning.\nSudoku variants form an unusually effective domain for reasoning research: each\npuzzle introduces unique or subtly interacting constraints, making memorization\ninfeasible and requiring solvers to identify novel logical breakthroughs\n(``break-ins''). Despite their diversity, Sudoku variants maintain a common and\ncompact structure, enabling clear and consistent evaluation. Sudoku-Bench\nincludes a carefully chosen puzzle set, a standardized text-based puzzle\nrepresentation, and flexible tools compatible with thousands of publicly\navailable puzzles -- making it easy to extend into a general research\nenvironment. Baseline experiments show that state-of-the-art LLMs solve fewer\nthan 15\\% of puzzles unaided, highlighting significant opportunities to advance\nlong-horizon, strategic reasoning capabilities.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 02:24:35", "updated": "2025-05-22 02:24:35", "pdf_url": "http://arxiv.org/pdf/2505.16135v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16136v1", "title": "Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study", "authors": ["Yuke Zhang"], "abstract": "This study introduces an interpretable machine learning (ML) framework to\nextract macroeconomic alpha from global news sentiment. We process the Global\nDatabase of Events, Language, and Tone (GDELT) Project's worldwide news feed\nusing FinBERT -- a Bidirectional Encoder Representations from Transformers\n(BERT) based model pretrained on finance-specific language -- to construct\ndaily sentiment indices incorporating mean tone, dispersion, and event impact.\nThese indices drive an XGBoost classifier, benchmarked against logistic\nregression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S.\nTreasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold\nexpanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates\nexceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios\nachieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective\ncompound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and\n22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment\ndispersion and article impact are key predictive features. Our findings\nestablish that integrating domain-specific Natural Language Processing (NLP)\nwith interpretable ML offers a potent and explainable source of macro alpha.", "categories": ["q-fin.CP", "cs.AI", "cs.LG", "q-fin.TR"], "published": "2025-05-22 02:24:45", "updated": "2025-05-22 02:24:45", "pdf_url": "http://arxiv.org/pdf/2505.16136v1", "comment": "18 pages (including references), 1 figure, 1 table. Code available at\n  \\url{https://github.com/yukepenn/macro-news-sentiment-trading}. Keywords:\n  Macro Sentiment, News Sentiment, Algorithmic Trading, GDELT, FinBERT, NLP,\n  Alternative Data, Foreign Exchange, Treasury Futures, Quantitative Finance,\n  Machine Learning, SHAP, Interpretability", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16146v1", "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "abstract": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 02:45:45", "updated": "2025-05-22 02:45:45", "pdf_url": "http://arxiv.org/pdf/2505.16146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16147v1", "title": "Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value", "authors": ["Le Ma", "Shirao Yang", "Zihao Wang", "Yinggui Wang", "Lei Wang", "Tao Wei", "Kejun Zhang"], "abstract": "The proliferation of large models has intensified the need for efficient data\nvaluation methods to quantify the contribution of individual data providers.\nTraditional approaches, such as game-theory-based Shapley value and\ninfluence-function-based techniques, face prohibitive computational costs or\nrequire access to full data and model training details, making them hardly\nachieve partial data valuation. To address this, we propose Unlearning Shapley,\na novel framework that leverages machine unlearning to estimate data values\nefficiently. By unlearning target data from a pretrained model and measuring\nperformance shifts on a reachable test set, our method computes Shapley values\nvia Monte Carlo sampling, avoiding retraining and eliminating dependence on\nfull data. Crucially, Unlearning Shapley supports both full and partial data\nvaluation, making it scalable for large models (e.g., LLMs) and practical for\ndata markets. Experiments on benchmark datasets and large-scale text corpora\ndemonstrate that our approach matches the accuracy of state-of-the-art methods\nwhile reducing computational overhead by orders of magnitude. Further analysis\nconfirms a strong correlation between estimated values and the true impact of\ndata subsets, validating its reliability in real-world scenarios. This work\nbridges the gap between data valuation theory and practical deployment,\noffering a scalable, privacy-compliant solution for modern AI ecosystems.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-22 02:46:03", "updated": "2025-05-22 02:46:03", "pdf_url": "http://arxiv.org/pdf/2505.16147v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16149v1", "title": "When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification", "authors": ["Zirui Pang", "Haosheng Tan", "Yuhan Pu", "Zhijie Deng", "Zhouan Shen", "Keyu Hu", "Jiaheng Wei"], "abstract": "Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet\nserve as critical tools for model evaluation. However, despite the cleaning\nefforts, these datasets still suffer from pervasive noisy labels and often\ncontain missing labels due to the co-existing image pattern where multiple\nclasses appear in an image sample. This results in misleading model comparisons\nand unfair evaluations. Existing label cleaning methods focus primarily on\nnoisy labels, but the issue of missing labels remains largely overlooked.\nMotivated by these challenges, we present a comprehensive framework named\nREVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,\nLLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods\n(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and\nmissing label detection in widely-used image classification test sets. REVEAL\ndetects potential noisy labels and omissions, aggregates predictions from\nvarious methods, and refines label accuracy through confidence-informed\npredictions and consensus-based filtering. Additionally, we provide a thorough\nanalysis of state-of-the-art vision-language models and pre-trained image\nclassifiers, highlighting their strengths and limitations within the context of\ndataset renovation by revealing 10 observations. Our method effectively reveals\nmissing labels from public datasets and provides soft-labeled results with\nlikelihoods. Through human verifications, REVEAL significantly improves the\nquality of 6 benchmark test sets, highly aligning to human judgments and\nenabling more accurate and meaningful comparisons in image classification.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-22 02:47:36", "updated": "2025-05-22 02:47:36", "pdf_url": "http://arxiv.org/pdf/2505.16149v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16172v1", "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "abstract": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 03:19:49", "updated": "2025-05-22 03:19:49", "pdf_url": "http://arxiv.org/pdf/2505.16172v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16175v1", "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design", "authors": ["Benjamin Schneider", "Dongfu Jiang", "Chao Du", "Tianyu Pang", "Wenhu Chen"], "abstract": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 03:26:50", "updated": "2025-05-22 03:26:50", "pdf_url": "http://arxiv.org/pdf/2505.16175v1", "comment": "19 pages, 6 figures, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16176v1", "title": "Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning", "authors": ["Jun Rao", "Xuebo Liu", "Hexuan Deng", "Zepeng Lin", "Zixiong Yu", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "abstract": "In the realm of data selection for reasoning tasks, existing approaches\npredominantly rely on externally predefined static metrics such as difficulty\nand diversity, which are often designed for supervised fine-tuning (SFT) and\nlack adaptability to continuous training processes. A critical limitation of\nthese methods is their inability to dynamically align with the evolving\ncapabilities of models during online training, a gap that becomes increasingly\npronounced with the rise of dynamic training paradigms and online reinforcement\nlearning (RL) frameworks (e.g., R1 models). To address this, we introduce\nSAI-DPO, an algorithm that dynamically selects training data by continuously\nassessing a model's stage-specific reasoning abilities across different\ntraining phases. By integrating real-time model performance feedback, SAI-DPO\nadaptively adapts data selection to the evolving strengths and weaknesses of\nthe model, thus enhancing both data utilization efficiency and final task\nperformance. Extensive experiments on three state-of-the-art models and eight\nmathematical reasoning benchmarks, including challenging competition-level\ndatasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average\nperformance boost of up to 21.3 percentage points, with particularly notable\nimprovements of 10 and 15 points on AIME24 and AMC23, respectively. These\nresults highlight the superiority of dynamic, model-adaptive data selection\nover static, externally defined strategies in advancing reasoning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 03:27:05", "updated": "2025-05-22 03:27:05", "pdf_url": "http://arxiv.org/pdf/2505.16176v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16181v1", "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks", "authors": ["Mohammad Reza Taesiri", "Brandon Collins", "Logan Bolton", "Viet Dac Lai", "Franck Dernoncourt", "Trung Bui", "Anh Totti Nguyen"], "abstract": "Generative AI (GenAI) holds significant promise for automating everyday image\nediting tasks, especially following the recent release of GPT-4o on March 25,\n2025. However, what subjects do people most often want edited? What kinds of\nediting actions do they want to perform (e.g., removing or stylizing the\nsubject)? Do people prefer precise edits with predictable outcomes or highly\ncreative ones? By understanding the characteristics of real-world requests and\nthe corresponding edits made by freelance photo-editing wizards, can we draw\nlessons for improving AI-based editors and determine which types of requests\ncan currently be handled successfully by AI editors? In this paper, we present\na unique study addressing these questions by analyzing 83k requests from the\npast 12 years (2013-2025) on the Reddit community, which collected 305k\nPSR-wizard edits. According to human ratings, approximately only 33% of\nrequests can be fulfilled by the best AI editors (including GPT-4o,\nGemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on\nlow-creativity requests that require precise editing than on more open-ended\ntasks. They often struggle to preserve the identity of people and animals, and\nfrequently make non-requested touch-ups. On the other side of the table, VLM\njudges (e.g., o1) perform differently from human judges and may prefer AI edits\nmore than human edits. Code and qualitative examples are available at:\nhttps://psrdataset.github.io", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 03:35:15", "updated": "2025-05-22 03:35:15", "pdf_url": "http://arxiv.org/pdf/2505.16181v1", "comment": "Code and qualitative examples are available at:\n  https://psrdataset.github.io", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16186v1", "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning", "authors": ["Kaiwen Zhou", "Xuandong Zhao", "Gaowen Liu", "Jayanth Srinivasa", "Aosong Feng", "Dawn Song", "Xin Eric Wang"], "abstract": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.", "categories": ["cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-22 03:46:03", "updated": "2025-05-22 03:46:03", "pdf_url": "http://arxiv.org/pdf/2505.16186v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16187v1", "title": "EasyInsert: A Data-Efficient and Generalizable Insertion Policy", "authors": ["Guanghe Li", "Junming Zhao", "Shengjie Wang", "Yang Gao"], "abstract": "Insertion task is highly challenging that requires robots to operate with\nexceptional precision in cluttered environments. Existing methods often have\npoor generalization capabilities. They typically function in restricted and\nstructured environments, and frequently fail when the plug and socket are far\napart, when the scene is densely cluttered, or when handling novel objects.\nThey also rely on strong assumptions such as access to CAD models or a digital\ntwin in simulation. To address this, we propose EasyInsert, a framework which\nleverages the human intuition that relative pose (delta pose) between plug and\nsocket is sufficient for successful insertion, and employs efficient and\nautomated real-world data collection with minimal human labor to train a\ngeneralizable model for relative pose prediction. During execution, EasyInsert\nfollows a coarse-to-fine execution procedure based on predicted delta pose, and\nsuccessfully performs various insertion tasks. EasyInsert demonstrates strong\nzero-shot generalization capability for unseen objects in cluttered\nenvironments, handling cases with significant initial pose deviations while\nmaintaining high sample efficiency and requiring little human effort. In\nreal-world experiments, with just 5 hours of training data, EasyInsert achieves\nover 90% success in zero-shot insertion for 13 out of 15 unseen novel objects,\nincluding challenging objects like Type-C cables, HDMI cables, and Ethernet\ncables. Furthermore, with only one human demonstration and 4 minutes of\nautomatically collected data for fine-tuning, it reaches over 90% success rate\nfor all 15 objects.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 03:46:05", "updated": "2025-05-22 03:46:05", "pdf_url": "http://arxiv.org/pdf/2505.16187v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16192v1", "title": "VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "authors": ["Chaoya Jiang", "Yongrui Heng", "Wei Ye", "Han Yang", "Haiyang Xu", "Ming Yan", "Ji Zhang", "Fei Huang", "Shikun Zhang"], "abstract": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce \\textbf{VLM-R$^3$} (\\textbf{V}isual\n\\textbf{L}anguage \\textbf{M}odel with \\textbf{R}egion \\textbf{R}ecognition and\n\\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)\ndecide \\emph{when} additional visual evidence is needed, (ii) determine\n\\emph{where} to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is \\textbf{Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO)}, a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 03:50:13", "updated": "2025-05-22 03:50:13", "pdf_url": "http://arxiv.org/pdf/2505.16192v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16195v1", "title": "SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet", "authors": ["Zhi Zhong", "Akira Takahashi", "Shuyang Cui", "Keisuke Toyama", "Shusuke Takahashi", "Yuki Mitsufuji"], "abstract": "Foley synthesis aims to synthesize high-quality audio that is both\nsemantically and temporally aligned with video frames. Given its broad\napplication in creative industries, the task has gained increasing attention in\nthe research community. To avoid the non-trivial task of training audio\ngenerative models from scratch, adapting pretrained audio generative models for\nvideo-synchronized foley synthesis presents an attractive direction.\nControlNet, a method for adding fine-grained controls to pretrained generative\nmodels, has been applied to foley synthesis, but its use has been limited to\nhandcrafted human-readable temporal conditions. In contrast, from-scratch\nmodels achieved success by leveraging high-dimensional deep features extracted\nusing pretrained video encoders. We have observed a performance gap between\nControlNet-based and from-scratch foley models. To narrow this gap, we propose\nSpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward\nvideo-synchronized foley synthesis via ControlNet. To unlock the potential of a\nsingle ControlNet branch, we resolve the discrepancy between the temporal video\nfeatures and the time-frequency nature of the pretrained SpecMaskGIT via a\nfrequency-aware temporal feature aligner, eliminating the need for complicated\nconditioning mechanisms widely used in prior arts. Evaluations on a common\nfoley synthesis benchmark demonstrate that SpecMaskFoley could even outperform\nstrong from-scratch baselines, substantially advancing the development of\nControlNet-based foley synthesis models. Demo page:\nhttps://zzaudio.github.io/SpecMaskFoley_Demo/", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.IV"], "published": "2025-05-22 03:58:16", "updated": "2025-05-22 03:58:16", "pdf_url": "http://arxiv.org/pdf/2505.16195v1", "comment": "4 pages, 2 figures, 2 tables. Demo page:\n  https://zzaudio.github.io/SpecMaskFoley_Demo/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16196v1", "title": "SEM: Enhancing Spatial Understanding for Robust Robot Manipulation", "authors": ["Xuewu Lin", "Tianwei Lin", "Lichao Huang", "Hongyu Xie", "Yiwei Jin", "Keyu Li", "Zhizhong Su"], "abstract": "A key challenge in robot manipulation lies in developing policy models with\nstrong spatial understanding, the ability to reason about 3D geometry, object\nrelations, and robot embodiment. Existing methods often fall short: 3D point\ncloud models lack semantic abstraction, while 2D image encoders struggle with\nspatial reasoning. To address this, we propose SEM (Spatial Enhanced\nManipulation model), a novel diffusion-based policy framework that explicitly\nenhances spatial understanding from two complementary perspectives. A spatial\nenhancer augments visual representations with 3D geometric context, while a\nrobot state encoder captures embodiment-aware structure through graphbased\nmodeling of joint dependencies. By integrating these modules, SEM significantly\nimproves spatial understanding, leading to robust and generalizable\nmanipulation across diverse tasks that outperform existing baselines.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-05-22 04:00:12", "updated": "2025-05-22 04:00:12", "pdf_url": "http://arxiv.org/pdf/2505.16196v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16199v1", "title": "Velocity Completion Task and Method for Event-based Player Positional Data in Soccer", "authors": ["Rikuhei Umemoto", "Keisuke Fujii"], "abstract": "In many real-world complex systems, the behavior can be observed as a\ncollection of discrete events generated by multiple interacting agents.\nAnalyzing the dynamics of these multi-agent systems, especially team sports,\noften relies on understanding the movement and interactions of individual\nagents. However, while providing valuable snapshots, event-based positional\ndata typically lacks the continuous temporal information needed to directly\ncalculate crucial properties such as velocity. This absence severely limits the\ndepth of dynamic analysis, preventing a comprehensive understanding of\nindividual agent behaviors and emergent team strategies. To address this\nchallenge, we propose a new method to simultaneously complete the velocity of\nall agents using only the event-based positional data from team sports. Based\non this completed velocity information, we investigate the applicability of\nexisting team sports analysis and evaluation methods. Experiments using soccer\nevent data demonstrate that neural network-based approaches outperformed\nrule-based methods regarding velocity completion error, considering the\nunderlying temporal dependencies and graph structure of player-to-player or\nplayer-to-ball interaction. Moreover, the space evaluation results obtained\nusing the completed velocity are closer to those derived from complete tracking\ndata, highlighting our method's potential for enhanced team sports system\nanalysis.", "categories": ["cs.AI"], "published": "2025-05-22 04:01:49", "updated": "2025-05-22 04:01:49", "pdf_url": "http://arxiv.org/pdf/2505.16199v1", "comment": "24 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16208v1", "title": "Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems", "authors": ["Anton Erofeev", "Balasubramanya T. Nadiga", "Ilya Timofeyev"], "abstract": "We apply the Echo-State Networks to predict the time series and statistical\nproperties of the competitive Lotka-Volterra model in the chaotic regime. In\nparticular, we demonstrate that Echo-State Networks successfully learn the\nchaotic attractor of the competitive Lotka-Volterra model and reproduce\nhistograms of dependent variables, including tails and rare events. We use the\nGeneralized Extreme Value distribution to quantify the tail behavior.", "categories": ["nlin.CD", "cs.AI", "cs.LG", "math.DS", "37N99, 68T30"], "published": "2025-05-22 04:21:05", "updated": "2025-05-22 04:21:05", "pdf_url": "http://arxiv.org/pdf/2505.16208v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16210v1", "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 04:23:19", "updated": "2025-05-22 04:23:19", "pdf_url": "http://arxiv.org/pdf/2505.16210v1", "comment": "11 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16211v1", "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhuo Chen", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "abstract": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-22 04:27:46", "updated": "2025-05-22 04:27:46", "pdf_url": "http://arxiv.org/pdf/2505.16211v1", "comment": "Technical Report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16221v1", "title": "LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead", "authors": ["Yifan Zhang", "Xinkui Zhao", "Zuxin Wang", "Guanjie Cheng", "Yueshen Xu", "Shuiguang Deng", "Jianwei Yin"], "abstract": "The rapid advancement of large language models has unlocked remarkable\ncapabilities across a diverse array of natural language processing tasks.\nHowever, the considerable differences among available LLMs-in terms of cost,\nperformance, and computational demands-pose significant challenges for users\naiming to identify the most suitable model for specific tasks. In this work, we\npresent LightRouter, a novel framework designed to systematically select and\nintegrate a small subset of LLMs from a larger pool, with the objective of\njointly optimizing both task performance and cost efficiency. LightRouter\nleverages an adaptive selection mechanism to identify models that require only\na minimal number of boot tokens, thereby reducing costs, and further employs an\neffective integration strategy to combine their outputs. Extensive experiments\nacross multiple benchmarks demonstrate that LightRouter matches or outperforms\nwidely-used ensemble baselines, achieving up to a 25% improvement in accuracy.\nCompared with leading high-performing models, LightRouter achieves comparable\nperformance while reducing inference costs by up to 27%. Importantly, our\nframework operates without any prior knowledge of individual models and relies\nexclusively on inexpensive, lightweight models. This work introduces a\npractical approach for efficient LLM selection and provides valuable insights\ninto optimal strategies for model combination.", "categories": ["cs.AI"], "published": "2025-05-22 04:46:04", "updated": "2025-05-22 04:46:04", "pdf_url": "http://arxiv.org/pdf/2505.16221v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16223v1", "title": "MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network", "authors": ["Sangyong Lee", "Subo Hwang", "Dohoon Kim"], "abstract": "In this paper, we propose MADCluster, a novel model-agnostic anomaly\ndetection framework utilizing self-supervised clustering. MADCluster is\napplicable to various deep learning architectures and addresses the\n'hypersphere collapse' problem inherent in existing deep learning-based anomaly\ndetection methods. The core idea is to cluster normal pattern data into a\n'single cluster' while simultaneously learning the cluster center and mapping\ndata close to this center. Also, to improve expressiveness and enable effective\nsingle clustering, we propose a new 'One-directed Adaptive loss'. The\noptimization of this loss is mathematically proven. MADCluster consists of\nthree main components: Base Embedder capturing high-dimensional temporal\ndynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous\ncenter updates. Its model-agnostic characteristics are achieved by applying\nvarious architectures to the Base Embedder. Experiments on four time series\nbenchmark datasets demonstrate that applying MADCluster improves the overall\nperformance of comparative models. In conclusion, the compatibility of\nMADCluster shows potential for enhancing model performance across various\narchitectures.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-22 04:50:44", "updated": "2025-05-22 04:50:44", "pdf_url": "http://arxiv.org/pdf/2505.16223v1", "comment": "24 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16225v1", "title": "MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning", "authors": ["Zihan Chen", "Song Wang", "Zhen Tan", "Jundong Li", "Cong Shen"], "abstract": "In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle\ndiverse tasks by incorporating multiple input-output examples, known as\ndemonstrations, into the input of LLMs. More recently, advancements in the\nexpanded context windows of LLMs have led to many-shot ICL, which uses hundreds\nof demonstrations and outperforms few-shot ICL, which relies on fewer examples.\nHowever, this approach is often hindered by the high cost of obtaining large\namounts of labeled data. To address this challenge, we propose Many-Shot\nAdaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL\nframework that utilizes pseudo-labeled samples to compensate for the lack of\nlabel information. We first identify a subset of impactful unlabeled samples\nand perform pseudo-labeling on them by querying LLMs. These pseudo-labeled\nsamples are then adaptively selected and tailored to each test query as input\nto improve the performance of many-shot ICL, without significant labeling\ncosts. Extensive experiments on real-world datasets demonstrate the\neffectiveness of our framework, showcasing its ability to enhance LLM\nadaptability and performance with limited labeled data.", "categories": ["cs.AI"], "published": "2025-05-22 04:54:27", "updated": "2025-05-22 04:54:27", "pdf_url": "http://arxiv.org/pdf/2505.16225v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16227v1", "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "abstract": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 04:55:41", "updated": "2025-05-22 04:55:41", "pdf_url": "http://arxiv.org/pdf/2505.16227v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16234v1", "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "abstract": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 05:08:27", "updated": "2025-05-22 05:08:27", "pdf_url": "http://arxiv.org/pdf/2505.16234v1", "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16249v1", "title": "Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control", "authors": ["Zhen Zhang", "Xiangyu Chu", "Yunxi Tang", "Lulu Zhao", "Jing Huang", "Zhongliang Jiang", "K. W. Samuel Au"], "abstract": "Manipulating elasto-plastic objects remains a significant challenge due to\nsevere self-occlusion, difficulties of representation, and complicated\ndynamics. This work proposes a novel framework for elasto-plastic object\nmanipulation with a quasi-static assumption for motions, leveraging 3D\noccupancy to represent such objects, a learned dynamics model trained with 3D\noccupancy, and a learning-based predictive control algorithm to address these\nchallenges effectively. We build a novel data collection platform to collect\nfull spatial information and propose a pipeline for generating a 3D occupancy\ndataset. To infer the 3D occupancy during manipulation, an occupancy prediction\nnetwork is trained with multiple RGB images supervised by the generated\ndataset. We design a deep neural network empowered by a 3D convolution neural\nnetwork (CNN) and a graph neural network (GNN) to predict the complex\ndeformation with the inferred 3D occupancy results. A learning-based predictive\ncontrol algorithm is introduced to plan the robot actions, incorporating a\nnovel shape-based action initialization module specifically designed to improve\nthe planner efficiency. The proposed framework in this paper can successfully\nshape the elasto-plastic objects into a given goal shape and has been verified\nin various experiments both in simulation and the real world.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 05:36:00", "updated": "2025-05-22 05:36:00", "pdf_url": "http://arxiv.org/pdf/2505.16249v1", "comment": "8 Pages, 5 figures, accepted for publication in IEEE Robotics and\n  Automation Letters (RA-L)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16256v1", "title": "DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor", "authors": ["Yan Zhao", "Zhengxue Cheng", "Junxuan Zhang", "Qunshan Gu", "Qi Wang", "Li Song"], "abstract": "Most learning-based lossless compressors are designed for a single modality,\nrequiring separate models for multi-modal data and lacking flexibility.\nHowever, different modalities vary significantly in format and statistical\nproperties, making it ineffective to use compressors that lack\nmodality-specific adaptations. While multi-modal large language models (MLLMs)\noffer a potential solution for modality-unified compression, their excessive\ncomplexity hinders practical deployment. To address these challenges, we focus\non the two most common modalities, image and text, and propose DualComp, the\nfirst unified and lightweight learning-based dual-modality lossless compressor.\nBuilt on a lightweight backbone, DualComp incorporates three key structural\nenhancements to handle modality heterogeneity: modality-unified tokenization,\nmodality-switching contextual learning, and modality-routing\nmixture-of-experts. A reparameterization training strategy is also used to\nboost compression performance. DualComp integrates both modality-specific and\nshared parameters for efficient parameter utilization, enabling near real-time\ninference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp\nachieves compression performance on par with the SOTA LLM-based methods for\nboth text and image datasets. Its simplified single-modality variant surpasses\nthe previous best image compressor on the Kodak dataset by about 9% using just\n1.2% of the model size.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "published": "2025-05-22 05:46:14", "updated": "2025-05-22 05:46:14", "pdf_url": "http://arxiv.org/pdf/2505.16256v1", "comment": "18 pages, 11 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16258v1", "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "abstract": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "published": "2025-05-22 05:49:01", "updated": "2025-05-22 05:49:01", "pdf_url": "http://arxiv.org/pdf/2505.16258v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16259v1", "title": "Dialogue in Resonance: An Interactive Music Piece for Piano and Real-Time Automatic Transcription System", "authors": ["Hayeon Bang", "Taegyun Kwon", "Juhan Nam"], "abstract": "This paper presents <Dialogue in Resonance>, an interactive music piece for a\nhuman pianist and a computer-controlled piano that integrates real-time\nautomatic music transcription into a score-driven framework. Unlike previous\napproaches that primarily focus on improvisation-based interactions, our work\nestablishes a balanced framework that combines composed structure with dynamic\ninteraction. Through real-time automatic transcription as its core mechanism,\nthe computer interprets and responds to the human performer's input in real\ntime, creating a musical dialogue that balances compositional intent with live\ninteraction while incorporating elements of unpredictability. In this paper, we\npresent the development process from composition to premiere performance,\nincluding technical implementation, rehearsal process, and performance\nconsiderations.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-22 05:50:13", "updated": "2025-05-22 05:50:13", "pdf_url": "http://arxiv.org/pdf/2505.16259v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16270v1", "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "abstract": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 06:00:45", "updated": "2025-05-22 06:00:45", "pdf_url": "http://arxiv.org/pdf/2505.16270v1", "comment": "33 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16276v1", "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance", "authors": ["Desiree Heim", "Lars-Peter Meyer", "Markus Schr\u00f6der", "Johannes Frey", "Andreas Dengel"], "abstract": "When using Large Language Models (LLMs) to support Knowledge Graph\nEngineering (KGE), one of the first indications when searching for an\nappropriate model is its size. According to the scaling laws, larger models\ntypically show higher capabilities. However, in practice, resource costs are\nalso an important factor and thus it makes sense to consider the ratio between\nmodel performance and costs. The LLM-KG-Bench framework enables the comparison\nof LLMs in the context of KGE tasks and assesses their capabilities of\nunderstanding and producing KGs and KG queries. Based on a dataset created in\nan LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the\nmodel size scaling laws specific to KGE tasks. In our analyses, we assess how\nbenchmark scores evolve between different model size categories. Additionally,\nwe inspect how the general score development of single models and families of\nmodels correlates to their size. Our analyses revealed that, with a few\nexceptions, the model size scaling laws generally also apply to the selected\nKGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,\nthe task performance did not change much between a model and the next larger\nmodel. In these cases, smaller models could be considered to achieve high\ncost-effectiveness. Regarding models of the same family, sometimes larger\nmodels performed worse than smaller models of the same family. These effects\noccurred only locally. Hence it is advisable to additionally test the next\nsmallest and largest model of the same family.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 06:21:40", "updated": "2025-05-22 06:21:40", "pdf_url": "http://arxiv.org/pdf/2505.16276v1", "comment": "Peer reviewed and to appear in the ESWC 2025 Workshops and Tutorials\n  Joint Proceedings (Workshop on Evaluation of Language Models in Knowledge\n  Engineering [ELMKE])", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16278v1", "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "authors": ["Zhenjie Yang", "Yilin Chai", "Xiaosong Jia", "Qifeng Li", "Yuqian Shao", "Xuekai Zhu", "Haisheng Su", "Junchi Yan"], "abstract": "End-to-end autonomous driving (E2E-AD) demands effective processing of\nmulti-view sensory data and robust handling of diverse and complex driving\nscenarios, particularly rare maneuvers such as aggressive turns. Recent success\nof Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)\ndemonstrates that specialization of parameters enables strong scalability. In\nthis work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a\nScene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is\nbuilt upon our $\\pi_0$ Vision-Language-Action (VLA) baseline (originally from\nthe embodied AI field), called Drive-$\\pi_0$. Specifically, we add Vision MoE\nto Drive-$\\pi_0$ by training a router to select relevant cameras according to\nthe driving context dynamically. This design mirrors human driving cognition,\nwhere drivers selectively attend to crucial visual cues rather than\nexhaustively processing all visual information. In addition, we add Action MoE\nby training another router to activate specialized expert modules for different\ndriving behaviors. Through explicit behavioral specialization, DriveMoE is able\nto handle diverse scenarios without suffering from modes averaging like\nexisting models. In Bench2Drive closed-loop evaluation experiments, DriveMoE\nachieves state-of-the-art (SOTA) performance, demonstrating the effectiveness\nof combining vision and action MoE in autonomous driving tasks. We will release\nour code and models of DriveMoE and Drive-$\\pi_0$.", "categories": ["cs.CV", "cs.AI", "cs.RO"], "published": "2025-05-22 06:23:04", "updated": "2025-05-22 06:23:04", "pdf_url": "http://arxiv.org/pdf/2505.16278v1", "comment": "Project Page: https://thinklab-sjtu.github.io/DriveMoE/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16288v1", "title": "No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery", "authors": ["Xiaoxue Han", "Pengfei Hu", "Jun-En Ding", "Chang Lu", "Feng Liu", "Yue Ning"], "abstract": "Deep learning models trained on extensive Electronic Health Records (EHR)\ndata have achieved high accuracy in diagnosis prediction, offering the\npotential to assist clinicians in decision-making and treatment planning.\nHowever, these models lack two crucial features that clinicians highly value:\ninterpretability and interactivity. The ``black-box'' nature of these models\nmakes it difficult for clinicians to understand the reasoning behind\npredictions, limiting their ability to make informed decisions. Additionally,\nthe absence of interactive mechanisms prevents clinicians from incorporating\ntheir own knowledge and experience into the decision-making process. To address\nthese limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal\ndiscovery framework that integrates personalized knowledge databases and\nagentic LLMs. II-KEA enhances interpretability through explicit reasoning and\ncausal analysis, while also improving interactivity by allowing clinicians to\ninject their knowledge and experience through customized knowledge bases and\nprompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating\nsuperior performance along with enhanced interpretability and interactivity, as\nevidenced by its strong results from extensive case studies.", "categories": ["cs.AI"], "published": "2025-05-22 06:36:30", "updated": "2025-05-22 06:36:30", "pdf_url": "http://arxiv.org/pdf/2505.16288v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16290v1", "title": "Multimodal Generative AI for Story Point Estimation in Software Development", "authors": ["Mohammad Rubyet Islam", "Peter Sandborn"], "abstract": "This research explores the application of Multimodal Generative AI to enhance\nstory point estimation in Agile software development. By integrating text,\nimage, and categorical data using advanced models like BERT, CNN, and XGBoost,\nour approach surpasses the limitations of traditional single-modal estimation\nmethods. The results demonstrate strong accuracy for simpler story points,\nwhile also highlighting challenges in more complex categories due to data\nimbalance. This study further explores the impact of categorical data,\nparticularly severity, on the estimation process, emphasizing its influence on\nmodel performance. Our findings emphasize the transformative potential of\nmultimodal data integration in refining AI-driven project management, paving\nthe way for more precise, adaptable, and domain-specific AI capabilities.\nAdditionally, this work outlines future directions for addressing data\nvariability and enhancing the robustness of AI in Agile methodologies.", "categories": ["cs.SE", "cs.AI", "68T07, 68T45", "I.2.6; I.2.10; D.2.9; H.2.8"], "published": "2025-05-22 06:40:41", "updated": "2025-05-22 06:40:41", "pdf_url": "http://arxiv.org/pdf/2505.16290v1", "comment": null, "doi": null, "journal_ref": "A revised version of this work is published in the proceedings of\n  the IEEE Conference on Artificial Intelligence 2025"}
{"arxiv_id": "2505.16301v1", "title": "Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space", "authors": ["Fuchun Ge", "Pavlo O. Dral"], "abstract": "Molecular dynamics (MD) is a powerful tool for exploring the behavior of\natomistic systems, but its reliance on sequential numerical integration limits\nsimulation efficiency. We present MDtrajNet-1, a foundational AI model that\ndirectly generates MD trajectories across chemical space, bypassing force\ncalculations and integration. This approach accelerates simulations by up to\ntwo orders of magnitude compared to traditional MD, even those enhanced by\nmachine-learning interatomic potentials. MDtrajNet-1 combines equivariant\nneural networks with a Transformer-based architecture to achieve strong\naccuracy and transferability in predicting long-time trajectories for both\nknown and unseen systems. Remarkably, the errors of the trajectories generated\nby MDtrajNet-1 for various molecular systems are close to those of the\nconventional ab initio MD. The model's flexible design supports diverse\napplication scenarios, including different statistical ensembles, boundary\nconditions, and interaction types. By overcoming the intrinsic speed barrier of\nconventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable\natomistic simulations.", "categories": ["physics.chem-ph", "cs.AI", "cs.LG"], "published": "2025-05-22 06:56:19", "updated": "2025-05-22 06:56:19", "pdf_url": "http://arxiv.org/pdf/2505.16301v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16306v1", "title": "Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models", "authors": ["Yizhi Zhou", "Haina Zhu", "Hangting Chen"], "abstract": "Recently, pre-trained models for music information retrieval based on\nself-supervised learning (SSL) are becoming popular, showing success in various\ndownstream tasks. However, there is limited research on the specific meanings\nof the encoded information and their applicability. Exploring these aspects can\nhelp us better understand their capabilities and limitations, leading to more\neffective use in downstream tasks.\n  In this study, we analyze the advanced music representation model MusicFM and\nthe newly emerged SSL model MuQ. We focus on three main aspects: (i) validating\nthe advantages of SSL models across multiple downstream tasks, (ii) exploring\nthe specialization of layer-wise information for different tasks, and (iii)\ncomparing performance differences when selecting specific layers. Through this\nanalysis, we reveal insights into the structure and potential applications of\nSSL models in music information retrieval.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-22 06:58:24", "updated": "2025-05-22 06:58:24", "pdf_url": "http://arxiv.org/pdf/2505.16306v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16307v1", "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "abstract": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 06:59:10", "updated": "2025-05-22 06:59:10", "pdf_url": "http://arxiv.org/pdf/2505.16307v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16312v1", "title": "EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning", "authors": ["Jiawei Liu", "Qisi Chen", "Jianshu Zhang", "Quan Liu", "Defu Lian"], "abstract": "Large Language Models (LLMs) excel at complex reasoning through search\nalgorithms, yet current strategies often suffer from massive token consumption\ndue to redundant exploration of semantically equivalent steps. Existing\nsemantic similarity methods struggle to accurately identify such equivalence in\ndomain-specific contexts like mathematical reasoning. To address this, we\npropose EquivPruner, a simple yet effective approach that identifies and prunes\nsemantically equivalent actions during LLM reasoning search. We also introduce\nMathEquiv, the first dataset we created for mathematical statement equivalence,\nwhich enables the training of a lightweight equivalence detector. Extensive\nexperiments across various models and tasks demonstrate that EquivPruner\nsignificantly reduces token consumption, improving searching efficiency and\noften bolstering reasoning accuracy. For instance, when applied to\nQwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by\n48.1\\% while also improving accuracy. Our code is available at\nhttps://github.com/Lolo1222/EquivPruner.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 07:07:43", "updated": "2025-05-22 07:07:43", "pdf_url": "http://arxiv.org/pdf/2505.16312v1", "comment": "11 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16314v1", "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment", "authors": ["Shuhao Han", "Haotian Fan", "Fangyuan Kong", "Wenjie Liao", "Chunle Guo", "Chongyi Li", "Radu Timofte", "Liang Li", "Tao Li", "Junhui Cui", "Yunqiu Wang", "Yang Tai", "Jingwei Sun", "Jianhui Sun", "Xinli Yue", "Tianyi Wang", "Huan Hou", "Junda Lu", "Xinyang Huang", "Zitang Zhou", "Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao", "Trong-Hieu Nguyen-Mau", "Minh-Hoang Le", "Minh-Khoa Le-Phan", "Duy-Nam Ly", "Hai-Dang Nguyen", "Minh-Triet Tran", "Yukang Lin", "Yan Hong", "Chuanbiao Song", "Siyuan Li", "Jun Lan", "Zhichao Zhang", "Xinyue Li", "Wei Sun", "Zicheng Zhang", "Yunhao Li", "Xiaohong Liu", "Guangtao Zhai", "Zitong Xu", "Huiyu Duan", "Jiarui Wang", "Guangji Ma", "Liu Yang", "Lu Liu", "Qiang Hu", "Xiongkuo Min", "Zichuan Wang", "Zhenchen Tang", "Bo Peng", "Jing Dong", "Fengbin Guan", "Zihao Yu", "Yiting Lu", "Wei Luo", "Xin Li", "Minhao Lin", "Haofeng Chen", "Xuanxuan He", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Bo-Cheng Qiu", "Chih-Chung Hsu", "Chia-ming Lee", "Yu-Fan Lin", "Bo Yu", "Zehao Wang", "Da Mu", "Mingxiu Chen", "Junkang Fang", "Huamei Sun", "Wending Zhao", "Zhiyu Wang", "Wang Liu", "Weikang Yu", "Puhong Duan", "Bin Sun", "Xudong Kang", "Shutao Li", "Shuai He", "Lingzhi Fu", "Heng Cong", "Rongyu Zhang", "Jiarong He", "Zhishan Qiao", "Yongqing Huang", "Zewen Chen", "Zhe Pang", "Juan Wang", "Jian Guo", "Zhizhuo Shao", "Ziyu Feng", "Bing Li", "Weiming Hu", "Hesong Li", "Dehua Liu", "Zeming Liu", "Qingsong Xie", "Ruichen Wang", "Zhihao Li", "Yuqi Liang", "Jianqi Bi", "Jun Luo", "Junfeng Yang", "Can Li", "Jing Fu", "Hongwei Xu", "Mingrui Long", "Lulin Tang"], "abstract": "This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)\ngeneration model quality assessment, which will be held in conjunction with the\nNew Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.\nThe aim of this challenge is to address the fine-grained quality assessment of\ntext-to-image generation models. This challenge evaluates text-to-image models\nfrom two aspects: image-text alignment and image structural distortion\ndetection, and is divided into the alignment track and the structural track.\nThe alignment track uses the EvalMuse-40K, which contains around 40K\nAI-Generated Images (AIGIs) generated by 20 popular generative models. The\nalignment track has a total of 371 registered participants. A total of 1,883\nsubmissions are received in the development phase, and 507 submissions are\nreceived in the test phase. Finally, 12 participating teams submitted their\nmodels and fact sheets. The structure track uses the EvalMuse-Structure, which\ncontains 10,000 AI-Generated Images (AIGIs) with corresponding structural\ndistortion mask. A total of 211 participants have registered in the structure\ntrack. A total of 1155 submissions are received in the development phase, and\n487 submissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Almost all methods have achieved better\nresults than baseline methods, and the winning methods in both tracks have\ndemonstrated superior prediction performance on T2I model quality assessment.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 07:12:36", "updated": "2025-05-22 07:12:36", "pdf_url": "http://arxiv.org/pdf/2505.16314v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16315v1", "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning", "authors": ["Xiaoxue Cheng", "Junyi Li", "Zhenduo Zhang", "Xinyu Tang", "Wayne Xin Zhao", "Xinyu Kong", "Zhiqiang Zhang"], "abstract": "Large reasoning models (LRMs) have demonstrated strong performance on complex\nreasoning tasks, but often suffer from overthinking, generating redundant\ncontent regardless of task difficulty. Inspired by the dual process theory in\ncognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a\nreinforcement learning framework that enables LRMs to achieve efficient\nreasoning through adaptive cognitive allocation and dynamic system switch. ACPO\nincorporates two key components: (1) introducing system-aware reasoning tokens\nto explicitly represent the thinking modes thereby making the model's cognitive\nprocess transparent, and (2) integrating online difficulty estimation and token\nlength budget to guide adaptive system switch and reasoning during\nreinforcement learning. To this end, we propose a two-stage training strategy.\nThe first stage begins with supervised fine-tuning to cold start the model,\nenabling it to generate reasoning paths with explicit thinking modes. In the\nsecond stage, we apply ACPO to further enhance adaptive system switch for\ndifficulty-aware reasoning. Experimental results demonstrate that ACPO\neffectively reduces redundant reasoning while adaptively adjusting cognitive\nallocation based on task complexity, achieving efficient hybrid reasoning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 07:15:08", "updated": "2025-05-22 07:15:08", "pdf_url": "http://arxiv.org/pdf/2505.16315v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16322v1", "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "abstract": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 07:24:11", "updated": "2025-05-22 07:24:11", "pdf_url": "http://arxiv.org/pdf/2505.16322v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16325v1", "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "abstract": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-22 07:32:12", "updated": "2025-05-22 07:32:12", "pdf_url": "http://arxiv.org/pdf/2505.16325v1", "comment": "18 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16330v1", "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "abstract": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM.", "categories": ["cs.CL", "cs.AI", "cs.DL"], "published": "2025-05-22 07:34:59", "updated": "2025-05-22 07:34:59", "pdf_url": "http://arxiv.org/pdf/2505.16330v1", "comment": null, "doi": "10.1016/j.eswa.2025.126778", "journal_ref": "Expert Systems With Applications, 2025"}
{"arxiv_id": "2505.16332v1", "title": "Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing", "authors": ["Zhehui Wanga", "Benjamin Chen Ming Choonga", "Tian Huang", "Daniel Gerlinghoffa", "Rick Siow Mong Goh", "Cheng Liu", "Tao Luo"], "abstract": "Quantum optimization is the most mature quantum computing technology to date,\nproviding a promising approach towards efficiently solving complex\ncombinatorial problems. Methods such as adiabatic quantum computing (AQC) have\nbeen employed in recent years on important optimization problems across various\ndomains. In deep learning, deep neural networks (DNN) have reached immense\nsizes to support new predictive capabilities. Optimization of large-scale\nmodels is critical for sustainable deployment, but becomes increasingly\nchallenging with ever-growing model sizes and complexity. While quantum\noptimization is suitable for solving complex problems, its application to DNN\noptimization is not straightforward, requiring thorough reformulation for\ncompatibility with commercially available quantum devices. In this work, we\nexplore the potential of adopting AQC for fine-grained pruning-quantization of\nconvolutional neural networks. We rework established heuristics to formulate\nmodel compression as a quadratic unconstrained binary optimization (QUBO)\nproblem, and assess the solution space offered by commercial quantum annealing\ndevices. Through our exploratory efforts of reformulation, we demonstrate that\nAQC can achieve effective compression of practical DNN models. Experiments\ndemonstrate that adiabatic quantum computing (AQC) not only outperforms\nclassical algorithms like genetic algorithms and reinforcement learning in\nterms of time efficiency but also excels at identifying global optima.", "categories": ["quant-ph", "cs.AI"], "published": "2025-05-22 07:40:23", "updated": "2025-05-22 07:40:23", "pdf_url": "http://arxiv.org/pdf/2505.16332v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16335v1", "title": "FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design", "authors": ["Renjie Wei", "Songqiang Xu", "Qingyu Guo", "Meng Li"], "abstract": "Visual autoregressive (VAR) modeling has marked a paradigm shift in image\ngeneration from next-token prediction to next-scale prediction. VAR predicts a\nset of tokens at each step from coarse to fine scale, leading to better image\nquality and faster inference speed compared to existing diffusion models.\nHowever, the large parameter size and computation cost hinder its deployment on\nedge devices. To reduce the memory and computation cost, we propose FPQVAR, an\nefficient post-training floating-point (FP) quantization framework for VAR\nfeaturing algorithm and hardware co-design. At the algorithm level, we first\nidentify the challenges of quantizing VAR. To address them, we propose Dual\nFormat Quantization for the highly imbalanced input activation. We further\npropose Group-wise Hadamard Transformation and GHT-Aware Learnable\nTransformation to address the time-varying outlier channels. At the hardware\nlevel, we design the first low-bit FP quantizer and multiplier with lookup\ntables on FPGA and propose the first FPGA-based VAR accelerator featuring\nlow-bit FP computation and an elaborate two-level pipeline. Extensive\nexperiments show that compared to the state-of-the-art quantization method, our\nproposed FPQVAR significantly improves Fr\\'echet Inception Distance (FID) from\n10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit\nquantization. FPQVAR also significantly improves the performance of 6-bit\nquantized VAR, bringing it on par with the FP16 model. Our accelerator on\nAMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x\nhigher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x\nhigher energy efficiency compared to the integer-based accelerator and GPU\nbaseline, respectively.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 07:47:51", "updated": "2025-05-22 07:47:51", "pdf_url": "http://arxiv.org/pdf/2505.16335v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16351v1", "title": "Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection", "authors": ["Chenxu Guo", "Jiachen Lian", "Xuanru Zhou", "Jinming Zhang", "Shuhe Li", "Zongli Ye", "Hwi Joo Park", "Anaisha Das", "Zoe Ezzes", "Jet Vonk", "Brittany Morin", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "abstract": "Automatic detection of speech dysfluency aids speech-language pathologists in\nefficient transcription of disordered speech, enhancing diagnostics and\ntreatment planning. Traditional methods, often limited to classification,\nprovide insufficient clinical insight, and text-independent models misclassify\ndysfluency, especially in context-dependent cases. This work introduces\nDysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes\nand detects dysfluency. Unlike previous models, Dysfluent-WFST operates with\nupstream encoders like WavLM and requires no additional training. It achieves\nstate-of-the-art performance in both phonetic error rate and dysfluency\ndetection on simulated and real speech data. Our approach is lightweight,\ninterpretable, and effective, demonstrating that explicit modeling of\npronunciation behavior in decoding, rather than complex architectures, is key\nto improving dysfluency processing systems.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-22 08:02:50", "updated": "2025-05-22 08:02:50", "pdf_url": "http://arxiv.org/pdf/2505.16351v1", "comment": null, "doi": null, "journal_ref": "Interspeech 2025"}
{"arxiv_id": "2505.16362v1", "title": "Neuromorphic-based metaheuristics: A new generation of low power, low latency and small footprint optimization algorithms", "authors": ["El-ghazali Talbi"], "abstract": "Neuromorphic computing (NC) introduces a novel algorithmic paradigm\nrepresenting a major shift from traditional digital computing of Von Neumann\narchitectures. NC emulates or simulates the neural dynamics of brains in the\nform of Spiking Neural Networks (SNNs). Much of the research in NC has\nconcentrated on machine learning applications and neuroscience simulations.\nThis paper investigates the modelling and implementation of optimization\nalgorithms and particularly metaheuristics using the NC paradigm as an\nalternative to Von Neumann architectures, leading to breakthroughs in solving\noptimization problems.\n  Neuromorphic-based metaheuristics (Nheuristics) are supposed to be\ncharacterized by low power, low latency and small footprint. Since NC systems\nare fundamentally different from conventional Von Neumann computers, several\nchallenges are posed to the design and implementation of Nheuristics. A\nguideline based on a classification and critical analysis is conducted on the\ndifferent families of metaheuristics and optimization problems they address. We\nalso discuss future directions that need to be addressed to expand both the\ndevelopment and application of Nheuristics.", "categories": ["cs.NE", "cs.AI"], "published": "2025-05-22 08:14:07", "updated": "2025-05-22 08:14:07", "pdf_url": "http://arxiv.org/pdf/2505.16362v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16363v1", "title": "AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training", "authors": ["Huishuai Zhang", "Bohan Wang", "Luoxin Chen"], "abstract": "We introduce AdamS, a simple yet effective alternative to Adam for large\nlanguage model (LLM) pretraining and post-training. By leveraging a novel\ndenominator, i.e., the root of weighted sum of squares of the momentum and the\ncurrent gradient, AdamS eliminates the need for second-moment estimates. Hence,\nAdamS is efficient, matching the memory and compute footprint of SGD with\nmomentum while delivering superior optimization performance. Moreover, AdamS is\neasy to adopt: it can directly inherit hyperparameters of AdamW, and is\nentirely model-agnostic, integrating seamlessly into existing pipelines without\nmodifications to optimizer APIs or architectures. The motivation behind AdamS\nstems from the observed $(L_0, L_1)$ smoothness properties in transformer\nobjectives, where local smoothness is governed by gradient magnitudes that can\nbe further approximated by momentum magnitudes. We establish rigorous\ntheoretical convergence guarantees and provide practical guidelines for\nhyperparameter selection. Empirically, AdamS demonstrates strong performance in\nvarious tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B\nparameters) and reinforcement learning in post-training regimes. With its\nefficiency, simplicity, and theoretical grounding, AdamS stands as a compelling\nalternative to existing optimizers.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-22 08:16:48", "updated": "2025-05-22 08:16:48", "pdf_url": "http://arxiv.org/pdf/2505.16363v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16365v1", "title": "A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules", "authors": ["Manuel Ruiz-Botella", "Marta Sales-Pardo", "Roger Guimer\u00e0"], "abstract": "Developing new molecular compounds is crucial to address pressing challenges,\nfrom health to environmental sustainability. However, exploring the molecular\nspace to discover new molecules is difficult due to the vastness of the space.\nHere we introduce CoCoGraph, a collaborative and constrained graph diffusion\nmodel capable of generating molecules that are guaranteed to be chemically\nvalid. Thanks to the constraints built into the model and to the collaborative\nmechanism, CoCoGraph outperforms state-of-the-art approaches on standard\nbenchmarks while requiring up to an order of magnitude fewer parameters.\nAnalysis of 36 chemical properties also demonstrates that CoCoGraph generates\nmolecules with distributions more closely matching real molecules than current\nmodels. Leveraging the model's efficiency, we created a database of 8.2M\nmillion synthetically generated molecules and conducted a Turing-like test with\norganic chemistry experts to further assess the plausibility of the generated\nmolecules, and potential biases and limitations of CoCoGraph.", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "q-bio.QM"], "published": "2025-05-22 08:21:27", "updated": "2025-05-22 08:21:27", "pdf_url": "http://arxiv.org/pdf/2505.16365v1", "comment": "28 pages, 10 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16368v1", "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning", "authors": ["Huanyu Liu", "Jia Li", "Hao Zhu", "Kechi Zhang", "Yihong Dong", "Ge Li"], "abstract": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLM\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 08:23:10", "updated": "2025-05-22 08:23:10", "pdf_url": "http://arxiv.org/pdf/2505.16368v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16372v1", "title": "Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition", "authors": ["Feng Liu", "Bingyu Nan", "Xuezhong Qian", "Xiaolan Fu"], "abstract": "When emotions are repressed, an individual's true feelings may be revealed\nthrough micro-expressions. Consequently, micro-expressions are regarded as a\ngenuine source of insight into an individual's authentic emotions. However, the\ntransient and highly localised nature of micro-expressions poses a significant\nchallenge to their accurate recognition, with the accuracy rate of\nmicro-expression recognition being as low as 50%, even for professionals. In\norder to address these challenges, it is necessary to explore the field of\ndynamic micro expression recognition (DMER) using multimodal fusion techniques,\nwith special attention to the diverse fusion of temporal and spatial modal\nfeatures. In this paper, we propose a novel Temporal and Spatial feature Fusion\nframework for DMER (TSFmicro). This framework integrates a Retention Network\n(RetNet) and a transformer-based DMER network, with the objective of efficient\nmicro-expression recognition through the capture and fusion of temporal and\nspatial relations. Meanwhile, we propose a novel parallel time-space fusion\nmethod from the perspective of modal fusion, which fuses spatio-temporal\ninformation in high-dimensional feature space, resulting in complementary\n\"where-how\" relationships at the semantic level and providing richer semantic\ninformation for the model. The experimental results demonstrate the superior\nperformance of the TSFmicro method in comparison to other contemporary\nstate-of-the-art methods. This is evidenced by its effectiveness on three\nwell-recognised micro-expression datasets.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 08:26:19", "updated": "2025-05-22 08:26:19", "pdf_url": "http://arxiv.org/pdf/2505.16372v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16376v1", "title": "DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos", "authors": ["Zijia Lu", "A S M Iftekhar", "Gaurav Mittal", "Tianjian Meng", "Xiawei Wang", "Cheng Zhao", "Rohith Kukkala", "Ehsan Elhamifar", "Mei Chen"], "abstract": "Long Video Temporal Grounding (LVTG) aims at identifying specific moments\nwithin lengthy videos based on user-provided text queries for effective content\nretrieval. The approach taken by existing methods of dividing video into clips\nand processing each clip via a full-scale expert encoder is challenging to\nscale due to prohibitive computational costs of processing a large number of\nclips in long videos. To address this issue, we introduce DeCafNet, an approach\nemploying ``delegate-and-conquer'' strategy to achieve computation efficiency\nwithout sacrificing grounding performance. DeCafNet introduces a sidekick\nencoder that performs dense feature extraction over all video clips in a\nresource-efficient manner, while generating a saliency map to identify the most\nrelevant clips for full processing by the expert encoder. To effectively\nleverage features from sidekick and expert encoders that exist at different\ntemporal resolutions, we introduce DeCaf-Grounder, which unifies and refines\nthem via query-aware temporal aggregation and multi-scale temporal refinement\nfor accurate grounding. Experiments on two LTVG benchmark datasets demonstrate\nthat DeCafNet reduces computation by up to 47\\% while still outperforming\nexisting methods, establishing a new state-of-the-art for LTVG in terms of both\nefficiency and performance. Our code is available at\nhttps://github.com/ZijiaLewisLu/CVPR2025-DeCafNet.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 08:29:57", "updated": "2025-05-22 08:29:57", "pdf_url": "http://arxiv.org/pdf/2505.16376v1", "comment": "Accepted by CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16377v1", "title": "VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving", "authors": ["Yansong Qu", "Zilin Huang", "Zihao Sheng", "Jiancong Chen", "Sikai Chen", "Samuel Labi"], "abstract": "Reinforcement learning (RL)-based autonomous driving policy learning faces\ncritical limitations such as low sample efficiency and poor generalization; its\nreliance on online interactions and trial-and-error learning is especially\nunacceptable in safety-critical scenarios. Existing methods including safe RL\noften fail to capture the true semantic meaning of \"safety\" in complex driving\ncontexts, leading to either overly conservative driving behavior or constraint\nviolations. To address these challenges, we propose VL-SAFE, a world\nmodel-based safe RL framework with Vision-Language model\n(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.\nSpecifically, we construct offline datasets containing data collected by expert\nagents and labeled with safety scores derived from VLMs. A world model is\ntrained to generate imagined rollouts together with safety estimations,\nallowing the agent to perform safe planning without interacting with the real\nenvironment. Based on these imagined trajectories and safety evaluations,\nactor-critic learning is conducted under VLM-based safety guidance to optimize\nthe driving policy more safely and efficiently. Extensive evaluations\ndemonstrate that VL-SAFE achieves superior sample efficiency, generalization,\nsafety, and overall performance compared to existing baselines. To the best of\nour knowledge, this is the first work that introduces a VLM-guided world\nmodel-based approach for safe autonomous driving. The demo video and code can\nbe accessed at: https://ys-qu.github.io/vlsafe-website/", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 08:29:59", "updated": "2025-05-22 08:29:59", "pdf_url": "http://arxiv.org/pdf/2505.16377v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16379v1", "title": "Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey", "authors": ["Zhixun Li", "Bin Cao", "Rui Jiao", "Liang Wang", "Ding Wang", "Yang Liu", "Dingshuo Chen", "Jia Li", "Qiang Liu", "Yu Rong", "Liang Wang", "Tong-yi Zhang", "Jeffrey Xu Yu"], "abstract": "Materials are the foundation of modern society, underpinning advancements in\nenergy, electronics, healthcare, transportation, and infrastructure. The\nability to discover and design new materials with tailored properties is\ncritical to solving some of the most pressing global challenges. In recent\nyears, the growing availability of high-quality materials data combined with\nrapid advances in Artificial Intelligence (AI) has opened new opportunities for\naccelerating materials discovery. Data-driven generative models provide a\npowerful tool for materials design by directly create novel materials that\nsatisfy predefined property requirements. Despite the proliferation of related\nwork, there remains a notable lack of up-to-date and systematic surveys in this\narea. To fill this gap, this paper provides a comprehensive overview of recent\nprogress in AI-driven materials generation. We first organize various types of\nmaterials and illustrate multiple representations of crystalline materials. We\nthen provide a detailed summary and taxonomy of current AI-driven materials\ngeneration approaches. Furthermore, we discuss the common evaluation metrics\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future directions and challenges in this fast-growing field. The\nrelated sources can be found at\nhttps://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation.", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "published": "2025-05-22 08:33:21", "updated": "2025-05-22 08:33:21", "pdf_url": "http://arxiv.org/pdf/2505.16379v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16388v1", "title": "Serious Games: Human-AI Interaction, Evolution, and Coevolution", "authors": ["Nandini Doreswamy", "Louise Horstmanshof"], "abstract": "The serious games between humans and AI have only just begun. Evolutionary\nGame Theory (EGT) models the competitive and cooperative strategies of\nbiological entities. EGT could help predict the potential evolutionary\nequilibrium of humans and AI. The objective of this work was to examine some of\nthe EGT models relevant to human-AI interaction, evolution, and coevolution. Of\nthirteen EGT models considered, three were examined: the Hawk-Dove Game,\nIterated Prisoner's Dilemma, and the War of Attrition. This selection was based\non the widespread acceptance and clear relevance of these models to potential\nhuman-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove\nGame predicts balanced mixed-strategy equilibria based on the costs of\nconflict. It also shows the potential for balanced coevolution rather than\ndominance. Iterated Prisoner's Dilemma suggests that repeated interaction may\nlead to cognitive coevolution. It demonstrates how memory and reciprocity can\nlead to cooperation. The War of Attrition suggests that competition for\nresources may result in strategic coevolution, asymmetric equilibria, and\nconventions on sharing resources. Therefore, EGT may provide a suitable\nframework to understand and predict the human-AI evolutionary dynamic. However,\nfuture research could extend beyond EGT and explore additional frameworks,\nempirical validation methods, and interdisciplinary perspectives. AI is being\nshaped by human input and is evolving in response to it. So too,\nneuroplasticity allows the human brain to grow and evolve in response to\nstimuli. If humans and AI converge in future, what might be the result of human\nneuroplasticity combined with an ever-evolving AI? Future research should be\nmindful of the ethical and cognitive implications of human-AI interaction,\nevolution, and coevolution.", "categories": ["cs.AI", "cs.GT", "91A22 (Primary), 68T99 (Secondary)", "J.4; I.2.0; K.4.1; J.3; K.4.0"], "published": "2025-05-22 08:41:37", "updated": "2025-05-22 08:41:37", "pdf_url": "http://arxiv.org/pdf/2505.16388v1", "comment": "8 pages, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16392v1", "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "abstract": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts.", "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "published": "2025-05-22 08:45:14", "updated": "2025-05-22 08:45:14", "pdf_url": "http://arxiv.org/pdf/2505.16392v1", "comment": "Accepted at SIGIR 2025", "doi": "10.1145/3726302.3730304", "journal_ref": null}
{"arxiv_id": "2505.16394v1", "title": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "authors": ["Zhenjie Yang", "Xiaosong Jia", "Qifeng Li", "Xue Yang", "Maoqing Yao", "Junchi Yan"], "abstract": "Reinforcement Learning (RL) can mitigate the causal confusion and\ndistribution shift inherent to imitation learning (IL). However, applying RL to\nend-to-end autonomous driving (E2E-AD) remains an open problem for its training\ndifficulty, and IL is still the mainstream paradigm in both academia and\nindustry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated\npromising results in neural planning; however, these methods typically require\nprivileged information as input rather than raw sensor data. We fill this gap\nby designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently\ntrain an auxiliary privileged world model paired with a neural planner that\nuses privileged information as input. Subsequently, we introduce a raw sensor\nworld model trained via our proposed Guidance Mechanism, which ensures\nconsistency between the raw sensor world model and the privileged world model\nduring rollouts. Finally, the raw sensor world model combines the prior\nknowledge embedded in the heads of the privileged world model to effectively\nguide the training of the raw sensor policy. Raw2Drive is so far the only RL\nbased end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it\nachieves state-of-the-art performance.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-05-22 08:46:53", "updated": "2025-05-22 08:46:53", "pdf_url": "http://arxiv.org/pdf/2505.16394v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16400v1", "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "abstract": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 08:50:47", "updated": "2025-05-22 08:50:47", "pdf_url": "http://arxiv.org/pdf/2505.16400v1", "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16409v1", "title": "FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS", "authors": ["Chaeeun Kim", "Seungone Kim"], "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\nmulti-step reasoning and calling search engines at appropriate steps. However,\nexisting retrieval-augmented reasoning approaches rely on separate retrieval\nmodels, limiting the LRM's role in retrieval to deciding when to retrieve and\nhow to query. This separation not only increases hardware and operational costs\nbut also leads to errors in the retrieval process due to the representation\nbottleneck, a phenomenon where the retriever's embedding space is not\nexpressive enough to meet the generator's requirements. To address this, we\nshift our perspective from sequence-to-sequence matching to locating the\nanswer-containing paths within the corpus, and propose a novel framework called\nFREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables\nLRMs to retrieve relevant knowledge on their own by acting as both a generator\nand retriever. To achieve this, we introduce a variant of the MCTS algorithm\nspecialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing\nMonte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus\ntoward answer-containing regions. Our results on five open-domain QA\nbenchmarks, including single-hop and multi-hop questions, show that FREESON\nachieves an average improvement of 14.4% in EM and F1 over four multi-step\nreasoning models with a separate retriever, and it also performs comparably to\nthe strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.", "categories": ["cs.AI"], "published": "2025-05-22 09:00:08", "updated": "2025-05-22 09:00:08", "pdf_url": "http://arxiv.org/pdf/2505.16409v1", "comment": "Work In Progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16410v1", "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 09:00:19", "updated": "2025-05-22 09:00:19", "pdf_url": "http://arxiv.org/pdf/2505.16410v1", "comment": "Working in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16415v1", "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "abstract": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 09:04:03", "updated": "2025-05-22 09:04:03", "pdf_url": "http://arxiv.org/pdf/2505.16415v1", "comment": "Work in process", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16416v1", "title": "Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models", "authors": ["Chengcheng Wang", "Jianyuan Guo", "Hongguang Li", "Yuchuan Tian", "Ying Nie", "Chang Xu", "Kai Han"], "abstract": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding\nrelative positional information in large language models (LLMs). However, when\nextended to large vision-language models (LVLMs), its variants introduce\nunintended cross-modal positional biases. Specifically, they enforce relative\npositional dependencies between text token indices and image tokens, causing\nspurious alignments. This issue arises because image tokens representing the\nsame content but located at different spatial positions are assigned distinct\npositional biases, leading to inconsistent cross-modal associations. To address\nthis, we propose Per-Token Distance (PTD) - a simple yet effective metric for\nquantifying the independence of positional encodings across modalities.\nInformed by this analysis, we introduce Circle-RoPE, a novel encoding scheme\nthat maps image token indices onto a circular trajectory orthogonal to the\nlinear path of text token indices, forming a cone-like structure. This\nconfiguration ensures that each text token maintains an equal distance to all\nimage tokens, reducing artificial cross-modal biases while preserving\nintra-image spatial information. To further enhance performance, we propose a\nstaggered layer strategy that applies different RoPE variants across layers.\nThis design leverages the complementary strengths of each RoPE variant, thereby\nenhancing the model's overall performance. Our experimental results demonstrate\nthat our method effectively preserves spatial information from images while\nreducing relative positional bias, offering a more robust and flexible\npositional encoding framework for LVLMs. The code is available at\n[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 09:05:01", "updated": "2025-05-22 09:05:01", "pdf_url": "http://arxiv.org/pdf/2505.16416v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16419v1", "title": "Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment", "authors": ["Soh Takahashi", "Masaru Sasaki", "Ken Takeda", "Masafumi Oizumi"], "abstract": "The learning mechanisms by which humans acquire internal representations of\nobjects are not fully understood. Deep neural networks (DNNs) have emerged as a\nuseful tool for investigating this question, as they have internal\nrepresentations similar to those of humans as a byproduct of optimizing their\nobjective functions. While previous studies have shown that models trained with\nvarious learning paradigms - such as supervised, self-supervised, and CLIP -\nacquire human-like representations, it remains unclear whether their similarity\nto human representations is primarily at a coarse category level or extends to\nfiner details. Here, we employ an unsupervised alignment method based on\nGromov-Wasserstein Optimal Transport to compare human and model object\nrepresentations at both fine-grained and coarse-grained levels. The unique\nfeature of this method compared to conventional representational similarity\nanalysis is that it estimates optimal fine-grained mappings between the\nrepresentation of each object in human and model representations. We used this\nunsupervised alignment method to assess the extent to which the representation\nof each object in humans is correctly mapped to the corresponding\nrepresentation of the same object in models. Using human similarity judgments\nof 1,854 objects from the THINGS dataset, we find that models trained with CLIP\nconsistently achieve strong fine- and coarse-grained matching with human object\nrepresentations. In contrast, self-supervised models showed limited matching at\nboth fine- and coarse-grained levels, but still formed object clusters that\nreflected human coarse category structure. Our results offer new insights into\nthe role of linguistic information in acquiring precise object representations\nand the potential of self-supervised learning to capture coarse categorical\nstructures.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 09:06:06", "updated": "2025-05-22 09:06:06", "pdf_url": "http://arxiv.org/pdf/2505.16419v1", "comment": "34 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16425v1", "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "abstract": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 09:10:09", "updated": "2025-05-22 09:10:09", "pdf_url": "http://arxiv.org/pdf/2505.16425v1", "comment": "13 pages, 5 figures, under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16429v1", "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "abstract": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 09:14:23", "updated": "2025-05-22 09:14:23", "pdf_url": "http://arxiv.org/pdf/2505.16429v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16430v1", "title": "AutoMCQ -- Automatically Generate Code Comprehension Questions using GenAI", "authors": ["Martin Goodfellow", "Robbie Booth", "Andrew Fagan", "Alasdair Lambert"], "abstract": "Students often do not fully understand the code they have written. This\nsometimes does not become evident until later in their education, which can\nmean it is harder to fix their incorrect knowledge or misunderstandings. In\naddition, being able to fully understand code is increasingly important in a\nworld where students have access to generative artificial intelligence (GenAI)\ntools, such as GitHub Copilot. One effective solution is to utilise code\ncomprehension questions, where a marker asks questions about a submission to\ngauge understanding, this can also have the side effect of helping to detect\nplagiarism. However, this approach is time consuming and can be difficult\nand/or expensive to scale. This paper introduces AutoMCQ, which uses GenAI for\nthe automatic generation of multiple-choice code comprehension questions. This\nis integrated with the CodeRunner automated assessment platform.", "categories": ["cs.SE", "cs.AI", "cs.PL"], "published": "2025-05-22 09:14:41", "updated": "2025-05-22 09:14:41", "pdf_url": "http://arxiv.org/pdf/2505.16430v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16448v1", "title": "Internal Bias in Reasoning Models leads to Overthinking", "authors": ["Renfei Dang", "Shujian Huang", "Jiajun Chen"], "abstract": "While current reasoning models possess strong exploratory capabilities, they\nare often criticized for overthinking due to redundant and unnecessary\nreflections. In this work, we reveal for the first time that overthinking in\nreasoning models may stem from their internal bias towards input texts. Upon\nencountering a reasoning problem, the model immediately forms a preliminary\nguess about the answer, which we term as an internal bias since it is not\nderived through actual reasoning. When this guess conflicts with its reasoning\nresult, the model tends to engage in reflection, leading to the waste of\ncomputational resources. Through further interpretability experiments, we find\nthat this behavior is largely driven by the model's excessive attention to the\ninput section, which amplifies the influence of internal bias on its\ndecision-making process. Additionally, by masking out the original input\nsection, the affect of internal bias can be effectively alleviated and the\nreasoning length could be reduced by 31%-53% across different complex reasoning\ntasks. Notably, in most cases, this approach also leads to improvements in\naccuracy. These findings demonstrate a causal relationship between internal\nbias and overthinking.", "categories": ["cs.AI"], "published": "2025-05-22 09:35:52", "updated": "2025-05-22 09:35:52", "pdf_url": "http://arxiv.org/pdf/2505.16448v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16452v1", "title": "CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI", "authors": ["Mohamed S. Elmahdy", "Marius Staring", "Patrick J. H. de Koning", "Samer Alabed", "Mahan Salehi", "Faisal Alandejani", "Michael Sharkey", "Ziad Aldabbagh", "Andrew J. Swift", "Rob J. van der Geest"], "abstract": "Accurate and efficient quantification of cardiac function is essential for\nthe estimation of prognosis of cardiovascular diseases (CVDs). One of the most\ncommonly used metrics for evaluating cardiac pumping performance is left\nventricular ejection fraction (LVEF). However, LVEF can be affected by factors\nsuch as inter-observer variability and varying pre-load and after-load\nconditions, which can reduce its reproducibility. Additionally, cardiac\ndysfunction may not always manifest as alterations in LVEF, such as in heart\nfailure and cardiotoxicity diseases. An alternative measure that can provide a\nrelatively load-independent quantitative assessment of myocardial contractility\nis myocardial strain and strain rate. By using LVEF in combination with\nmyocardial strain, it is possible to obtain a thorough description of cardiac\nfunction. Automated estimation of LVEF and other volumetric measures from\ncine-MRI sequences can be achieved through segmentation models, while strain\ncalculation requires the estimation of tissue displacement between sequential\nframes, which can be accomplished using registration models. These tasks are\noften performed separately, potentially limiting the assessment of cardiac\nfunction. To address this issue, in this study we propose an end-to-end deep\nlearning (DL) model that jointly estimates groupwise (GW) registration and\nsegmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep\nGW network was trained and validated on a large dataset of 4-chamber view\ncine-MRI image series of 374 subjects. A quantitative comparison with\nconventional GW registration using elastix and two DL-based methods showed that\nthe proposed model improved performance and substantially reduced computation\ntime.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-22 09:36:42", "updated": "2025-05-22 09:36:42", "pdf_url": "http://arxiv.org/pdf/2505.16452v1", "comment": "15 pages, 7 figures, 1 appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16455v1", "title": "Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events", "authors": ["Mengzhu Liu", "Zhengqiu Zhu", "Chuan Ai", "Chen Gao", "Xinghong Li", "Lingnan He", "Kaisheng Lai", "Yingfeng Chen", "Xin Lu", "Yong Li", "Quanjun Yin"], "abstract": "During sudden disaster events, accurately predicting public panic sentiment\non social media is crucial for proactive governance and crisis management.\nCurrent efforts on this problem face three main challenges: lack of finely\nannotated data hinders emotion prediction studies, unmodeled risk perception\ncauses prediction inaccuracies, and insufficient interpretability of panic\nformation mechanisms. We address these issues by proposing a Psychology-driven\ngenerative Agent framework (PsychoAgent) for explainable panic prediction based\non emotion arousal theory. Specifically, we first construct a fine-grained open\npanic emotion dataset (namely COPE) via human-large language models (LLMs)\ncollaboration to mitigate semantic bias. Then, we develop a framework\nintegrating cross-domain heterogeneous data grounded in psychological\nmechanisms to model risk perception and cognitive differences in emotion\ngeneration. To enhance interpretability, we design an LLM-based role-playing\nagent that simulates individual psychological chains through dedicatedly\ndesigned prompts. Experimental results on our annotated dataset show that\nPsychoAgent improves panic emotion prediction performance by 12.6% to 21.7%\ncompared to baseline models. Furthermore, the explainability and generalization\nof our approach is validated. Crucially, this represents a paradigm shift from\nopaque \"data-driven fitting\" to transparent \"role-based simulation with\nmechanistic interpretation\" for panic emotion prediction during emergencies.\nOur implementation is publicly available at:\nhttps://anonymous.4open.science/r/PsychoAgent-19DD.", "categories": ["cs.AI", "cs.CY"], "published": "2025-05-22 09:39:39", "updated": "2025-05-22 09:39:39", "pdf_url": "http://arxiv.org/pdf/2505.16455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16459v1", "title": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks", "authors": ["Guiyao Tie", "Xueyang Zhou", "Tianhe Gu", "Ruihang Zhang", "Chaoran Hu", "Sizhe Zhang", "Mengqu Sun", "Yan Zhang", "Pan Zhou", "Lichao Sun"], "abstract": "Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled\nunified processing of language, vision, and structured inputs, opening the door\nto complex tasks such as logical deduction, spatial reasoning, and scientific\nanalysis. Despite their promise, the reasoning capabilities of MLLMs,\nparticularly those augmented with intermediate thinking traces (MLLMs-T),\nremain poorly understood and lack standardized evaluation benchmarks. Existing\nwork focuses primarily on perception or final answer correctness, offering\nlimited insight into how models reason or fail across modalities. To address\nthis gap, we introduce the MMMR, a new benchmark designed to rigorously\nevaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a\nhigh-difficulty dataset of 1,083 questions spanning six diverse reasoning types\nwith symbolic depth and multi-hop demands and 2) a modular Reasoning Trace\nEvaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy\nthrough metrics like relevance, consistency, and structured error annotations.\nEmpirical results show that MLLMs-T overall outperform non-thinking\ncounterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro\nsuffer from reasoning pathologies such as inconsistency and overthinking. This\nbenchmark reveals persistent gaps between accuracy and reasoning quality and\nprovides an actionable evaluation pipeline for future model development.\nOverall, the MMMR offers a scalable foundation for evaluating, comparing, and\nimproving the next generation of multi-modal reasoning systems.", "categories": ["cs.AI"], "published": "2025-05-22 09:41:55", "updated": "2025-05-22 09:41:55", "pdf_url": "http://arxiv.org/pdf/2505.16459v1", "comment": "39 pages, 28 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16460v1", "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "abstract": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-22 09:42:11", "updated": "2025-05-22 09:42:11", "pdf_url": "http://arxiv.org/pdf/2505.16460v1", "comment": "16 pages, 13 tables, 1 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16466v1", "title": "Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for GNN-based Recommendation Methods", "authors": ["Meng Yan", "Cai Xu", "Xujing Wang", "Ziyu Guan", "Wei Zhao", "Yuhang Zhou"], "abstract": "Recommender systems based on graph neural networks perform well in tasks such\nas rating and ranking. However, in real-world recommendation scenarios, noise\nsuch as user misuse and malicious advertisement gradually accumulates through\nthe message propagation mechanism. Even if existing studies mitigate their\neffects by reducing the noise propagation weights, the severe sparsity of the\nrecommender system still leads to the low-weighted noisy neighbors being\nmistaken as meaningful information, and the prediction result obtained based on\nthe polluted nodes is not entirely trustworthy. Therefore, it is crucial to\nmeasure the confidence of the prediction results in this highly noisy\nframework. Furthermore, our evaluation of the existing representative GNN-based\nrecommendation shows that it suffers from overconfidence. Based on the above\nconsiderations, we propose a new method to quantify and calibrate the\nprediction confidence of GNN-based recommendations (Conf-GNNRec). Specifically,\nwe propose a rating calibration method that dynamically adjusts excessive\nratings to mitigate overconfidence based on user personalization. We also\ndesign a confidence loss function to reduce the overconfidence of negative\nsamples and effectively improve recommendation performance. Experiments on\npublic datasets demonstrate the validity of Conf-GNNRec in prediction\nconfidence and recommendation performance.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-22 09:48:17", "updated": "2025-05-22 09:48:17", "pdf_url": "http://arxiv.org/pdf/2505.16466v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16475v1", "title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection", "authors": ["Jiaqi Li", "Xinyi Dong", "Yang Liu", "Zhizhuo Yang", "Quansen Wang", "Xiaobo Wang", "SongChun Zhu", "Zixia Jia", "Zilong Zheng"], "abstract": "We present a novel pipeline, ReflectEvo, to demonstrate that small language\nmodels (SLMs) can enhance meta introspection through reflection learning. This\nprocess iteratively generates self-reflection for self-training, fostering a\ncontinuous and self-evolving process. Leveraging this pipeline, we construct\nReflectEvo-460k, a large-scale, comprehensive, self-generated reflection\ndataset with broadened instructions and diverse multi-domain tasks. Building\nupon this dataset, we demonstrate the effectiveness of reflection learning to\nimprove SLMs' reasoning abilities using SFT and DPO with remarkable\nperformance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral\nfrom 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the\nreasoning capability of the three prominent open-sourced models on BIG-bench\nwithout distillation from superior models or fine-grained human annotation. We\nfurther conduct a deeper analysis of the high quality of self-generated\nreflections and their impact on error localization and correction. Our work\nhighlights the potential of continuously enhancing the reasoning performance of\nSLMs through iterative reflection learning in the long run.", "categories": ["cs.AI"], "published": "2025-05-22 10:03:05", "updated": "2025-05-22 10:03:05", "pdf_url": "http://arxiv.org/pdf/2505.16475v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16477v1", "title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery", "authors": ["Yanbo Zhang", "Sumeer A. Khan", "Adnan Mahmud", "Huck Yang", "Alexander Lavin", "Michael Levin", "Jeremy Frey", "Jared Dunnmon", "James Evans", "Alan Bundy", "Saso Dzeroski", "Jesper Tegner", "Hector Zenil"], "abstract": "With recent Nobel Prizes recognising AI contributions to science, Large\nLanguage Models (LLMs) are transforming scientific research by enhancing\nproductivity and reshaping the scientific method. LLMs are now involved in\nexperimental design, data analysis, and workflows, particularly in chemistry\nand biology. However, challenges such as hallucinations and reliability\npersist. In this contribution, we review how Large Language Models (LLMs) are\nredefining the scientific method and explore their potential applications\nacross different stages of the scientific cycle, from hypothesis testing to\ndiscovery. We conclude that, for LLMs to serve as relevant and effective\ncreative engines and productivity enhancers, their deep integration into all\nsteps of the scientific process should be pursued in collaboration and\nalignment with human scientific goals, with clear evaluation metrics. The\ntransition to AI-driven science raises ethical questions about creativity,\noversight, and responsibility. With careful guidance, LLMs could evolve into\ncreative engines, driving transformative breakthroughs across scientific\ndisciplines responsibly and effectively. However, the scientific community must\nalso decide how much it leaves to LLMs to drive science, even when associations\nwith 'reasoning', mostly currently undeserved, are made in exchange for the\npotential to explore hypothesis and solution regions that might otherwise\nremain unexplored by human exploration alone.", "categories": ["cs.AI"], "published": "2025-05-22 10:05:48", "updated": "2025-05-22 10:05:48", "pdf_url": "http://arxiv.org/pdf/2505.16477v1", "comment": "45 pages", "doi": null, "journal_ref": "npj Artificial Intelligence, 2025"}
{"arxiv_id": "2505.16482v1", "title": "Minimizing the energy depletion in wireless rechargeable sensor networks using bi-level metaheuristic charging schemes", "authors": ["Huynh Thi Thanh Binh", "Le Van Cuong", "Dang Hai Dang", "Le Trong Vinh"], "abstract": "Recently, Wireless Rechargeable Sensor Networks (WRSNs) that leveraged the\nadvantage of wireless energy transfer technology have opened a promising\nopportunity in solving the limited energy issue. However, an ineffective\ncharging strategy may reduce the charging performance. Although many practical\ncharging algorithms have been introduced, these studies mainly focus on\noptimizing the charging path with a fully charging approach. This approach may\nlead to the death of a series of sensors due to their extended charging\nlatency. This paper introduces a novel partial charging approach that follows a\nbi-level optimized scheme to minimize energy depletion in WRSNs. We aim at\noptimizing simultaneously two factors: the charging path and time. To\naccomplish this, we first formulate a mathematical model of the investigated\nproblem. We then propose two approximate algorithms in which the optimization\nof the charging path and the charging time are considered as the upper and\nlower level, respectively. The first algorithm combines a Multi-start Local\nSearch method and a Genetic Algorithm to find a solution. The second algorithm\nadopts a nested approach that utilizes the advantages of the Multitasking and\nCovariance Matrix Adaptation Evolutionary Strategies. Experimental validations\non various network scenarios demonstrate that our proposed algorithms\noutperform the existing works.", "categories": ["cs.AI", "cs.NE"], "published": "2025-05-22 10:09:21", "updated": "2025-05-22 10:09:21", "pdf_url": "http://arxiv.org/pdf/2505.16482v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16483v1", "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "abstract": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:10:07", "updated": "2025-05-22 10:10:07", "pdf_url": "http://arxiv.org/pdf/2505.16483v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16491v1", "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "abstract": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:22:39", "updated": "2025-05-22 10:22:39", "pdf_url": "http://arxiv.org/pdf/2505.16491v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16498v1", "title": "Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models", "authors": ["Augusto Luis Ballardini", "Miguel \u00c1ngel Sotelo"], "abstract": "Achieving full automation in self-driving vehicles remains a challenge,\nespecially in dynamic urban environments where navigation requires real-time\nadaptability. Existing systems struggle to handle navigation plans when faced\nwith unpredictable changes in road layouts, spontaneous detours, or missing map\ndata, due to their heavy reliance on predefined cartographic information. In\nthis work, we explore the use of Large Language Models to generate Answer Set\nProgramming rules by translating informal navigation instructions into\nstructured, logic-based reasoning. ASP provides non-monotonic reasoning,\nallowing autonomous vehicles to adapt to evolving scenarios without relying on\npredefined maps. We present an experimental evaluation in which LLMs generate\nASP constraints that encode real-world urban driving logic into a formal\nknowledge representation. By automating the translation of informal navigation\ninstructions into logical rules, our method improves adaptability and\nexplainability in autonomous navigation. Results show that LLM-driven ASP rule\ngeneration supports semantic-based decision-making, offering an explainable\nframework for dynamic navigation planning that aligns closely with how humans\ncommunicate navigational intent.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 10:32:43", "updated": "2025-05-22 10:32:43", "pdf_url": "http://arxiv.org/pdf/2505.16498v1", "comment": "7 pages, 5 figures, submitted for IEEE conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16499v1", "title": "Smaller, Smarter, Closer: The Edge of Collaborative Generative AI", "authors": ["Roberto Morabito", "SiYoung Jang"], "abstract": "The rapid adoption of generative AI (GenAI), particularly Large Language\nModels (LLMs), has exposed critical limitations of cloud-centric deployments,\nincluding latency, cost, and privacy concerns. Meanwhile, Small Language Models\n(SLMs) are emerging as viable alternatives for resource-constrained edge\nenvironments, though they often lack the capabilities of their larger\ncounterparts. This article explores the potential of collaborative inference\nsystems that leverage both edge and cloud resources to address these\nchallenges. By presenting distinct cooperation strategies alongside practical\ndesign principles and experimental insights, we offer actionable guidance for\ndeploying GenAI across the computing continuum.", "categories": ["cs.DC", "cs.AI", "cs.NI"], "published": "2025-05-22 10:34:48", "updated": "2025-05-22 10:34:48", "pdf_url": "http://arxiv.org/pdf/2505.16499v1", "comment": "This paper is currently under review for publication in an IEEE\n  magazine. If accepted, the copyright will be transferred to IEEE", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16505v1", "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "abstract": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-22 10:41:35", "updated": "2025-05-22 10:41:35", "pdf_url": "http://arxiv.org/pdf/2505.16505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16507v1", "title": "Relevance for Stability of Verification Status of a Set of Arguments in Incomplete Argumentation Frameworks (with Proofs)", "authors": ["Anshu Xiong", "Songmao Zhang"], "abstract": "The notion of relevance was proposed for stability of justification status of\na single argument in incomplete argumentation frameworks (IAFs) in 2024 by\nOdekerken et al. To extend the notion, we study the relevance for stability of\nverification status of a set of arguments in this paper, i.e., the\nuncertainties in an IAF that have to be resolved in some situations so that\nanswering whether a given set of arguments is an extension obtains the same\nresult in every completion of the IAF. Further we propose the notion of strong\nrelevance for describing the necessity of resolution in all situations reaching\nstability. An analysis of complexity reveals that detecting the (strong)\nrelevance for stability of sets of arguments can be accomplished in P time\nunder the most semantics discussed in the paper. We also discuss the difficulty\nin finding tractable methods for relevance detection under grounded semantics.", "categories": ["cs.AI"], "published": "2025-05-22 10:42:16", "updated": "2025-05-22 10:42:16", "pdf_url": "http://arxiv.org/pdf/2505.16507v1", "comment": "This is a version of paper 'Relevance for Stability of Verification\n  Status of a Set of Arguments in Incomplete Argumentation Frameworks' extented\n  with proofs of the results in the paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16508v1", "title": "Edge-First Language Model Inference: Models, Metrics, and Tradeoffs", "authors": ["SiYoung Jang", "Roberto Morabito"], "abstract": "The widespread adoption of Language Models (LMs) across industries is driving\ninterest in deploying these services across the computing continuum, from the\ncloud to the network edge. This shift aims to reduce costs, lower latency, and\nimprove reliability and privacy. Small Language Models (SLMs), enabled by\nadvances in model compression, are central to this shift, offering a path to\non-device inference on resource-constrained edge platforms. This work examines\nthe interplay between edge and cloud deployments, starting from detailed\nbenchmarking of SLM capabilities on single edge devices, and extending to\ndistributed edge clusters. We identify scenarios where edge inference offers\ncomparable performance with lower costs, and others where cloud fallback\nbecomes essential due to limits in scalability or model capacity. Rather than\nproposing a one-size-fits-all solution, we present platform-level comparisons\nand design insights for building efficient, adaptive LM inference systems\nacross heterogeneous environments.", "categories": ["cs.DC", "cs.AI", "cs.NI", "cs.PF"], "published": "2025-05-22 10:43:00", "updated": "2025-05-22 10:43:00", "pdf_url": "http://arxiv.org/pdf/2505.16508v1", "comment": "This paper has been accepted for publication and presentation at the\n  45th IEEE International Conference on Distributed Computing Systems (IEEE\n  ICDCS 2025). The copyright will be transferred to IEEE upon publication in\n  the conference proceedings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16512v1", "title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection", "authors": ["Jiaxin Liu", "Jia Wang", "Saihui Hou", "Min Ren", "Huijia Wu", "Zhaofeng He"], "abstract": "In recent years, the rapid development of deepfake technology has given rise\nto an emerging and serious threat to public security: diffusion model-based\ndigital human generation. Unlike traditional face manipulation methods, such\nmodels can generate highly realistic videos with consistency through multimodal\ncontrol signals. Their flexibility and covertness pose severe challenges to\nexisting detection strategies. To bridge this gap, we introduce DigiFakeAV, the\nfirst large-scale multimodal digital human forgery dataset based on diffusion\nmodels. Employing five latest digital human generation methods (Sonic, Hallo,\netc.) and voice cloning method, we systematically produce a dataset comprising\n60,000 videos (8.4 million frames), covering multiple nationalities, skin\ntones, genders, and real-world scenarios, significantly enhancing data\ndiversity and realism. User studies show that the confusion rate between forged\nand real videos reaches 68%, and existing state-of-the-art (SOTA) detection\nmodels exhibit large drops in AUC values on DigiFakeAV, highlighting the\nchallenge of the dataset. To address this problem, we further propose\nDigiShield, a detection baseline based on spatiotemporal and cross-modal\nfusion. By jointly modeling the 3D spatiotemporal features of videos and the\nsemantic-acoustic features of audio, DigiShield achieves SOTA performance on\nboth the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method\neffectively identifies covert artifacts through fine-grained analysis of the\ntemporal evolution of facial features in synthetic videos.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 10:46:37", "updated": "2025-05-22 10:46:37", "pdf_url": "http://arxiv.org/pdf/2505.16512v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16516v1", "title": "Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods", "authors": ["Majid Mohammadi", "Siu Lun Chau", "Krikamol Muandet"], "abstract": "Kernel methods are widely used in machine learning due to their flexibility\nand expressive power. However, their black-box nature poses significant\nchallenges to interpretability, limiting their adoption in high-stakes\napplications. Shapley value-based feature attribution techniques, such as SHAP\nand kernel-specific variants like RKHS-SHAP, offer a promising path toward\nexplainability. Yet, computing exact Shapley values remains computationally\nintractable in general, motivating the development of various approximation\nschemes. In this work, we introduce PKeX-Shapley, a novel algorithm that\nutilizes the multiplicative structure of product kernels to enable the exact\ncomputation of Shapley values in polynomial time. We show that product-kernel\nmodels admit a functional decomposition that allows for a recursive formulation\nof Shapley values. This decomposition not only yields computational efficiency\nbut also enhances interpretability in kernel-based learning. We also\ndemonstrate how our framework can be generalized to explain kernel-based\nstatistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the\nHilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for\ninterpretable statistical inference.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 10:53:04", "updated": "2025-05-22 10:53:04", "pdf_url": "http://arxiv.org/pdf/2505.16516v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16518v1", "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "authors": ["Lovisa Hagstr\u00f6m", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "abstract": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:57:08", "updated": "2025-05-22 10:57:08", "pdf_url": "http://arxiv.org/pdf/2505.16518v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16520v1", "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "abstract": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 11:00:53", "updated": "2025-05-22 11:00:53", "pdf_url": "http://arxiv.org/pdf/2505.16520v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16522v1", "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "abstract": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 11:04:09", "updated": "2025-05-22 11:04:09", "pdf_url": "http://arxiv.org/pdf/2505.16522v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16530v1", "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection", "authors": ["Yuliang Yan", "Haochun Tang", "Shuo Yan", "Enyan Dai"], "abstract": "Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-22 11:16:46", "updated": "2025-05-22 11:16:46", "pdf_url": "http://arxiv.org/pdf/2505.16530v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16540v1", "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation", "authors": ["Inbal Cohen", "Boaz Meivar", "Peihan Tu", "Shai Avidan", "Gal Oren"], "abstract": "Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 11:31:56", "updated": "2025-05-22 11:31:56", "pdf_url": "http://arxiv.org/pdf/2505.16540v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16547v1", "title": "Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation", "authors": ["Nitesh Subedi", "Hsin-Jung Yang", "Devesh K. Jha", "Soumik Sarkar"], "abstract": "This paper presents an end-to-end deep reinforcement learning (RL) framework\nfor occlusion-aware robotic manipulation in cluttered plant environments. Our\napproach enables a robot to interact with a deformable plant to reveal hidden\nobjects of interest, such as fruits, using multimodal observations. We decouple\nthe kinematic planning problem from robot control to simplify zero-shot\nsim2real transfer for the trained policy. Our results demonstrate that the\ntrained policy, deployed using our framework, achieves up to 86.7% success in\nreal-world trials across diverse initial conditions. Our findings pave the way\ntoward autonomous, perception-driven agricultural robots that intelligently\ninteract with complex foliage plants to \"find the fruit\" in challenging\noccluded scenarios, without the need for explicitly designed geometric and\ndynamic models of every plant scenario.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 11:37:39", "updated": "2025-05-22 11:37:39", "pdf_url": "http://arxiv.org/pdf/2505.16547v1", "comment": "18 Pages, 15 Figures, 5 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16561v1", "title": "Auto-nnU-Net: Towards Automated Medical Image Segmentation", "authors": ["Jannis Becktepe", "Leona Hennig", "Steffen Oeltze-Jafra", "Marius Lindauer"], "abstract": "Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ\nsegmentation, each with its own challenges in finding the best segmentation\nmodel. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many\naspects of model configuration but remains constrained by fixed hyperparameters\nand heuristic design choices. As a full-AutoML framework for MIS, we propose\nAuto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization\n(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).\nAdditionally, we propose Regularized PriorBand to balance model accuracy with\nthe computational resources required for training, addressing the resource\nconstraints often faced in real-world medical settings that limit the\nfeasibility of extensive training procedures. We evaluate our approach across\ndiverse MIS datasets from the well-established Medical Segmentation Decathlon,\nanalyzing the impact of AutoML techniques on segmentation performance,\ncomputational efficiency, and model design choices. The results demonstrate\nthat our AutoML approach substantially improves the segmentation performance of\nnnU-Net on 6 out of 10 datasets and is on par on the other datasets while\nmaintaining practical resource requirements. Our code is available at\nhttps://github.com/LUH-AI/AutonnUNet.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 11:52:16", "updated": "2025-05-22 11:52:16", "pdf_url": "http://arxiv.org/pdf/2505.16561v1", "comment": "31 pages, 19 figures. Accepted for publication at AutoML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16567v1", "title": "Finetuning-Activated Backdoors in LLMs", "authors": ["Thibaud Gloaguen", "Mark Vero", "Robin Staab", "Martin Vechev"], "abstract": "Finetuning openly accessible Large Language Models (LLMs) has become standard\npractice for achieving task-specific performance improvements. Until now,\nfinetuning has been regarded as a controlled and secure process in which\ntraining on benign datasets led to predictable behaviors. In this paper, we\ndemonstrate for the first time that an adversary can create poisoned LLMs that\ninitially appear benign but exhibit malicious behaviors once finetuned by\ndownstream users. To this end, our proposed attack, FAB (Finetuning-Activated\nBackdoor), poisons an LLM via meta-learning techniques to simulate downstream\nfinetuning, explicitly optimizing for the emergence of malicious behaviors in\nthe finetuned models. At the same time, the poisoned LLM is regularized to\nretain general capabilities and to exhibit no malicious behaviors prior to\nfinetuning. As a result, when users finetune the seemingly benign model on\ntheir own datasets, they unknowingly trigger its hidden backdoor behavior. We\ndemonstrate the effectiveness of FAB across multiple LLMs and three target\nbehaviors: unsolicited advertising, refusal, and jailbreakability.\nAdditionally, we show that FAB-backdoors are robust to various finetuning\nchoices made by the user (e.g., dataset, number of steps, scheduler). Our\nfindings challenge prevailing assumptions about the security of finetuning,\nrevealing yet another critical attack vector exploiting the complexities of\nLLMs.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "published": "2025-05-22 11:59:44", "updated": "2025-05-22 11:59:44", "pdf_url": "http://arxiv.org/pdf/2505.16567v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16573v1", "title": "From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling", "authors": ["Yi Hu", "Hanchi Ren", "Jingjing Deng", "Xianghua Xie"], "abstract": "Stock price prediction is a critical area of financial forecasting,\ntraditionally approached by training models using the historical price data of\nindividual stocks. While these models effectively capture single-stock\npatterns, they fail to leverage potential correlations among stock trends,\nwhich could improve predictive performance. Current single-stock learning\nmethods are thus limited in their ability to provide a broader understanding of\nprice dynamics across multiple stocks. To address this, we propose a novel\nmethod that merges local patterns into a global understanding through\ncross-stock pattern integration. Our strategy is inspired by Federated Learning\n(FL), a paradigm designed for decentralized model training. FL enables\ncollaborative learning across distributed datasets without sharing raw data,\nfacilitating the aggregation of global insights while preserving data privacy.\nIn our adaptation, we train models on individual stock data and iteratively\nmerge them to create a unified global model. This global model is subsequently\nfine-tuned on specific stock data to retain local relevance. The proposed\nstrategy enables parallel training of individual stock models, facilitating\nefficient utilization of computational resources and reducing overall training\ntime. We conducted extensive experiments to evaluate the proposed method,\ndemonstrating that it outperforms benchmark models and enhances the predictive\ncapabilities of state-of-the-art approaches. Our results highlight the efficacy\nof Cross-Stock Trend Integration (CSTI) in advancing stock price prediction,\noffering a robust alternative to traditional single-stock learning\nmethodologies.", "categories": ["cs.CE", "cs.AI"], "published": "2025-05-22 12:04:10", "updated": "2025-05-22 12:04:10", "pdf_url": "http://arxiv.org/pdf/2505.16573v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16579v1", "title": "Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning", "authors": ["Siqu Ou", "Hongcheng Liu", "Pingjie Wang", "Yusheng Liao", "Chuan Xuan", "Yanfeng Wang", "Yu Wang"], "abstract": "While chains-of-thought (CoT) have advanced complex reasoning in multimodal\nlarge language models (MLLMs), existing methods remain confined to text or\nstatic visual domains, often faltering in dynamic spatial reasoning tasks. To\nbridge this gap, we present GRASSLAND, a novel maze navigation benchmark\ndesigned to evaluate dynamic spatial reasoning. Our experiments show that\naugmenting textual reasoning chains with dynamic visual drafts, overlaid on\ninput images, significantly outperforms conventional approaches, offering new\ninsights into spatial reasoning in evolving environments. To generalize this\ncapability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free\nframework that seamlessly integrates textual CoT with corresponding visual\ndrafts into MLLMs. Extensive evaluations demonstrate that D2R consistently\nenhances performance across diverse tasks, establishing a robust baseline for\ndynamic spatial reasoning without requiring model fine-tuning. Project is open\nat https://github.com/Cratileo/D2R.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-22 12:14:23", "updated": "2025-05-22 12:14:23", "pdf_url": "http://arxiv.org/pdf/2505.16579v1", "comment": "19 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16581v1", "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning", "authors": ["Max Weltevrede", "Moritz A. Zanger", "Matthijs T. J. Spaan", "Wendelin B\u00f6hmer"], "abstract": "In the zero-shot policy transfer setting in reinforcement learning, the goal\nis to train an agent on a fixed set of training environments so that it can\ngeneralise to similar, but unseen, testing environments. Previous work has\nshown that policy distillation after training can sometimes produce a policy\nthat outperforms the original in the testing environments. However, it is not\nyet entirely clear why that is, or what data should be used to distil the\npolicy. In this paper, we prove, under certain assumptions, a generalisation\nbound for policy distillation after training. The theory provides two practical\ninsights: for improved generalisation, you should 1) train an ensemble of\ndistilled policies, and 2) distil it on as much data from the training\nenvironments as possible. We empirically verify that these insights hold in\nmore general settings, when the assumptions required for the theory no longer\nhold. Finally, we demonstrate that an ensemble of policies distilled on a\ndiverse dataset can generalise significantly better than the original agent.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 12:15:52", "updated": "2025-05-22 12:15:52", "pdf_url": "http://arxiv.org/pdf/2505.16581v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16582v1", "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "abstract": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 12:17:13", "updated": "2025-05-22 12:17:13", "pdf_url": "http://arxiv.org/pdf/2505.16582v1", "comment": "25 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16596v1", "title": "Safe Uncertainty-Aware Learning of Robotic Suturing", "authors": ["Wilbert Peter Empleo", "Yitaek Kim", "Hansoul Kim", "Thiusius Rajeeth Savarimuthu", "I\u00f1igo Iturrate"], "abstract": "Robot-Assisted Minimally Invasive Surgery is currently fully manually\ncontrolled by a trained surgeon. Automating this has great potential for\nalleviating issues, e.g., physical strain, highly repetitive tasks, and\nshortages of trained surgeons. For these reasons, recent works have utilized\nArtificial Intelligence methods, which show promising adaptability. Despite\nthese advances, there is skepticism of these methods because they lack\nexplainability and robust safety guarantees. This paper presents a framework\nfor a safe, uncertainty-aware learning method. We train an Ensemble Model of\nDiffusion Policies using expert demonstrations of needle insertion. Using an\nEnsemble model, we can quantify the policy's epistemic uncertainty, which is\nused to determine Out-Of-Distribution scenarios. This allows the system to\nrelease control back to the surgeon in the event of an unsafe scenario.\nAdditionally, we implement a model-free Control Barrier Function to place\nformal safety guarantees on the predicted action. We experimentally evaluate\nour proposed framework using a state-of-the-art robotic suturing simulator. We\nevaluate multiple scenarios, such as dropping the needle, moving the camera,\nand moving the phantom. The learned policy is robust to these perturbations,\nshowing corrective behaviors and generalization, and it is possible to detect\nOut-Of-Distribution scenarios. We further demonstrate that the Control Barrier\nFunction successfully limits the action to remain within our specified safety\nset in the case of unsafe predictions.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-22 12:31:18", "updated": "2025-05-22 12:31:18", "pdf_url": "http://arxiv.org/pdf/2505.16596v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16612v1", "title": "Steering Large Language Models for Machine Translation Personalization", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "abstract": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 12:47:16", "updated": "2025-05-22 12:47:16", "pdf_url": "http://arxiv.org/pdf/2505.16612v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "abstract": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "categories": ["cs.AI", "q-bio.OT", "92", "J.3"], "published": "2025-05-22 12:52:34", "updated": "2025-05-22 12:52:34", "pdf_url": "http://arxiv.org/pdf/2505.16619v1", "comment": "1 PDF, 24 Pages, 2 figures within. Co-corresponding authors:\n  Institute of Applied Biosciences, Centre for Research and Technology Hellas,\n  Thessaloniki, Greece and Department of Biomedical Sciences, University of\n  Padova, Padova, Italy. E-mails: fpsom@certh.gr, silvio.tosatto@unipd.it", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16630v1", "title": "SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding", "authors": ["Sushant Gautam", "Cise Midoglu", "Vajira Thambawita", "Michael A. Riegler", "P\u00e5l Halvorsen", "Mubarak Shah"], "abstract": "The integration of artificial intelligence in sports analytics has\ntransformed soccer video understanding, enabling real-time, automated insights\ninto complex game dynamics. Traditional approaches rely on isolated data\nstreams, limiting their effectiveness in capturing the full context of a match.\nTo address this, we introduce SoccerChat, a multimodal conversational AI\nframework that integrates visual and textual data for enhanced soccer video\ncomprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey\ncolor annotations and automatic speech recognition (ASR) transcripts,\nSoccerChat is fine-tuned on a structured video instruction dataset to\nfacilitate accurate game understanding, event classification, and referee\ndecision making. We benchmark SoccerChat on action classification and referee\ndecision-making tasks, demonstrating its performance in general soccer event\ncomprehension while maintaining competitive accuracy in referee decision\nmaking. Our findings highlight the importance of multimodal integration in\nadvancing soccer analytics, paving the way for more interactive and explainable\nAI-driven sports analysis. https://github.com/simula/SoccerChat", "categories": ["cs.CV", "cs.AI", "68T45, 68T50", "I.2.10; I.2.7; H.5.2"], "published": "2025-05-22 13:01:51", "updated": "2025-05-22 13:01:51", "pdf_url": "http://arxiv.org/pdf/2505.16630v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16637v1", "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "abstract": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 13:08:25", "updated": "2025-05-22 13:08:25", "pdf_url": "http://arxiv.org/pdf/2505.16637v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16640v1", "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization", "authors": ["Xueyang Zhou", "Guiyao Tie", "Guowen Zhang", "Hechang Wang", "Pan Zhou", "Lichao Sun"], "abstract": "Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/.", "categories": ["cs.CR", "cs.AI", "68T07", "I.2.6; I.2.9"], "published": "2025-05-22 13:12:46", "updated": "2025-05-22 13:12:46", "pdf_url": "http://arxiv.org/pdf/2505.16640v1", "comment": "19 pages, 12 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16643v1", "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models", "authors": ["Yiwei Sun", "Peiqi Jiang", "Chuanbin Liu", "Luohao Lin", "Zhiying Lu", "Hongtao Xie"], "abstract": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 13:16:53", "updated": "2025-05-22 13:16:53", "pdf_url": "http://arxiv.org/pdf/2505.16643v1", "comment": "49 pages, 12 figures, 17 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16646v1", "title": "SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving", "authors": ["Yujie Hou", "Ting Zhang", "Mei Wang", "Xuetao Ma", "Hu Huang"], "abstract": "Large Language Models have achieved remarkable results on a variety of\nmathematical benchmarks. However, concerns remain as to whether these successes\nreflect genuine mathematical reasoning or superficial pattern recognition.\nCommon evaluation metrics, such as final answer accuracy, fail to disentangle\nthe underlying competencies involved, offering limited diagnostic value. To\naddress these limitations, we introduce SMART: a Self-Generating and\nSelf-Validating Multi-Dimensional Assessment Framework. SMART decomposes\nmathematical problem solving into four distinct dimensions: understanding,\nreasoning, arithmetic, and reflection \\& refinement. Each dimension is\nevaluated independently through tailored tasks, enabling interpretable and\nfine-grained analysis of LLM behavior. Crucially, SMART integrates an automated\nself-generating and self-validating mechanism to produce and verify benchmark\ndata, ensuring both scalability and reliability. We apply SMART to 21\nstate-of-the-art open- and closed-source LLMs, uncovering significant\ndiscrepancies in their abilities across different dimensions. Our findings\ndemonstrate the inadequacy of final answer accuracy as a sole metric and\nmotivate a new holistic metric to better capture true problem-solving\ncapabilities. Code and benchmarks will be released upon acceptance.", "categories": ["cs.AI"], "published": "2025-05-22 13:18:24", "updated": "2025-05-22 13:18:24", "pdf_url": "http://arxiv.org/pdf/2505.16646v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16647v1", "title": "Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models", "authors": ["Sushant Gautam", "Michael A. Riegler", "P\u00e5l Halvorsen"], "abstract": "We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount.", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.4.8"], "published": "2025-05-22 13:18:44", "updated": "2025-05-22 13:18:44", "pdf_url": "http://arxiv.org/pdf/2505.16647v1", "comment": "Accepted as a full paper at the 38th IEEE International Symposium on\n  Computer-Based Medical Systems (CBMS) 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16648v1", "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "abstract": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:18:45", "updated": "2025-05-22 13:18:45", "pdf_url": "http://arxiv.org/pdf/2505.16648v1", "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16660v1", "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "abstract": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:24:52", "updated": "2025-05-22 13:24:52", "pdf_url": "http://arxiv.org/pdf/2505.16660v1", "comment": "29pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16664v1", "title": "End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries", "authors": ["Khoa Tran", "Tri Le", "Bao Huynh", "Hung-Cuong Trinh", "Vy-Rin Nguyen"], "abstract": "Accurate prediction of the Remaining Useful Life (RUL) is essential for\nenabling timely maintenance of lithium-ion batteries, impacting the operational\nefficiency of electric applications that rely on them. This paper proposes a\nRUL prediction approach that leverages data from recent charge-discharge cycles\nto estimate the number of remaining usable cycles. The approach introduces both\na novel signal processing pipeline and a deep learning prediction model. In the\nsignal preprocessing pipeline, a derived capacity feature is computed based on\ncurrent and capacity signals. Alongside original capacity, voltage and current,\nthese features are denoised and enhanced using statistical metrics and a\ndelta-based method to capture differences between the current and previous\ncycles. In the prediction model, the processed features are then fed into a\nhybrid deep learning architecture composed of 1D Convolutional Neural Networks\n(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential\nEquation-based LSTM (ODE-LSTM) modules. This architecture is designed to\ncapture both local signal characteristics and long-range temporal dependencies\nwhile modeling the continuous-time dynamics of battery degradation. The model\nis further evaluated using transfer learning across different learning\nstrategies and target data partitioning scenarios. Results indicate that the\nmodel maintains robust performance, even when fine-tuned on limited target\ndata. Experimental results on two publicly available large-scale datasets\ndemonstrate that the proposed method outperforms a baseline deep learning\napproach and machine learning techniques, achieving an RMSE of 101.59,\nhighlighting its strong potential for real-world RUL prediction applications.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 13:28:18", "updated": "2025-05-22 13:28:18", "pdf_url": "http://arxiv.org/pdf/2505.16664v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16667v1", "title": "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming", "authors": ["Xinwei Yang", "Zhaofeng Liu", "Chen Huang", "Jiashuai Zhang", "Tong Zhang", "Yifan Zhang", "Wenqiang Lei"], "abstract": "While recent research increasingly emphasizes the value of human-LLM\ncollaboration in competitive programming and proposes numerous empirical\nmethods, a comprehensive understanding remains elusive due to the fragmented\nnature of existing studies and their use of diverse, application-specific human\nfeedback. Thus, our work serves a three-fold purpose: First, we present the\nfirst taxonomy of human feedback consolidating the entire programming process,\nwhich promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a\nnovel programming dataset specifically designed for human-LLM collaboration,\nmeticulously annotated to enable large-scale simulated human feedback and\nfacilitate costeffective real human interaction studies. Third, we introduce\nELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM\ncompetitive programming. With ELABORATION, we pinpoint strengthes and\nweaknesses of existing methods, thereby setting the foundation for future\nimprovement. Our code and dataset are available at\nhttps://github.com/SCUNLP/ELABORATION", "categories": ["cs.AI"], "published": "2025-05-22 13:32:39", "updated": "2025-05-22 13:32:39", "pdf_url": "http://arxiv.org/pdf/2505.16667v1", "comment": "ACL 2025 Main. Our code and dataset are available at\n  https://github.com/SCUNLP/ELABORATION", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16670v1", "title": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models", "authors": ["Xiaobei Yan", "Yiming Li", "Zhaoxin Fan", "Han Qiu", "Tianwei Zhang"], "abstract": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-22 13:36:00", "updated": "2025-05-22 13:36:00", "pdf_url": "http://arxiv.org/pdf/2505.16670v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16673v1", "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "authors": ["Huanjin Yao", "Qixiang Yin", "Jingyi Zhang", "Min Yang", "Yibo Wang", "Wenhao Wu", "Fei Su", "Li Shen", "Minghui Qiu", "Dacheng Tao", "Jiaxing Huang"], "abstract": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-22 13:39:32", "updated": "2025-05-22 13:39:32", "pdf_url": "http://arxiv.org/pdf/2505.16673v1", "comment": "Technical report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16679v1", "title": "Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds", "authors": ["Jordan Dotzel", "Tony Montes", "Mohamed S. Abdelfattah", "Zhiru Zhang"], "abstract": "Traditional methods for 3D object compression operate only on structural\ninformation within the object vertices, polygons, and textures. These methods\nare effective at compression rates up to 10x for standard object sizes but\nquickly deteriorate at higher compression rates with texture artifacts,\nlow-polygon counts, and mesh gaps. In contrast, semantic compression ignores\nstructural information and operates directly on the core concepts to push to\nextreme levels of compression. In addition, it uses natural language as its\nstorage format, which makes it natively human-readable and a natural fit for\nemerging applications built around large-scale, collaborative projects within\naugmented and virtual reality. It deprioritizes structural information like\nlocation, size, and orientation and predicts the missing information with\nstate-of-the-art deep generative models. In this work, we construct a pipeline\nfor 3D semantic compression from public generative models and explore the\nquality-compression frontier for 3D object compression. We apply this pipeline\nto achieve rates as high as 105x for 3D objects taken from the Objaverse\ndataset and show that semantic compression can outperform traditional methods\nin the important quality-preserving region around 100x compression.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 13:45:35", "updated": "2025-05-22 13:45:35", "pdf_url": "http://arxiv.org/pdf/2505.16679v1", "comment": "First two authors have equal contribution", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16686v1", "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge", "authors": ["Lars Benedikt Kaesberg", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "abstract": "Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 13:53:50", "updated": "2025-05-22 13:53:50", "pdf_url": "http://arxiv.org/pdf/2505.16686v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16690v1", "title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator", "authors": ["Beier Luo", "Shuoyuan Wang", "Yixuan Li", "Hongxin Wei"], "abstract": "Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 13:55:39", "updated": "2025-05-22 13:55:39", "pdf_url": "http://arxiv.org/pdf/2505.16690v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16691v1", "title": "EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion", "authors": ["Advait Joglekar", "Divyanshu Singh", "Rooshil Rohit Bhatia", "S. Umesh"], "abstract": "Voice Conversion research in recent times has increasingly focused on\nimproving the zero-shot capabilities of existing methods. Despite remarkable\nadvancements, current architectures still tend to struggle in zero-shot\ncross-lingual settings. They are also often unable to generalize for speakers\nof unseen languages and accents. In this paper, we adopt a simple yet effective\napproach that combines discrete speech representations from self-supervised\nmodels with a non-autoregressive Diffusion-Transformer based conditional flow\nmatching speech decoder. We show that this architecture allows us to train a\nvoice-conversion model in a purely textless, self-supervised fashion. Our\ntechnique works without requiring multiple encoders to disentangle speech\nfeatures. Our model also manages to excel in zero-shot cross-lingual settings\neven for unseen languages.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-22 13:57:02", "updated": "2025-05-22 13:57:02", "pdf_url": "http://arxiv.org/pdf/2505.16691v1", "comment": "Submitted to EMNLP 2025, 7 pages, 2 figures, 5 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16694v1", "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:59:30", "updated": "2025-05-22 13:59:30", "pdf_url": "http://arxiv.org/pdf/2505.16694v1", "comment": "Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16700v1", "title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "authors": ["Xuanqi Gao", "Siyi Xie", "Juan Zhai", "Shqing Ma", "Chao Shen"], "abstract": "As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143.", "categories": ["cs.AI"], "published": "2025-05-22 14:02:37", "updated": "2025-05-22 14:02:37", "pdf_url": "http://arxiv.org/pdf/2505.16700v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16705v1", "title": "An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations", "authors": ["Seonghwan Park", "Jueun Mun", "Donghyun Oh", "Namhoon Lee"], "abstract": "Concept bottleneck models (CBMs) ensure interpretability by decomposing\npredictions into human interpretable concepts. Yet the annotations used for\ntraining CBMs that enable this transparency are often noisy, and the impact of\nsuch corruption is not well understood. In this study, we present the first\nsystematic study of noise in CBMs and show that even moderate corruption\nsimultaneously impairs prediction performance, interpretability, and the\nintervention effectiveness. Our analysis identifies a susceptible subset of\nconcepts whose accuracy declines far more than the average gap between noisy\nand clean supervision and whose corruption accounts for most performance loss.\nTo mitigate this vulnerability we propose a two-stage framework. During\ntraining, sharpness-aware minimization stabilizes the learning of\nnoise-sensitive concepts. During inference, where clean labels are unavailable,\nwe rank concepts by predictive entropy and correct only the most uncertain\nones, using uncertainty as a proxy for susceptibility. Theoretical analysis and\nextensive ablations elucidate why sharpness-aware training confers robustness\nand why uncertainty reliably identifies susceptible concepts, providing a\nprincipled basis that preserves both interpretability and resilience in the\npresence of noise.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 14:06:55", "updated": "2025-05-22 14:06:55", "pdf_url": "http://arxiv.org/pdf/2505.16705v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16710v1", "title": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Daohai Yu", "Rongrong Ji"], "abstract": "While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 14:11:34", "updated": "2025-05-22 14:11:34", "pdf_url": "http://arxiv.org/pdf/2505.16710v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16722v1", "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "abstract": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 14:30:14", "updated": "2025-05-22 14:30:14", "pdf_url": "http://arxiv.org/pdf/2505.16722v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16724v1", "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model", "authors": ["Konstantinos Barmpas", "Na Lee", "Yannis Panagakis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "abstract": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs.", "categories": ["cs.LG", "cs.AI", "cs.HC"], "published": "2025-05-22 14:32:56", "updated": "2025-05-22 14:32:56", "pdf_url": "http://arxiv.org/pdf/2505.16724v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16732v1", "title": "Sequential Monte Carlo for Policy Optimization in Continuous POMDPs", "authors": ["Hany Abdulsamad", "Sahel Iqbal", "Simo S\u00e4rkk\u00e4"], "abstract": "Optimal decision-making under partial observability requires agents to\nbalance reducing uncertainty (exploration) against pursuing immediate\nobjectives (exploitation). In this paper, we introduce a novel policy\noptimization framework for continuous partially observable Markov decision\nprocesses (POMDPs) that explicitly addresses this challenge. Our method casts\npolicy learning as probabilistic inference in a non-Markovian Feynman--Kac\nmodel that inherently captures the value of information gathering by\nanticipating future observations, without requiring extrinsic exploration\nbonuses or handcrafted heuristics. To optimize policies under this model, we\ndevelop a nested sequential Monte Carlo~(SMC) algorithm that efficiently\nestimates a history-dependent policy gradient under samples from the optimal\ntrajectory distribution induced by the POMDP. We demonstrate the effectiveness\nof our algorithm across standard continuous POMDP benchmarks, where existing\nmethods struggle to act under uncertainty.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-22 14:45:46", "updated": "2025-05-22 14:45:46", "pdf_url": "http://arxiv.org/pdf/2505.16732v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16735v1", "title": "Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting", "authors": ["Youngmoon Jung", "Yong-Hyeok Lee", "Myunghun Jung", "Jaeyoung Roh", "Chang Woo Han", "Hoon-Young Cho"], "abstract": "For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic\nand text embeddings are typically compared at either the phoneme or utterance\nlevel. To facilitate this, we optimize acoustic and text encoders using deep\nmetric learning (DML), enabling direct comparison of multi-modal embeddings in\na shared embedding space. However, the inherent heterogeneity between audio and\ntext modalities presents a significant challenge. To address this, we propose\nModality Adversarial Learning (MAL), which reduces the domain gap in\nheterogeneous modality representations. Specifically, we train a modality\nclassifier adversarially to encourage both encoders to generate\nmodality-invariant embeddings. Additionally, we apply DML to achieve\nphoneme-level alignment between audio and text, and conduct comprehensive\ncomparisons across various DML objectives. Experiments on the Wall Street\nJournal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the\nproposed approach.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-22 14:49:46", "updated": "2025-05-22 14:49:46", "pdf_url": "http://arxiv.org/pdf/2505.16735v1", "comment": "5 pages, 1 figures, Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16737v1", "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "abstract": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "published": "2025-05-22 14:52:10", "updated": "2025-05-22 14:52:10", "pdf_url": "http://arxiv.org/pdf/2505.16737v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16740v1", "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP", "authors": ["Alya Zouzou", "L\u00e9o and\u00e9ol", "M\u00e9lanie Ducoffe", "Ryma Boumazouza"], "abstract": "We explore the use of conformal prediction to provide statistical uncertainty\nguarantees for runway detection in vision-based landing systems (VLS). Using\nfine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal\nprediction to quantify localization reliability under user-defined risk levels.\nWe also introduce Conformal mean Average Precision (C-mAP), a novel metric\naligning object detection performance with conformal guarantees. Our results\nshow that conformal prediction can improve the reliability of runway detection\nby quantifying uncertainty in a statistically sound way, increasing safety\non-board and paving the way for certification of ML system in the aerospace\ndomain.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 14:52:59", "updated": "2025-05-22 14:52:59", "pdf_url": "http://arxiv.org/pdf/2505.16740v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16743v1", "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "abstract": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "published": "2025-05-22 14:53:53", "updated": "2025-05-22 14:53:53", "pdf_url": "http://arxiv.org/pdf/2505.16743v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16752v1", "title": "Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation", "authors": ["Hao Guo", "Erpeng Xue", "Lei Huang", "Shichao Wang", "Xiaolei Wang", "Lei Wang", "Jinpeng Wang", "Sheng Chen"], "abstract": "We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream\narchitecture designed for recommendation systems. DFGR integrates innovative\ninteraction patterns between real and fake flows within the QKV modules of the\nself-attention mechanism, enhancing both training and inference efficiency.\nThis approach effectively addresses a key limitation observed in Meta's\nproposed HSTU generative recommendation approach, where heterogeneous\ninformation volumes are mapped into identical vector spaces, leading to\ntraining instability. Unlike traditional recommendation models, DFGR only\nrelies on user history behavior sequences and minimal attribute information,\neliminating the need for extensive manual feature engineering. Comprehensive\nevaluations on open-source and industrial datasets reveal DFGR's superior\nperformance compared to established baselines such as DIN, DCN, DIEN, and\nDeepFM. We also investigate optimal parameter allocation strategies under\ncomputational constraints, establishing DFGR as an efficient and effective\nnext-generation generate ranking paradigm.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-22 14:58:53", "updated": "2025-05-22 14:58:53", "pdf_url": "http://arxiv.org/pdf/2505.16752v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16765v1", "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques", "authors": ["Jianing Geng", "Biao Yi", "Zekun Fei", "Tongxi Wu", "Lihai Nie", "Zheli Liu"], "abstract": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-22 15:07:34", "updated": "2025-05-22 15:07:34", "pdf_url": "http://arxiv.org/pdf/2505.16765v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16771v1", "title": "Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review", "authors": ["Beyazit Bestami Yuksel", "Ayse Yilmazer Metin"], "abstract": "This paper presents a comprehensive synthesis of major breakthroughs in\nartificial intelligence (AI) over the past fifteen years, integrating\nhistorical, theoretical, and technological perspectives. It identifies key\ninflection points in AI' s evolution by tracing the convergence of\ncomputational resources, data access, and algorithmic innovation. The analysis\nhighlights how researchers enabled GPU based model training, triggered a data\ncentric shift with ImageNet, simplified architectures through the Transformer,\nand expanded modeling capabilities with the GPT series. Rather than treating\nthese advances as isolated milestones, the paper frames them as indicators of\ndeeper paradigm shifts. By applying concepts from statistical learning theory\nsuch as sample complexity and data efficiency, the paper explains how\nresearchers translated breakthroughs into scalable solutions and why the field\nmust now embrace data centric approaches. In response to rising privacy\nconcerns and tightening regulations, the paper evaluates emerging solutions\nlike federated learning, privacy enhancing technologies (PETs), and the data\nsite paradigm, which reframe data access and security. In cases where real\nworld data remains inaccessible, the paper also assesses the utility and\nconstraints of mock and synthetic data generation. By aligning technical\ninsights with evolving data infrastructure, this study offers strategic\nguidance for future AI research and policy development.", "categories": ["cs.AI"], "published": "2025-05-22 15:12:48", "updated": "2025-05-22 15:12:48", "pdf_url": "http://arxiv.org/pdf/2505.16771v1", "comment": "10 pages, 6 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16773v1", "title": "Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis", "authors": ["Iv\u00e1n Matas", "Carmen Serrano", "Miguel Nogales", "David Moreno", "Lara Ferr\u00e1ndiz", "Teresa Ojeda", "Bego\u00f1a Acha"], "abstract": "Deep learning has transformed computer vision but relies heavily on large\nlabeled datasets and computational resources. Transfer learning, particularly\nfine-tuning pretrained models, offers a practical alternative; however, models\npretrained on natural image datasets such as ImageNet may fail to capture\ndomain-specific characteristics in medical imaging. This study introduces an\nunsupervised learning framework that extracts high-value dermatological\nfeatures instead of relying solely on ImageNet-based pretraining. We employ a\nVariational Autoencoder (VAE) trained from scratch on a proprietary\ndermatological dataset, allowing the model to learn a structured and clinically\nrelevant latent space. This self-supervised feature extractor is then compared\nto an ImageNet-pretrained backbone under identical classification conditions,\nhighlighting the trade-offs between general-purpose and domain-specific\npretraining. Our results reveal distinct learning patterns. The self-supervised\nmodel achieves a final validation loss of 0.110 (-33.33%), while the\nImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.\nAccuracy trends confirm this: the self-supervised model improves from 45% to\n65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained\nmodel reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting\ngap increasing to +0.060. These findings suggest that while ImageNet\npretraining accelerates convergence, it also amplifies overfitting on\nnon-clinically relevant features. In contrast, self-supervised learning\nachieves steady improvements, stronger generalization, and superior\nadaptability, underscoring the importance of domain-specific feature extraction\nin medical imaging.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 15:15:17", "updated": "2025-05-22 15:15:17", "pdf_url": "http://arxiv.org/pdf/2505.16773v1", "comment": "6 pages, 2 tables, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16781v1", "title": "Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making", "authors": ["Qianlei Jia", "Xinliang Zhou", "Ondrej Krejcar", "Enrique Herrera-Viedma"], "abstract": "In group decision-making (GDM) scenarios, uncertainty, dynamic social\nstructures, and vague information present major challenges for traditional\nopinion dynamics models. To address these issues, this study proposes a novel\nsocial network group decision-making (SNGDM) framework that integrates\nthree-way decision (3WD) theory, dynamic network reconstruction, and linguistic\nopinion representation. First, the 3WD mechanism is introduced to explicitly\nmodel hesitation and ambiguity in agent judgments, thereby preventing\nirrational decisions. Second, a connection adjustment rule based on opinion\nsimilarity is developed, enabling agents to adaptively update their\ncommunication links and better reflect the evolving nature of social\nrelationships. Third, linguistic terms are used to describe agent opinions,\nallowing the model to handle subjective, vague, or incomplete information more\neffectively. Finally, an integrated multi-agent decision-making framework is\nconstructed, which simultaneously considers individual uncertainty, opinion\nevolution, and network dynamics. The proposed model is applied to a multi-UAV\ncooperative decision-making scenario, where simulation results and consensus\nanalysis demonstrate its effectiveness. Experimental comparisons further verify\nthe advantages of the algorithm in enhancing system stability and representing\nrealistic decision-making behaviors.", "categories": ["cs.AI"], "published": "2025-05-22 15:26:48", "updated": "2025-05-22 15:26:48", "pdf_url": "http://arxiv.org/pdf/2505.16781v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16785v1", "title": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models", "authors": ["Zhenzhen Ren", "GuoBiao Li", "Sheng Li", "Zhenxing Qian", "Xinpeng Zhang"], "abstract": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-22 15:28:25", "updated": "2025-05-22 15:28:25", "pdf_url": "http://arxiv.org/pdf/2505.16785v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16787v1", "title": "Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce", "authors": ["Ashish Sundar", "Chunbo Luo", "Xiaoyang Wang"], "abstract": "Model-based reinforcement learning (MBRL) offers an intuitive way to increase\nthe sample efficiency of model-free RL methods by simultaneously training a\nworld model that learns to predict the future. MBRL methods have progressed by\nlargely prioritising the actor; optimising the world model learning has been\nneglected meanwhile. Improving the fidelity of the world model and reducing its\ntime to convergence can yield significant downstream benefits, one of which is\nimproving the ensuing performance of any actor it may train. We propose a novel\napproach that anticipates and actively seeks out high-entropy states using\nshort-horizon latent predictions generated by the world model, offering a\nprincipled alternative to traditional curiosity-driven methods that chase\nonce-novel states well after they were stumbled into. While many model\npredictive control (MPC) based methods offer similar alternatives, they\ntypically lack commitment, synthesising multi step plans after every step. To\nmitigate this, we present a hierarchical planner that dynamically decides when\nto replan, planning horizon length, and the weighting between reward and\nentropy. While our method can theoretically be applied to any model that trains\nits own actors with solely model generated data, we have applied it to just\nDreamer as a proof of concept. Our method finishes the Miniworld procedurally\ngenerated mazes 50% faster than base Dreamer at convergence and the policy\ntrained in imagination converges in only 60% of the environment steps that base\nDreamer needs.", "categories": ["cs.AI"], "published": "2025-05-22 15:28:50", "updated": "2025-05-22 15:28:50", "pdf_url": "http://arxiv.org/pdf/2505.16787v1", "comment": "9 pages without appendix, 15 Figures, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16789v1", "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "abstract": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 15:30:00", "updated": "2025-05-22 15:30:00", "pdf_url": "http://arxiv.org/pdf/2505.16789v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16790v1", "title": "Learning Flexible Forward Trajectories for Masked Molecular Diffusion", "authors": ["Hyunjin Seo", "Taewon Kim", "Sihyun Yu", "SungSoo Ahn"], "abstract": "Masked diffusion models (MDMs) have achieved notable progress in modeling\ndiscrete data, while their potential in molecular generation remains\nunderexplored. In this work, we explore their potential and introduce the\nsurprising result that naively applying standards MDMs severely degrades the\nperformance. We identify the critical cause of this issue as a state-clashing\nproblem-where the forward diffusion of distinct molecules collapse into a\ncommon state, resulting in a mixture of reconstruction targets that cannot be\nlearned using typical reverse diffusion process with unimodal predictions. To\nmitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that\norchestrates per-element corruption trajectories to avoid collision between\ndistinct molecular graphs. This is achieved through a parameterized noise\nscheduling network that assigns distinct corruption rates to individual graph\nelements, i.e., atoms and bonds. Extensive experiments on diverse molecular\nbenchmarks reveal that MELD markedly enhances overall generation quality\ncompared to element-agnostic noise scheduling, increasing the chemical validity\nof vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves\nstate-of-the-art property alignment in conditional generation tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 15:30:17", "updated": "2025-05-22 15:30:17", "pdf_url": "http://arxiv.org/pdf/2505.16790v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16791v1", "title": "Cohort-Based Active Modality Acquisition", "authors": ["Tillmann Rheude", "Roland Eils", "Benjamin Wild"], "abstract": "Real-world machine learning applications often involve data from multiple\nmodalities that must be integrated effectively to make robust predictions.\nHowever, in many practical settings, not all modalities are available for every\nsample, and acquiring additional modalities can be costly. This raises the\nquestion: which samples should be prioritized for additional modality\nacquisition when resources are limited? While prior work has explored\nindividual-level acquisition strategies and training-time active learning\nparadigms, test-time and cohort-based acquisition remain underexplored despite\ntheir importance in many real-world settings. We introduce Cohort-based Active\nModality Acquisition (CAMA), a novel test-time setting to formalize the\nchallenge of selecting which samples should receive additional modalities. We\nderive acquisition strategies that leverage a combination of generative\nimputation and discriminative modeling to estimate the expected benefit of\nacquiring missing modalities based on common evaluation metrics. We also\nintroduce upper-bound heuristics that provide performance ceilings to benchmark\nacquisition strategies. Experiments on common multimodal datasets demonstrate\nthat our proposed imputation-based strategies can more effectively guide the\nacquisition of new samples in comparison to those relying solely on unimodal\ninformation, entropy guidance, and random selections. Our work provides an\neffective solution for optimizing modality acquisition at the cohort level,\nenabling better utilization of resources in constrained settings.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 15:32:50", "updated": "2025-05-22 15:32:50", "pdf_url": "http://arxiv.org/pdf/2505.16791v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16792v1", "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training", "authors": ["Ziqiao Wang", "Wangbo Zhao", "Yuhao Zhou", "Zekai Li", "Zhiyuan Liang", "Mingjia Shi", "Xuanlei Zhao", "Pengfei Zhou", "Kaipeng Zhang", "Zhangyang Wang", "Kai Wang", "Yang You"], "abstract": "Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet\ntheir training remains notoriously slow. A recent remedy -- representation\nalignment (REPA) that matches DiT hidden features to those of a non-generative\nteacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus\nor even degrades performance later. We trace this failure to a capacity\nmismatch: once the generative student begins modelling the joint data\ndistribution, the teacher's lower-dimensional embeddings and attention patterns\nbecome a straitjacket rather than a guide. We then introduce HASTE (Holistic\nAlignment with Stage-wise Termination for Efficient training), a two-phase\nschedule that keeps the help and drops the hindrance. Phase I applies a\nholistic alignment loss that simultaneously distills attention maps (relational\npriors) and feature projections (semantic anchors) from the teacher into\nmid-level layers of the DiT, yielding rapid convergence. Phase II then performs\none-shot termination that deactivates the alignment loss, once a simple trigger\nsuch as a fixed iteration is hit, freeing the DiT to focus on denoising and\nexploit its generative capacity. HASTE speeds up training of diverse DiTs\nwithout architecture changes. On ImageNet 256X256, it reaches the vanilla\nSiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,\namounting to a 28X reduction in optimization steps. HASTE also improves\ntext-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled\nrecipe for efficient diffusion training across various tasks. Our code is\navailable at https://github.com/NUS-HPC-AI-Lab/HASTE .", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 15:34:33", "updated": "2025-05-22 15:34:33", "pdf_url": "http://arxiv.org/pdf/2505.16792v1", "comment": "24 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16798v1", "title": "SEED: Speaker Embedding Enhancement Diffusion Model", "authors": ["KiHyun Nam", "Jungwoo Heo", "Jee-weon Jung", "Gangin Park", "Chaeyoung Jung", "Ha-Jin Yu", "Joon Son Chung"], "abstract": "A primary challenge when deploying speaker recognition systems in real-world\napplications is performance degradation caused by environmental mismatch. We\npropose a diffusion-based method that takes speaker embeddings extracted from a\npre-trained speaker recognition model and generates refined embeddings. For\ntraining, our approach progressively adds Gaussian noise to both clean and\nnoisy speaker embeddings extracted from clean and noisy speech, respectively,\nvia forward process of a diffusion model, and then reconstructs them to clean\nembeddings in the reverse process. While inferencing, all embeddings are\nregenerated via diffusion process. Our method needs neither speaker label nor\nany modification to the existing speaker recognition pipeline. Experiments on\nevaluation sets simulating environment mismatch scenarios show that our method\ncan improve recognition accuracy by up to 19.6% over baseline models while\nretaining performance on conventional scenarios. We publish our code here\nhttps://github.com/kaistmm/seed-pytorch", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-22 15:38:37", "updated": "2025-05-22 15:38:37", "pdf_url": "http://arxiv.org/pdf/2505.16798v1", "comment": "Accepted to Interspeech 2025. The official code can be found at\n  https://github.com/kaistmm/seed-pytorch", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16801v1", "title": "A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents", "authors": ["Eleftherios Kalafatis", "Konstantinos Mitsis", "Konstantia Zarkogianni", "Maria Athanasiou", "Konstantina Nikita"], "abstract": "Serious Games (SGs) are nowadays shifting focus to include procedural content\ngeneration (PCG) in the development process as a means of offering personalized\nand enhanced player experience. However, the development of a framework to\nassess the impact of PCG techniques when integrated into SGs remains\nparticularly challenging. This study proposes a methodology for automated\nevaluation of PCG integration in SGs, incorporating deep reinforcement learning\n(DRL) game testing agents. To validate the proposed framework, a previously\nintroduced SG featuring card game mechanics and incorporating three different\nversions of PCG for nonplayer character (NPC) creation has been deployed.\nVersion 1 features random NPC creation, while versions 2 and 3 utilize a\ngenetic algorithm approach. These versions are used to test the impact of\ndifferent dynamic SG environments on the proposed framework's agents. The\nobtained results highlight the superiority of the DRL game testing agents\ntrained on Versions 2 and 3 over those trained on Version 1 in terms of win\nrate (i.e. number of wins per played games) and training time. More\nspecifically, within the execution of a test emulating regular gameplay, both\nVersions 2 and 3 peaked at a 97% win rate and achieved statistically\nsignificant higher (p=0009) win rates compared to those achieved in Version 1\nthat peaked at 94%. Overall, results advocate towards the proposed framework's\ncapability to produce meaningful data for the evaluation of procedurally\ngenerated content in SGs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 15:40:56", "updated": "2025-05-22 15:40:56", "pdf_url": "http://arxiv.org/pdf/2505.16801v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16813v1", "title": "Dynamic Reservoir Computing with Physical Neuromorphic Networks", "authors": ["Yinhao Xu", "Georg A. Gottwald", "Zdenka Kuncic"], "abstract": "Reservoir Computing (RC) with physical systems requires an understanding of\nthe underlying structure and internal dynamics of the specific physical\nreservoir. In this study, physical nano-electronic networks with neuromorphic\ndynamics are investigated for their use as physical reservoirs in an RC\nframework. These neuromorphic networks operate as dynamic reservoirs, with node\nactivities in general coupled to the edge dynamics through nonlinear\nnano-electronic circuit elements, and the reservoir outputs influenced by the\nunderlying network connectivity structure. This study finds that networks with\nvarying degrees of sparsity generate more useful nonlinear temporal outputs for\ndynamic RC compared to dense networks. Dynamic RC is also tested on an\nautonomous multivariate chaotic time series prediction task with networks of\nvarying densities, which revealed the importance of network sparsity in\nmaintaining network activity and overall dynamics, that in turn enabled the\nlearning of the chaotic Lorenz63 system's attractor behavior.", "categories": ["cs.ET", "cond-mat.dis-nn", "cs.AI"], "published": "2025-05-22 15:50:45", "updated": "2025-05-22 15:50:45", "pdf_url": "http://arxiv.org/pdf/2505.16813v1", "comment": "8 pages, 8 figures, IJCNN 2025, accepted", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16826v1", "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning", "authors": ["Wei Sun", "Wen Yang", "Pu Jian", "Qianlong Du", "Fuwei Cui", "Shuo Ren", "Jiajun Zhang"], "abstract": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 16:00:33", "updated": "2025-05-22 16:00:33", "pdf_url": "http://arxiv.org/pdf/2505.16826v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16827v1", "title": "GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent", "authors": ["Bin Xie", "Rui Shao", "Gongwei Chen", "Kaiwen Zhou", "Yinchuan Li", "Jie Liu", "Min Zhang", "Liqiang Nie"], "abstract": "GUI automation faces critical challenges in dynamic environments. MLLMs\nsuffer from two key issues: misinterpreting UI components and outdated\nknowledge. Traditional fine-tuning methods are costly for app-specific\nknowledge updates. We propose GUI-explorer, a training-free GUI agent that\nincorporates two fundamental mechanisms: (1) Autonomous Exploration of\nFunction-aware Trajectory. To comprehensively cover all application\nfunctionalities, we design a Function-aware Task Goal Generator that\nautomatically constructs exploration goals by analyzing GUI structural\ninformation (e.g., screenshots and activity hierarchies). This enables\nsystematic exploration to collect diverse trajectories. (2) Unsupervised Mining\nof Transition-aware Knowledge. To establish precise screen-operation logic, we\ndevelop a Transition-aware Knowledge Extractor that extracts effective\nscreen-operation logic through unsupervised analysis the state transition of\nstructured interaction triples (observation, action, outcome). This eliminates\nthe need for human involvement in knowledge extraction. With a task success\nrate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows\nsignificant improvements over SOTA agents. It requires no parameter updates for\nnew apps. GUI-explorer is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/GUI-explorer.", "categories": ["cs.AI"], "published": "2025-05-22 16:01:06", "updated": "2025-05-22 16:01:06", "pdf_url": "http://arxiv.org/pdf/2505.16827v1", "comment": "ACL 2025. Github: https://github.com/JiuTian-VL/GUI-explorer", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16831v1", "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "abstract": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "published": "2025-05-22 16:02:10", "updated": "2025-05-22 16:02:10", "pdf_url": "http://arxiv.org/pdf/2505.16831v1", "comment": "44 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16832v1", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "abstract": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "published": "2025-05-22 16:02:18", "updated": "2025-05-22 16:02:18", "pdf_url": "http://arxiv.org/pdf/2505.16832v1", "comment": "16 pages; 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16834v1", "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "abstract": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-22 16:05:02", "updated": "2025-05-22 16:05:02", "pdf_url": "http://arxiv.org/pdf/2505.16834v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16836v1", "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning", "authors": ["Fanrui Zhang", "Dian Li", "Qiang Zhang", "Chenjun", "sinbadliu", "Junxiong Lin", "Jiahong Yan", "Jiawei Liu", "Zheng-Jun Zha"], "abstract": "The rapid spread of multimodal misinformation on social media has raised\ngrowing concerns, while research on video misinformation detection remains\nlimited due to the lack of large-scale, diverse datasets. Existing methods\noften overfit to rigid templates and lack deep reasoning over deceptive\ncontent. To address these challenges, we introduce FakeVV, a large-scale\nbenchmark comprising over 100,000 video-text pairs with fine-grained,\ninterpretable annotations. In addition, we further propose Fact-R1, a novel\nframework that integrates deep reasoning with collaborative rule-based\nreinforcement learning. Fact-R1 is trained through a three-stage process: (1)\nmisinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference\nalignment via Direct Preference Optimization (DPO), and (3) Group Relative\nPolicy Optimization (GRPO) using a novel verifiable reward function. This\nenables Fact-R1 to exhibit emergent reasoning behaviors comparable to those\nobserved in advanced text-based reinforcement learning systems, but in the more\ncomplex multimodal misinformation setting. Our work establishes a new paradigm\nfor misinformation detection, bridging large-scale video understanding,\nreasoning-guided alignment, and interpretable verification.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 16:05:06", "updated": "2025-05-22 16:05:06", "pdf_url": "http://arxiv.org/pdf/2505.16836v1", "comment": "28 pages, 27 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16845v1", "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate", "authors": ["Hanglei Zhang", "Yiwei Guo", "Zhihan Li", "Xiang Hao", "Xie Chen", "Kai Yu"], "abstract": "Most neural speech codecs achieve bitrate adjustment through intra-frame\nmechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However,\nspeech segments inherently have time-varying information density (e.g., silent\nintervals versus voiced regions). This property makes CFR not optimal in terms\nof bitrate and token sequence length, hindering efficiency in real-time\napplications. In this work, we propose a Temporally Flexible Coding (TFC)\ntechnique, introducing variable frame rate (VFR) into neural speech codecs for\nthe first time. TFC enables seamlessly tunable average frame rates and\ndynamically allocates frame rates based on temporal entropy. Experimental\nresults show that a codec with TFC achieves optimal reconstruction quality with\nhigh flexibility, and maintains competitive performance even at lower frame\nrates. Our approach is promising for the integration with other efforts to\ndevelop low-frame-rate neural speech codecs for more efficient downstream\ntasks.", "categories": ["eess.AS", "cs.AI", "cs.SD"], "published": "2025-05-22 16:10:01", "updated": "2025-05-22 16:10:01", "pdf_url": "http://arxiv.org/pdf/2505.16845v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16854v1", "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "authors": ["Jiaqi Wang", "Kevin Qinghong Lin", "James Cheng", "Mike Zheng Shou"], "abstract": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-22 16:13:29", "updated": "2025-05-22 16:13:29", "pdf_url": "http://arxiv.org/pdf/2505.16854v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16856v1", "title": "Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only", "authors": ["Wei Xiao", "Jiacheng Liu", "Zifeng Zhuang", "Runze Suo", "Shangke Lyu", "Donglin Wang"], "abstract": "Improving the performance of pre-trained policies through online\nreinforcement learning (RL) is a critical yet challenging topic. Existing\nonline RL fine-tuning methods require continued training with offline\npretrained Q-functions for stability and performance. However, these offline\npretrained Q-functions commonly underestimate state-action pairs beyond the\noffline dataset due to the conservatism in most offline RL methods, which\nhinders further exploration when transitioning from the offline to the online\nsetting. Additionally, this requirement limits their applicability in scenarios\nwhere only pre-trained policies are available but pre-trained Q-functions are\nabsent, such as in imitation learning (IL) pre-training. To address these\nchallenges, we propose a method for efficient online RL fine-tuning using\nsolely the offline pre-trained policy, eliminating reliance on pre-trained\nQ-functions. We introduce PORL (Policy-Only Reinforcement Learning\nFine-Tuning), which rapidly initializes the Q-function from scratch during the\nonline phase to avoid detrimental pessimism. Our method not only achieves\ncompetitive performance with advanced offline-to-online RL algorithms and\nonline RL approaches that leverage data or policies prior, but also pioneers a\nnew path for directly fine-tuning behavior cloning (BC) policies.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-22 16:14:08", "updated": "2025-05-22 16:14:08", "pdf_url": "http://arxiv.org/pdf/2505.16856v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16860v1", "title": "GCAL: Adapting Graph Models to Evolving Domain Shifts", "authors": ["Ziyue Qiao", "Qianyi Cai", "Hao Dong", "Jiawei Gu", "Pengyang Wang", "Meng Xiao", "Xiao Luo", "Hui Xiong"], "abstract": "This paper addresses the challenge of graph domain adaptation on evolving,\nmultiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation\nmethods are confined to single-step adaptation, making them ineffective in\nhandling continuous domain shifts and prone to catastrophic forgetting. This\npaper introduces the Graph Continual Adaptive Learning (GCAL) method, designed\nto enhance model sustainability and adaptability across various graph domains.\nGCAL employs a bilevel optimization strategy. The \"adapt\" phase uses an\ninformation maximization approach to fine-tune the model with new graph domains\nwhile re-adapting past memories to mitigate forgetting. Concurrently, the\n\"generate memory\" phase, guided by a theoretical lower bound derived from\ninformation bottleneck theory, involves a variational memory graph generation\nmodule to condense original graphs into memories. Extensive experimental\nevaluations demonstrate that GCAL substantially outperforms existing methods in\nterms of adaptability and knowledge retention.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 16:19:19", "updated": "2025-05-22 16:19:19", "pdf_url": "http://arxiv.org/pdf/2505.16860v1", "comment": "Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16875v1", "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training", "authors": ["Zhehao Huang", "Yuhang Liu", "Yixin Lou", "Zhengbao He", "Mingzhen He", "Wenxing Zhou", "Tao Li", "Kehan Li", "Zeyi Huang", "Xiaolin Huang"], "abstract": "Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-22 16:31:43", "updated": "2025-05-22 16:31:43", "pdf_url": "http://arxiv.org/pdf/2505.16875v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16877v1", "title": "Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings", "authors": ["Yuqicheng Zhu", "Daniel Hern\u00e1ndez", "Yuan He", "Zifeng Ding", "Bo Xiong", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is\ncrucial for ensuring the reliability of downstream applications. A recent work\napplies conformal prediction to KGE methods, providing uncertainty estimates by\ngenerating a set of answers that is guaranteed to include the true answer with\na predefined confidence level. However, existing methods provide probabilistic\nguarantees averaged over a reference set of queries and answers (marginal\ncoverage guarantee). In high-stakes applications such as medical diagnosis, a\nstronger guarantee is often required: the predicted sets must provide\nconsistent coverage per query (conditional coverage guarantee). We propose\nCondKGCP, a novel method that approximates predicate-conditional coverage\nguarantees while maintaining compact prediction sets. CondKGCP merges\npredicates with similar vector representations and augments calibration with\nrank information. We prove the theoretical guarantees and demonstrate empirical\neffectiveness of CondKGCP by comprehensive evaluations.", "categories": ["cs.AI"], "published": "2025-05-22 16:33:20", "updated": "2025-05-22 16:33:20", "pdf_url": "http://arxiv.org/pdf/2505.16877v1", "comment": "Accepted to the Finding of ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16881v1", "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "abstract": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 16:35:33", "updated": "2025-05-22 16:35:33", "pdf_url": "http://arxiv.org/pdf/2505.16881v1", "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16886v1", "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "abstract": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 16:41:37", "updated": "2025-05-22 16:41:37", "pdf_url": "http://arxiv.org/pdf/2505.16886v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16888v1", "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "authors": ["Viet Pham", "Thai Le"], "abstract": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-22 16:47:15", "updated": "2025-05-22 16:47:15", "pdf_url": "http://arxiv.org/pdf/2505.16888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16896v1", "title": "Structure-Aligned Protein Language Model", "authors": ["Can Chen", "David Heurtel-Depeiges", "Robert M. Vernon", "Christopher James Langmead", "Yoshua Bengio", "Quentin Fournier"], "abstract": "Protein language models (pLMs) pre-trained on vast protein sequence databases\nexcel at various downstream tasks but lack the structural knowledge essential\nfor many biological applications. To address this, we integrate structural\ninsights from pre-trained protein graph neural networks (pGNNs) into pLMs\nthrough a latent-level contrastive learning task. This task aligns residue\nrepresentations from pLMs with those from pGNNs across multiple proteins,\nenriching pLMs with inter-protein structural knowledge. Additionally, we\nincorporate a physical-level task that infuses intra-protein structural\nknowledge by optimizing pLMs to predict structural tokens. The proposed\ndual-task framework effectively incorporates both inter-protein and\nintra-protein structural knowledge into pLMs. Given the variability in the\nquality of protein structures in PDB, we further introduce a residue loss\nselection module, which uses a small model trained on high-quality structures\nto select reliable yet challenging residue losses for the pLM to learn.\nApplying our structure alignment method to the state-of-the-art ESM2 and\nAMPLIFY results in notable performance gains across a wide range of tasks,\nincluding a 12.7% increase in ESM2 contact prediction. The data, code, and\nresulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 16:56:12", "updated": "2025-05-22 16:56:12", "pdf_url": "http://arxiv.org/pdf/2505.16896v1", "comment": "16 pages, 8 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16899v1", "title": "Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships", "authors": ["Kerem Oktar", "Katherine M. Collins", "Jose Hernandez-Orallo", "Diane Coyle", "Stephen Cave", "Adrian Weller", "Ilia Sucholutsky"], "abstract": "Artificial Intelligence (AI) systems have historically been used as tools\nthat execute narrowly defined tasks. Yet recent advances in AI have unlocked\npossibilities for a new class of models that genuinely collaborate with humans\nin complex reasoning, from conceptualizing problems to brainstorming solutions.\nSuch AI thought partners enable novel forms of collaboration and extended\ncognition, yet they also pose major risks-including and beyond risks of typical\nAI tools and agents. In this commentary, we systematically identify risks of AI\nthought partners through a novel framework that identifies risks at multiple\nlevels of analysis, including Real-time, Individual, and Societal risks arising\nfrom collaborative cognition (RISc). We leverage this framework to propose\nconcrete metrics for risk evaluation, and finally suggest specific mitigation\nstrategies for developers and policymakers. As AI thought partners continue to\nproliferate, these strategies can help prevent major harms and ensure that\nhumans actively benefit from productive thought partnerships.", "categories": ["cs.AI"], "published": "2025-05-22 16:58:48", "updated": "2025-05-22 16:58:48", "pdf_url": "http://arxiv.org/pdf/2505.16899v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16911v1", "title": "Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation", "authors": ["Ofir Yaish", "Yehuda Mishaly", "Eliya Nachmani"], "abstract": "We introduce a new paradigm for active sound modification: Active Speech\nEnhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on\nsuppressing external interference, ASE goes further by actively shaping the\nspeech signal -- both attenuating unwanted noise components and amplifying\nspeech-relevant frequencies -- to improve intelligibility and perceptual\nquality. To enable this, we propose a novel Transformer-Mamba-based\narchitecture, along with a task-specific loss function designed to jointly\noptimize interference suppression and signal enrichment. Our method outperforms\nexisting baselines across multiple speech processing tasks -- including\ndenoising, dereverberation, and declipping -- demonstrating the effectiveness\nof active, targeted modulation in challenging acoustic environments.", "categories": ["eess.AS", "cs.AI"], "published": "2025-05-22 17:10:18", "updated": "2025-05-22 17:10:18", "pdf_url": "http://arxiv.org/pdf/2505.16911v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16915v1", "title": "DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?", "authors": ["Qirui Jiao", "Daoyuan Chen", "Yilun Huang", "Xika Lin", "Ying Shen", "Yaliang Li"], "abstract": "While recent text-to-image (T2I) models show impressive capabilities in\nsynthesizing images from brief descriptions, their performance significantly\ndegrades when confronted with long, detail-intensive prompts required in\nprofessional applications. We present DetailMaster, the first comprehensive\nbenchmark specifically designed to evaluate T2I models' systematical abilities\nto handle extended textual inputs that contain complex compositional\nrequirements. Our benchmark introduces four critical evaluation dimensions:\nCharacter Attributes, Structured Character Locations, Multi-Dimensional Scene\nAttributes, and Explicit Spatial/Interactive Relationships. The benchmark\ncomprises long and detail-rich prompts averaging 284.89 tokens, with high\nquality validated by expert annotators. Evaluation on 7 general-purpose and 5\nlong-prompt-optimized T2I models reveals critical performance limitations:\nstate-of-the-art models achieve merely ~50% accuracy in key dimensions like\nattribute binding and spatial reasoning, while all models showing progressive\nperformance degradation as prompt length increases. Our analysis highlights\nsystemic failures in structural comprehension and detail overload handling,\nmotivating future research into architectures with enhanced compositional\nreasoning. We open-source the dataset, data curation code, and evaluation tools\nto advance detail-rich T2I generation and enable broad applications that would\notherwise be infeasible due to the lack of a dedicated benchmark.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 17:11:27", "updated": "2025-05-22 17:11:27", "pdf_url": "http://arxiv.org/pdf/2505.16915v1", "comment": "22 pages, 8 figures, 10 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16927v1", "title": "Latent Principle Discovery for Language Model Self-Improvement", "authors": ["Keshav Ramji", "Tahira Naseem", "Ram\u00f3n Fernandez Astudillo"], "abstract": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 17:20:18", "updated": "2025-05-22 17:20:18", "pdf_url": "http://arxiv.org/pdf/2505.16927v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16928v1", "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning", "authors": ["Bosung Kim", "Prithviraj Ammanabrolu"], "abstract": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.", "categories": ["cs.AI", "cs.LG", "cs.RO"], "published": "2025-05-22 17:20:38", "updated": "2025-05-22 17:20:38", "pdf_url": "http://arxiv.org/pdf/2505.16928v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16932v1", "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "abstract": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "published": "2025-05-22 17:23:14", "updated": "2025-05-22 17:23:14", "pdf_url": "http://arxiv.org/pdf/2505.16932v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16938v1", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.", "categories": ["cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-22 17:27:43", "updated": "2025-05-22 17:27:43", "pdf_url": "http://arxiv.org/pdf/2505.16938v1", "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16941v1", "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "abstract": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-22 17:29:52", "updated": "2025-05-22 17:29:52", "pdf_url": "http://arxiv.org/pdf/2505.16941v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16944v1", "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 17:31:10", "updated": "2025-05-22 17:31:10", "pdf_url": "http://arxiv.org/pdf/2505.16944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16947v1", "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs", "authors": ["Csaba D\u00e9k\u00e1ny", "Stefan Balauca", "Robin Staab", "Dimitar I. Dimitrov", "Martin Vechev"], "abstract": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT.", "categories": ["cs.LG", "cs.AI", "I.2.7; K.4.1"], "published": "2025-05-22 17:32:50", "updated": "2025-05-22 17:32:50", "pdf_url": "http://arxiv.org/pdf/2505.16947v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16950v1", "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning", "authors": ["Adnan Oomerjee", "Zafeirios Fountas", "Zhongwei Yu", "Haitham Bou-Ammar", "Jun Wang"], "abstract": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "published": "2025-05-22 17:33:49", "updated": "2025-05-22 17:33:49", "pdf_url": "http://arxiv.org/pdf/2505.16950v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16957v1", "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models", "authors": ["Junjie Xiong", "Changjia Zhu", "Shuhang Lin", "Chong Zhang", "Yongfeng Zhang", "Yao Liu", "Lingyao Li"], "abstract": "Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-22 17:36:33", "updated": "2025-05-22 17:36:33", "pdf_url": "http://arxiv.org/pdf/2505.16957v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16965v1", "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "abstract": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 17:46:23", "updated": "2025-05-22 17:46:23", "pdf_url": "http://arxiv.org/pdf/2505.16965v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16967v1", "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval", "authors": ["Nandan Thakur", "Crystina Zhang", "Xueguang Ma", "Jimmy Lin"], "abstract": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "published": "2025-05-22 17:47:57", "updated": "2025-05-22 17:47:57", "pdf_url": "http://arxiv.org/pdf/2505.16967v1", "comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16968v1", "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "abstract": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "published": "2025-05-22 17:48:53", "updated": "2025-05-22 17:48:53", "pdf_url": "http://arxiv.org/pdf/2505.16968v1", "comment": "20 pages, 11 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16978v1", "title": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation", "authors": ["Weizhi Tang", "Yixuan Li", "Chris Sypherd", "Elizabeth Polgreen", "Vaishak Belle"], "abstract": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs.", "categories": ["cs.AI", "cs.PL"], "published": "2025-05-22 17:52:31", "updated": "2025-05-22 17:52:31", "pdf_url": "http://arxiv.org/pdf/2505.16978v1", "comment": "Accepted to ACL 2025 Findings. Code available at\n  https://github.com/RutaTang/HyGenar", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16979v1", "title": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design", "authors": ["Zhenkun Li", "Lingyao Li", "Shuhang Lin", "Yongfeng Zhang"], "abstract": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-22 17:52:33", "updated": "2025-05-22 17:52:33", "pdf_url": "http://arxiv.org/pdf/2505.16979v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "abstract": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "categories": ["cs.AI", "physics.med-ph"], "published": "2025-05-22 17:52:59", "updated": "2025-05-22 17:52:59", "pdf_url": "http://arxiv.org/pdf/2505.16982v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16985v1", "title": "Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation", "authors": ["Moru Liu", "Hao Dong", "Jessica Kelly", "Olga Fink", "Mario Trapp"], "abstract": "Out-of-distribution (OOD) detection and segmentation are crucial for\ndeploying machine learning models in safety-critical applications such as\nautonomous driving and robot-assisted surgery. While prior research has\nprimarily focused on unimodal image data, real-world applications are\ninherently multimodal, requiring the integration of multiple modalities for\nimproved OOD detection. A key challenge is the lack of supervision signals from\nunknown data, leading to overconfident predictions on OOD samples. To address\nthis challenge, we propose Feature Mixing, an extremely simple and fast method\nfor multimodal outlier synthesis with theoretical support, which can be further\noptimized to help the model better distinguish between in-distribution (ID) and\nOOD data. Feature Mixing is modality-agnostic and applicable to various\nmodality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal\ndataset for OOD segmentation, featuring synthetic OOD objects across diverse\nscenes and weather conditions. Extensive experiments on SemanticKITTI,\nnuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that\nFeature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370\n\\times$ speedup. Our source code and dataset will be available at\nhttps://github.com/mona4399/FeatureMixing.", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "published": "2025-05-22 17:54:30", "updated": "2025-05-22 17:54:30", "pdf_url": "http://arxiv.org/pdf/2505.16985v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16986v1", "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 17:54:32", "updated": "2025-05-22 17:54:32", "pdf_url": "http://arxiv.org/pdf/2505.16986v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16988v1", "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-22 17:54:38", "updated": "2025-05-22 17:54:38", "pdf_url": "http://arxiv.org/pdf/2505.16988v1", "comment": "18 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16994v1", "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning", "authors": ["Runyang You", "Yongqi Li", "Xinyu Lin", "Xin Zhang", "Wenjie Wang", "Wenjie Li", "Liqiang Nie"], "abstract": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "published": "2025-05-22 17:55:43", "updated": "2025-05-22 17:55:43", "pdf_url": "http://arxiv.org/pdf/2505.16994v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16997v1", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.", "categories": ["cs.AI", "cs.CL", "cs.MA"], "published": "2025-05-22 17:56:39", "updated": "2025-05-22 17:56:39", "pdf_url": "http://arxiv.org/pdf/2505.16997v1", "comment": "19 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16998v1", "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "abstract": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 17:57:23", "updated": "2025-05-22 17:57:23", "pdf_url": "http://arxiv.org/pdf/2505.16998v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17002v1", "title": "PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association", "authors": ["Abdul Hannan", "Muhammad Arslan Manzoor", "Shah Nawaz", "Muhammad Irzam Liaqat", "Markus Schedl", "Mubashir Noman"], "abstract": "We study the task of learning association between faces and voices, which is\ngaining interest in the multimodal community lately. These methods suffer from\nthe deliberate crafting of negative mining procedures as well as the reliance\non the distant margin parameter. These issues are addressed by learning a joint\nembedding space in which orthogonality constraints are applied to the fused\nembeddings of faces and voices. However, embedding spaces of faces and voices\npossess different characteristics and require spaces to be aligned before\nfusing them. To this end, we propose a method that accurately aligns the\nembedding spaces and fuses them with an enhanced gated fusion thereby improving\nthe performance of face-voice association. Extensive experiments on the\nVoxCeleb dataset reveals the merits of the proposed approach.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 17:57:55", "updated": "2025-05-22 17:57:55", "pdf_url": "http://arxiv.org/pdf/2505.17002v1", "comment": "Accepted at InterSpeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17004v1", "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs", "authors": ["Jiachen Yao", "Abbas Mammadov", "Julius Berner", "Gavin Kerrigan", "Jong Chul Ye", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "abstract": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "stat.ML"], "published": "2025-05-22 17:58:12", "updated": "2025-05-22 17:58:12", "pdf_url": "http://arxiv.org/pdf/2505.17004v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17005v1", "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "abstract": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-22 17:58:26", "updated": "2025-05-22 17:58:26", "pdf_url": "http://arxiv.org/pdf/2505.17005v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17010v1", "title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "authors": ["Tim Genewein", "Kevin Wenliang Li", "Jordi Grau-Moya", "Anian Ruoss", "Laurent Orseau", "Marcus Hutter"], "abstract": "Prompting is one of the main ways to adapt a pretrained model to target\ntasks. Besides manually constructing prompts, many prompt optimization methods\nhave been proposed in the literature. Method development is mainly empirically\ndriven, with less emphasis on a conceptual understanding of prompting. In this\npaper we discuss how optimal prompting can be understood through a Bayesian\nview, which also implies some fundamental limitations of prompting that can\nonly be overcome by tuning weights. The paper explains in detail how\nmeta-trained neural networks behave as Bayesian predictors over the pretraining\ndistribution, whose hallmark feature is rapid in-context adaptation. Optimal\nprompting can be studied formally as conditioning these Bayesian predictors,\nyielding criteria for target tasks where optimal prompting is and is not\npossible. We support the theory with educational experiments on LSTMs and\nTransformers, where we compare different versions of prefix-tuning and\ndifferent weight-tuning methods. We also confirm that soft prefixes, which are\nsequences of real-valued vectors outside the token alphabet, can lead to very\neffective prompts for trained and even untrained networks by manipulating\nactivations in ways that are not achievable by hard tokens. This adds an\nimportant mechanistic aspect beyond the conceptual Bayesian theory.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-22 17:58:53", "updated": "2025-05-22 17:58:53", "pdf_url": "http://arxiv.org/pdf/2505.17010v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17012v1", "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding", "authors": ["Haoning Wu", "Xiao Huang", "Yaohui Chen", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "abstract": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-22 17:59:03", "updated": "2025-05-22 17:59:03", "pdf_url": "http://arxiv.org/pdf/2505.17012v1", "comment": "Technical Report; Project Page:\n  https://haoningwu3639.github.io/SpatialScore", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17016v1", "title": "Interactive Post-Training for Vision-Language-Action Models", "authors": ["Shuhan Tan", "Kairan Dou", "Yue Zhao", "Philipp Kr\u00e4henb\u00fchl"], "abstract": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "published": "2025-05-22 17:59:45", "updated": "2025-05-22 17:59:45", "pdf_url": "http://arxiv.org/pdf/2505.17016v1", "comment": "Project page: https://ariostgx.github.io/ript_vla/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17017v1", "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "abstract": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 17:59:49", "updated": "2025-05-22 17:59:49", "pdf_url": "http://arxiv.org/pdf/2505.17017v1", "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17019v1", "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework", "authors": ["Chenhao Zhang", "Yazhe Niu"], "abstract": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.", "categories": ["cs.CV", "cs.AI", "cs.CY"], "published": "2025-05-22 17:59:53", "updated": "2025-05-22 17:59:53", "pdf_url": "http://arxiv.org/pdf/2505.17019v1", "comment": "16 pages, 9 figures. Code & Dataset:\n  https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17022v1", "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "abstract": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "published": "2025-05-22 17:59:58", "updated": "2025-05-22 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.17022v1", "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16086v1", "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "abstract": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 00:00:27", "updated": "2025-05-22 00:00:27", "pdf_url": "http://arxiv.org/pdf/2505.16086v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16088v1", "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "abstract": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday).", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 00:06:29", "updated": "2025-05-22 00:06:29", "pdf_url": "http://arxiv.org/pdf/2505.16088v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16090v1", "title": "Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance", "authors": ["Dominick Kubica", "Dylan T. Gordon", "Nanami Emura", "Derleen Saini", "Charlie Goldenberg"], "abstract": "As of 2025, Generative Artificial Intelligence (GenAI) has become a central\ntool for productivity across industries. Beyond text generation, GenAI now\nplays a critical role in coding, data analysis, and research workflows. As\nlarge language models (LLMs) continue to evolve, it is essential to assess the\nreliability and accuracy of their outputs, especially in specialized,\nhigh-stakes domains like finance. Most modern LLMs transform text into\nnumerical vectors, which are used in operations such as cosine similarity\nsearches to generate responses. However, this abstraction process can lead to\nmisinterpretation of emotional tone, particularly in nuanced financial\ncontexts. While LLMs generally excel at identifying sentiment in everyday\nlanguage, these models often struggle with the nuanced, strategically ambiguous\nlanguage found in earnings call transcripts. Financial disclosures frequently\nembed sentiment in hedged statements, forward-looking language, and\nindustry-specific jargon, making it difficult even for human analysts to\ninterpret consistently, let alone AI models. This paper presents findings from\nthe Santa Clara Microsoft Practicum Project, led by Professor Charlie\nGoldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's\nChatGPT, Google's Gemini, and traditional machine learning models for sentiment\nanalysis of financial text. Using Microsoft earnings call transcripts, the\nanalysis assesses how well LLM-derived sentiment correlates with market\nsentiment and stock movements and evaluates the accuracy of model outputs.\nPrompt engineering techniques are also examined to improve sentiment analysis\nresults. Visualizations of sentiment consistency are developed to evaluate\nalignment between tone and stock performance, with sentiment trends analyzed\nacross Microsoft's lines of business to determine which segments exert the\ngreatest influence.", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.7"], "published": "2025-05-22 00:09:11", "updated": "2025-05-22 00:09:11", "pdf_url": "http://arxiv.org/pdf/2505.16090v1", "comment": "6 pages, 4 figures. Research conducted as part of a\n  Microsoft-sponsored Capstone Project at Santa Clara University", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16094v1", "title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization", "authors": ["Ziqing Wang", "Kexin Zhang", "Zihan Zhao", "Yibo Wen", "Abhishek Pandey", "Han Liu", "Kaize Ding"], "abstract": "Large language models (LLMs) are introducing a paradigm shift in molecular\ndiscovery by enabling text-guided interaction with chemical spaces through\nnatural language, symbolic notations, with emerging extensions to incorporate\nmulti-modal inputs. To advance the new field of LLM for molecular discovery,\nthis survey provides an up-to-date and forward-looking review of the emerging\nuse of LLMs for two central tasks: molecule generation and molecule\noptimization. Based on our proposed taxonomy for both problems, we analyze\nrepresentative techniques in each category, highlighting how LLM capabilities\nare leveraged across different learning settings. In addition, we include the\ncommonly used datasets and evaluation protocols. We conclude by discussing key\nchallenges and future directions, positioning this survey as a resource for\nresearchers working at the intersection of LLMs and molecular science. A\ncontinuously updated reading list is available at\nhttps://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-22 00:26:27", "updated": "2025-05-22 00:26:27", "pdf_url": "http://arxiv.org/pdf/2505.16094v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16100v1", "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "abstract": "Validating scientific hypotheses is a central challenge in biomedical\nresearch, and remains difficult for artificial intelligence (AI) agents due to\nthe complexity of real-world data analysis and evidence interpretation. In this\nwork, we present BioDSA-1K, a benchmark designed to evaluate AI agents on\nrealistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K\nconsists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,\ncurated from over 300 published biomedical studies to reflect the structure and\nreasoning found in authentic research workflows. Each task includes a\nstructured hypothesis derived from the original study's conclusions, expressed\nin the affirmative to reflect the language of scientific reporting, and one or\nmore pieces of supporting evidence grounded in empirical data tables. While\nthese hypotheses mirror published claims, they remain testable using standard\nstatistical or machine learning methods. The benchmark enables evaluation along\nfour axes: (1) hypothesis decision accuracy, (2) alignment between evidence and\nconclusion, (3) correctness of the reasoning process, and (4) executability of\nthe AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable\nhypotheses: cases where the available data are insufficient to support or\nrefute a claim, reflecting a common yet underexplored scenario in real-world\nscience. We propose BioDSA-1K as a foundation for building and evaluating\ngeneralizable, trustworthy AI agents for biomedical discovery.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 01:02:21", "updated": "2025-05-22 01:02:21", "pdf_url": "http://arxiv.org/pdf/2505.16100v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16102v1", "title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "authors": ["Yash Kumar Atri", "Thomas H Shin", "Thomas Hartvigsen"], "abstract": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses.", "categories": ["cs.CL"], "published": "2025-05-22 01:02:51", "updated": "2025-05-22 01:02:51", "pdf_url": "http://arxiv.org/pdf/2505.16102v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16104v1", "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "abstract": "With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning.", "categories": ["cs.CL", "cs.CV", "cs.LG"], "published": "2025-05-22 01:06:28", "updated": "2025-05-22 01:06:28", "pdf_url": "http://arxiv.org/pdf/2505.16104v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16107v1", "title": "MPL: Multiple Programming Languages with Large Language Models for Information Extraction", "authors": ["Bo Li", "Gexiang Fang", "Wei Ye", "Zhenghua Xu", "Jinglei Zhang", "Hao Cheng", "Shikun Zhang"], "abstract": "Recent research in information extraction (IE) focuses on utilizing\ncode-style inputs to enhance structured output generation. The intuition behind\nthis is that the programming languages (PLs) inherently exhibit greater\nstructural organization than natural languages (NLs). This structural advantage\nmakes PLs particularly suited for IE tasks. Nevertheless, existing research\nprimarily focuses on Python for code-style simulation, overlooking the\npotential of other widely-used PLs (e.g., C++ and Java) during the supervised\nfine-tuning (SFT) phase. In this research, we propose \\textbf{M}ultiple\n\\textbf{P}rogramming \\textbf{L}anguages with large language models for\ninformation extraction (abbreviated as \\textbf{MPL}), a novel framework that\nexplores the potential of incorporating different PLs in the SFT phase.\nAdditionally, we introduce \\texttt{function-prompt} with virtual running to\nsimulate code-style inputs more effectively and efficiently. Experimental\nresults on a wide range of datasets demonstrate the effectiveness of MPL.\nFurthermore, we conduct extensive experiments to provide a comprehensive\nanalysis. We have released our code for future research.", "categories": ["cs.CL"], "published": "2025-05-22 01:28:23", "updated": "2025-05-22 01:28:23", "pdf_url": "http://arxiv.org/pdf/2505.16107v1", "comment": "Findings of ACL2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16118v1", "title": "Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics", "authors": ["Haotian Lan", "Yao Gao", "Yujun Cheng", "Wei Yuan", "Kun Wang"], "abstract": "Social media's rise establishes user-generated content (UGC) as pivotal for\ntravel decisions, yet analytical methods lack scalability. This study\nintroduces a dual-method LLM framework: unsupervised expectation extraction\nfrom UGC paired with survey-informed supervised fine-tuning. Findings reveal\nleisure/social expectations drive engagement more than foundational\nnatural/emotional factors. By establishing LLMs as precision tools for\nexpectation quantification, we advance tourism analytics methodology and\npropose targeted strategies for experience personalization and social travel\npromotion. The framework's adaptability extends to consumer behavior research,\ndemonstrating computational social science's transformative potential in\nmarketing optimization.", "categories": ["cs.CL", "stat.AP"], "published": "2025-05-22 01:52:01", "updated": "2025-05-22 01:52:01", "pdf_url": "http://arxiv.org/pdf/2505.16118v1", "comment": "33 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16125v1", "title": "KoBALT: Korean Benchmark For Advanced Linguistic Tasks", "authors": ["Hyopil Shin", "Sangah Lee", "Dongjun Jang", "Wooseok Song", "Jaeyoon Kim", "Chaeyoung Oh", "Hyemi Jo", "Youngchae Ahn", "Sihyun Oh", "Hyohyeong Chang", "Sunkyoung Kim", "Jinsik Lee"], "abstract": "We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a\ncomprehensive linguistically-motivated benchmark comprising 700 multiple-choice\nquestions spanning 24 phenomena across five linguistic domains: syntax,\nsemantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed\nto advance the evaluation of large language models (LLMs) in Korean, a\nmorphologically rich language, by addressing the limitations of conventional\nbenchmarks that often lack linguistic depth and typological grounding. It\nintroduces a suite of expert-curated, linguistically motivated questions with\nminimal n-gram overlap with standard Korean corpora, substantially mitigating\nthe risk of data contamination and allowing a more robust assessment of true\nlanguage understanding. Our evaluation of 20 contemporary LLMs reveals\nsignificant performance disparities, with the highest-performing model\nachieving 61\\% general accuracy but showing substantial variation across\nlinguistic domains - from stronger performance in semantics (66\\%) to\nconsiderable weaknesses in phonology (31\\%) and morphology (36\\%). Through\nhuman preference evaluation with 95 annotators, we demonstrate a strong\ncorrelation between KoBALT scores and human judgments, validating our\nbenchmark's effectiveness as a discriminative measure of Korean language\nunderstanding. KoBALT addresses critical gaps in linguistic evaluation for\ntypologically diverse languages and provides a robust framework for assessing\ngenuine linguistic competence in Korean language models.", "categories": ["cs.CL"], "published": "2025-05-22 02:03:07", "updated": "2025-05-22 02:03:07", "pdf_url": "http://arxiv.org/pdf/2505.16125v1", "comment": "Under Reveiw", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16128v1", "title": "Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning", "authors": ["Yue Zhou", "Barbara Di Eugenio"], "abstract": "Despite LLMs' explicit alignment against demographic stereotypes, they have\nbeen shown to exhibit biases under various social contexts. In this work, we\nfind that LLMs exhibit concerning biases in how they associate solution\nveracity with demographics. Through experiments across five human value-aligned\nLLMs on mathematics, coding, commonsense, and writing problems, we reveal two\nforms of such veracity biases: Attribution Bias, where models\ndisproportionately attribute correct solutions to certain demographic groups,\nand Evaluation Bias, where models' assessment of identical solutions varies\nbased on perceived demographic authorship. Our results show pervasive biases:\nLLMs consistently attribute fewer correct solutions and more incorrect ones to\nAfrican-American groups in math and coding, while Asian authorships are least\npreferred in writing evaluation. In additional studies, we show LLMs\nautomatically assign racially stereotypical colors to demographic groups in\nvisualization code, suggesting these biases are deeply embedded in models'\nreasoning processes. Our findings indicate that demographic bias extends beyond\nsurface-level stereotypes and social context provocations, raising concerns\nabout LLMs' deployment in educational and evaluation settings.", "categories": ["cs.CL"], "published": "2025-05-22 02:13:48", "updated": "2025-05-22 02:13:48", "pdf_url": "http://arxiv.org/pdf/2505.16128v1", "comment": "Accepted to ACL 2025 (Main)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16129v1", "title": "LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods", "authors": ["Hyang Cui"], "abstract": "Recent studies have applied large language models (LLMs) to machine\ntranslation quality estimation (MTQE) by prompting models to assign numeric\nscores. Nonetheless, these direct scoring methods tend to show low\nsegment-level correlation with human judgments. In this paper, we propose a\ngeneration-based evaluation paradigm that leverages decoder-only LLMs to\nproduce high-quality references, followed by semantic similarity scoring using\nsentence embeddings. We conduct the most extensive evaluation to date in MTQE,\ncovering 8 LLMs and 8 language pairs. Empirical results show that our method\noutperforms both intra-LLM direct scoring baselines and external non-LLM\nreference-free metrics from MTME. These findings demonstrate the strength of\ngeneration-based evaluation and support a shift toward hybrid approaches that\ncombine fluent generation with accurate semantic assessment.", "categories": ["cs.CL", "I.2.7"], "published": "2025-05-22 02:14:38", "updated": "2025-05-22 02:14:38", "pdf_url": "http://arxiv.org/pdf/2505.16129v1", "comment": "5 pages, 2 figures, 2 tables. Conforms to the ACL Rolling Review\n  (ARR) short paper track. Code and data available at:\n  https://github.com/CuiNiki/LLMs-Are-Not-Scorers", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16134v1", "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models", "authors": ["Menschikov Mikhail", "Alexander Kharitonov", "Maiia Kotyga", "Vadim Porvatov", "Anna Zhukovskaya", "David Kagramanyan", "Egor Shvetsov", "Evgeny Burnaev"], "abstract": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-22 02:23:00", "updated": "2025-05-22 02:23:00", "pdf_url": "http://arxiv.org/pdf/2505.16134v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16142v1", "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Jia Gu", "Zihao Wei", "Jingcheng Deng", "Feiyang Pan", "Huawei Shen", "Xueqi Cheng"], "abstract": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation.", "categories": ["cs.CL"], "published": "2025-05-22 02:36:36", "updated": "2025-05-22 02:36:36", "pdf_url": "http://arxiv.org/pdf/2505.16142v1", "comment": "15 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16146v1", "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "abstract": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 02:45:45", "updated": "2025-05-22 02:45:45", "pdf_url": "http://arxiv.org/pdf/2505.16146v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16148v1", "title": "NAN: A Training-Free Solution to Coefficient Estimation in Model Merging", "authors": ["Chongjie Si", "Kangtao Lv", "Jingjing Jiang", "Yadao Wang", "Yongwei Wang", "Xiaokang Yang", "Wenbo Su", "Bo Zheng", "Wei Shen"], "abstract": "Model merging offers a training-free alternative to multi-task learning by\ncombining independently fine-tuned models into a unified one without access to\nraw data. However, existing approaches often rely on heuristics to determine\nthe merging coefficients, limiting their scalability and generality. In this\nwork, we revisit model merging through the lens of least-squares optimization\nand show that the optimal merging weights should scale with the amount of\ntask-specific information encoded in each model. Based on this insight, we\npropose NAN, a simple yet effective method that estimates model merging\ncoefficients via the inverse of parameter norm. NAN is training-free,\nplug-and-play, and applicable to a wide range of merging strategies. Extensive\nexperiments on show that NAN consistently improves performance of baseline\nmethods.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-22 02:46:08", "updated": "2025-05-22 02:46:08", "pdf_url": "http://arxiv.org/pdf/2505.16148v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16149v1", "title": "When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification", "authors": ["Zirui Pang", "Haosheng Tan", "Yuhan Pu", "Zhijie Deng", "Zhouan Shen", "Keyu Hu", "Jiaheng Wei"], "abstract": "Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet\nserve as critical tools for model evaluation. However, despite the cleaning\nefforts, these datasets still suffer from pervasive noisy labels and often\ncontain missing labels due to the co-existing image pattern where multiple\nclasses appear in an image sample. This results in misleading model comparisons\nand unfair evaluations. Existing label cleaning methods focus primarily on\nnoisy labels, but the issue of missing labels remains largely overlooked.\nMotivated by these challenges, we present a comprehensive framework named\nREVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,\nLLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods\n(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and\nmissing label detection in widely-used image classification test sets. REVEAL\ndetects potential noisy labels and omissions, aggregates predictions from\nvarious methods, and refines label accuracy through confidence-informed\npredictions and consensus-based filtering. Additionally, we provide a thorough\nanalysis of state-of-the-art vision-language models and pre-trained image\nclassifiers, highlighting their strengths and limitations within the context of\ndataset renovation by revealing 10 observations. Our method effectively reveals\nmissing labels from public datasets and provides soft-labeled results with\nlikelihoods. Through human verifications, REVEAL significantly improves the\nquality of 6 benchmark test sets, highly aligning to human judgments and\nenabling more accurate and meaningful comparisons in image classification.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-22 02:47:36", "updated": "2025-05-22 02:47:36", "pdf_url": "http://arxiv.org/pdf/2505.16149v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16160v1", "title": "EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios", "authors": ["Bin Xu", "Yu Bai", "Huashan Sun", "Yiguan Lin", "Siming Liu", "Xinyue Liang", "Yaolin Li", "Yang Gao", "Heyan Huang"], "abstract": "As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench.", "categories": ["cs.CL"], "published": "2025-05-22 03:01:28", "updated": "2025-05-22 03:01:28", "pdf_url": "http://arxiv.org/pdf/2505.16160v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16162v1", "title": "KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization", "authors": ["Mingbo Song", "Heming Xia", "Jun Zhang", "Chak Tou Leong", "Qiancheng Xu", "Wenjie Li", "Sujian Li"], "abstract": "Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by efficiently drafting multiple tokens using a compact model\nand then verifying them in parallel using the target LLM. Notably,\nSelf-Speculative Decoding proposes skipping certain layers to construct the\ndraft model, which eliminates the need for additional parameters or training.\nDespite its strengths, we observe in this work that drafting with layer\nskipping exhibits significant sensitivity to domain shifts, leading to a\nsubstantial drop in acceleration performance. To enhance the domain\ngeneralizability of this paradigm, we introduce KNN-SSD, an algorithm that\nleverages K-Nearest Neighbor (KNN) search to match different skipped layers\nwith various domain inputs. We evaluated our algorithm in various models and\nmultiple tasks, observing that its application leads to 1.3x-1.6x speedup in\nLLM inference.", "categories": ["cs.CL"], "published": "2025-05-22 03:04:47", "updated": "2025-05-22 03:04:47", "pdf_url": "http://arxiv.org/pdf/2505.16162v1", "comment": "8 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16164v1", "title": "Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task", "authors": ["Mengyang Qiu", "Zoe Brisebois", "Siena Sun"], "abstract": "Large language models (LLMs) are increasingly explored as substitutes for\nhuman participants in cognitive tasks, but their ability to simulate human\nbehavioral variability remains unclear. This study examines whether LLMs can\napproximate individual differences in the phonemic fluency task, where\nparticipants generate words beginning with a target letter. We evaluated 34\nmodel configurations, varying prompt specificity, sampling temperature, and\nmodel type, and compared outputs to responses from 106 human participants.\nWhile some configurations, especially Claude 3.7 Sonnet, matched human averages\nand lexical preferences, none reproduced the scope of human variability. LLM\noutputs were consistently less diverse and structurally rigid, and LLM\nensembles failed to increase diversity. Network analyses further revealed\nfundamental differences in retrieval structure between humans and models. These\nresults highlight key limitations in using LLMs to simulate human cognition and\nbehavior.", "categories": ["cs.CL"], "published": "2025-05-22 03:08:27", "updated": "2025-05-22 03:08:27", "pdf_url": "http://arxiv.org/pdf/2505.16164v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16170v1", "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction", "authors": ["Yuqing Yang", "Robin Jia"], "abstract": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction.", "categories": ["cs.CL"], "published": "2025-05-22 03:16:00", "updated": "2025-05-22 03:16:00", "pdf_url": "http://arxiv.org/pdf/2505.16170v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16172v1", "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "abstract": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 03:19:49", "updated": "2025-05-22 03:19:49", "pdf_url": "http://arxiv.org/pdf/2505.16172v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16176v1", "title": "Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning", "authors": ["Jun Rao", "Xuebo Liu", "Hexuan Deng", "Zepeng Lin", "Zixiong Yu", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "abstract": "In the realm of data selection for reasoning tasks, existing approaches\npredominantly rely on externally predefined static metrics such as difficulty\nand diversity, which are often designed for supervised fine-tuning (SFT) and\nlack adaptability to continuous training processes. A critical limitation of\nthese methods is their inability to dynamically align with the evolving\ncapabilities of models during online training, a gap that becomes increasingly\npronounced with the rise of dynamic training paradigms and online reinforcement\nlearning (RL) frameworks (e.g., R1 models). To address this, we introduce\nSAI-DPO, an algorithm that dynamically selects training data by continuously\nassessing a model's stage-specific reasoning abilities across different\ntraining phases. By integrating real-time model performance feedback, SAI-DPO\nadaptively adapts data selection to the evolving strengths and weaknesses of\nthe model, thus enhancing both data utilization efficiency and final task\nperformance. Extensive experiments on three state-of-the-art models and eight\nmathematical reasoning benchmarks, including challenging competition-level\ndatasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average\nperformance boost of up to 21.3 percentage points, with particularly notable\nimprovements of 10 and 15 points on AIME24 and AMC23, respectively. These\nresults highlight the superiority of dynamic, model-adaptive data selection\nover static, externally defined strategies in advancing reasoning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 03:27:05", "updated": "2025-05-22 03:27:05", "pdf_url": "http://arxiv.org/pdf/2505.16176v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16178v1", "title": "Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge", "authors": ["Ying Zhang", "Benjamin Heinzerling", "Dongyuan Li", "Ryoma Ishigaki", "Yuta Hitomi", "Kentaro Inui"], "abstract": "Fact recall, the ability of language models (LMs) to retrieve specific\nfactual knowledge, remains a challenging task despite their impressive general\ncapabilities. Common training strategies often struggle to promote robust\nrecall behavior with two-stage training, which first trains a model with\nfact-storing examples (e.g., factual statements) and then with fact-recalling\nexamples (question-answer pairs), tending to encourage rote memorization rather\nthan generalizable fact retrieval. In contrast, mixed training, which jointly\nuses both types of examples, has been empirically shown to improve the ability\nto recall facts, but the underlying mechanisms are still poorly understood. In\nthis work, we investigate how these training strategies affect how model\nparameters are shaped during training and how these differences relate to their\nability to recall facts. We introduce cross-task gradient trace to identify\nshared parameters, those strongly influenced by both fact-storing and\nfact-recalling examples. Our analysis on synthetic fact recall datasets with\nthe Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a\nlarger and more centralized set of shared parameters. These findings suggest\nthat the emergence of parameters may play a key role in enabling LMs to\ngeneralize factual knowledge across task formulations.", "categories": ["cs.CL"], "published": "2025-05-22 03:34:29", "updated": "2025-05-22 03:34:29", "pdf_url": "http://arxiv.org/pdf/2505.16178v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16180v1", "title": "Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics", "authors": ["Ashim Dahal", "Ankit Ghimire", "Saydul Akbar Murad", "Nick Rahimi"], "abstract": "Evaluating image captions requires cohesive assessment of both visual\nsemantics and language pragmatics, which is often not entirely captured by most\nmetrics. We introduce Redemption Score, a novel hybrid framework that ranks\nimage captions by triangulating three complementary signals: (1) Mutual\nInformation Divergence (MID) for global image-text distributional alignment,\n(2) DINO-based perceptual similarity of cycle-generated images for visual\ngrounding, and (3) BERTScore for contextual text similarity against human\nreferences. A calibrated fusion of these signals allows Redemption Score to\noffer a more holistic assessment. On the Flickr8k benchmark, Redemption Score\nachieves a Kendall-$\\tau$ of 56.43, outperforming twelve prior methods and\ndemonstrating superior correlation with human judgments without requiring\ntask-specific training. Our framework provides a more robust and nuanced\nevaluation by effectively redeeming image semantics and linguistic\ninterpretability indicated by strong transfer of knowledge in the Conceptual\nCaptions and MS COCO datasets.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-22 03:35:12", "updated": "2025-05-22 03:35:12", "pdf_url": "http://arxiv.org/pdf/2505.16180v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16186v1", "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning", "authors": ["Kaiwen Zhou", "Xuandong Zhao", "Gaowen Liu", "Jayanth Srinivasa", "Aosong Feng", "Dawn Song", "Xin Eric Wang"], "abstract": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.", "categories": ["cs.AI", "cs.CL", "cs.CR"], "published": "2025-05-22 03:46:03", "updated": "2025-05-22 03:46:03", "pdf_url": "http://arxiv.org/pdf/2505.16186v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16188v1", "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models", "authors": ["Zirui He", "Mingyu Jin", "Bo Shen", "Ali Payani", "Yongfeng Zhang", "Mengnan Du"], "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but controlling their behavior\nreliably remains challenging, especially in open-ended generation settings.\nThis paper introduces a novel supervised steering approach that operates in\nsparse, interpretable representation spaces. We employ sparse autoencoders\n(SAEs)to obtain sparse latent representations that aim to disentangle semantic\nattributes from model activations. Then we train linear classifiers to identify\na small subspace of task-relevant dimensions in latent representations.\nFinally, we learn supervised steering vectors constrained to this subspace,\noptimized to align with target behaviors. Experiments across sentiment,\ntruthfulness, and politics polarity steering tasks with multiple LLMs\ndemonstrate that our supervised steering vectors achieve higher success rates\nwith minimal degradation in generation quality compared to existing methods.\nFurther analysis reveals that a notably small subspace is sufficient for\neffective steering, enabling more targeted and interpretable interventions.", "categories": ["cs.CL"], "published": "2025-05-22 03:46:57", "updated": "2025-05-22 03:46:57", "pdf_url": "http://arxiv.org/pdf/2505.16188v1", "comment": "30 pages, 24 figures, 12 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16189v1", "title": "The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions", "authors": ["Sophie Wu", "Jan Philip Wahle", "Saif M. Mohammad"], "abstract": "This paper is the first investigation of the connection between emotion,\nembodiment, and everyday language in a large sample of natural language data.\nWe created corpora of body part mentions (BPMs) in online English text (blog\nposts and tweets). This includes a subset featuring human annotations for the\nemotions of the person whose body part is mentioned in the text. We show that\nBPMs are common in personal narratives and tweets (~5% to 10% of posts include\nBPMs) and that their usage patterns vary markedly by time and %geographic\nlocation. Using word-emotion association lexicons and our annotated data, we\nshow that text containing BPMs tends to be more emotionally charged, even when\nthe BPM is not explicitly used to describe a physical reaction to the emotion\nin the text. Finally, we discover a strong and statistically significant\ncorrelation between body-related language and a variety of poorer health\noutcomes. In sum, we argue that investigating the role of body-part related\nwords in language can open up valuable avenues of future research at the\nintersection of NLP, the affective sciences, and the study of human wellbeing.", "categories": ["cs.CL"], "published": "2025-05-22 03:47:12", "updated": "2025-05-22 03:47:12", "pdf_url": "http://arxiv.org/pdf/2505.16189v1", "comment": "8 pages, 26 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16193v1", "title": "An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability", "authors": ["Daiqing Wu", "Dongbao Yang", "Sicheng Zhao", "Can Ma", "Yu Zhou"], "abstract": "The advancements in Multimodal Large Language Models (MLLMs) have enabled\nvarious multimodal tasks to be addressed under a zero-shot paradigm. This\nparadigm sidesteps the cost of model fine-tuning, emerging as a dominant trend\nin practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a\npivotal challenge in the quest for general artificial intelligence, fails to\naccommodate this convenience. The zero-shot paradigm exhibits undesirable\nperformance on MSA, casting doubt on whether MLLMs can perceive sentiments as\ncompetent as supervised models. By extending the zero-shot paradigm to\nIn-Context Learning (ICL) and conducting an in-depth study on configuring\ndemonstrations, we validate that MLLMs indeed possess such capability.\nSpecifically, three key factors that cover demonstrations' retrieval,\npresentation, and distribution are comprehensively investigated and optimized.\nA sentimental predictive bias inherent in MLLMs is also discovered and later\neffectively counteracted. By complementing each other, the devised strategies\nfor three factors result in average accuracy improvements of 15.9% on six MSA\ndatasets against the zero-shot paradigm and 11.2% against the random ICL\nbaseline.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-22 03:51:41", "updated": "2025-05-22 03:51:41", "pdf_url": "http://arxiv.org/pdf/2505.16193v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16210v1", "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 04:23:19", "updated": "2025-05-22 04:23:19", "pdf_url": "http://arxiv.org/pdf/2505.16210v1", "comment": "11 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16211v1", "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhuo Chen", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "abstract": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "published": "2025-05-22 04:27:46", "updated": "2025-05-22 04:27:46", "pdf_url": "http://arxiv.org/pdf/2505.16211v1", "comment": "Technical Report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16212v1", "title": "Large Language Models based ASR Error Correction for Child Conversations", "authors": ["Anfeng Xu", "Tiantian Feng", "So Hyun Kim", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "abstract": "Automatic Speech Recognition (ASR) has recently shown remarkable progress,\nbut accurately transcribing children's speech remains a significant challenge.\nRecent developments in Large Language Models (LLMs) have shown promise in\nimproving ASR transcriptions. However, their applications in child speech\nincluding conversational scenarios are underexplored. In this study, we explore\nthe use of LLMs in correcting ASR errors for conversational child speech. We\ndemonstrate the promises and challenges of LLMs through experiments on two\nchildren's conversational speech datasets with both zero-shot and fine-tuned\nASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR\noutputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs\nto improve ASR performance when incorporating contextual information or when\nusing fine-tuned autoregressive ASR (e.g., Whisper) outputs.", "categories": ["cs.CL", "eess.AS"], "published": "2025-05-22 04:28:02", "updated": "2025-05-22 04:28:02", "pdf_url": "http://arxiv.org/pdf/2505.16212v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16216v1", "title": "Memorization or Reasoning? Exploring the Idiom Understanding of LLMs", "authors": ["Jisu Kim", "Youngwoo Shin", "Uiji Hwang", "Jihun Choi", "Richeng Xuan", "Taeuk Kim"], "abstract": "Idioms have long posed a challenge due to their unique linguistic properties,\nwhich set them apart from other common expressions. While recent studies have\nleveraged large language models (LLMs) to handle idioms across various tasks,\ne.g., idiom-containing sentence generation and idiomatic machine translation,\nlittle is known about the underlying mechanisms of idiom processing in LLMs,\nparticularly in multilingual settings. To this end, we introduce MIDAS, a new\nlarge-scale dataset of idioms in six languages, each paired with its\ncorresponding meaning. Leveraging this resource, we conduct a comprehensive\nevaluation of LLMs' idiom processing ability, identifying key factors that\ninfluence their performance. Our findings suggest that LLMs rely not only on\nmemorization, but also adopt a hybrid approach that integrates contextual cues\nand reasoning, especially when processing compositional idioms. This implies\nthat idiom understanding in LLMs emerges from an interplay between internal\nknowledge retrieval and reasoning-based inference.", "categories": ["cs.CL"], "published": "2025-05-22 04:31:25", "updated": "2025-05-22 04:31:25", "pdf_url": "http://arxiv.org/pdf/2505.16216v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16220v1", "title": "Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning", "authors": ["Liang-Yeh Shen", "Shi-Xin Fang", "Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "abstract": "This paper introduces Meta-PerSER, a novel meta-learning framework that\npersonalizes Speech Emotion Recognition (SER) by adapting to each listener's\nunique way of interpreting emotion. Conventional SER systems rely on aggregated\nannotations, which often overlook individual subtleties and lead to\ninconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic\nMeta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,\nDerivative Annealing, and per-layer per-step learning rates, enabling rapid\nadaptation with only a few labeled examples. By integrating robust\nrepresentations from pre-trained self-supervised models, our framework first\ncaptures general emotional cues and then fine-tunes itself to personal\nannotation styles. Experiments on the IEMOCAP corpus demonstrate that\nMeta-PerSER significantly outperforms baseline methods in both seen and unseen\ndata scenarios, highlighting its promise for personalized emotion recognition.", "categories": ["eess.AS", "cs.CL"], "published": "2025-05-22 04:44:20", "updated": "2025-05-22 04:44:20", "pdf_url": "http://arxiv.org/pdf/2505.16220v1", "comment": "Accepted by INTERSPEECH 2025. 7 pages, including 2 pages of appendix", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16222v1", "title": "Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation", "authors": ["Jiwon Moon", "Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "abstract": "With the growing use of large language models(LLMs) as evaluators, their\napplication has expanded to code evaluation tasks, where they assess the\ncorrectness of generated code without relying on reference implementations.\nWhile this offers scalability and flexibility, it also raises a critical,\nunresolved question: Can LLM judges fairly and robustly evaluate semantically\nequivalent code with superficial variations? Functionally correct code often\nexhibits variations-such as differences in variable names, comments, or\nformatting-that should not influence its correctness. Yet, whether LLM judges\ncan reliably handle these variations remains unclear. We present the first\ncomprehensive study of this issue, defining six types of potential bias in code\nevaluation and revealing their systematic impact on LLM judges. Across five\nprogramming languages and multiple LLMs, we empirically demonstrate that all\ntested LLM judges are susceptible to both positive and negative biases,\nresulting in inflated or unfairly low scores. Moreover, we observe that LLM\njudges remain vulnerable to these biases even when prompted to generate test\ncases before scoring, highlighting the need for more robust code evaluation\nmethods.", "categories": ["cs.CL", "cs.SE"], "published": "2025-05-22 04:49:33", "updated": "2025-05-22 04:49:33", "pdf_url": "http://arxiv.org/pdf/2505.16222v1", "comment": "26 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16227v1", "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "abstract": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 04:55:41", "updated": "2025-05-22 04:55:41", "pdf_url": "http://arxiv.org/pdf/2505.16227v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16232v1", "title": "MuseRAG: Idea Originality Scoring At Scale", "authors": ["Ali Sarosh Bangash", "Krish Veera", "Ishfat Abrar Islam", "Raiyan Abdul Baten"], "abstract": "An objective, face-valid way to assess the originality of creative ideas is\nto measure how rare each idea is within a population -- an approach long used\nin creativity research but difficult to automate at scale. Tabulating response\nfrequencies via manual bucketing of idea rephrasings is labor-intensive,\nerror-prone, and brittle under large corpora. We introduce a fully automated,\npsychometrically validated pipeline for frequency-based originality scoring.\nOur method, MuseRAG, combines large language models (LLMs) with an externally\norchestrated retrieval-augmented generation (RAG) framework. Given a new idea,\nthe system retrieves semantically similar prior idea buckets and zero-shot\nprompts the LLM to judge whether the new idea belongs to an existing bucket or\nforms a new one. The resulting buckets enable computation of frequency-based\noriginality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG\nmatches human annotators in idea clustering structure and resolution (AMI =\n0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong\nconvergent and external validity. Our work enables intent-sensitive,\nhuman-aligned originality scoring at scale to aid creativity research.", "categories": ["cs.CL"], "published": "2025-05-22 05:05:25", "updated": "2025-05-22 05:05:25", "pdf_url": "http://arxiv.org/pdf/2505.16232v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16234v1", "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "abstract": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 05:08:27", "updated": "2025-05-22 05:08:27", "pdf_url": "http://arxiv.org/pdf/2505.16234v1", "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16237v1", "title": "Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation", "authors": ["Derong Xu", "Pengyue Jia", "Xiaopeng Li", "Yingyi Zhang", "Maolin Wang", "Qidong Liu", "Xiangyu Zhao", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Enhong Chen", "Tong Xu"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nstill struggle with issues like hallucinations and outdated information.\nRetrieval-augmented generation (RAG) addresses these issues by grounding LLM\noutputs in external knowledge with an Information Retrieval (IR) system.\nBuilding on this foundation, graph-based RAG systems go a step further by\nretrieving subgraphs, which preserve the relationships between knowledge\nentities and provide more comprehensive context. However, graph RAG faces two\nchallenges: (1) Retrieving relevant information introduces irrelevant nodes\n(especially in dense graph databases, where retrieval usually extends to\nadjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)\nThe representation gap between graph and language during generation with LLMs\nlimits the ability to fully leverage graph structures for enhanced\nunderstanding. To address these limitations, we propose Align-GRAG, a novel\nreasoning-guided dual alignment framework in post-retrieval phrase. It first\nformulates a subgraph by retrieving nodes and edges. Then an Aligner is\nproposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It\nachieves dual alignment of graph node and representation by leveraging KL\ndivergence loss and contrastive loss, facilitating efficient pruning of\nirrelevant knowledge and establishing a unified semantic space. The Generator\nintegrates the aligned graph data with LLM to produce coherent and accurate\nanswers. Experiments on GraphQA benchmark across three tasks (including common\nsense reasoning, scene graph understanding, and knowledge graph reasoning)\nvalidate the effectiveness of our method. The code will be available upon\naccepted.", "categories": ["cs.CL"], "published": "2025-05-22 05:15:27", "updated": "2025-05-22 05:15:27", "pdf_url": "http://arxiv.org/pdf/2505.16237v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16241v1", "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers", "authors": ["Viet-Anh Nguyen", "Shiqian Zhao", "Gia Dao", "Runyi Hu", "Yi Xie", "Luu Anh Tuan"], "abstract": "Recently, Large Reasoning Models (LRMs) have demonstrated superior logical\ncapabilities compared to traditional Large Language Models (LLMs), gaining\nsignificant attention. Despite their impressive performance, the potential for\nstronger reasoning abilities to introduce more severe security vulnerabilities\nremains largely underexplored. Existing jailbreak methods often struggle to\nbalance effectiveness with robustness against adaptive safety mechanisms. In\nthis work, we propose SEAL, a novel jailbreak attack that targets LRMs through\nan adaptive encryption pipeline designed to override their reasoning processes\nand evade potential adaptive alignment. Specifically, SEAL introduces a stacked\nencryption approach that combines multiple ciphers to overwhelm the models\nreasoning capabilities, effectively bypassing built-in safety mechanisms. To\nfurther prevent LRMs from developing countermeasures, we incorporate two\ndynamic strategies - random and adaptive - that adjust the cipher length,\norder, and combination. Extensive experiments on real-world reasoning models,\nincluding DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the\neffectiveness of our approach. Notably, SEAL achieves an attack success rate of\n80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant\nmargin of 27.2%. Warning: This paper contains examples of inappropriate,\noffensive, and harmful content.", "categories": ["cs.CL"], "published": "2025-05-22 05:19:42", "updated": "2025-05-22 05:19:42", "pdf_url": "http://arxiv.org/pdf/2505.16241v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16245v1", "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models", "authors": ["Vijeta Deshpande", "Debasmita Ghose", "John D. Patterson", "Roger Beaty", "Anna Rumshisky"], "abstract": "Diverse language model responses are crucial for creative generation,\nopen-ended tasks, and self-improvement training. We show that common diversity\nmetrics, and even reward models used for preference optimization,\nsystematically bias models toward shorter outputs, limiting expressiveness. To\naddress this, we introduce Diverse, not Short (Diverse-NS), a length-controlled\nself-learning framework that improves response diversity while maintaining\nlength parity. By generating and filtering preference data that balances\ndiversity, quality, and length, Diverse-NS enables effective training using\nonly 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,\nDiverse-NS substantially enhances lexical and semantic diversity. We show\nconsistent improvement in diversity with minor reduction or gains in response\nquality on four creative generation tasks: Divergent Associations, Persona\nGeneration, Alternate Uses, and Creative Writing. Surprisingly, experiments\nwith the Olmo-2 model family (7B, and 13B) show that smaller models like\nOlmo-2-7B can serve as effective \"diversity teachers\" for larger models. By\nexplicitly addressing length bias, our method efficiently pushes models toward\nmore diverse and expressive outputs.", "categories": ["cs.CL"], "published": "2025-05-22 05:29:47", "updated": "2025-05-22 05:29:47", "pdf_url": "http://arxiv.org/pdf/2505.16245v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16252v1", "title": "Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models", "authors": ["Hwiyeong Lee", "Uiji Hwang", "Hyelim Lim", "Taeuk Kim"], "abstract": "Large language models often retain unintended content, prompting growing\ninterest in knowledge unlearning. Recent approaches emphasize localized\nunlearning, which restricts parameter updates to specific regions in an effort\nto remove target knowledge while preserving unrelated general knowledge.\nHowever, their effectiveness remains uncertain due to the lack of robust and\nthorough evaluation of the trade-off between the competing goals of unlearning.\nIn this paper, we begin by revisiting existing localized unlearning approaches.\nWe then conduct controlled experiments to rigorously evaluate whether local\nparameter updates causally contribute to unlearning. Our findings reveal that\nthe set of parameters that must be modified for effective unlearning is not\nstrictly determined, challenging the core assumption of localized unlearning\nthat parameter locality is inherently indicative of effective knowledge\nremoval.", "categories": ["cs.CL", "I.2.7"], "published": "2025-05-22 05:41:53", "updated": "2025-05-22 05:41:53", "pdf_url": "http://arxiv.org/pdf/2505.16252v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16258v1", "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "abstract": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "published": "2025-05-22 05:49:01", "updated": "2025-05-22 05:49:01", "pdf_url": "http://arxiv.org/pdf/2505.16258v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16263v1", "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI", "authors": ["Sampanna Yashwant Kahu", "Naman Ahuja"], "abstract": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text.", "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5"], "published": "2025-05-22 05:55:26", "updated": "2025-05-22 05:55:26", "pdf_url": "http://arxiv.org/pdf/2505.16263v1", "comment": "10 pages, 22 figures, The source code and data used in this work is\n  available at: https://github.com/SampannaKahu/all_you_need_is_leet", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16270v1", "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "abstract": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 06:00:45", "updated": "2025-05-22 06:00:45", "pdf_url": "http://arxiv.org/pdf/2505.16270v1", "comment": "33 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16276v1", "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance", "authors": ["Desiree Heim", "Lars-Peter Meyer", "Markus Schr\u00f6der", "Johannes Frey", "Andreas Dengel"], "abstract": "When using Large Language Models (LLMs) to support Knowledge Graph\nEngineering (KGE), one of the first indications when searching for an\nappropriate model is its size. According to the scaling laws, larger models\ntypically show higher capabilities. However, in practice, resource costs are\nalso an important factor and thus it makes sense to consider the ratio between\nmodel performance and costs. The LLM-KG-Bench framework enables the comparison\nof LLMs in the context of KGE tasks and assesses their capabilities of\nunderstanding and producing KGs and KG queries. Based on a dataset created in\nan LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the\nmodel size scaling laws specific to KGE tasks. In our analyses, we assess how\nbenchmark scores evolve between different model size categories. Additionally,\nwe inspect how the general score development of single models and families of\nmodels correlates to their size. Our analyses revealed that, with a few\nexceptions, the model size scaling laws generally also apply to the selected\nKGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,\nthe task performance did not change much between a model and the next larger\nmodel. In these cases, smaller models could be considered to achieve high\ncost-effectiveness. Regarding models of the same family, sometimes larger\nmodels performed worse than smaller models of the same family. These effects\noccurred only locally. Hence it is advisable to additionally test the next\nsmallest and largest model of the same family.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 06:21:40", "updated": "2025-05-22 06:21:40", "pdf_url": "http://arxiv.org/pdf/2505.16276v1", "comment": "Peer reviewed and to appear in the ESWC 2025 Workshops and Tutorials\n  Joint Proceedings (Workshop on Evaluation of Language Models in Knowledge\n  Engineering [ELMKE])", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16277v1", "title": "Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility", "authors": ["Sheng-Fu Wang", "Laurent Prevot", "Jou-an Chi", "Ri-Sheng Huang", "Shu-Kai Hsieh"], "abstract": "The achievements of Large Language Models in Natural Language Processing,\nespecially for high-resource languages, call for a better understanding of\ntheir characteristics from a cognitive perspective. Researchers have attempted\nto evaluate artificial models by testing their ability to predict behavioral\n(e.g., eye-tracking fixations) and physiological (e.g., brain responses)\nvariables during language processing (e.g., reading/listening). In this paper,\nwe propose using spontaneous speech corpora to derive production variables\n(speech reductions, prosodic prominences) and applying them in a similar\nfashion. More precisely, we extract. We then test models trained with a\nstandard procedure on different pretraining datasets (written, spoken, and\nmixed genres) for their ability to predict these two variables. Our results\nshow that, after some fine-tuning, the models can predict these production\nvariables well above baselines. We also observe that spoken genre training data\nprovides more accurate predictions than written genres. These results\ncontribute to the broader effort of using high-quality speech corpora as\nbenchmarks for LLMs.", "categories": ["cs.CL"], "published": "2025-05-22 06:23:02", "updated": "2025-05-22 06:23:02", "pdf_url": "http://arxiv.org/pdf/2505.16277v1", "comment": "The 14th Workshop on Cognitive Modeling and Computational Linguistics\n  (CMCL). May 3, 2025. Collocated with NAACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16281v1", "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "abstract": "The advancement of Large Language Models (LLMs) enables flexible and\ninterpretable automatic evaluations. In the field of machine translation\nevaluation, utilizing LLMs with translation error annotations based on\nMultidimensional Quality Metrics (MQM) yields more human-aligned judgments.\nHowever, current LLM-based evaluation methods still face challenges in\naccurately identifying error spans and assessing their severity. In this paper,\nwe propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation\nEvaluation. We argue that existing approaches inadequately exploit the\nfine-grained structural and semantic information within the MQM hierarchy. To\naddress this, we develop a hierarchical multi-agent system grounded in the MQM\nerror typology, enabling granular evaluation of subtype errors. Two key\nstrategies are incorporated to further mitigate systemic hallucinations within\nthe framework: the utilization of the model's self-reflection capability and\nthe facilitation of agent discussion involving asymmetric information.\nEmpirically, HiMATE outperforms competitive baselines across different datasets\nin conducting human-aligned evaluations. Further analyses underscore its\nsignificant advantage in error span detection and severity assessment,\nachieving an average F1-score improvement of 89% over the best-performing\nbaseline. We make our code and data publicly available at\nhttps://anonymous.4open.science/r/HiMATE-Anony.", "categories": ["cs.CL"], "published": "2025-05-22 06:24:08", "updated": "2025-05-22 06:24:08", "pdf_url": "http://arxiv.org/pdf/2505.16281v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16293v1", "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA", "authors": ["Rishabh Maheshwary", "Masoud Hashemi", "Khyati Mahajan", "Shiva Krishna Reddy Malay", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Spandana Gella", "Vikas Yadav"], "abstract": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.", "categories": ["cs.CL"], "published": "2025-05-22 06:45:05", "updated": "2025-05-22 06:45:05", "pdf_url": "http://arxiv.org/pdf/2505.16293v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16297v1", "title": "ToDi: Token-wise Distillation via Fine-Grained Divergence Control", "authors": ["Seongryong Jung", "Suwan Yoon", "DongGeon Kim", "Hwanhee Lee"], "abstract": "Large language models (LLMs) offer impressive performance but are impractical\nfor resource-constrained deployment due to high latency and energy consumption.\nKnowledge distillation (KD) addresses this by transferring knowledge from a\nlarge teacher to a smaller student model. However, conventional KD, notably\napproaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence\nloss across the entire vocabulary, neglecting token-level prediction\ndiscrepancies. By investigating these representative divergences via gradient\nanalysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses\noverestimated ones, showing their complementary roles. Based on this\nobservation, we propose Token-wise Distillation (ToDi), a novel method that\nadaptively combines FKL and RKL per token using a sigmoid-based weighting\nfunction derived from the teacher-student probability log-ratio. ToDi\ndynamically emphasizes the appropriate divergence for each token, enabling\nprecise distribution alignment. We demonstrate that ToDi consistently\noutperforms recent distillation baselines using uniform or less granular\nstrategies across instruction-following benchmarks. Extensive ablation studies\nand efficiency analysis further validate ToDi's effectiveness and practicality.", "categories": ["cs.CL"], "published": "2025-05-22 06:51:16", "updated": "2025-05-22 06:51:16", "pdf_url": "http://arxiv.org/pdf/2505.16297v1", "comment": "13 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16303v1", "title": "INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling", "authors": ["Haochen Shi", "Tianshi Zheng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Chunkit Chan", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "abstract": "Large Language Model (LLM) routing is a pivotal technique for navigating a\ndiverse landscape of LLMs, aiming to select the best-performing LLMs tailored\nto the domains of user queries, while managing computational resources.\nHowever, current routing approaches often face limitations in scalability when\ndealing with a large pool of specialized LLMs, or in their adaptability to\nextending model scope and evolving capability domains. To overcome those\nchallenges, we propose InferenceDynamics, a flexible and scalable\nmulti-dimensional routing framework by modeling the capability and knowledge of\nmodels. We operate it on our comprehensive dataset RouteMix, and demonstrate\nits effectiveness and generalizability in group-level routing using modern\nbenchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its\nability to identify and leverage top-performing models for given tasks, leading\nto superior outcomes with efficient resource utilization. The broader adoption\nof Inference Dynamics can empower users to harness the full specialized\npotential of the LLM ecosystem, and our code will be made publicly available to\nencourage further research.", "categories": ["cs.CL"], "published": "2025-05-22 06:56:51", "updated": "2025-05-22 06:56:51", "pdf_url": "http://arxiv.org/pdf/2505.16303v1", "comment": "17 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16307v1", "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "abstract": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 06:59:10", "updated": "2025-05-22 06:59:10", "pdf_url": "http://arxiv.org/pdf/2505.16307v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16315v1", "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning", "authors": ["Xiaoxue Cheng", "Junyi Li", "Zhenduo Zhang", "Xinyu Tang", "Wayne Xin Zhao", "Xinyu Kong", "Zhiqiang Zhang"], "abstract": "Large reasoning models (LRMs) have demonstrated strong performance on complex\nreasoning tasks, but often suffer from overthinking, generating redundant\ncontent regardless of task difficulty. Inspired by the dual process theory in\ncognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a\nreinforcement learning framework that enables LRMs to achieve efficient\nreasoning through adaptive cognitive allocation and dynamic system switch. ACPO\nincorporates two key components: (1) introducing system-aware reasoning tokens\nto explicitly represent the thinking modes thereby making the model's cognitive\nprocess transparent, and (2) integrating online difficulty estimation and token\nlength budget to guide adaptive system switch and reasoning during\nreinforcement learning. To this end, we propose a two-stage training strategy.\nThe first stage begins with supervised fine-tuning to cold start the model,\nenabling it to generate reasoning paths with explicit thinking modes. In the\nsecond stage, we apply ACPO to further enhance adaptive system switch for\ndifficulty-aware reasoning. Experimental results demonstrate that ACPO\neffectively reduces redundant reasoning while adaptively adjusting cognitive\nallocation based on task complexity, achieving efficient hybrid reasoning.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 07:15:08", "updated": "2025-05-22 07:15:08", "pdf_url": "http://arxiv.org/pdf/2505.16315v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16322v1", "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "abstract": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 07:24:11", "updated": "2025-05-22 07:24:11", "pdf_url": "http://arxiv.org/pdf/2505.16322v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16325v1", "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "abstract": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-05-22 07:32:12", "updated": "2025-05-22 07:32:12", "pdf_url": "http://arxiv.org/pdf/2505.16325v1", "comment": "18 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16330v1", "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "abstract": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM.", "categories": ["cs.CL", "cs.AI", "cs.DL"], "published": "2025-05-22 07:34:59", "updated": "2025-05-22 07:34:59", "pdf_url": "http://arxiv.org/pdf/2505.16330v1", "comment": null, "doi": "10.1016/j.eswa.2025.126778", "journal_ref": "Expert Systems With Applications, 2025"}
{"arxiv_id": "2505.16348v1", "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance", "authors": ["Taeyoon Kwon", "Dongwook Choi", "Sunghwan Kim", "Hyojun Kim", "Seungjun Moon", "Beong-woo Kwak", "Kuan-Hao Huang", "Jinyoung Yeo"], "abstract": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO", "categories": ["cs.CL"], "published": "2025-05-22 08:00:10", "updated": "2025-05-22 08:00:10", "pdf_url": "http://arxiv.org/pdf/2505.16348v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16349v1", "title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization", "authors": ["Pierre Achkar", "Tim Gollub", "Martin Potthast"], "abstract": "The exponential growth of scientific publications has made it increasingly\ndifficult for researchers to stay updated and synthesize knowledge effectively.\nThis paper presents XSum, a modular pipeline for multi-document summarization\n(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The\npipeline includes two core components: a question-generation module and an\neditor module. The question-generation module dynamically generates questions\nadapted to the input papers, ensuring the retrieval of relevant and accurate\ninformation. The editor module synthesizes the retrieved content into coherent\nand well-structured summaries that adhere to academic standards for proper\ncitation. Evaluated on the SurveySum dataset, XSum demonstrates strong\nperformance, achieving considerable improvements in metrics such as CheckEval,\nG-Eval and Ref-F1 compared to existing approaches. This work provides a\ntransparent, adaptable framework for scientific summarization with potential\napplications in a wide range of domains. Code available at\nhttps://github.com/webis-de/scolia25-xsum", "categories": ["cs.CL"], "published": "2025-05-22 08:00:59", "updated": "2025-05-22 08:00:59", "pdf_url": "http://arxiv.org/pdf/2505.16349v1", "comment": "Accepted at SCOLIA@ECIR 2025 Workshop", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16381v1", "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "abstract": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-22 08:36:09", "updated": "2025-05-22 08:36:09", "pdf_url": "http://arxiv.org/pdf/2505.16381v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16385v1", "title": "Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models", "authors": ["Kaiyu He", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "abstract": "Large language models (LLMs) demonstrate remarkable ability in cross-lingual\ntasks. Understanding how LLMs acquire this ability is crucial for their\ninterpretability. To quantify the cross-lingual ability of LLMs accurately, we\npropose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn\ncross-lingual ability, we trace the outputs of LLMs' intermediate layers in the\nword translation task. We identify and distinguish two distinct behaviors in\nthe forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.\nWe attribute LLMs' two distinct behaviors to the co-occurrence frequency of\nwords and find the semantic pivot from the pre-training dataset. Finally, to\napply our findings to improve the cross-lingual ability of LLMs, we reconstruct\na semantic pivot-aware pre-training dataset using documents with a high\nproportion of semantic pivots. Our experiments validate the effectiveness of\nour approach in enhancing cross-lingual ability. Our research contributes\ninsights into the interpretability of LLMs and offers a method for improving\nLLMs' cross-lingual ability.", "categories": ["cs.CL"], "published": "2025-05-22 08:37:04", "updated": "2025-05-22 08:37:04", "pdf_url": "http://arxiv.org/pdf/2505.16385v1", "comment": "14 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16392v1", "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "abstract": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts.", "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "published": "2025-05-22 08:45:14", "updated": "2025-05-22 08:45:14", "pdf_url": "http://arxiv.org/pdf/2505.16392v1", "comment": "Accepted at SIGIR 2025", "doi": "10.1145/3726302.3730304", "journal_ref": null}
{"arxiv_id": "2505.16400v1", "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "abstract": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-22 08:50:47", "updated": "2025-05-22 08:50:47", "pdf_url": "http://arxiv.org/pdf/2505.16400v1", "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16406v1", "title": "On the reliability of feature attribution methods for speech classification", "authors": ["Gaofei Shen", "Hosein Mohebbi", "Arianna Bisazza", "Afra Alishahi", "Grzegorz Chrupa\u0142a"], "abstract": "As the capabilities of large-scale pre-trained models evolve, understanding\nthe determinants of their outputs becomes more important. Feature attribution\naims to reveal which parts of the input elements contribute the most to model\noutputs. In speech processing, the unique characteristics of the input signal\nmake the application of feature attribution methods challenging. We study how\nfactors such as input type and aggregation and perturbation timespan impact the\nreliability of standard feature attribution methods, and how these factors\ninteract with characteristics of each classification task. We find that\nstandard approaches to feature attribution are generally unreliable when\napplied to the speech domain, with the exception of word-aligned perturbation\nmethods when applied to word-based classification tasks.", "categories": ["cs.CL", "eess.AS"], "published": "2025-05-22 08:59:25", "updated": "2025-05-22 08:59:25", "pdf_url": "http://arxiv.org/pdf/2505.16406v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16408v1", "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs", "authors": ["Muhammad Farid Adilazuarda", "Chen Cecilia Liu", "Iryna Gurevych", "Alham Fikri Aji"], "abstract": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior.", "categories": ["cs.CL"], "published": "2025-05-22 09:00:01", "updated": "2025-05-22 09:00:01", "pdf_url": "http://arxiv.org/pdf/2505.16408v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16410v1", "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 09:00:19", "updated": "2025-05-22 09:00:19", "pdf_url": "http://arxiv.org/pdf/2505.16410v1", "comment": "Working in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16415v1", "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "abstract": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 09:04:03", "updated": "2025-05-22 09:04:03", "pdf_url": "http://arxiv.org/pdf/2505.16415v1", "comment": "Work in process", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16418v1", "title": "Exploring the Relationship Between Diversity and Quality in Ad Text Generation", "authors": ["Yoichi Aoki", "Soichiro Murakami", "Ukyo Honda", "Akihiko Kato"], "abstract": "In natural language generation for advertising, creating diverse and engaging\nad texts is crucial for capturing a broad audience and avoiding advertising\nfatigue. Regardless of the importance of diversity, the impact of the\ndiversity-enhancing methods in ad text generation -- mainly tested on tasks\nsuch as summarization and machine translation -- has not been thoroughly\nexplored. Ad text generation significantly differs from these tasks owing to\nthe text style and requirements. This research explores the relationship\nbetween diversity and ad quality in ad text generation by considering multiple\nfactors, such as diversity-enhancing methods, their hyperparameters,\ninput-output formats, and the models.", "categories": ["cs.CL"], "published": "2025-05-22 09:05:44", "updated": "2025-05-22 09:05:44", "pdf_url": "http://arxiv.org/pdf/2505.16418v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16421v1", "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "abstract": "While reinforcement learning (RL) has demonstrated remarkable success in\nenhancing large language models (LLMs), it has primarily focused on single-turn\ntasks such as solving math problems. Training effective web agents for\nmulti-turn interactions remains challenging due to the complexity of\nlong-horizon decision-making across dynamic web interfaces. In this work, we\npresent WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework\nfor training web agents. It learns directly from online interactions with web\nenvironments by asynchronously generating diverse trajectories, entirely guided\nby binary rewards depending on task success. Experiments on the WebArena-Lite\nbenchmark demonstrate the effectiveness of WebAgent-R1, boosting the task\nsuccess rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to\n44.8%, significantly outperforming existing state-of-the-art methods and strong\nproprietary models such as OpenAI o3. In-depth analyses reveal the\neffectiveness of the thinking-based prompting strategy and test-time scaling\nthrough increased interactions for web tasks. We further investigate different\nRL initialization policies by introducing two variants, namely WebAgent-R1-Zero\nand WebAgent-R1-CoT, which highlight the importance of the warm-up training\nstage (i.e., behavior cloning) and provide insights on incorporating long\nchain-of-thought (CoT) reasoning in web agents.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-22 09:07:43", "updated": "2025-05-22 09:07:43", "pdf_url": "http://arxiv.org/pdf/2505.16421v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16425v1", "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "abstract": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 09:10:09", "updated": "2025-05-22 09:10:09", "pdf_url": "http://arxiv.org/pdf/2505.16425v1", "comment": "13 pages, 5 figures, under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16429v1", "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "abstract": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 09:14:23", "updated": "2025-05-22 09:14:23", "pdf_url": "http://arxiv.org/pdf/2505.16429v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16460v1", "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "abstract": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-22 09:42:11", "updated": "2025-05-22 09:42:11", "pdf_url": "http://arxiv.org/pdf/2505.16460v1", "comment": "16 pages, 13 tables, 1 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16467v1", "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fern\u00e1ndez"], "abstract": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity.", "categories": ["cs.CL"], "published": "2025-05-22 09:48:51", "updated": "2025-05-22 09:48:51", "pdf_url": "http://arxiv.org/pdf/2505.16467v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16470v1", "title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "authors": ["Kuicai Dong", "Yujing Chang", "Shijie Huang", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "abstract": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/.", "categories": ["cs.IR", "cs.CL", "cs.CV"], "published": "2025-05-22 09:52:57", "updated": "2025-05-22 09:52:57", "pdf_url": "http://arxiv.org/pdf/2505.16470v1", "comment": "preprint. code available at\n  \\url{https://mmdocrag.github.io/MMDocRAG/}", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16483v1", "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "abstract": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:10:07", "updated": "2025-05-22 10:10:07", "pdf_url": "http://arxiv.org/pdf/2505.16483v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16491v1", "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "abstract": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:22:39", "updated": "2025-05-22 10:22:39", "pdf_url": "http://arxiv.org/pdf/2505.16491v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16505v1", "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "abstract": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-22 10:41:35", "updated": "2025-05-22 10:41:35", "pdf_url": "http://arxiv.org/pdf/2505.16505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16514v1", "title": "AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios", "authors": ["Yuting Huang", "Meitong Guo", "Yiquan Wu", "Ang Li", "Xiaozhong Liu", "Keting Yin", "Changlong Sun", "Fei Wu", "Kun Kuang"], "abstract": "Recent advances in LegalAI have primarily focused on individual case judgment\nanalysis, often overlooking the critical appellate process within the judicial\nsystem. Appeals serve as a core mechanism for error correction and ensuring\nfair trials, making them highly significant both in practice and in research.\nTo address this gap, we present the AppealCase dataset, consisting of 10,000\npairs of real-world, matched first-instance and second-instance documents\nacross 91 categories of civil cases. The dataset also includes detailed\nannotations along five dimensions central to appellate review: judgment\nreversals, reversal reasons, cited legal provisions, claim-level decisions, and\nwhether there is new information in the second instance. Based on these\nannotations, we propose five novel LegalAI tasks and conduct a comprehensive\nevaluation across 20 mainstream models. Experimental results reveal that all\ncurrent models achieve less than 50% F1 scores on the judgment reversal\nprediction task, highlighting the complexity and challenge of the appeal\nscenario. We hope that the AppealCase dataset will spur further research in\nLegalAI for appellate case analysis and contribute to improving consistency in\njudicial decision-making.", "categories": ["cs.CL"], "published": "2025-05-22 10:50:33", "updated": "2025-05-22 10:50:33", "pdf_url": "http://arxiv.org/pdf/2505.16514v1", "comment": "15 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16518v1", "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "authors": ["Lovisa Hagstr\u00f6m", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "abstract": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 10:57:08", "updated": "2025-05-22 10:57:08", "pdf_url": "http://arxiv.org/pdf/2505.16518v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16520v1", "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "abstract": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 11:00:53", "updated": "2025-05-22 11:00:53", "pdf_url": "http://arxiv.org/pdf/2505.16520v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16522v1", "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "abstract": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 11:04:09", "updated": "2025-05-22 11:04:09", "pdf_url": "http://arxiv.org/pdf/2505.16522v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16526v1", "title": "EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance", "authors": ["Heejae Suh", "Yejin Jeon", "Deokhyung Kang", "Taehee Park", "Yejin Min", "Gary Geunbae Lee"], "abstract": "Small large language models (sLLMs) offer the advantage of being lightweight\nand efficient, which makes them suitable for resource-constrained environments.\nHowever, sLLMs often struggle to maintain topic consistency in task-oriented\ndialogue systems, which is critical for scenarios such as service chatbots.\nSpecifically, it is important to ensure that the model denies off-topic or\nmalicious inputs and adheres to its intended functionality so as to prevent\npotential misuse and uphold reliability. Towards this, existing activation\nengineering approaches have been proposed to manipulate internal activations\nduring inference. While these methods are effective in certain scenarios, our\npreliminary experiments reveal their limitations in ensuring topic adherence.\nTherefore, to address this, we propose a novel approach termed Entropy-scaled\nSteering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the\nsteering intensity based on input uncertainty, which allows the model to handle\noff-topic distractors effectively while preserving on-topic accuracy. Our\nexperiments demonstrate that EnSToM achieves significant performance gain with\na relatively small data size compared to fine-tuning approaches. By improving\ntopic adherence without compromising efficiency, our approach provides a robust\nsolution for enhancing sLLM-based dialogue systems.", "categories": ["cs.CL"], "published": "2025-05-22 11:12:27", "updated": "2025-05-22 11:12:27", "pdf_url": "http://arxiv.org/pdf/2505.16526v1", "comment": "Accepted at ACL 2025 (Findings, long paper)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16530v1", "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection", "authors": ["Yuliang Yan", "Haochun Tang", "Shuo Yan", "Enyan Dai"], "abstract": "Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-22 11:16:46", "updated": "2025-05-22 11:16:46", "pdf_url": "http://arxiv.org/pdf/2505.16530v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16538v1", "title": "Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models", "authors": ["Ercong Nie", "Helmut Schmid", "Hinrich Sch\u00fctze"], "abstract": "Language confusion -- where large language models (LLMs) generate unintended\nlanguages against the user's need -- remains a critical challenge, especially\nfor English-centric models. We present the first mechanistic interpretability\n(MI) study of language confusion, combining behavioral benchmarking with\nneuron-level analysis. Using the Language Confusion Benchmark (LCB), we show\nthat confusion points (CPs) -- specific positions where language switches occur\n-- are central to this phenomenon. Through layer-wise analysis with TunedLens\nand targeted neuron attribution, we reveal that transition failures in the\nfinal layers drive confusion. We further demonstrate that editing a small set\nof critical neurons, identified via comparative analysis with\nmultilingual-tuned models, substantially mitigates confusion without harming\ngeneral competence or fluency. Our approach matches multilingual alignment in\nconfusion reduction for most languages and yields cleaner, higher-quality\noutputs. These findings provide new insights into the internal dynamics of LLMs\nand highlight neuron-level interventions as a promising direction for robust,\ninterpretable multilingual language modeling.", "categories": ["cs.CL"], "published": "2025-05-22 11:29:17", "updated": "2025-05-22 11:29:17", "pdf_url": "http://arxiv.org/pdf/2505.16538v1", "comment": "16 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16552v1", "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "authors": ["Wenhui Tan", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Ruihua Song"], "abstract": "Large Language Models (LLMs) achieve superior performance through\nChain-of-Thought (CoT) reasoning, but these token-level reasoning chains are\ncomputationally expensive and inefficient. In this paper, we introduce\nCompressed Latent Reasoning (CoLaR), a novel framework that dynamically\ncompresses reasoning processes in latent space through a two-stage training\napproach. First, during supervised fine-tuning, CoLaR extends beyond next-token\nprediction by incorporating an auxiliary next compressed embedding prediction\nobjective. This process merges embeddings of consecutive tokens using a\ncompression factor randomly sampled from a predefined range, and trains a\nspecialized latent head to predict distributions of subsequent compressed\nembeddings. Second, we enhance CoLaR through reinforcement learning (RL) that\nleverages the latent head's non-deterministic nature to explore diverse\nreasoning paths and exploit more compact ones. This approach enables CoLaR to:\ni) perform reasoning at a dense latent level (i.e., silently), substantially\nreducing reasoning chain length, and ii) dynamically adjust reasoning speed at\ninference time by simply prompting the desired compression factor. Extensive\nexperiments across four mathematical reasoning datasets demonstrate that CoLaR\nachieves 14.1% higher accuracy than latent-based baseline methods at comparable\ncompression ratios, and reduces reasoning chain length by 53.3% with only 4.8%\nperformance degradation compared to explicit CoT method. Moreover, when applied\nto more challenging mathematical reasoning tasks, our RL-enhanced CoLaR\ndemonstrates performance gains of up to 5.4% while dramatically reducing latent\nreasoning chain length by 82.8%. The code and models will be released upon\nacceptance.", "categories": ["cs.CL"], "published": "2025-05-22 11:40:26", "updated": "2025-05-22 11:40:26", "pdf_url": "http://arxiv.org/pdf/2505.16552v1", "comment": "15 pages, 8 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16559v1", "title": "CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning", "authors": ["Biao Yi", "Tiansheng Huang", "Baolei Zhang", "Tong Li", "Lihai Nie", "Zheli Liu", "Li Shen"], "abstract": "Fine-tuning-as-a-service, while commercially successful for Large Language\nModel (LLM) providers, exposes models to harmful fine-tuning attacks. As a\nwidely explored defense paradigm against such attacks, unlearning attempts to\nremove malicious knowledge from LLMs, thereby essentially preventing them from\nbeing used to perform malicious tasks. However, we highlight a critical flaw:\nthe powerful general adaptability of LLMs allows them to easily bypass\nselective unlearning by rapidly relearning or repurposing their capabilities\nfor harmful tasks. To address this fundamental limitation, we propose a\nparadigm shift: instead of selective removal, we advocate for inducing model\ncollapse--effectively forcing the model to \"unlearn everything\"--specifically\nin response to updates characteristic of malicious adaptation. This collapse\ndirectly neutralizes the very general capabilities that attackers exploit,\ntackling the core issue unaddressed by selective unlearning. We introduce the\nCollapse Trap (CTRAP) as a practical mechanism to implement this concept\nconditionally. Embedded during alignment, CTRAP pre-configures the model's\nreaction to subsequent fine-tuning dynamics. If updates during fine-tuning\nconstitute a persistent attempt to reverse safety alignment, the pre-configured\ntrap triggers a progressive degradation of the model's core language modeling\nabilities, ultimately rendering it inert and useless for the attacker.\nCrucially, this collapse mechanism remains dormant during benign fine-tuning,\nensuring the model's utility and general capabilities are preserved for\nlegitimate users. Extensive empirical results demonstrate that CTRAP\neffectively counters harmful fine-tuning risks across various LLMs and attack\nsettings, while maintaining high performance in benign scenarios. Our code is\navailable at https://anonymous.4open.science/r/CTRAP.", "categories": ["cs.CR", "cs.CL"], "published": "2025-05-22 11:47:08", "updated": "2025-05-22 11:47:08", "pdf_url": "http://arxiv.org/pdf/2505.16559v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16566v1", "title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts", "authors": ["Dongwon Noh", "Donghyeok Koh", "Junghun Yuk", "Gyuwan Kim", "Jaeyong Lee", "Kyungtae Lim", "Cheoneum Park"], "abstract": "Prior benchmarks for evaluating the domain-specific knowledge of large\nlanguage models (LLMs) lack the scalability to handle complex academic tasks.\nTo address this, we introduce \\texttt{ScholarBench}, a benchmark centered on\ndeep expert knowledge and complex academic problem-solving, which evaluates the\nacademic reasoning ability of LLMs and is constructed through a three-step\nprocess. \\texttt{ScholarBench} targets more specialized and logically complex\ncontexts derived from academic literature, encompassing five distinct problem\ntypes. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the\nabstraction, comprehension, and reasoning capabilities of LLMs across eight\ndistinct research domains. To ensure high-quality evaluation data, we define\ncategory-specific example attributes and design questions that are aligned with\nthe characteristic research methodologies and discourse structures of each\ndomain. Additionally, this benchmark operates as an English-Korean bilingual\ndataset, facilitating simultaneous evaluation for linguistic capabilities of\nLLMs in both languages. The benchmark comprises 5,031 examples in Korean and\n5,309 in English, with even state-of-the-art models like o3-mini achieving an\naverage evaluation score of only 0.543, demonstrating the challenging nature of\nthis benchmark.", "categories": ["cs.CL"], "published": "2025-05-22 11:59:06", "updated": "2025-05-22 11:59:06", "pdf_url": "http://arxiv.org/pdf/2505.16566v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16570v1", "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training", "authors": ["Dongyang Fan", "Vinko Sabol\u010dec", "Martin Jaggi"], "abstract": "Large Language Models (LLMs) are commonly pretrained on vast corpora of text\nwithout utilizing contextual metadata such as source, quality, or topic,\nleading to a context-free learning paradigm. While recent studies suggest that\nadding metadata like URL information as context (i.e., auxiliary inputs not\nused in the loss calculation) can improve training efficiency and downstream\nperformance, they offer limited understanding of which types of metadata are\ntruly effective and under what conditions. In this work, we conduct a\nsystematic evaluation and find that not all metadata types contribute equally.\nOnly URL context speeds up training, whereas quality scores and topic/format\ndomain information offer no clear benefit. Furthermore, the improved downstream\nperformances of URL conditioning emerge only when longer prompts are used at\ninference time. In addition, we demonstrate that context-aware pretraining\nenables more controllable generation than context-free pretraining, in a\nclassifier-free guidance fashion. Although topic and format metadata do not\naccelerate training, they are effective for steering outputs, offering\nhuman-interpretable control over generation.", "categories": ["cs.CL"], "published": "2025-05-22 12:01:20", "updated": "2025-05-22 12:01:20", "pdf_url": "http://arxiv.org/pdf/2505.16570v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16576v1", "title": "EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions", "authors": ["Spencer Hong", "Meng Luo", "Xinyi Wan"], "abstract": "Determining the veracity of atomic claims is an imperative component of many\nrecently proposed fact-checking systems. Many approaches tackle this problem by\nfirst retrieving evidence by querying a search engine and then performing\nclassification by providing the evidence set and atomic claim to a large\nlanguage model, but this process deviates from what a human would do in order\nto perform the task. Recent work attempted to address this issue by proposing\niterative evidence retrieval, allowing for evidence to be collected several\ntimes and only when necessary. Continuing along this line of research, we\npropose a novel claim verification system, called EMULATE, which is designed to\nbetter emulate human actions through the use of a multi-agent framework where\neach agent performs a small part of the larger task, such as ranking search\nresults according to predefined criteria or evaluating webpage content.\nExtensive experiments on several benchmarks show clear improvements over prior\nwork, demonstrating the efficacy of our new multi-agent framework.", "categories": ["cs.CL"], "published": "2025-05-22 12:08:08", "updated": "2025-05-22 12:08:08", "pdf_url": "http://arxiv.org/pdf/2505.16576v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16582v1", "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "abstract": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 12:17:13", "updated": "2025-05-22 12:17:13", "pdf_url": "http://arxiv.org/pdf/2505.16582v1", "comment": "25 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16591v1", "title": "Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering", "authors": ["Bowen Jiang", "Runchuan Zhu", "Jiang Wu", "Zinco Jiang", "Yifan He", "Junyuan Gao", "Jia Yu", "Rui Min", "Yinfan Wang", "Haote Yang", "Songyang Zhang", "Dahua Lin", "Lijun Wu", "Conghui He"], "abstract": "We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual\nfactual ability of Large Language Models (LLMs). Inspired by existing research,\nwe created the question set with features such as single knowledge point\ncoverage, absolute objectivity, unique answers, and temporal stability. These\nquestions enable efficient evaluation using the LLM-as-judge paradigm, testing\nboth the LLMs' factual memory and self-awareness (\"know what they don't know\").\nKoLasSimpleQA expands existing research in two key dimensions: (1) Breadth\n(Multilingual Coverage): It includes 9 languages, supporting global\napplicability evaluation. (2) Depth (Dual Domain Design): It covers both the\ngeneral domain (global facts) and the language-specific domain (such as\nhistory, culture, and regional traditions) for a comprehensive assessment of\nmultilingual capabilities. We evaluated mainstream LLMs, including traditional\nLLM and emerging Large Reasoning Models. Results show significant performance\ndifferences between the two domains, particularly in performance metrics,\nranking, calibration, and robustness. This highlights the need for targeted\nevaluation and optimization in multilingual contexts. We hope KoLasSimpleQA\nwill help the research community better identify LLM capability boundaries in\nmultilingual contexts and provide guidance for model optimization. We will\nrelease KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .", "categories": ["cs.CL"], "published": "2025-05-22 12:27:02", "updated": "2025-05-22 12:27:02", "pdf_url": "http://arxiv.org/pdf/2505.16591v1", "comment": "Equal contribution: Bowen Jiang, Runchuan Zhu, Jiang Wu;\n  Corresponding author: Conghui He", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16592v1", "title": "What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse", "authors": ["Shijia Zhou", "Siyao Peng", "Simon Luebke", "J\u00f6rg Ha\u00dfler", "Mario Haim", "Saif M. Mohammad", "Barbara Plank"], "abstract": "Media framing refers to the emphasis on specific aspects of perceived reality\nto shape how an issue is defined and understood. Its primary purpose is to\nshape public perceptions often in alignment with the authors' opinions and\nstances. However, the interaction between stance and media frame remains\nlargely unexplored. In this work, we apply an interdisciplinary approach to\nconceptualize and computationally explore this interaction with internet memes\non climate change. We curate CLIMATEMEMES, the first dataset of climate-change\nmemes annotated with both stance and media frames, inspired by research in\ncommunication science. CLIMATEMEMES includes 1,184 memes sourced from 47\nsubreddits, enabling analysis of frame prominence over time and communities,\nand sheds light on the framing preferences of different stance holders. We\npropose two meme understanding tasks: stance detection and media frame\ndetection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the\ncorresponding results on their LLM backbone. Human captions consistently\nenhance performance. Synthetic captions and human-corrected OCR also help\noccasionally. Our findings highlight that VLMs perform well on stance, but\nstruggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'\nlimitations in handling nuanced frames and stance expressions on climate change\ninternet memes.", "categories": ["cs.CL", "cs.MM"], "published": "2025-05-22 12:27:12", "updated": "2025-05-22 12:27:12", "pdf_url": "http://arxiv.org/pdf/2505.16592v1", "comment": "19 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16610v1", "title": "From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "abstract": "Effective emotional support hinges on understanding users' emotions and needs\nto provide meaningful comfort during multi-turn interactions. Large Language\nModels (LLMs) show great potential for expressing empathy; however, they often\ndeliver generic and one-size-fits-all responses that fail to address users'\nspecific needs. To tackle this issue, we propose a self-evolution framework\ndesigned to help LLMs improve their responses to better align with users'\nimplicit preferences concerning user profiles (personalities), emotional\nstates, and specific situations. Our framework consists of two distinct phases:\n\\textit{(1)} \\textit{Emotional Support Experience Acquisition}, where LLMs are\nfine-tuned on limited emotional support conversation data to provide basic\nsupport, and \\textit{(2)} \\textit{Self-Improvement for Personalized Emotional\nSupport}, where LLMs leverage self-reflection and self-refinement to generate\npersonalized responses. Through iterative direct preference optimization\nbetween the pre- and post-refined responses, our model generates responses that\nreflect a better understanding of the user's implicit preferences. Extensive\nexperiments and evaluations demonstrate that our method significantly enhances\nthe model's performance in emotional support, reducing unhelpful responses and\nminimizing discrepancies between user preferences and model outputs.", "categories": ["cs.CL"], "published": "2025-05-22 12:45:12", "updated": "2025-05-22 12:45:12", "pdf_url": "http://arxiv.org/pdf/2505.16610v1", "comment": "27 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16612v1", "title": "Steering Large Language Models for Machine Translation Personalization", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "abstract": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 12:47:16", "updated": "2025-05-22 12:47:16", "pdf_url": "http://arxiv.org/pdf/2505.16612v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16624v1", "title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports", "authors": ["Francesco Dalla Serra", "Patrick Schrempf", "Chaoyang Wang", "Zaiqiao Meng", "Fani Deligianni", "Alison Q. O'Neil"], "abstract": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-22 12:57:35", "updated": "2025-05-22 12:57:35", "pdf_url": "http://arxiv.org/pdf/2505.16624v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16631v1", "title": "MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries", "authors": ["Jonghwi Kim", "Deokhyung Kang", "Seonjeong Hwang", "Yunsu Kim", "Jungseul Ok", "Gary Lee"], "abstract": "Despite bilingual speakers frequently using mixed-language queries in web\nsearches, Information Retrieval (IR) research on them remains scarce. To\naddress this, we introduce MiLQ,Mixed-Language Query test set, the first public\nbenchmark of mixed-language queries, confirmed as realistic and highly\npreferred. Experiments show that multilingual IR models perform moderately on\nMiLQ and inconsistently across native, English, and mixed-language queries,\nalso suggesting code-switched training data's potential for robust IR models\nhandling such queries. Meanwhile, intentional English mixing in queries proves\nan effective strategy for bilinguals searching English documents, which our\nanalysis attributes to enhanced token matching compared to native queries.", "categories": ["cs.IR", "cs.CL"], "published": "2025-05-22 13:03:15", "updated": "2025-05-22 13:03:15", "pdf_url": "http://arxiv.org/pdf/2505.16631v1", "comment": "16 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16637v1", "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "abstract": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 13:08:25", "updated": "2025-05-22 13:08:25", "pdf_url": "http://arxiv.org/pdf/2505.16637v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16648v1", "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "abstract": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:18:45", "updated": "2025-05-22 13:18:45", "pdf_url": "http://arxiv.org/pdf/2505.16648v1", "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16660v1", "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "abstract": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:24:52", "updated": "2025-05-22 13:24:52", "pdf_url": "http://arxiv.org/pdf/2505.16660v1", "comment": "29pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16661v1", "title": "A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP", "authors": ["Issey Sukeda", "Takuro Fujii", "Kosei Buma", "Shunsuke Sasaki", "Shinnosuke Ono"], "abstract": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.", "categories": ["cs.CL"], "published": "2025-05-22 13:27:37", "updated": "2025-05-22 13:27:37", "pdf_url": "http://arxiv.org/pdf/2505.16661v1", "comment": "15 pages, 9 tables, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16673v1", "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "authors": ["Huanjin Yao", "Qixiang Yin", "Jingyi Zhang", "Min Yang", "Yibo Wang", "Wenhao Wu", "Fei Su", "Li Shen", "Minghui Qiu", "Dacheng Tao", "Jiaxing Huang"], "abstract": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-22 13:39:32", "updated": "2025-05-22 13:39:32", "pdf_url": "http://arxiv.org/pdf/2505.16673v1", "comment": "Technical report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16686v1", "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge", "authors": ["Lars Benedikt Kaesberg", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "abstract": "Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 13:53:50", "updated": "2025-05-22 13:53:50", "pdf_url": "http://arxiv.org/pdf/2505.16686v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16694v1", "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 13:59:30", "updated": "2025-05-22 13:59:30", "pdf_url": "http://arxiv.org/pdf/2505.16694v1", "comment": "Accepted to ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16703v1", "title": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs", "authors": ["Zeping Yu", "Sophia Ananiadou"], "abstract": "Although multimodal large language models (MLLMs) have achieved impressive\nperformance, the multimodal instruction tuning stage often causes catastrophic\nforgetting of the base LLM's language ability, even in strong models like\nLlama3. To address this, we propose Locate-then-Merge, a training-free\nparameter fusion framework that first locates important parameters and then\nselectively merges them. We further introduce Neuron-Fusion, a neuron-level\nstrategy that preserves the influence of neurons with large parameter\nshifts--neurons likely responsible for newly acquired visual\ncapabilities--while attenuating the influence of neurons with smaller changes\nthat likely encode general-purpose language skills. This design enables better\nretention of visual adaptation while mitigating language degradation.\nExperiments on 13 benchmarks across both language and visual tasks show that\nNeuron-Fusion consistently outperforms existing model merging methods. Further\nanalysis reveals that our method effectively reduces context hallucination in\ngeneration.", "categories": ["cs.CL"], "published": "2025-05-22 14:04:43", "updated": "2025-05-22 14:04:43", "pdf_url": "http://arxiv.org/pdf/2505.16703v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16722v1", "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "abstract": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 14:30:14", "updated": "2025-05-22 14:30:14", "pdf_url": "http://arxiv.org/pdf/2505.16722v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16737v1", "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "abstract": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "published": "2025-05-22 14:52:10", "updated": "2025-05-22 14:52:10", "pdf_url": "http://arxiv.org/pdf/2505.16737v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16743v1", "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "abstract": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "published": "2025-05-22 14:53:53", "updated": "2025-05-22 14:53:53", "pdf_url": "http://arxiv.org/pdf/2505.16743v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16774v1", "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models", "authors": ["Yiming Gao", "Bin Wang", "Chengwei Wei", "Shuo Sun", "AiTi Aw"], "abstract": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area.", "categories": ["cs.CL"], "published": "2025-05-22 15:15:29", "updated": "2025-05-22 15:15:29", "pdf_url": "http://arxiv.org/pdf/2505.16774v1", "comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16782v1", "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning", "authors": ["Xinghao Chen", "Anhao Zhao", "Heming Xia", "Xuan Lu", "Hanlin Wang", "Yanjun Chen", "Wei Zhang", "Jian Wang", "Wenjie Li", "Xiaoyu Shen"], "abstract": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT.", "categories": ["cs.CL"], "published": "2025-05-22 15:26:51", "updated": "2025-05-22 15:26:51", "pdf_url": "http://arxiv.org/pdf/2505.16782v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16789v1", "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "abstract": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 15:30:00", "updated": "2025-05-22 15:30:00", "pdf_url": "http://arxiv.org/pdf/2505.16789v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16800v1", "title": "Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation", "authors": ["Changbing Yang", "Garrett Nicolai"], "abstract": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages.", "categories": ["cs.CL"], "published": "2025-05-22 15:40:09", "updated": "2025-05-22 15:40:09", "pdf_url": "http://arxiv.org/pdf/2505.16800v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16806v1", "title": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement", "authors": ["Kexin Zhang", "Junlan Chen", "Daifeng Li", "Yuxuan Zhang", "Yangyang Feng", "Bowen Deng", "Weixu Chen"], "abstract": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-22 15:45:29", "updated": "2025-05-22 15:45:29", "pdf_url": "http://arxiv.org/pdf/2505.16806v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16814v1", "title": "Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?", "authors": ["Gaurav Kamath", "Sowmya Vajjala"], "abstract": "Named Entity Recognition(NER) for low-resource languages aims to produce\nrobust systems for languages where there is limited labeled training data\navailable, and has been an area of increasing interest within NLP. Data\naugmentation for increasing the amount of low-resource labeled data is a common\npractice. In this paper, we explore the role of synthetic data in the context\nof multilingual, low-resource NER, considering 11 languages from diverse\nlanguage families. Our results suggest that synthetic data does in fact hold\npromise for low-resource language NER, though we see significant variation\nbetween languages.", "categories": ["cs.CL"], "published": "2025-05-22 15:50:47", "updated": "2025-05-22 15:50:47", "pdf_url": "http://arxiv.org/pdf/2505.16814v1", "comment": "pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16826v1", "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning", "authors": ["Wei Sun", "Wen Yang", "Pu Jian", "Qianlong Du", "Fuwei Cui", "Shuo Ren", "Jiajun Zhang"], "abstract": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 16:00:33", "updated": "2025-05-22 16:00:33", "pdf_url": "http://arxiv.org/pdf/2505.16826v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16831v1", "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "abstract": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "published": "2025-05-22 16:02:10", "updated": "2025-05-22 16:02:10", "pdf_url": "http://arxiv.org/pdf/2505.16831v1", "comment": "44 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16832v1", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "abstract": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "published": "2025-05-22 16:02:18", "updated": "2025-05-22 16:02:18", "pdf_url": "http://arxiv.org/pdf/2505.16832v1", "comment": "16 pages; 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16834v1", "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "abstract": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-22 16:05:02", "updated": "2025-05-22 16:05:02", "pdf_url": "http://arxiv.org/pdf/2505.16834v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16838v1", "title": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search", "authors": ["Yibo Wang", "Li Shen", "Huanjin Yao", "Tiansheng Huang", "Rui Liu", "Naiqiang Tan", "Jiaxing Huang", "Kai Zhang", "Dacheng Tao"], "abstract": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress", "categories": ["cs.CL"], "published": "2025-05-22 16:06:59", "updated": "2025-05-22 16:06:59", "pdf_url": "http://arxiv.org/pdf/2505.16838v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16847v1", "title": "Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study", "authors": ["Baran Barbarestani", "Isa Maks", "Piek Vossen"], "abstract": "This paper introduces a method for detecting inappropriately targeting\nlanguage in online conversations by integrating crowd and expert annotations\nwith ChatGPT. We focus on English conversation threads from Reddit, examining\ncomments that target individuals or groups. Our approach involves a\ncomprehensive annotation framework that labels a diverse data set for various\ntarget categories and specific target words within the conversational context.\nWe perform a comparative analysis of annotations from human experts, crowd\nannotators, and ChatGPT, revealing strengths and limitations of each method in\nrecognizing both explicit hate speech and subtler discriminatory language. Our\nfindings highlight the significant role of contextual factors in identifying\nhate speech and uncover new categories of targeting, such as social belief and\nbody image. We also address the challenges and subjective judgments involved in\nannotation and the limitations of ChatGPT in grasping nuanced language. This\nstudy provides insights for improving automated content moderation strategies\nto enhance online safety and inclusivity.", "categories": ["cs.CL"], "published": "2025-05-22 16:10:43", "updated": "2025-05-22 16:10:43", "pdf_url": "http://arxiv.org/pdf/2505.16847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16850v1", "title": "ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning", "authors": ["Tajamul Ashraf", "Mohammed Mohsen Peerzada", "Moloud Abdar", "Yutong Xie", "Yuyin Zhou", "Xiaofeng Liu", "Iqra Altaf Gillani", "Janibul Bashir"], "abstract": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy across decentralized participants.\nAs FL adoption grows, numerous techniques have been proposed to tackle its\npractical challenges. However, the lack of standardized evaluation across key\ndimensions hampers systematic progress and fair comparison of FL methods. In\nthis work, we introduce ATR-Bench, a unified framework for analyzing federated\nlearning through three foundational dimensions: Adaptation, Trust, and\nReasoning. We provide an in-depth examination of the conceptual foundations,\ntask formulations, and open research challenges associated with each theme. We\nhave extensively benchmarked representative methods and datasets for adaptation\nto heterogeneous clients and trustworthiness in adversarial or unreliable\nenvironments. Due to the lack of reliable metrics and models for reasoning in\nFL, we only provide literature-driven insights for this dimension. ATR-Bench\nlays the groundwork for a systematic and holistic evaluation of federated\nlearning with real-world relevance. We will make our complete codebase publicly\naccessible and a curated repository that continuously tracks new developments\nand research in the FL literature.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-22 16:11:38", "updated": "2025-05-22 16:11:38", "pdf_url": "http://arxiv.org/pdf/2505.16850v1", "comment": "Federated Learning Benchmark for Domain Adaptation, Trustworthiness,\n  and Reasoning", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16855v1", "title": "Nested Named Entity Recognition as Single-Pass Sequence Labeling", "authors": ["Alberto Mu\u00f1oz-Ortiz", "David Vilares", "Caio COrro", "Carlos G\u00f3mez-Rodr\u00edguez"], "abstract": "We cast nested named entity recognition (NNER) as a sequence labeling task by\nleveraging prior work that linearizes constituency structures, effectively\nreducing the complexity of this structured prediction problem to\nstraightforward token classification. By combining these constituency\nlinearizations with pretrained encoders, our method captures nested entities\nwhile performing exactly $n$ tagging actions. Our approach achieves competitive\nperformance compared to less efficient systems, and it can be trained using any\noff-the-shelf sequence labeling library.", "categories": ["cs.CL", "68T50", "I.2.7"], "published": "2025-05-22 16:13:39", "updated": "2025-05-22 16:13:39", "pdf_url": "http://arxiv.org/pdf/2505.16855v1", "comment": "Submitted to EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16868v1", "title": "Comparative analysis of subword tokenization approaches for Indian languages", "authors": ["Sudhansu Bala Das", "Samujjal Choudhury", "Tapas Kumar Mishra", "Bidyut Kr. Patra"], "abstract": "Tokenization is the act of breaking down text into smaller parts, or tokens,\nthat are easier for machines to process. This is a key phase in machine\ntranslation (MT) models. Subword tokenization enhances this process by breaking\ndown words into smaller subword units, which is especially beneficial in\nlanguages with complicated morphology or a vast vocabulary. It is useful in\ncapturing the intricate structure of words in Indian languages (ILs), such as\nprefixes, suffixes, and other morphological variations. These languages\nfrequently use agglutinative structures, in which words are formed by the\ncombination of multiple morphemes such as suffixes, prefixes, and stems. As a\nresult, a suitable tokenization strategy must be chosen to address these\nscenarios. This paper examines how different subword tokenization techniques,\nsuch as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,\naffect ILs. The effectiveness of these subword tokenization techniques is\ninvestigated in statistical, neural, and multilingual neural machine\ntranslation models. All models are examined using standard evaluation metrics,\nsuch as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,\nRIBES, and COMET. Based on the results, it appears that for the majority of\nlanguage pairs for the Statistical and Neural MT models, the SentencePiece\ntokenizer continuously performed better than other tokenizers in terms of BLEU\nscore. However, BPE tokenization outperformed other tokenization techniques in\nthe context of Multilingual Neural Machine Translation model. The results show\nthat, despite using the same tokenizer and dataset for each model, translations\nfrom ILs to English surpassed translations from English to ILs.", "categories": ["cs.CL"], "published": "2025-05-22 16:24:37", "updated": "2025-05-22 16:24:37", "pdf_url": "http://arxiv.org/pdf/2505.16868v1", "comment": "24 pages, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16869v1", "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Jiahe Guo", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "abstract": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility.", "categories": ["cs.CL"], "published": "2025-05-22 16:24:51", "updated": "2025-05-22 16:24:51", "pdf_url": "http://arxiv.org/pdf/2505.16869v1", "comment": "To Appear at ACL 2025 (Main)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16881v1", "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "abstract": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 16:35:33", "updated": "2025-05-22 16:35:33", "pdf_url": "http://arxiv.org/pdf/2505.16881v1", "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16886v1", "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "abstract": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 16:41:37", "updated": "2025-05-22 16:41:37", "pdf_url": "http://arxiv.org/pdf/2505.16886v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16888v1", "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "authors": ["Viet Pham", "Thai Le"], "abstract": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-22 16:47:15", "updated": "2025-05-22 16:47:15", "pdf_url": "http://arxiv.org/pdf/2505.16888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16894v1", "title": "Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs", "authors": ["Zeyu Wei", "Shuo Wang", "Xiaohui Rong", "Xuemin Liu", "He Li"], "abstract": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms.", "categories": ["cs.CL"], "published": "2025-05-22 16:50:58", "updated": "2025-05-22 16:50:58", "pdf_url": "http://arxiv.org/pdf/2505.16894v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16900v1", "title": "Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "abstract": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-22 16:59:26", "updated": "2025-05-22 16:59:26", "pdf_url": "http://arxiv.org/pdf/2505.16900v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16922v1", "title": "UNCLE: Uncertainty Expressions in Long-Form Generation", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Dong Yu", "Nigel Collier", "Deqing Yang"], "abstract": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE.", "categories": ["cs.CL"], "published": "2025-05-22 17:16:08", "updated": "2025-05-22 17:16:08", "pdf_url": "http://arxiv.org/pdf/2505.16922v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16927v1", "title": "Latent Principle Discovery for Language Model Self-Improvement", "authors": ["Keshav Ramji", "Tahira Naseem", "Ram\u00f3n Fernandez Astudillo"], "abstract": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 17:20:18", "updated": "2025-05-22 17:20:18", "pdf_url": "http://arxiv.org/pdf/2505.16927v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16931v1", "title": "PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues", "authors": ["Matthew Zent", "Digory Smith", "Simon Woodhead"], "abstract": "Personally identifiable information (PII) anonymization is a high-stakes task\nthat poses a barrier to many open-science data sharing initiatives. While PII\nidentification has made large strides in recent years, in practice, error\nthresholds and the recall/precision trade-off still limit the uptake of these\nanonymization pipelines. We present PIIvot, a lighter-weight framework for PII\nanonymization that leverages knowledge of the data context to simplify the PII\ndetection problem. To demonstrate its effectiveness, we also contribute\nQATD-2k, the largest open-source real-world tutoring dataset of its kind, to\nsupport the demand for quality educational dialogue data.", "categories": ["cs.CL"], "published": "2025-05-22 17:22:28", "updated": "2025-05-22 17:22:28", "pdf_url": "http://arxiv.org/pdf/2505.16931v1", "comment": "6 pages, 2 figures, submitted to EMNLP 2025, for associated dataset,\n  see\n  https://huggingface.co/datasets/Eedi/Question-Anchored-Tutoring-Dialogues-2k", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16932v1", "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "abstract": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "published": "2025-05-22 17:23:14", "updated": "2025-05-22 17:23:14", "pdf_url": "http://arxiv.org/pdf/2505.16932v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16933v1", "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "abstract": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.", "categories": ["cs.LG", "cs.CL", "cs.CV"], "published": "2025-05-22 17:23:26", "updated": "2025-05-22 17:23:26", "pdf_url": "http://arxiv.org/pdf/2505.16933v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16934v1", "title": "In-Context Watermarks for Large Language Models", "authors": ["Yepeng Liu", "Xuandong Zhao", "Christopher Kruegel", "Dawn Song", "Yuheng Bu"], "abstract": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution.", "categories": ["cs.CL"], "published": "2025-05-22 17:24:51", "updated": "2025-05-22 17:24:51", "pdf_url": "http://arxiv.org/pdf/2505.16934v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16938v1", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.", "categories": ["cs.AI", "cs.CL", "cs.CV"], "published": "2025-05-22 17:27:43", "updated": "2025-05-22 17:27:43", "pdf_url": "http://arxiv.org/pdf/2505.16938v1", "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16944v1", "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-22 17:31:10", "updated": "2025-05-22 17:31:10", "pdf_url": "http://arxiv.org/pdf/2505.16944v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16956v1", "title": "On Multilingual Encoder Language Model Compression for Low-Resource Languages", "authors": ["Daniil Gurgurov", "Michal Gregor", "Josef van Genabith", "Simon Ostermann"], "abstract": "In this paper, we combine two-step knowledge distillation, structured\npruning, truncation, and vocabulary trimming for extremely compressing\nmultilingual encoder-only language models for low-resource languages. Our novel\napproach systematically combines existing techniques and takes them to the\nextreme, reducing layer depth, feed-forward hidden size, and intermediate layer\nembedding size to create significantly smaller monolingual models while\nretaining essential language-specific knowledge. We achieve compression rates\nof up to 92% with only a marginal performance drop of 2-10% in four downstream\ntasks, including sentiment analysis, topic classification, named entity\nrecognition, and part-of-speech tagging, across three low-resource languages.\nNotably, the performance degradation correlates with the amount of\nlanguage-specific data in the teacher model, with larger datasets resulting in\nsmaller performance losses. Additionally, we conduct extensive ablation studies\nto identify best practices for multilingual model compression using these\ntechniques.", "categories": ["cs.CL"], "published": "2025-05-22 17:35:39", "updated": "2025-05-22 17:35:39", "pdf_url": "http://arxiv.org/pdf/2505.16956v1", "comment": "Pre-print", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16964v1", "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning", "authors": ["Suhao Yu", "Haojin Wang", "Juncheng Wu", "Cihang Xie", "Yuyin Zhou"], "abstract": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-22 17:46:11", "updated": "2025-05-22 17:46:11", "pdf_url": "http://arxiv.org/pdf/2505.16964v1", "comment": "9 pages, 4 Figures Benchmark data:\n  https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16965v1", "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "abstract": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-22 17:46:23", "updated": "2025-05-22 17:46:23", "pdf_url": "http://arxiv.org/pdf/2505.16965v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16967v1", "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval", "authors": ["Nandan Thakur", "Crystina Zhang", "Xueguang Ma", "Jimmy Lin"], "abstract": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "published": "2025-05-22 17:47:57", "updated": "2025-05-22 17:47:57", "pdf_url": "http://arxiv.org/pdf/2505.16967v1", "comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16968v1", "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "abstract": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "published": "2025-05-22 17:48:53", "updated": "2025-05-22 17:48:53", "pdf_url": "http://arxiv.org/pdf/2505.16968v1", "comment": "20 pages, 11 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16972v1", "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition", "authors": ["Tianduo Wang", "Lu Xu", "Wei Lu", "Shanbo Cheng"], "abstract": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-22 17:51:05", "updated": "2025-05-22 17:51:05", "pdf_url": "http://arxiv.org/pdf/2505.16972v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16973v1", "title": "VeriFastScore: Speeding up long-form factuality evaluation", "authors": ["Rishanth Rajendhran", "Amir Zadeh", "Matthew Sarte", "Chuan Li", "Mohit Iyyer"], "abstract": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets.", "categories": ["cs.CL"], "published": "2025-05-22 17:51:25", "updated": "2025-05-22 17:51:25", "pdf_url": "http://arxiv.org/pdf/2505.16973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16975v1", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "abstract": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.", "categories": ["cs.SE", "cs.CL"], "published": "2025-05-22 17:51:49", "updated": "2025-05-22 17:51:49", "pdf_url": "http://arxiv.org/pdf/2505.16975v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16983v1", "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding", "authors": ["Junlong Tong", "Jinlan Fu", "Zixuan Lin", "Yingqi Fan", "Anhao Zhao", "Hui Su", "Xiaoyu Shen"], "abstract": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM.", "categories": ["cs.CL"], "published": "2025-05-22 17:53:28", "updated": "2025-05-22 17:53:28", "pdf_url": "http://arxiv.org/pdf/2505.16983v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16984v1", "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning", "authors": ["Mingyang Liu", "Gabriele Farina", "Asuman Ozdaglar"], "abstract": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-22 17:53:57", "updated": "2025-05-22 17:53:57", "pdf_url": "http://arxiv.org/pdf/2505.16984v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16986v1", "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 17:54:32", "updated": "2025-05-22 17:54:32", "pdf_url": "http://arxiv.org/pdf/2505.16986v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16988v1", "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-22 17:54:38", "updated": "2025-05-22 17:54:38", "pdf_url": "http://arxiv.org/pdf/2505.16988v1", "comment": "18 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16994v1", "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning", "authors": ["Runyang You", "Yongqi Li", "Xinyu Lin", "Xin Zhang", "Wenjie Wang", "Wenjie Li", "Liqiang Nie"], "abstract": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "published": "2025-05-22 17:55:43", "updated": "2025-05-22 17:55:43", "pdf_url": "http://arxiv.org/pdf/2505.16994v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16995v1", "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization", "authors": ["Chao Zhang", "Xin Shi", "Xueqiao Zhang", "Yifan Zhu", "Yi Yang", "Yawei Luo"], "abstract": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality.", "categories": ["cs.CL"], "published": "2025-05-22 17:56:21", "updated": "2025-05-22 17:56:21", "pdf_url": "http://arxiv.org/pdf/2505.16995v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16997v1", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.", "categories": ["cs.AI", "cs.CL", "cs.MA"], "published": "2025-05-22 17:56:39", "updated": "2025-05-22 17:56:39", "pdf_url": "http://arxiv.org/pdf/2505.16997v1", "comment": "19 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.16998v1", "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "abstract": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-22 17:57:23", "updated": "2025-05-22 17:57:23", "pdf_url": "http://arxiv.org/pdf/2505.16998v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17005v1", "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "abstract": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-22 17:58:26", "updated": "2025-05-22 17:58:26", "pdf_url": "http://arxiv.org/pdf/2505.17005v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17015v1", "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models", "authors": ["Runsen Xu", "Weiyao Wang", "Hao Tang", "Xingyu Chen", "Xiaodong Wang", "Fu-Jen Chu", "Dahua Lin", "Matt Feiszli", "Kevin J. Liang"], "abstract": "Multi-modal large language models (MLLMs) have rapidly advanced in visual\ntasks, yet their spatial understanding remains limited to single images,\nleaving them ill-suited for robotics and other real-world applications that\nrequire multi-frame reasoning. In this paper, we propose a framework to equip\nMLLMs with robust multi-frame spatial understanding by integrating depth\nperception, visual correspondence, and dynamic perception. Central to our\napproach is the MultiSPA dataset, a novel, large-scale collection of more than\n27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we\nintroduce a comprehensive benchmark that tests a wide spectrum of spatial tasks\nunder uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves\nsignificant gains over baselines and proprietary systems, demonstrating\nscalable, generalizable multi-frame reasoning. We further observe multi-task\nbenefits and early indications of emergent capabilities in challenging\nscenarios, and showcase how our model can serve as a multi-frame reward\nannotator for robotics.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-22 17:59:39", "updated": "2025-05-22 17:59:39", "pdf_url": "http://arxiv.org/pdf/2505.17015v1", "comment": "24 pages. An MLLM, dataset, and benchmark for multi-frame spatial\n  understanding. Project page: https://runsenxu.com/projects/Multi-SpatialMLLM", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17017v1", "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "abstract": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-22 17:59:49", "updated": "2025-05-22 17:59:49", "pdf_url": "http://arxiv.org/pdf/2505.17017v1", "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.17022v1", "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "abstract": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "published": "2025-05-22 17:59:58", "updated": "2025-05-22 17:59:58", "pdf_url": "http://arxiv.org/pdf/2505.17022v1", "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19381v1", "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving", "authors": ["Anqing Jiang", "Yu Gao", "Zhigang Sun", "Yiru Wang", "Jijun Wang", "Jinghao Chai", "Qian Cao", "Yuweng Heng", "Hao Jiang", "Zongzheng Zhang", "Xianda Guo", "Hao Sun", "Hao Zhao"], "abstract": "Research interest in end-to-end autonomous driving has surged owing to its\nfully differentiable design integrating modular tasks, i.e. perception,\nprediction and planing, which enables optimization in pursuit of the ultimate\ngoal. Despite the great potential of the end-to-end paradigm, existing methods\nsuffer from several aspects including expensive BEV (bird's eye view)\ncomputation, action diversity, and sub-optimal decision in complex real-world\nscenarios. To address these challenges, we propose a novel hybrid sparse-dense\ndiffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.\nWe explore the sparse diffusion representation for efficient multi-modal\ndriving behavior. Moreover, we rethink the effectiveness of VLM driving\ndecision and improve the trajectory generation guidance through deep\ninteraction across agent, map instances and VLM output. Our method shows\nsuperior performance in Autonomous Grand Challenge 2025 which contains\nchallenging real and reactive synthetic scenarios. Our methods achieves 45.0\nPDMS.", "categories": ["cs.AI", "cs.CV", "cs.RO"], "published": "2025-05-26 00:49:35", "updated": "2025-05-26 00:49:35", "pdf_url": "http://arxiv.org/pdf/2505.19381v1", "comment": "4pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19383v1", "title": "CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models", "authors": ["Varun Reddy", "Yen-Ling Kuo"], "abstract": "Large language models (LLMs) exhibit strong performance on factual recall and\ngeneral reasoning but struggle to adapt to user-specific, commonsense\nknowledge, a challenge particularly acute in small-parameter settings where\ncomputational efficiency is prioritized. We introduce CaseEdit, a new dataset\nand generation pipeline for evaluating localized, personalized commonsense\nknowledge editing in small LLMs to address this. Built upon the ATOMIC20/20\ncommonsense graph, CaseEdit uses a multi-stage inference process to generate\nboth typical and atypical contextual edits for household objects, paired with\ntargeted evaluation questions across four axes: reliability, generalization,\nlocality, and portability. We evaluate established knowledge editing methods\nusing CaseEdit and demonstrate that AlphaEdit, a technique employing null-space\nprojection to minimize interference with unrelated knowledge, consistently\noutperforms other methods when applied to an LLaMA 3.2 3B model, even in\nscalability tests, showing minimal ripple effects. Our results indicate that\nusing CaseEdit with effective editing techniques like AlphaEdit allows small\nmodels to internalize high-quality, context-sensitive common-sense knowledge,\npaving the way for lightweight, personalized assistants.", "categories": ["cs.AI"], "published": "2025-05-26 00:54:04", "updated": "2025-05-26 00:54:04", "pdf_url": "http://arxiv.org/pdf/2505.19383v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19385v1", "title": "Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion", "authors": ["Jiaqi Guo", "Santiago Lopez-Tapia", "Aggelos K. Katsaggelos"], "abstract": "Limited Angle Computed Tomography (LACT) often faces significant challenges\ndue to missing angular information. Unlike previous methods that operate in the\nimage domain, we propose a new method that focuses on sinogram inpainting. We\nleverage MR-SDEs, a variant of diffusion models that characterize the diffusion\nprocess with mean-reverting stochastic differential equations, to fill in\nmissing angular data at the projection level. Furthermore, by combining\ndistillation with constraining the output of the model using the pseudo-inverse\nof the inpainting matrix, the diffusion process is accelerated and done in a\nstep, enabling efficient and accurate sinogram completion. A subsequent\npost-processing module back-projects the inpainted sinogram into the image\ndomain and further refines the reconstruction, effectively suppressing\nartifacts while preserving critical structural details. Quantitative\nexperimental results demonstrate that the proposed method achieves\nstate-of-the-art performance in both perceptual and fidelity quality, offering\na promising solution for LACT reconstruction in scientific and clinical\napplications.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 00:59:58", "updated": "2025-05-26 00:59:58", "pdf_url": "http://arxiv.org/pdf/2505.19385v1", "comment": "Accepted at the 2025 IEEE International Conference on Image\n  Processing (Oral)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19386v1", "title": "Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals", "authors": ["Nate Gillman", "Charles Herrmann", "Michael Freeman", "Daksh Aggarwal", "Evan Luo", "Deqing Sun", "Chen Sun"], "abstract": "Recent advances in video generation models have sparked interest in world\nmodels capable of simulating realistic environments. While navigation has been\nwell-explored, physically meaningful interactions that mimic real-world forces\nremain largely understudied. In this work, we investigate using physical forces\nas a control signal for video generation and propose force prompts which enable\nusers to interact with images through both localized point forces, such as\npoking a plant, and global wind force fields, such as wind blowing on fabric.\nWe demonstrate that these force prompts can enable videos to respond\nrealistically to physical control signals by leveraging the visual and motion\nprior in the original pretrained model, without using any 3D asset or physics\nsimulator at inference. The primary challenge of force prompting is the\ndifficulty in obtaining high quality paired force-video training data, both in\nthe real world due to the difficulty of obtaining force signals, and in\nsynthetic data due to limitations in the visual quality and domain diversity of\nphysics simulators. Our key finding is that video generation models can\ngeneralize remarkably well when adapted to follow physical force conditioning\nfrom videos synthesized by Blender, even with limited demonstrations of few\nobjects. Our method can generate videos which simulate forces across diverse\ngeometries, settings, and materials. We also try to understand the source of\nthis generalization and perform ablations that reveal two key elements: visual\ndiversity and the use of specific text keywords during training. Our approach\nis trained on only around 15k training examples for a single day on four A100\nGPUs, and outperforms existing methods on force adherence and physics realism,\nbringing world models closer to real-world physics interactions. We release all\ndatasets, code, weights, and interactive video demos at our project page.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 01:04:02", "updated": "2025-05-26 01:04:02", "pdf_url": "http://arxiv.org/pdf/2505.19386v1", "comment": "Project page: https://force-prompting.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19392v1", "title": "Simple and Effective Baselines for Code Summarisation Evaluation", "authors": ["Jade Robinson", "Jonathan K. Kummerfeld"], "abstract": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias.", "categories": ["cs.CL", "cs.AI", "cs.SE", "68T50", "I.2.7"], "published": "2025-05-26 01:16:41", "updated": "2025-05-26 01:16:41", "pdf_url": "http://arxiv.org/pdf/2505.19392v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19395v1", "title": "VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation", "authors": ["Ethan TS. Liu", "Austin Wang", "Spencer Mateega", "Carlos Georgescu", "Danny Tang"], "abstract": "Ensuring that large language models (LLMs) can effectively assess, detect,\nexplain, and remediate software vulnerabilities is critical for building robust\nand secure software systems. We introduce VADER, a human-evaluated benchmark\ndesigned explicitly to assess LLM performance across four key\nvulnerability-handling dimensions: assessment, detection, explanation, and\nremediation. VADER comprises 174 real-world software vulnerabilities, each\ncarefully curated from GitHub repositories and annotated by security experts.\nFor each vulnerability case, models are tasked with identifying the flaw,\nclassifying it using Common Weakness Enumeration (CWE), explaining its\nunderlying cause, proposing a patch, and formulating a test plan. Using a\none-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7\nSonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and\nhuman security experts evaluated each response according to a rigorous scoring\nrubric emphasizing remediation (quality of the code fix, 50%), explanation\n(20%), and classification and test plan (30%) according to a standardized\nrubric. Our results show that current state-of-the-art LLMs achieve only\nmoderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with\nothers in the 49-54% range, indicating ample room for improvement. Notably,\nremediation quality is strongly correlated (Pearson r > 0.97) with accurate\nclassification and test plans, suggesting that models that effectively\ncategorize vulnerabilities also tend to fix them well. VADER's comprehensive\ndataset, detailed evaluation rubrics, scoring tools, and visualized results\nwith confidence intervals are publicly released, providing the community with\nan interpretable, reproducible benchmark to advance vulnerability-aware LLMs.\nAll code and data are available at: https://github.com/AfterQuery/vader", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-26 01:20:44", "updated": "2025-05-26 01:20:44", "pdf_url": "http://arxiv.org/pdf/2505.19395v1", "comment": "16 pages, 8 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19402v1", "title": "Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods", "authors": ["Tai-Quan Peng", "Xuzhen Yang"], "abstract": "This paper examines how large language models (LLMs) are transforming core\nquantitative methods in communication research in particular, and in the social\nsciences more broadly-namely, content analysis, survey research, and\nexperimental studies. Rather than replacing classical approaches, LLMs\nintroduce new possibilities for coding and interpreting text, simulating\ndynamic respondents, and generating personalized and interactive stimuli.\nDrawing on recent interdisciplinary work, the paper highlights both the\npotential and limitations of LLMs as research tools, including issues of\nvalidity, bias, and interpretability. To situate these developments\ntheoretically, the paper revisits Lasswell's foundational framework -- \"Who\nsays what, in which channel, to whom, with what effect?\" -- and demonstrates\nhow LLMs reconfigure message studies, audience analysis, and effects research\nby enabling interpretive variation, audience trajectory modeling, and\ncounterfactual experimentation. Revisiting the metaphor of the methodological\ncompass, the paper argues that classical research logics remain essential as\nthe field integrates LLMs and generative AI. By treating LLMs not only as\ntechnical instruments but also as epistemic and cultural tools, the paper calls\nfor thoughtful, rigorous, and imaginative use of LLMs in future communication\nand social science research.", "categories": ["cs.AI", "cs.CY"], "published": "2025-05-26 01:38:02", "updated": "2025-05-26 01:38:02", "pdf_url": "http://arxiv.org/pdf/2505.19402v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19404v1", "title": "Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning", "authors": ["Yuta Ono", "Hiroshi Nakamura", "Hideki Takase"], "abstract": "Federated Active Learning (FAL) seeks to reduce the burden of annotation\nunder the realistic constraints of federated learning by leveraging Active\nLearning (AL). As FAL settings make it more expensive to obtain ground truth\nlabels, FAL strategies that work well in low-budget regimes, where the amount\nof annotation is very limited, are needed. In this work, we investigate the\neffectiveness of TypiClust, a successful low-budget AL strategy, in low-budget\nFAL settings. Our empirical results show that TypiClust works well even in\nlow-budget FAL settings contrasted with relatively low performances of other\nmethods, although these settings present additional challenges, such as data\nheterogeneity, compared to AL. In addition, we show that FAL settings cause\ndistribution shifts in terms of typicality, but TypiClust is not very\nvulnerable to the shifts. We also analyze the sensitivity of TypiClust to\nfeature extraction methods, and it suggests a way to perform FAL even in\nlimited data situations.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-26 01:40:52", "updated": "2025-05-26 01:40:52", "pdf_url": "http://arxiv.org/pdf/2505.19404v1", "comment": "6 pages. Accepted at COMPSAC 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19406v1", "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "authors": ["Tianle Li", "Jihai Zhang", "Yongming Rao", "Yu Cheng"], "abstract": "While large language models (LLMs) demonstrate strong reasoning capabilities\nutilizing reinforcement learning (RL) with verifiable reward, whether large\nvision-language models (VLMs) can directly inherit such capabilities through\nsimilar post-training strategies remains underexplored. In this work, we\nconduct a systematic compositional probing study to evaluate whether current\nVLMs trained with RL or other post-training strategies can compose capabilities\nacross modalities or tasks under out-of-distribution conditions. We design a\nsuite of diagnostic tasks that train models on unimodal tasks or isolated\nreasoning skills, and evaluate them on multimodal, compositional variants\nrequiring skill integration. Through comparisons between supervised fine-tuning\n(SFT) and RL-trained models, we identify three key findings: (1) RL-trained\nmodels consistently outperform SFT on compositional generalization,\ndemonstrating better integration of learned skills; (2) although VLMs achieve\nstrong performance on individual tasks, they struggle to generalize\ncompositionally under cross-modal and cross-task scenario, revealing a\nsignificant gap in current training strategies; (3) enforcing models to\nexplicitly describe visual content before reasoning (e.g.,\ncaption-before-thinking), along with rewarding progressive vision-to-text\ngrounding, yields notable gains. It highlights two essential ingredients for\nimproving compositionality in VLMs: visual-to-text alignment and accurate\nvisual grounding. Our findings shed light on the current limitations of\nRL-based reasoning VLM training and provide actionable insights toward building\nmodels that reason compositionally across modalities and tasks.", "categories": ["cs.AI"], "published": "2025-05-26 01:42:38", "updated": "2025-05-26 01:42:38", "pdf_url": "http://arxiv.org/pdf/2505.19406v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19409v1", "title": "Fusion Intelligence for Digital Twinning AI Data Centers: A Synergistic GenAI-PhyAI Approach", "authors": ["Ruihang Wang", "Minghao Li", "Zhiwei Cao", "Jimin Jia", "Kyle Guan", "Yonggang Wen"], "abstract": "The explosion in artificial intelligence (AI) applications is pushing the\ndevelopment of AI-dedicated data centers (AIDCs), creating management\nchallenges that traditional methods and standalone AI solutions struggle to\naddress. While digital twins are beneficial for AI-based design validation and\noperational optimization, current AI methods for their creation face\nlimitations. Specifically, physical AI (PhyAI) aims to capture the underlying\nphysical laws, which demands extensive, case-specific customization, and\ngenerative AI (GenAI) can produce inaccurate or hallucinated results. We\npropose Fusion Intelligence, a novel framework synergizing GenAI's automation\nwith PhyAI's domain grounding. In this dual-agent collaboration, GenAI\ninterprets natural language prompts to generate tokenized AIDC digital twins.\nSubsequently, PhyAI optimizes these generated twins by enforcing physical\nconstraints and assimilating real-time data. Case studies demonstrate the\nadvantages of our framework in automating the creation and validation of AIDC\ndigital twins. These twins deliver predictive analytics to support power usage\neffectiveness (PUE) optimization in the design stage. With operational data\ncollected, the digital twin accuracy is further improved compared with pure\nphysics-based models developed by human experts. Fusion Intelligence offers a\npromising pathway to accelerate digital transformation. It enables more\nreliable and efficient AI-driven digital transformation for a broad range of\nmission-critical infrastructures.", "categories": ["cs.AI"], "published": "2025-05-26 01:58:34", "updated": "2025-05-26 01:58:34", "pdf_url": "http://arxiv.org/pdf/2505.19409v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19414v1", "title": "Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study", "authors": ["Ruihang Wang", "Zhiwei Cao", "Qingang Zhang", "Rui Tan", "Yonggang Wen", "Tommy Leung", "Stuart Kennedy", "Justin Teoh"], "abstract": "Data centers are the backbone of computing capacity. Operating data centers\nin the tropical regions faces unique challenges due to consistently high\nambient temperature and elevated relative humidity throughout the year. These\nconditions result in increased cooling costs to maintain the reliability of the\ncomputing systems. While existing machine learning-based approaches have\ndemonstrated potential to elevate operations to a more proactive and\nintelligent level, their deployment remains dubious due to concerns about model\nextrapolation capabilities and associated system safety issues. To address\nthese concerns, this article proposes incorporating the physical\ncharacteristics of data centers into traditional data-driven machine learning\nsolutions. We begin by introducing the data center system, including the\nrelevant multiphysics processes and the data-physics availability. Next, we\noutline the associated modeling and optimization problems and propose an\nintegrated, physics-informed machine learning system to address them. Using the\nproposed system, we present relevant applications across varying levels of\noperational intelligence. A case study on an industry-grade tropical data\ncenter is provided to demonstrate the effectiveness of our approach. Finally,\nwe discuss key challenges and highlight potential future directions.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-26 02:06:45", "updated": "2025-05-26 02:06:45", "pdf_url": "http://arxiv.org/pdf/2505.19414v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19419v1", "title": "It's Not Just Labeling\" -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features", "authors": ["Baichuan Li", "Larry Powell", "Tracy Hammond"], "abstract": "The quality of training data is critical to the performance of machine\nlearning applications in domains like transportation, healthcare, and robotics.\nAccurate image labeling, however, often relies on time-consuming, expert-driven\nmethods with limited feedback. This research introduces a sketch-based\nannotation approach supported by large language models (LLMs) to reduce\ntechnical barriers and enhance accessibility. Using a synthetic dataset, we\nexamine how sketch recognition features relate to LLM feedback metrics, aiming\nto improve the reliability and interpretability of LLM-assisted labeling. We\nalso explore how prompting strategies and sketch variations influence feedback\nquality. Our main contribution is a sketch-based virtual assistant that\nsimplifies annotation for non-experts and advances LLM-driven labeling tools in\nterms of scalability, accessibility, and explainability.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-26 02:13:52", "updated": "2025-05-26 02:13:52", "pdf_url": "http://arxiv.org/pdf/2505.19419v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19423v1", "title": "Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network", "authors": ["Bingdong Li", "Mei Jiang", "Hong Qian", "Peng Yang", "Wenjing Hong", "Hong Qian", "Ke Tang"], "abstract": "Evolutionary Reinforcement Learning (ERL), training the Reinforcement\nLearning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated\nenhanced exploration capabilities and greater robustness than using traditional\npolicy gradient. However, ERL suffers from the high computational costs and low\nsearch efficiency, as EAs require evaluating numerous candidate policies with\nexpensive simulations, many of which are ineffective and do not contribute\nmeaningfully to the training. One intuitive way to reduce the ineffective\nevaluations is to adopt the surrogates. Unfortunately, existing ERL policies\nare often modeled as deep neural networks (DNNs) and thus naturally represented\nas high-dimensional vectors containing millions of weights, which makes the\nbuilding of effective surrogates for ERL policies extremely challenging. This\npaper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)\nand Hyperbolic Neural Networks (HNN). Specifically, AE compresses\nhigh-dimensional policies into low-dimensional representations while extracting\nkey features as the inputs for the surrogate. HNN, functioning as a\nclassification-based surrogate model, can learn complex nonlinear relationships\nfrom sampled data and enable more accurate pre-selection of the sampled\npolicies without real evaluations. The experiments on 10 Atari and 4 Mujoco\ngames have verified that the proposed method outperforms previous approaches\nsignificantly. The search trajectories guided by AE and HNN are also visually\ndemonstrated to be more effective, in terms of both exploration and\nconvergence. This paper not only presents the first learnable policy embedding\nand surrogate-modeling modules for high-dimensional ERL policies, but also\nempirically reveals when and why they can be successful.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 02:25:17", "updated": "2025-05-26 02:25:17", "pdf_url": "http://arxiv.org/pdf/2505.19423v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19426v1", "title": "The Role of Diversity in In-Context Learning for Large Language Models", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "abstract": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 02:37:26", "updated": "2025-05-26 02:37:26", "pdf_url": "http://arxiv.org/pdf/2505.19426v1", "comment": "30 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19427v1", "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference", "authors": ["Sihan Chen", "Dan Zhao", "Jongwoo Ko", "Colby Banbury", "Huiping Zhuang", "Luming Liang", "Tianyi Chen"], "abstract": "The growing computational demands of large language models (LLMs) make\nefficient inference and activation strategies increasingly critical. While\nrecent approaches, such as Mixture-of-Experts (MoE), leverage selective\nactivation but require specialized training, training-free sparse activation\nmethods offer broader applicability and superior resource efficiency through\ntheir plug-and-play design. However, many existing methods rely solely on\nhidden state magnitudes to determine activation, resulting in high\napproximation errors and suboptimal inference accuracy. To address these\nlimitations, we propose WINA (Weight Informed Neuron Activation), a novel,\nsimple, and training-free sparse activation framework that jointly considers\nhidden state magnitudes and the column-wise $\\ell_2$-norms of weight matrices.\nWe show that this leads to a sparsification strategy that obtains optimal\napproximation error bounds with theoretical guarantees tighter than existing\ntechniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,\nTEAL) by up to $2.94\\%$ in average performance at the same sparsity levels,\nacross a diverse set of LLM architectures and datasets. These results position\nWINA as a new performance frontier for training-free sparse activation in LLM\ninference, advancing training-free sparse activation methods and setting a\nrobust baseline for efficient inference. The source code is available at\nhttps://github.com/microsoft/wina.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 02:37:32", "updated": "2025-05-26 02:37:32", "pdf_url": "http://arxiv.org/pdf/2505.19427v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19430v1", "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation", "authors": ["Keane Ong", "Rui Mao", "Deeksha Varshney", "Paul Pu Liang", "Erik Cambria", "Gianmarco Mengaldo"], "abstract": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 02:41:50", "updated": "2025-05-26 02:41:50", "pdf_url": "http://arxiv.org/pdf/2505.19430v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19434v1", "title": "CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features", "authors": ["X. Feng", "D. Zhang", "S. Hu", "X. Li", "M. Wu", "J. Zhang", "X. Chen", "K. Huang"], "abstract": "Effectively modeling and utilizing spatiotemporal features from RGB and other\nmodalities (\\eg, depth, thermal, and event data, denoted as X) is the core of\nRGB-X tracker design. Existing methods often employ two parallel branches to\nseparately process the RGB and X input streams, requiring the model to\nsimultaneously handle two dispersed feature spaces, which complicates both the\nmodel structure and computation process. More critically, intra-modality\nspatial modeling within each dispersed space incurs substantial computational\noverhead, limiting resources for inter-modality spatial modeling and temporal\nmodeling. To address this, we propose a novel tracker, CSTrack, which focuses\non modeling Compact Spatiotemporal features to achieve simple yet effective\ntracking. Specifically, we first introduce an innovative Spatial Compact Module\nthat integrates the RGB-X dual input streams into a compact spatial feature,\nenabling thorough intra- and inter-modality spatial modeling. Additionally, we\ndesign an efficient Temporal Compact Module that compactly represents temporal\nfeatures by constructing the refined target distribution heatmap. Extensive\nexperiments validate the effectiveness of our compact spatiotemporal modeling\nmethod, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.\nThe code and models will be released at:\nhttps://github.com/XiaokunFeng/CSTrack.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 02:53:12", "updated": "2025-05-26 02:53:12", "pdf_url": "http://arxiv.org/pdf/2505.19434v1", "comment": "Accepted by ICML25!", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19436v1", "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "authors": ["Ye Ye"], "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings.", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.8; H.3.3"], "published": "2025-05-26 02:53:22", "updated": "2025-05-26 02:53:22", "pdf_url": "http://arxiv.org/pdf/2505.19436v1", "comment": "Under review. 9 pages main content, 15 pages appendix, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19441v1", "title": "Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems", "authors": ["Jing Nathan Yan", "Junxiong Wang", "Jeffrey M. Rzeszotarski", "Allison Koenecke"], "abstract": "The rapid proliferation of recommender systems necessitates robust fairness\npractices to address inherent biases. Assessing fairness, though, is\nchallenging due to constantly evolving metrics and best practices. This paper\nanalyzes how industry practitioners perceive and incorporate these changing\nfairness standards in their workflows. Through semi-structured interviews with\n11 practitioners from technical teams across a range of large technology\ncompanies, we investigate industry implementations of fairness in\nrecommendation system products. We focus on current debiasing practices,\napplied metrics, collaborative strategies, and integrating academic research\ninto practice. Findings show a preference for multi-dimensional debiasing over\ntraditional demographic methods, and a reliance on intuitive rather than\nacademic metrics. This study also highlights the difficulties in balancing\nfairness with both the practitioner's individual (bottom-up) roles and\norganizational (top-down) workplace constraints, including the interplay with\nlegal and compliance experts. Finally, we offer actionable recommendations for\nthe recommender system community and algorithmic fairness practitioners,\nunderlining the need to refine fairness practices continually.", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "published": "2025-05-26 02:59:57", "updated": "2025-05-26 02:59:57", "pdf_url": "http://arxiv.org/pdf/2505.19441v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19442v1", "title": "Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning", "authors": ["Dutao Zhang", "Sergey Kovalchuk", "YuLong He"], "abstract": "Controllable code generation, the ability to synthesize code that follows a\nspecified style while maintaining functionality, remains a challenging task. We\npropose a two-stage training framework combining contrastive learning and\nconditional decoding to enable flexible style control. The first stage aligns\ncode style representations with semantic and structural features. In the second\nstage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned\nstyle vector to guide generation. Our method supports style interpolation and\nuser personalization via lightweight mixing. Compared to prior work, our\nunified framework offers improved stylistic control without sacrificing code\ncorrectness. This is among the first approaches to combine contrastive\nalignment with conditional decoding for style-guided code generation.", "categories": ["cs.AI", "I.2.6; D.2.3"], "published": "2025-05-26 03:00:20", "updated": "2025-05-26 03:00:20", "pdf_url": "http://arxiv.org/pdf/2505.19442v1", "comment": "10 pages, 5 figures, submitted to EMNLP 2025 (Industry Track)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19443v1", "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "abstract": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle.", "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2025-05-26 03:00:21", "updated": "2025-05-26 03:00:21", "pdf_url": "http://arxiv.org/pdf/2505.19443v1", "comment": "35 Pages, 8 Figures, 6 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19455v1", "title": "MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering", "authors": ["Xu Li", "Fan Lyu"], "abstract": "Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)\nhas achieved promising progress by leveraging prompt tuning to enable continual\nmulti-modal learning. However, most existing methods adopt cross-modal prompt\nisolation, constructing visual and textual prompts separately, which\nexacerbates modality imbalance and leads to degraded performance over time. To\ntackle this issue, we propose MM-Prompt, a novel framework incorporating\ncross-modal prompt query and cross-modal prompt recovery. The former enables\nbalanced prompt selection by incorporating cross-modal signals during query\nformation, while the latter promotes joint prompt reconstruction through\niterative cross-modal interactions, guided by an alignment loss to prevent\nrepresentational drift. Extensive experiments show that MM-Prompt surpasses\nprior approaches in accuracy and knowledge retention, while maintaining\nbalanced modality engagement throughout continual learning.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-26 03:21:21", "updated": "2025-05-26 03:21:21", "pdf_url": "http://arxiv.org/pdf/2505.19455v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19457v1", "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs", "authors": ["Guilong Lu", "Xuntao Guo", "Rongjunchen Zhang", "Wenqiao Zhu", "Ji Liu"], "abstract": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.", "categories": ["cs.AI", "cs.CE", "cs.CL"], "published": "2025-05-26 03:23:02", "updated": "2025-05-26 03:23:02", "pdf_url": "http://arxiv.org/pdf/2505.19457v1", "comment": "Project Page: https://hithink-research.github.io/BizFinBench/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19459v1", "title": "Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation", "authors": ["Kaichao Jiang", "He Wang", "Xiaoshuai Hao", "Xiulong Yang", "Ajian Liu", "Qi Chu", "Yunfeng Diao"], "abstract": "Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative\nmodels, are well known for their ability to achieve both high classification\naccuracy and generative capability within a single model. However, their\nrobustness still lags significantly behind the classifiers based adversarial\ntraining (AT). Conversely, while AT is currently the most effective approach to\nimproving the classifier's robustness, it typically sacrifices accuracy on\nclean data and lacks generative capability. The triple trade-off between\nclassification accuracy, generative capability and robustness, raises a natural\nquestion: Can a single model simultaneously achieve high classification\naccuracy, adversarial robustness, and generative performance? -- a goal that\nhas been rarely explored. To address this question, we systematically analyze\nthe energy distribution differences of clean, adversarial, and generated\nsamples across various JEM variants and adversarially trained models. We\nobserve that AT tends to reduce the energy gap between clean and adversarial\nsamples, while JEMs reduce the gap between clean and synthetic ones. This\nobservation suggests a key insight: if the energy distributions of all three\ndata types can be aligned, we might unify the strengths of AT and JEMs,\nresolving their inherent trade-offs. Building on this idea, we propose\nEnergy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly\nmodel the clean data distribution, the adversarial distribution, and the\nclassifier by maximizing their joint probability. EB-JDAT is a general and\nflexible optimization method, compatible with various JEM variants. Extensive\nexperimental results demonstrate that EB-JDAT not only maintains near original\naccuracy and generative capability of JEMs, but also significantly enhances\nrobustness, even surpassing state-of-the-art ATs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 03:26:55", "updated": "2025-05-26 03:26:55", "pdf_url": "http://arxiv.org/pdf/2505.19459v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19465v1", "title": "Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding", "authors": ["Hengwei Zhang", "Minghui Wu", "Li Qiao", "Ling Liu", "Ziqi Han", "Zhen Gao"], "abstract": "This letter proposes a deep-learning (DL)-based multi-user channel state\ninformation (CSI) feedback framework for massive multiple-input multiple-output\nsystems, where the deep joint source-channel coding (DJSCC) is utilized to\nimprove the CSI reconstruction accuracy. Specifically, we design a multi-user\njoint CSI feedback framework, whereby the CSI correlation of nearby users is\nutilized to reduce the feedback overhead. Under the framework, we propose a new\nresidual cross-attention transformer architecture, which is deployed at the\nbase station to further improve the CSI feedback performance. Moreover, to\ntackle the \"cliff-effect\" of conventional bit-level CSI feedback approaches, we\nintegrated DJSCC into the multi-user CSI feedback, together with utilizing a\ntwo-stage training scheme to adapt to varying uplink noise levels. Experimental\nresults demonstrate the superiority of our methods in CSI feedback performance,\nwith low network complexity and better scalability.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 03:38:08", "updated": "2025-05-26 03:38:08", "pdf_url": "http://arxiv.org/pdf/2505.19465v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19466v1", "title": "Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs", "authors": ["Hongyu Liang", "Yuting Zheng", "Yihan Li", "Yiran Zhang", "Shiyu Liang"], "abstract": "As large language models (LLMs) continue to advance, their deployment often\ninvolves fine-tuning to enhance performance on specific downstream tasks.\nHowever, this customization is sometimes accompanied by misleading claims about\nthe origins, raising significant concerns about transparency and trust within\nthe open-source community. Existing model verification techniques typically\nassess functional, representational, and weight similarities. However, these\napproaches often struggle against obfuscation techniques, such as permutations\nand scaling transformations. To address this limitation, we propose a novel\ndetection method Origin-Tracer that rigorously determines whether a model has\nbeen fine-tuned from a specified base model. This method includes the ability\nto extract the LoRA rank utilized during the fine-tuning process, providing a\nmore robust verification framework. This framework is the first to provide a\nformalized approach specifically aimed at pinpointing the sources of model\nfine-tuning. We empirically validated our method on thirty-one diverse\nopen-source models under conditions that simulate real-world obfuscation\nscenarios. We empirically analyze the effectiveness of our framework and\nfinally, discuss its limitations. The results demonstrate the effectiveness of\nour approach and indicate its potential to establish new benchmarks for model\nverification.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-26 03:38:14", "updated": "2025-05-26 03:38:14", "pdf_url": "http://arxiv.org/pdf/2505.19466v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19469v1", "title": "Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory", "authors": ["Mingzhuo Li", "Guang Li", "Jiafeng Mao", "Takahiro Ogawa", "Miki Haseyama"], "abstract": "Dataset distillation enables the training of deep neural networks with\ncomparable performance in significantly reduced time by compressing large\ndatasets into small and representative ones. Although the introduction of\ngenerative models has made great achievements in this field, the distributions\nof their distilled datasets are not diverse enough to represent the original\nones, leading to a decrease in downstream validation accuracy. In this paper,\nwe present a diversity-driven generative dataset distillation method based on a\ndiffusion model to solve this problem. We introduce self-adaptive memory to\nalign the distribution between distilled and real datasets, assessing the\nrepresentativeness. The degree of alignment leads the diffusion model to\ngenerate more diverse datasets during the distillation process. Extensive\nexperiments show that our method outperforms existing state-of-the-art methods\nin most situations, proving its ability to tackle dataset distillation tasks.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-26 03:48:56", "updated": "2025-05-26 03:48:56", "pdf_url": "http://arxiv.org/pdf/2505.19469v1", "comment": "Accepted by ICIP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19474v1", "title": "Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models", "authors": ["Xinmiao Hu", "Chun Wang", "Ruihe An", "ChenYu Shao", "Xiaojun Ye", "Sheng Zhou", "Liangcheng Li"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance\nin visual understanding tasks, yet they often suffer from object\nhallucinations--generating descriptions of objects that are inconsistent with\nor entirely absent from the input. This issue is closely related to dataset\nbiases, where frequent co-occurrences of objects lead to entangled semantic\nrepresentations across modalities. As a result, models may erroneously activate\nobject representations that are commonly associated with the input but not\nactually present.\n  To address this, we propose a causality-driven disentanglement framework that\nmitigates hallucinations through causal intervention. Our approach includes a\nCausal-Driven Projector in the visual pathway and a Causal Intervention Module\nintegrated into the final transformer layer of the language model. These\ncomponents work together to reduce spurious correlations caused by biased\ntraining data.\n  Experimental results show that our method significantly reduces\nhallucinations while maintaining strong performance on multiple multimodal\nbenchmarks. Visualization analyses further confirm improved separability of\nobject representations.\n  The code is available at: https://github.com/IgniSavium/Causal-LLaVA", "categories": ["cs.AI"], "published": "2025-05-26 03:53:00", "updated": "2025-05-26 03:53:00", "pdf_url": "http://arxiv.org/pdf/2505.19474v1", "comment": "21 pages, 19 figures, Submitted to NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19477v1", "title": "Judging with Many Minds: Do More Perspectives Mean Less Prejudice?", "authors": ["Chiyu Ma", "Enpei Zhang", "Yilun Zhao", "Wenjun Liu", "Yaning Jia", "Peijun Qing", "Lin Shi", "Arman Cohan", "Yujun Yan", "Soroush Vosoughi"], "abstract": "LLM-as-Judge has emerged as a scalable alternative to human evaluation,\nenabling large language models (LLMs) to provide reward signals in trainings.\nWhile recent work has explored multi-agent extensions such as multi-agent\ndebate and meta-judging to enhance evaluation quality, the question of how\nintrinsic biases manifest in these settings remains underexplored. In this\nstudy, we conduct a systematic analysis of four diverse bias types: position\nbias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate\nthese biases across two widely adopted multi-agent LLM-as-Judge frameworks:\nMulti-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate\nframework amplifies biases sharply after the initial debate, and this increased\nbias is sustained in subsequent rounds, while meta-judge approaches exhibit\ngreater resistance. We further investigate the incorporation of PINE, a leading\nsingle-agent debiasing method, as a bias-free agent within these systems. The\nresults reveal that this bias-free agent effectively reduces biases in debate\nsettings but provides less benefit in meta-judge scenarios. Our work provides a\ncomprehensive study of bias behavior in multi-agent LLM-as-Judge systems and\nhighlights the need for targeted bias mitigation strategies in collaborative\nevaluation settings.", "categories": ["cs.AI"], "published": "2025-05-26 03:56:41", "updated": "2025-05-26 03:56:41", "pdf_url": "http://arxiv.org/pdf/2505.19477v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19481v1", "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs", "authors": ["Hao Kang", "Qingru Zhang", "Han Cai", "Weiyuan Xu", "Tushar Krishna", "Yilun Du", "Tsachy Weissman"], "abstract": "Large language models (LLMs) have shown remarkable performance across diverse\nreasoning and generation tasks, and are increasingly deployed as agents in\ndynamic environments such as code generation and recommendation systems.\nHowever, many real-world applications, such as high-frequency trading and\nreal-time competitive gaming, require decisions under strict latency\nconstraints, where faster responses directly translate into higher rewards.\nDespite the importance of this latency quality trade off, it remains\nunderexplored in the context of LLM based agents. In this work, we present the\nfirst systematic study of this trade off in real time decision making tasks. To\nsupport our investigation, we introduce two new benchmarks: HFTBench, a high\nfrequency trading simulation, and StreetFighter, a competitive gaming platform.\nOur analysis reveals that optimal latency quality balance varies by task, and\nthat sacrificing quality for lower latency can significantly enhance downstream\nperformance. To address this, we propose FPX, an adaptive framework that\ndynamically selects model size and quantization level based on real time\ndemands. Our method achieves the best performance on both benchmarks, improving\nwin rate by up to 80% in Street Fighter and boosting daily yield by up to\n26.52% in trading, underscoring the need for latency aware evaluation and\ndeployment strategies for LLM based agents. These results demonstrate the\ncritical importance of latency aware evaluation and deployment strategies for\nreal world LLM based agents. Our benchmarks are available at Latency Sensitive\nBenchmarks.", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA"], "published": "2025-05-26 04:03:48", "updated": "2025-05-26 04:03:48", "pdf_url": "http://arxiv.org/pdf/2505.19481v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19488v1", "title": "Understanding Transformer from the Perspective of Associative Memory", "authors": ["Shu Zhong", "Mingyu Xu", "Tenglong Ao", "Guang Shi"], "abstract": "In this paper, we share our reflections and insights on understanding\nTransformer architectures through the lens of associative memory--a classic\npsychological concept inspired by human cognition. We start with the basics of\nassociative memory (think simple linear attention) and then dive into two\ndimensions:\n  Memory Capacity: How much can a Transformer really remember, and how well? We\nintroduce retrieval SNR to measure this and use a kernel perspective to\nmathematically reveal why Softmax Attention is so effective. We also show how\nFFNs can be seen as a type of associative memory, leading to insights on their\ndesign and potential improvements.\n  Memory Update: How do these memories learn and evolve? We present a unified\nframework for understanding how different Transformer variants (like DeltaNet\nand Softmax Attention) update their \"knowledge base\". This leads us to tackle\ntwo provocative questions: 1. Are Transformers fundamentally limited in what\nthey can express, and can we break these barriers? 2. If a Transformer had\ninfinite context, would it become infinitely intelligent?\n  We want to demystify Transformer architecture, offering a clearer\nunderstanding of existing designs. This exploration aims to provide fresh\ninsights and spark new avenues for Transformer innovation.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 04:15:38", "updated": "2025-05-26 04:15:38", "pdf_url": "http://arxiv.org/pdf/2505.19488v1", "comment": "Consider this post less as a formal research paper and more as a\n  blog-style sharing of our current reflections, intended to spark discussion\n  as one might in a collaborative team meeting", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19489v1", "title": "Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs", "authors": ["Zhenhao Zhou", "Zhuochen Huang", "Yike He", "Chong Wang", "Jiajun Wang", "Yijian Wu", "Xin Peng", "Yiling Lou"], "abstract": "The Linux kernel is a critical system, serving as the foundation for numerous\nsystems. Bugs in the Linux kernel can cause serious consequences, affecting\nbillions of users. Fault localization (FL), which aims at identifying the buggy\ncode elements in software, plays an essential role in software quality\nassurance. While recent LLM agents have achieved promising accuracy in FL on\nrecent benchmarks like SWE-bench, it remains unclear how well these methods\nperform in the Linux kernel, where FL is much more challenging due to the\nlarge-scale code base, limited observability, and diverse impact factors. In\nthis paper, we introduce LinuxFLBench, a FL benchmark constructed from\nreal-world Linux kernel bugs. We conduct an empirical study to assess the\nperformance of state-of-the-art LLM agents on the Linux kernel. Our initial\nresults reveal that existing agents struggle with this task, achieving a best\ntop-1 accuracy of only 41.6% at file level. To address this challenge, we\npropose LinuxFL$^+$, an enhancement framework designed to improve FL\neffectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially\nimproves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy\nincrease) with minimal costs. Data and code are available at\nhttps://github.com/FudanSELab/LinuxFLBench.", "categories": ["cs.AI", "cs.SE"], "published": "2025-05-26 04:15:48", "updated": "2025-05-26 04:15:48", "pdf_url": "http://arxiv.org/pdf/2505.19489v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19490v1", "title": "Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models", "authors": ["Jianxing Liao", "Junyan Xu", "Yatao Sun", "Maowen Tang", "Sicheng He", "Jingxian Liao", "Shui Yu", "Yun Li", "Hongguan Xiao"], "abstract": "Designing complex computer-aided design (CAD) models is often time-consuming\ndue to challenges such as computational inefficiency and the difficulty of\ngenerating precise models. We propose a novel language-guided framework for\nindustrial design automation to address these issues, integrating large\nlanguage models (LLMs) with computer-automated design (CAutoD).Through this\nframework, CAD models are automatically generated from parameters and\nappearance descriptions, supporting the automation of design tasks during the\ndetailed CAD design phase. Our approach introduces three key innovations: (1) a\nsemi-automated data annotation pipeline that leverages LLMs and vision-language\nlarge models (VLLMs) to generate high-quality parameters and appearance\ndescriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts\nmodeling sequences via dual-channel feature aggregation; (3) an enhanced CAD\nmodeling generation model, called CADLLM, that is designed to refine the\ngenerated sequences by incorporating the confidence scores from TCADGen.\nExperimental results demonstrate that the proposed approach outperforms\ntraditional methods in both accuracy and efficiency, providing a powerful tool\nfor automating industrial workflows and generating complex CAD models from\ntextual prompts. The code is available at\nhttps://jianxliao.github.io/cadllm-page/", "categories": ["cs.AI", "I.2.7; I.2.6"], "published": "2025-05-26 04:17:51", "updated": "2025-05-26 04:17:51", "pdf_url": "http://arxiv.org/pdf/2505.19490v1", "comment": "Accepted by ACL 2025 Main Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19498v1", "title": "Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models", "authors": ["Nanxing Hu", "Xiaoyue Duan", "Jinchao Zhang", "Guoliang Kang"], "abstract": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy\ncontext coherence but don't match the visual input. Such a hallucination issue\nhinders LVLMs' applicability in the real world. The key to solving\nhallucination in LVLM is to make the text generation rely more on the visual\ncontent. Most previous works choose to enhance/adjust the features/output of a\nspecific modality (i.e., visual or textual) to alleviate hallucinations in\nLVLM, which do not explicitly or systematically enhance the visual reliance. In\nthis paper, we comprehensively investigate the factors which may degenerate the\nvisual reliance in text generation of LVLM from a Bayesian perspective. Based\non our observations, we propose to mitigate hallucination in LVLM from three\naspects. Firstly, we observe that not all visual tokens are informative in\ngenerating meaningful texts. We propose to evaluate and remove redundant visual\ntokens to avoid their disturbance. Secondly, LVLM may encode inappropriate\nprior information, making it lean toward generating unexpected words. We\npropose a simple yet effective way to rectify the prior from a Bayesian\nperspective. Thirdly, we observe that starting from certain steps, the\nposterior of next-token prediction conditioned on visual tokens may collapse to\na prior distribution which does not depend on any informative visual tokens at\nall. Thus, we propose to stop further text generation to avoid hallucination.\nExtensive experiments on three benchmarks including POPE, CHAIR, and MME\ndemonstrate that our method can consistently mitigate the hallucination issue\nof LVLM and performs favorably against previous state-of-the-arts.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 04:26:30", "updated": "2025-05-26 04:26:30", "pdf_url": "http://arxiv.org/pdf/2505.19498v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19501v1", "title": "Genome-Bench: A Scientific Reasoning Benchmark from Real-World Expert Discussions", "authors": ["Ming Yin", "Yuanhao Qu", "Dyllan Liu", "Ling Yang", "Le Cong", "Mengdi Wang"], "abstract": "In this short report, we present an automated pipeline tailored for the\ngenomics domain and introduce \\textit{Genome-Bench}, a new benchmark\nconstructed from over a decade of scientific forum discussions on genome\nengineering. Our pipeline transforms raw interactions into a reinforcement\nlearning friendly multiple-choice questions format, supported by 3000+ high\nquality question answer pairs spanning foundational biology, experimental\ntroubleshooting, tool usage, and beyond. To our knowledge, this is the first\nend-to-end pipeline for teaching LLMs to reason from scientific discussions,\nwith promising potential for generalization across scientific domains beyond\nbiology.", "categories": ["cs.AI"], "published": "2025-05-26 04:28:46", "updated": "2025-05-26 04:28:46", "pdf_url": "http://arxiv.org/pdf/2505.19501v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19502v1", "title": "CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation", "authors": ["Guang Yang", "Yu Zhou", "Xiang Chen", "Wei Zheng", "Xing Hu", "Xin Zhou", "David Lo", "Taolue Chen"], "abstract": "Trustworthy evaluation methods for code snippets play a crucial role in\nneural code generation. Traditional methods, which either rely on reference\nsolutions or require executable test cases, have inherent limitation in\nflexibility and scalability. The recent LLM-as-Judge methodology offers a\npromising alternative by directly evaluating functional consistency between the\nproblem description and the generated code. To systematically understand the\nlandscape of these LLM-as-Judge methods, we conduct a comprehensive empirical\nstudy across three diverse datasets. Our investigation reveals the pros and\ncons of two categories of LLM-as-Judge methods: the methods based on general\nfoundation models can achieve good performance but require complex prompts and\nlack explainability, while the methods based on reasoning foundation models\nprovide better explainability with simpler prompts but demand substantial\ncomputational resources due to their large parameter sizes. To address these\nlimitations, we propose CODE-DITING, a novel code evaluation method that\nbalances accuracy, efficiency and explainability. We develop a data\ndistillation framework that effectively transfers reasoning capabilities from\nDeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing\nevaluation explainability and reducing the computational cost. With the\nmajority vote strategy in the inference process, CODE-DITING 1.5B outperforms\nall models with the same magnitude of parameters and achieves performance which\nwould normally exhibit in a model with 5 times of parameter scale. CODE-DITING\n7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the\nparameter volume of these large models. Further experiments show that\nCODEDITING is robust to preference leakage and can serve as a promising\nalternative for code evaluation.", "categories": ["cs.SE", "cs.AI"], "published": "2025-05-26 04:29:14", "updated": "2025-05-26 04:29:14", "pdf_url": "http://arxiv.org/pdf/2505.19502v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19504v1", "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation", "authors": ["Pingzhi Li", "Zhen Tan", "Huaizhi Qu", "Huan Liu", "Tianlong Chen"], "abstract": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 04:31:38", "updated": "2025-05-26 04:31:38", "pdf_url": "http://arxiv.org/pdf/2505.19504v1", "comment": "Code is available at https://github.com/UNITES-Lab/DOGe", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19505v1", "title": "Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model", "authors": ["Yu Xia", "Rui Zhong", "Hao Gu", "Wei Yang", "Chi Lu", "Peng Jiang", "Kun Gai"], "abstract": "Large Language Models (LLMs) have garnered significant attention in\nRecommendation Systems (RS) due to their extensive world knowledge and robust\nreasoning capabilities. However, a critical challenge lies in enabling LLMs to\neffectively comprehend and extract insights from massive user behaviors.\nCurrent approaches that directly leverage LLMs for user interest learning face\nlimitations in handling long sequential behaviors, effectively extracting\ninterest, and applying interest in practical scenarios. To address these\nissues, we propose a Hierarchical Tree Search-based User Lifelong Behavior\nModeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior\nExtraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture\ndiverse interests and interest evolution of user. CUBE divides user lifelong\nbehaviors into multiple chunks and learns the interest and interest evolution\nwithin each chunk in a cascading manner. HTS generates candidate interests\nthrough hierarchical expansion and searches for the optimal interest with\nprocess rating model to ensure information gain for each behavior chunk.\nAdditionally, we design Temporal-Ware Interest Fusion (TIF) to integrate\ninterests from multiple behavior chunks, constructing a comprehensive\nrepresentation of user lifelong interests. The representation can be embedded\ninto any recommendation model to enhance performance. Extensive experiments\ndemonstrate the effectiveness of our approach, showing that it surpasses\nstate-of-the-art methods.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-26 04:32:57", "updated": "2025-05-26 04:32:57", "pdf_url": "http://arxiv.org/pdf/2505.19505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19509v1", "title": "Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models", "authors": ["Yifan Jia", "Kailin Jiang", "Yuyang Liang", "Qihan Ren", "Yi Xin", "Rui Yang", "Fenze Feng", "Mingcai Chen", "Hengyang Lu", "Haozhe Wang", "Xiaoye Qu", "Dongrui Liu", "Lizhen Cui", "Yuntao Du"], "abstract": "Large Multimodal Models(LMMs) face notable challenges when encountering\nmultimodal knowledge conflicts, particularly under retrieval-augmented\ngeneration(RAG) frameworks where the contextual information from external\nsources may contradict the model's internal parametric knowledge, leading to\nunreliable outputs. However, existing benchmarks fail to reflect such realistic\nconflict scenarios. Most focus solely on intra-memory conflicts, while\ncontext-memory and inter-context conflicts remain largely investigated.\nFurthermore, commonly used factual knowledge-based evaluations are often\noverlooked, and existing datasets lack a thorough investigation into conflict\ndetection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark\ndesigned to evaluate factual knowledge conflicts in both context-memory and\ninter-context scenarios. MMKC-Bench encompasses three types of multimodal\nknowledge conflicts and includes 1,573 knowledge instances and 3,381 images\nacross 23 broad types, collected through automated pipelines with human\nverification. We evaluate three representative series of LMMs on both model\nbehavior analysis and conflict detection tasks. Our findings show that while\ncurrent LMMs are capable of recognizing knowledge conflicts, they tend to favor\ninternal parametric knowledge over external evidence. We hope MMKC-Bench will\nfoster further research in multimodal knowledge conflict and enhance the\ndevelopment of multimodal RAG systems. The source code is available at\nhttps://github.com/MLLMKCBENCH/MLLMKC.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 04:39:30", "updated": "2025-05-26 04:39:30", "pdf_url": "http://arxiv.org/pdf/2505.19509v1", "comment": "The source code is available at https://github.com/MLLMKCBENCH/MLLMKC", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19514v1", "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "abstract": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 04:56:48", "updated": "2025-05-26 04:56:48", "pdf_url": "http://arxiv.org/pdf/2505.19514v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19525v1", "title": "Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate", "authors": ["Liangwei Nathan Zheng", "Wei Emma Zhang", "Mingyu Guo", "Miao Xu", "Olaf Maennel", "Weitong Chen"], "abstract": "Effectively managing missing modalities is a fundamental challenge in\nreal-world multimodal learning scenarios, where data incompleteness often\nresults from systematic collection errors or sensor failures. Sparse\nMixture-of-Experts (SMoE) architectures have the potential to naturally handle\nmultimodal data, with individual experts specializing in different modalities.\nHowever, existing SMoE approach often lacks proper ability to handle missing\nmodality, leading to performance degradation and poor generalization in\nreal-world applications. We propose Conf-SMoE to introduce a two-stage\nimputation module to handle the missing modality problem for the SMoE\narchitecture and reveal the insight of expert collapse from theoretical\nanalysis with strong empirical evidence. Inspired by our theoretical analysis,\nConf-SMoE propose a novel expert gating mechanism by detaching the softmax\nrouting score to task confidence score w.r.t ground truth. This naturally\nrelieves expert collapse without introducing additional load balance loss\nfunction. We show that the insights of expert collapse aligns with other gating\nmechanism such as Gaussian and Laplacian gate. We also evaluate the proposed\nmethod on four different real world dataset with three different experiment\nsettings to conduct comprehensive the analysis of Conf-SMoE on modality fusion\nand resistance to missing modality.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 05:18:55", "updated": "2025-05-26 05:18:55", "pdf_url": "http://arxiv.org/pdf/2505.19525v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19527v1", "title": "Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation", "authors": ["Mohammed D. Belgoumri", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Imran Razzak", "Sunil Aryal"], "abstract": "Training large neural networks through gradient-based optimization requires\nnavigating high-dimensional loss landscapes, which often exhibit pathological\ngeometry, leading to undesirable training dynamics. In particular, poor\ngeneralization frequently results from convergence to sharp minima that are\nhighly sensitive to input perturbations, causing the model to overfit the\ntraining data while failing to generalize to unseen examples. Furthermore,\nthese optimization procedures typically display strong dependence on the fine\nstructure of the loss landscape, leading to unstable training dynamics, due to\nthe fractal-like nature of the loss surface. In this work, we propose an\nalternative optimizer that simultaneously reduces this dependence, and avoids\nsharp minima, thereby improving generalization. This is achieved by simulating\nthe motion of the center of a ball rolling on the loss landscape. The degree to\nwhich our optimizer departs from the standard gradient descent is controlled by\na hyperparameter, representing the radius of the ball. Changing this\nhyperparameter allows for probing the loss landscape at different scales,\nmaking it a valuable tool for understanding its geometry.", "categories": ["cs.LG", "cs.AI", "math.OC"], "published": "2025-05-26 05:26:21", "updated": "2025-05-26 05:26:21", "pdf_url": "http://arxiv.org/pdf/2505.19527v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19528v1", "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "abstract": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness.", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "published": "2025-05-26 05:27:10", "updated": "2025-05-26 05:27:10", "pdf_url": "http://arxiv.org/pdf/2505.19528v1", "comment": "13 pages, 4 figures, Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19531v1", "title": "Minimalist Softmax Attention Provably Learns Constrained Boolean Functions", "authors": ["Jerry Yao-Chieh Hu", "Xiwen Zhang", "Maojiang Su", "Zhao Song", "Han Liu"], "abstract": "We study the computational limits of learning $k$-bit Boolean functions\n(specifically, $\\mathrm{AND}$, $\\mathrm{OR}$, and their noisy variants), using\na minimalist single-head softmax-attention mechanism, where $k=\\Theta(d)$\nrelevant bits are selected from $d$ inputs. We show that these simple\n$\\mathrm{AND}$ and $\\mathrm{OR}$ functions are unsolvable with a single-head\nsoftmax-attention mechanism alone. However, with teacher forcing, the same\nminimalist attention is capable of solving them. These findings offer two key\ninsights: Architecturally, solving these Boolean tasks requires only minimalist\nattention, without deep Transformer blocks or FFNs. Methodologically, one\ngradient descent update with supervision suffices and replaces the multi-step\nChain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for\nsolving Boolean problems. Together, the bounds expose a fundamental gap between\nwhat this minimal architecture achieves under ideal supervision and what is\nprovably impossible under standard training.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-26 05:33:26", "updated": "2025-05-26 05:33:26", "pdf_url": "http://arxiv.org/pdf/2505.19531v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19534v1", "title": "Training-Free Multi-Step Audio Source Separation", "authors": ["Yongyi Zang", "Jingyi Li", "Qiuqiang Kong"], "abstract": "Audio source separation aims to separate a mixture into target sources.\nPrevious audio source separation systems usually conduct one-step inference,\nwhich does not fully explore the separation ability of models. In this work, we\nreveal that pretrained one-step audio source separation models can be leveraged\nfor multi-step separation without additional training. We propose a simple yet\neffective inference method that iteratively applies separation by optimally\nblending the input mixture with the previous step's separation result. At each\nstep, we determine the optimal blending ratio by maximizing a metric. We prove\nthat our method always yield improvement over one-step inference, provide error\nbounds based on model smoothness and metric robustness, and provide theoretical\nanalysis connecting our method to denoising along linear interpolation paths\nbetween noise and clean distributions, a property we link to denoising\ndiffusion bridge models. Our approach effectively delivers improved separation\nperformance as a \"free lunch\" from existing models. Our empirical results\ndemonstrate that our multi-step separation approach consistently outperforms\none-step inference across both speech enhancement and music source separation\ntasks, and can achieve scaling performance similar to training a larger model,\nusing more data, or in some cases employing a multi-step training objective.\nThese improvements appear not only on the optimization metric during multi-step\ninference, but also extend to nearly all non-optimized metrics (with one\nexception). We also discuss limitations of our approach and directions for\nfuture research.", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "published": "2025-05-26 05:40:12", "updated": "2025-05-26 05:40:12", "pdf_url": "http://arxiv.org/pdf/2505.19534v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19536v1", "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models", "authors": ["Jintao Tong", "Wenwei Jin", "Pengda Qin", "Anqi Li", "Yixiong Zou", "Yuhong Li", "Yuhua Li", "Ruixuan Li"], "abstract": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-26 05:54:48", "updated": "2025-05-26 05:54:48", "pdf_url": "http://arxiv.org/pdf/2505.19536v1", "comment": "19 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19538v1", "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients", "authors": ["Yuxing Lu", "Gecheng Fu", "Wei Wu", "Xukai Zhao", "Sin Yee Goi", "Jinzhuo Wang"], "abstract": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.MA"], "published": "2025-05-26 05:56:23", "updated": "2025-05-26 05:56:23", "pdf_url": "http://arxiv.org/pdf/2505.19538v1", "comment": "32 pages, 5 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19547v1", "title": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization", "authors": ["Haoyu Zhang", "Wentao Zhang", "Hao Miao", "Xinke Jiang", "Yuchen Fang", "Yifan Zhang"], "abstract": "Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful\ntool for modeling dynamic graph-structured data across diverse domains.\nHowever, they often fail to generalize in Spatio-Temporal Out-of-Distribution\n(STOOD) scenarios, where both temporal dynamics and spatial structures evolve\nbeyond the training distribution. To address this problem, we propose an\ninnovative Spatio-Temporal Retrieval-Augmented Pattern Learning\nframework,STRAP, which enhances model generalization by integrating\nretrieval-augmented learning into the STGNN continue learning pipeline. The\ncore of STRAP is a compact and expressive pattern library that stores\nrepresentative spatio-temporal patterns enriched with historical, structural,\nand semantic information, which is obtained and optimized during the training\nphase. During inference, STRAP retrieves relevant patterns from this library\nbased on similarity to the current input and injects them into the model via a\nplug-and-play prompting mechanism. This not only strengthens spatio-temporal\nrepresentations but also mitigates catastrophic forgetting. Moreover, STRAP\nintroduces a knowledge-balancing objective to harmonize new information with\nretrieved knowledge. Extensive experiments across multiple real-world streaming\ngraph datasets show that STRAP consistently outperforms state-of-the-art STGNN\nbaselines on STOOD tasks, demonstrating its robustness, adaptability, and\nstrong generalization capability without task-specific fine-tuning.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 06:11:05", "updated": "2025-05-26 06:11:05", "pdf_url": "http://arxiv.org/pdf/2505.19547v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19548v1", "title": "How Syntax Specialization Emerges in Language Models", "authors": ["Xufeng Duan", "Zhaoqian Yao", "Yunhao Zhang", "Shaonan Wang", "Zhenguang G. Cai"], "abstract": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 06:11:18", "updated": "2025-05-26 06:11:18", "pdf_url": "http://arxiv.org/pdf/2505.19548v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19550v1", "title": "Turing Test 2.0: The General Intelligence Threshold", "authors": ["Georgios Mappouras"], "abstract": "With the rise of artificial intelligence (A.I.) and large language models\nlike Chat-GPT, a new race for achieving artificial general intelligence (A.G.I)\nhas started. While many speculate how and when A.I. will achieve A.G.I., there\nis no clear agreement on how A.G.I. can be detected in A.I. models, even when\npopular tools like the Turing test (and its modern variations) are used to\nmeasure their intelligence. In this work, we discuss why traditional methods\nlike the Turing test do not suffice for measuring or detecting A.G.I. and\nprovide a new, practical method that can be used to decide if a (computer or\nany other) system has reached or surpassed A.G.I. To achieve this, we make two\nnew contributions. First, we present a clear definition for general\nintelligence (G.I.) and set a G.I. threshold (G.I.T.) that can be used to\ndistinguish between systems that achieve A.G.I. and systems that do not.\nSecond, we present a new framework on how to construct tests that can detect if\na system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass\nway. We call this novel framework the Turing Tests 2.0. We then demonstrate\nreal-life examples of applying tests that follow our Turing Tests 2.0 framework\non modern A.I. models.", "categories": ["cs.AI"], "published": "2025-05-26 06:13:15", "updated": "2025-05-26 06:13:15", "pdf_url": "http://arxiv.org/pdf/2505.19550v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19562v1", "title": "AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare", "authors": ["Ying Xiao", "Jie Huang", "Ruijuan He", "Jing Xiao", "Mohammad Reza Mousavi", "Yepang Liu", "Kezhi Li", "Zhenpeng Chen", "Jie M. Zhang"], "abstract": "Large language models (LLMs) are reaching expert-level accuracy on medical\ndiagnosis questions, yet their mistakes and the biases behind them pose\nlife-critical risks. Bias linked to race, sex, and socioeconomic status is\nalready well known, but a consistent and automatic testbed for measuring it is\nmissing. To fill this gap, this paper presents AMQA -- an Adversarial Medical\nQuestion-Answering dataset -- built for automated, large-scale bias evaluation\nof LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the\nUnited States Medical Licensing Examination (USMLE) dataset, generated using a\nmulti-agent framework to create diverse adversarial descriptions and question\npairs. Using AMQA, we benchmark five representative LLMs and find surprisingly\nsubstantial disparities: even GPT-4.1, the least biased model tested, answers\nprivileged-group questions over 10 percentage points more accurately than\nunprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%\nlarger accuracy gaps on average between privileged and unprivileged groups. Our\ndataset and code are publicly available at https://github.com/XY-Showing/AMQA\nto support reproducible research and advance trustworthy, bias-aware medical\nAI.", "categories": ["cs.AI"], "published": "2025-05-26 06:24:20", "updated": "2025-05-26 06:24:20", "pdf_url": "http://arxiv.org/pdf/2505.19562v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19563v1", "title": "Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights", "authors": ["Shi-Yu Tian", "Zhi Zhou", "Wei Dong", "Ming Yang", "Kun-Yang Yu", "Zi-Jian Cheng", "Lan-Zhe Guo", "Yu-Feng Li"], "abstract": "Reasoning with tabular data holds increasing importance in modern\napplications, yet comprehensive evaluation methodologies for\nreasoning-intensive Table Question Answering (QA) tasks remain nascent.\nExisting research is constrained by two primary bottlenecks: 1) Reliance on\ncostly manually annotated real-world data, which is difficult to cover complex\nreasoning scenarios; 2) The heterogeneity of table structures hinders\nsystematic analysis of the intrinsic mechanisms behind the underperformance of\nLLMs, especially in reasoning-intensive tasks. To address these issues, we\npropose an automated generation pipeline AutoT2T that transforms mathematical\nword problems into table-based reasoning tasks, eliminating the need for manual\nannotation. The pipeline can generate multiple variants of a table for the same\nreasoning problem, including noisy versions to support robustness evaluation.\nBased on this, we construct a new benchmark TabularGSM, which systematically\nspans a range of table complexities and trap problems. Experimental analyses\nthrough AutoT2T and TabularGSM reveal that the tight coupling between reasoning\nand retrieval or identification processes is a key factor underlying the\nfailure of LLMs in complex Table QA tasks. This highlights the necessity for\nmodels to develop synergistic reasoning capabilities in order to perform\neffectively in complex Table QA tasks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 06:24:31", "updated": "2025-05-26 06:24:31", "pdf_url": "http://arxiv.org/pdf/2505.19563v1", "comment": "Paper under review, code and dataset are all available", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19567v1", "title": "LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer", "authors": ["Rasoul Zahedifar", "Sayyed Ali Mirghasemi", "Mahdieh Soleymani Baghshah", "Alireza Taheri"], "abstract": "This study presents the LLM-Agent-Controller, a multi-agent large language\nmodel (LLM) system developed to address a wide range of problems in control\nengineering (Control Theory). The system integrates a central controller agent\nwith multiple specialized auxiliary agents, responsible for tasks such as\ncontroller design, model representation, control analysis, time-domain\nresponse, and simulation. A supervisor oversees high-level decision-making and\nworkflow coordination, enhancing the system's reliability and efficiency. The\nLLM-Agent-Controller incorporates advanced capabilities, including\nRetrieval-Augmented Generation (RAG), Chain-of-Thought reasoning,\nself-criticism and correction, efficient memory handling, and user-friendly\nnatural language communication. It is designed to function without requiring\nusers to have prior knowledge of Control Theory, enabling them to input\nproblems in plain language and receive complete, real-time solutions. To\nevaluate the system, we propose new performance metrics assessing both\nindividual agents and the system as a whole. We test five categories of Control\nTheory problems and benchmark performance across three advanced LLMs.\nAdditionally, we conduct a comprehensive qualitative conversational analysis\ncovering all key services. Results show that the LLM-Agent-Controller\nsuccessfully solved 83% of general tasks, with individual agents achieving an\naverage success rate of 87%. Performance improved with more advanced LLMs. This\nresearch demonstrates the potential of multi-agent LLM architectures to solve\ncomplex, domain-specific problems. By integrating specialized agents,\nsupervisory control, and advanced reasoning, the LLM-Agent-Controller offers a\nscalable, robust, and accessible solution framework that can be extended to\nvarious technical domains.", "categories": ["cs.AI", "cs.MA"], "published": "2025-05-26 06:30:13", "updated": "2025-05-26 06:30:13", "pdf_url": "http://arxiv.org/pdf/2505.19567v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19568v1", "title": "MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model", "authors": ["Jiongchao Jin", "Xiuju Fu", "Xiaowei Gao", "Tao Cheng", "Ran Yan"], "abstract": "Maritime transportation is the backbone of global trade, making ship\ninspection essential for ensuring maritime safety and environmental protection.\nPort State Control (PSC), conducted by national ports, enforces compliance with\nsafety regulations, with ship detention being the most severe consequence,\nimpacting both ship schedules and company reputations. Traditional machine\nlearning methods for ship detention prediction are limited by the capacity of\nrepresentation learning and thus suffer from low accuracy. Meanwhile,\nautoencoder-based deep learning approaches face challenges due to the severe\ndata imbalance in learning historical PSC detention records. To address these\nlimitations, we propose Maritime Ship Detention with Large Language Models\n(MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based\nautoencoder with a progressive learning pipeline to handle imbalanced data and\nextract meaningful PSC representations. Then, a large language model groups and\nranks features to identify likely detention cases, enabling dynamic\nthresholding for flexible detention predictions. Extensive evaluations on\n31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM\noutperforms state-of-the-art methods more than 12\\% on Area Under the Curve\n(AUC) for Singapore ports. Additionally, it demonstrates robustness to\nreal-world challenges, making it adaptable to diverse maritime risk assessment\nscenarios.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-26 06:32:02", "updated": "2025-05-26 06:32:02", "pdf_url": "http://arxiv.org/pdf/2505.19568v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19572v1", "title": "DocMEdit: Towards Document-Level Model Editing", "authors": ["Li Zeng", "Zeming Liu", "Chong Feng", "Heyan Huang", "Yuhang Guo"], "abstract": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 06:37:24", "updated": "2025-05-26 06:37:24", "pdf_url": "http://arxiv.org/pdf/2505.19572v1", "comment": "Accepted by ACL 2025 findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19574v1", "title": "Situationally-Aware Dynamics Learning", "authors": ["Alejandro Murillo-Gonzalez", "Lantao Liu"], "abstract": "Autonomous robots operating in complex, unstructured environments face\nsignificant challenges due to latent, unobserved factors that obscure their\nunderstanding of both their internal state and the external world. Addressing\nthis challenge would enable robots to develop a more profound grasp of their\noperational context. To tackle this, we propose a novel framework for online\nlearning of hidden state representations, with which the robots can adapt in\nreal-time to uncertain and dynamic conditions that would otherwise be ambiguous\nand result in suboptimal or erroneous behaviors. Our approach is formalized as\na Generalized Hidden Parameter Markov Decision Process, which explicitly models\nthe influence of unobserved parameters on both transition dynamics and reward\nstructures. Our core innovation lies in learning online the joint distribution\nof state transitions, which serves as an expressive representation of latent\nego- and environmental-factors. This probabilistic approach supports the\nidentification and adaptation to different operational situations, improving\nrobustness and safety. Through a multivariate extension of Bayesian Online\nChangepoint Detection, our method segments changes in the underlying data\ngenerating process governing the robot's dynamics. The robot's transition model\nis then informed with a symbolic representation of the current situation\nderived from the joint distribution of latest state transitions, enabling\nadaptive and context-aware decision-making. To showcase the real-world\neffectiveness, we validate our approach in the challenging task of unstructured\nterrain navigation, where unmodeled and unmeasured terrain characteristics can\nsignificantly impact the robot's motion. Extensive experiments in both\nsimulation and real world reveal significant improvements in data efficiency,\npolicy performance, and the emergence of safer, adaptive navigation strategies.", "categories": ["cs.RO", "cs.AI", "cs.LG", "math.OC"], "published": "2025-05-26 06:40:11", "updated": "2025-05-26 06:40:11", "pdf_url": "http://arxiv.org/pdf/2505.19574v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19578v1", "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing", "authors": ["Dan Peng", "Zhihui Fu", "Zewen Ye", "Zhuoran Song", "Jun Wang"], "abstract": "Sparse attention methods exploit the inherent sparsity in attention to speed\nup the prefilling phase of long-context inference, mitigating the quadratic\ncomplexity of full attention computation. While existing sparse attention\nmethods rely on predefined patterns or inaccurate estimations to approximate\nattention behavior, they often fail to fully capture the true dynamics of\nattention, resulting in reduced efficiency and compromised accuracy. Instead,\nwe propose a highly accurate sparse attention mechanism that shares similar yet\nprecise attention patterns across heads, enabling a more realistic capture of\nthe dynamic behavior of attention. Our approach is grounded in two key\nobservations: (1) attention patterns demonstrate strong inter-head similarity,\nand (2) this similarity remains remarkably consistent across diverse inputs. By\nstrategically sharing computed accurate patterns across attention heads, our\nmethod effectively captures actual patterns while requiring full attention\ncomputation for only a small subset of heads. Comprehensive evaluations\ndemonstrate that our approach achieves superior or comparable speedup relative\nto state-of-the-art methods while delivering the best overall accuracy.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 06:48:53", "updated": "2025-05-26 06:48:53", "pdf_url": "http://arxiv.org/pdf/2505.19578v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19588v1", "title": "LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval", "authors": ["Yanzhen Shen", "Sihao Chen", "Xueqiang Xu", "Yunyi Zhang", "Chaitanya Malaviya", "Dan Roth"], "abstract": "While significant progress has been made with dual- and bi-encoder dense\nretrievers, they often struggle on queries with logical connectives, a use case\nthat is often overlooked yet important in downstream applications. Current\ndense retrievers struggle with such queries, such that the retrieved results do\nnot respect the logical constraints implied in the queries. To address this\nchallenge, we introduce LogiCoL, a logically-informed contrastive learning\nobjective for dense retrievers. LogiCoL builds upon in-batch supervised\ncontrastive learning, and learns dense retrievers to respect the subset and\nmutually-exclusive set relation between query results via two sets of soft\nconstraints expressed via t-norm in the learning objective. We evaluate the\neffectiveness of LogiCoL on the task of entity retrieval, where the model is\nexpected to retrieve a set of entities in Wikipedia that satisfy the implicit\nlogical constraints in the query. We show that models trained with LogiCoL\nyield improvement both in terms of retrieval performance and logical\nconsistency in the results. We provide detailed analysis and insights to\nuncover why queries with logical connectives are challenging for dense\nretrievers and why LogiCoL is most effective.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-26 07:00:32", "updated": "2025-05-26 07:00:32", "pdf_url": "http://arxiv.org/pdf/2505.19588v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19591v1", "title": "Multi-Agent Collaboration via Evolving Orchestration", "authors": ["Yufan Dang", "Chen Qian", "Xueheng Luo", "Jingru Fan", "Zihao Xie", "Ruijie Shi", "Weize Chen", "Cheng Yang", "Xiaoyin Che", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-26 07:02:17", "updated": "2025-05-26 07:02:17", "pdf_url": "http://arxiv.org/pdf/2505.19591v1", "comment": "Work in Progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19599v1", "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 07:08:47", "updated": "2025-05-26 07:08:47", "pdf_url": "http://arxiv.org/pdf/2505.19599v1", "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19601v1", "title": "Preference Optimization by Estimating the Ratio of the Data Distribution", "authors": ["Yeongmin Kim", "Heesun Bae", "Byeonghu Na", "Il-Chul Moon"], "abstract": "Direct preference optimization (DPO) is widely used as a simple and stable\nmethod for aligning large language models (LLMs) with human preferences. This\npaper investigates a generalized DPO loss that enables a policy model to match\nthe target policy from a likelihood ratio estimation perspective. The ratio of\nthe target policy provides a unique identification of the policy distribution\nwithout relying on reward models or partition functions. This allows the\ngeneralized loss to retain both simplicity and theoretical guarantees, which\nprior work such as $f$-PO fails to achieve simultaneously. We propose Bregman\npreference optimization (BPO), a generalized framework for ratio matching that\nprovides a family of objective functions achieving target policy optimality.\nBPO subsumes DPO as a special case and offers tractable forms for all\ninstances, allowing implementation with a few lines of code. We further develop\nscaled Basu's power divergence (SBA), a gradient scaling method that can be\nused for BPO instances. The BPO framework complements other DPO variants and is\napplicable to target policies defined by these variants. In experiments, unlike\nother probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a\ntrade-off between generation fidelity and diversity, instances of BPO improve\nboth win rate and entropy compared with DPO. When applied to\nLlama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B\nbackbones, with a 55.9\\% length-controlled win rate on AlpacaEval2.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 07:10:53", "updated": "2025-05-26 07:10:53", "pdf_url": "http://arxiv.org/pdf/2505.19601v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19607v1", "title": "Energy-based Preference Optimization for Test-time Adaptation", "authors": ["Yewon Han", "Seoyun Yang", "Taesup Kim"], "abstract": "Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation\nto target distributions that differ from training distributions, improving\nreal-world generalizability. Existing TTA approaches focus on adjusting the\nconditional distribution; however these methods often depend on uncertain\npredictions in the absence of label information, leading to unreliable\nperformance. Energy-based frameworks suggest a promising alternative to address\ndistribution shifts without relying on uncertain predictions, instead computing\nthe marginal distribution of target data. However, they involve the critical\nchallenge of requiring extensive SGLD sampling, which is impractical for\ntest-time scenarios requiring immediate adaptation. In this work, we propose\nEnergy-based Preference Optimization for Test-time Adaptation (EPOTTA), which\nis based on a sampling free strategy. We first parameterize the target model\nusing a pretrained model and residual energy function, enabling marginal\nlikelihood maximization of target data without sampling. Building on the\nobservation that the parameterization is mathematically equivalent to DPO\nobjective, we then directly adapt the model to a target distribution without\nexplicitly training the residual. Our experiments verify that EPOTTA is\nwell-calibrated and performant while achieving computational efficiency.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 07:21:32", "updated": "2025-05-26 07:21:32", "pdf_url": "http://arxiv.org/pdf/2505.19607v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19609v1", "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling", "authors": ["Hongtao Xu", "Wenting Shen", "Yuanxin Wei", "Ang Wang", "Guo Runfan", "Tianxing Wang", "Yong Li", "Mingzhen Li", "Weile Jia"], "abstract": "Long-context supervised fine-tuning (Long-SFT) plays a vital role in\nenhancing the performance of large language models (LLMs) on long-context\ntasks. To smoothly adapt LLMs to long-context scenarios, this process typically\nentails training on mixed datasets containing both long and short sequences.\nHowever, this heterogeneous sequence length distribution poses significant\nchallenges for existing training systems, as they fail to simultaneously\nachieve high training efficiency for both long and short sequences, resulting\nin sub-optimal end-to-end system performance in Long-SFT. In this paper, we\npresent a novel perspective on data scheduling to address the challenges posed\nby the heterogeneous data distributions in Long-SFT. We propose Skrull, a\ndynamic data scheduler specifically designed for efficient long-SFT. Through\ndynamic data scheduling, Skrull balances the computation requirements of long\nand short sequences, improving overall training efficiency. Furthermore, we\nformulate the scheduling process as a joint optimization problem and thoroughly\nanalyze the trade-offs involved. Based on those analysis, Skrull employs a\nlightweight scheduling algorithm to achieve near-zero cost online scheduling in\nLong-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art\ndistributed training system for LLMs. Experimental results demonstrate that\nSkrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world\nlong-SFT scenarios.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 07:22:39", "updated": "2025-05-26 07:22:39", "pdf_url": "http://arxiv.org/pdf/2505.19609v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19611v1", "title": "Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning", "authors": ["Ruolin Shen", "Xiaozhong Ji", "Kai WU", "Jiangning Zhang", "Yijun He", "HaiHua Yang", "Xiaobin Hu", "Xiaoyu Sun"], "abstract": "Current multi-modal models exhibit a notable misalignment with the human\nvisual system when identifying objects that are visually assimilated into the\nbackground. Our observations reveal that these multi-modal models cannot\ndistinguish concealed objects, demonstrating an inability to emulate human\ncognitive processes which effectively utilize foreground-background similarity\nprinciples for visual analysis. To analyze this hidden human-model visual\nthinking discrepancy, we build a visual system that mimicks human visual\ncamouflaged perception to progressively and iteratively `refocus' visual\nconcealed content. The refocus is a progressive guidance mechanism enabling\nmodels to logically localize objects in visual images through stepwise\nreasoning. The localization process of concealed objects requires hierarchical\nattention shifting with dynamic adjustment and refinement of prior cognitive\nknowledge. In this paper, we propose a visual refocus reinforcement framework\nvia the policy optimization algorithm to encourage multi-modal models to think\nand refocus more before answering, and achieve excellent reasoning abilities to\nalign and even surpass human camouflaged perception systems. Our extensive\nexperiments on camouflaged perception successfully demonstrate the emergence of\nrefocus visual phenomena, characterized by multiple reasoning tokens and\ndynamic adjustment of the detection box. Besides, experimental results on both\ncamouflaged object classification and detection tasks exhibit significantly\nsuperior performance compared to Supervised Fine-Tuning (SFT) baselines.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 07:27:18", "updated": "2025-05-26 07:27:18", "pdf_url": "http://arxiv.org/pdf/2505.19611v1", "comment": "Project Website: \\url{https://github.com/HUuxiaobin/VRRF}", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19616v1", "title": "Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models", "authors": ["Rui Cai", "Bangzheng Li", "Xiaofei Wen", "Muhao Chen", "Zhe Zhao"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across tasks, yet they often exhibit difficulty in distinguishing\ntask-relevant from irrelevant signals, particularly in tasks like Visual\nQuestion Answering (VQA), which can lead to susceptibility to misleading or\nspurious inputs. We refer to this broader limitation as the Cross-Modality\nCompetency Problem: the model's inability to fairly evaluate all modalities.\nThis vulnerability becomes more evident in modality-specific tasks such as\nimage classification or pure text question answering, where models are expected\nto rely solely on one modality. In such tasks, spurious information from\nirrelevant modalities often leads to significant performance degradation. We\nrefer to this failure as Modality Interference, which serves as a concrete and\nmeasurable instance of the cross-modality competency problem. We further design\na perturbation-based causal diagnostic experiment to verify and quantify this\nproblem. To mitigate modality interference, we propose a novel framework to\nfine-tune MLLMs, including perturbation-based data augmentations with both\nheuristic perturbations and adversarial perturbations via Projected Gradient\nDescent (PGD), and a consistency regularization strategy applied to model\noutputs with original and perturbed inputs. Experiments on multiple benchmark\ndatasets (image-heavy, text-heavy, and VQA tasks) and multiple model families\nwith different scales demonstrate significant improvements in robustness and\ncross-modality competency, indicating our method's effectiveness in boosting\nunimodal reasoning ability while enhancing performance on multimodal tasks.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "published": "2025-05-26 07:31:32", "updated": "2025-05-26 07:31:32", "pdf_url": "http://arxiv.org/pdf/2505.19616v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19620v1", "title": "Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs", "authors": ["Jiawen Chen", "Qi Shao", "Duxin Chen", "Wenwu Yu"], "abstract": "Spatio-temporal prediction is a pivotal task with broad applications in\ntraffic management, climate monitoring, energy scheduling, etc. However,\nexisting methodologies often struggle to balance model expressiveness and\ncomputational efficiency, especially when scaling to large real-world datasets.\nTo tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph\nSeparation Networks), a novel framework that decouples temporal and spatial\nmodeling to enhance both efficiency and precision. Therein, the temporal\ndimension is modeled using lightweight large language models, which effectively\ncapture low-rank temporal dynamics. Concurrently, the spatial dimension is\naddressed through an adaptive hypergraph neural network, which dynamically\nconstructs hyperedges to model intricate, higher-order interactions. A\ncarefully designed gating mechanism is integrated to seamlessly fuse temporal\nand spatial representations. By leveraging the fundamental principles of\nlow-rank temporal dynamics and spatial interactions, STH-SepNet offers a\npragmatic and scalable solution for spatio-temporal prediction in real-world\napplications. Extensive experiments on large-scale real-world datasets across\nmultiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting\npredictive performance while maintaining computational efficiency. This work\nmay provide a promising lightweight framework for spatio-temporal prediction,\naiming to reduce computational demands and while enhancing predictive\nperformance. Our code is avaliable at\nhttps://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 07:37:39", "updated": "2025-05-26 07:37:39", "pdf_url": "http://arxiv.org/pdf/2505.19620v1", "comment": null, "doi": "10.1145/3711896.3736904", "journal_ref": null}
{"arxiv_id": "2505.19621v1", "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models", "authors": ["George Kour", "Itay Nakash", "Ateret Anaby-Tavor", "Michal Shmueli-Scheuer"], "abstract": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 07:41:21", "updated": "2025-05-26 07:41:21", "pdf_url": "http://arxiv.org/pdf/2505.19621v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19623v1", "title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems", "authors": ["Yu Shang", "Peijie Liu", "Yuwei Yan", "Zijing Wu", "Leheng Sheng", "Yuanqing Yu", "Chumeng Jiang", "An Zhang", "Fengli Xu", "Yu Wang", "Min Zhang", "Yong Li"], "abstract": "The emergence of agentic recommender systems powered by Large Language Models\n(LLMs) represents a paradigm shift in personalized recommendations, leveraging\nLLMs' advanced reasoning and role-playing capabilities to enable autonomous,\nadaptive decision-making. Unlike traditional recommendation approaches, agentic\nrecommender systems can dynamically gather and interpret user-item interactions\nfrom complex environments, generating robust recommendation strategies that\ngeneralize across diverse scenarios. However, the field currently lacks\nstandardized evaluation protocols to systematically assess these methods. To\naddress this critical gap, we propose: (1) an interactive textual\nrecommendation simulator incorporating rich user and item metadata and three\ntypical evaluation scenarios (classic, evolving-interest, and cold-start\nrecommendation tasks); (2) a unified modular framework for developing and\nstudying agentic recommender systems; and (3) the first comprehensive benchmark\ncomparing 10 classical and agentic recommendation methods. Our findings\ndemonstrate the superiority of agentic systems and establish actionable design\nguidelines for their core components. The benchmark environment has been\nrigorously validated through an open challenge and remains publicly available\nwith a continuously maintained\nleaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},\nfostering ongoing community engagement and reproducible research. The benchmark\nis available at:\n\\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-26 07:45:11", "updated": "2025-05-26 07:45:11", "pdf_url": "http://arxiv.org/pdf/2505.19623v1", "comment": "15 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19624v1", "title": "Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat", "authors": ["Pusheng Xu", "Xia Gong", "Xiaolan Chen", "Weiyi Zhang", "Jiancheng Yang", "Bingjie Yan", "Meng Yuan", "Yalin Zheng", "Mingguang He", "Danli Shi"], "abstract": "Purpose: To develop a bilingual multimodal visual question answering (VQA)\nbenchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts\nand associated captions published between January 1, 2016, and December 31,\n2024, were collected from WeChat Official Accounts. Based on these captions,\nbilingual question-answer (QA) pairs in Chinese and English were generated\nusing GPT-4o-mini. QA pairs were categorized into six subsets by question type\nand language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,\nSingle-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark\nwas used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,\nand Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included\n3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548\nconditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0\nFlash achieved the highest overall accuracy (0.548), outperforming GPT-4o\n(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led\nin both Chinese (0.546) and English subsets (0.550). Subset-specific\nperformance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),\nSingle-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked\nhighest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),\nand Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study\npresents the first bilingual VQA benchmark for ophthalmology, distinguished by\nits real-world context and inclusion of multiple examinations per patient. The\ndataset reflects authentic clinical decision-making scenarios and enables\nquantitative evaluation of VLMs, supporting the development of accurate,\nspecialized, and trustworthy AI systems for eye care.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 07:45:42", "updated": "2025-05-26 07:45:42", "pdf_url": "http://arxiv.org/pdf/2505.19624v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19625v1", "title": "Search-Based Software Engineering in the Landscape of AI Foundation Models", "authors": ["Hassan Sartaj", "Shaukat Ali"], "abstract": "Search-based software engineering (SBSE), at the intersection of artificial\nintelligence (AI) and software engineering, has been an active area of research\nfor about 25 years. It has been applied to solve numerous problems across the\nentire software engineering lifecycle and has demonstrated its versatility in\nmultiple domains. With the recent advancements in AI, particularly the\nemergence of foundation models (FMs), the evolution of SBSE alongside FMs\nremains undetermined. In this window of opportunity, we propose a research\nroadmap that articulates the current landscape of SBSE in relation to\nfoundation models (FMs), highlights open challenges, and outlines potential\nresearch directions for advancing SBSE through its interplay with FMs. This\nroadmap aims to establish a forward-thinking and innovative perspective for the\nfuture of SBSE in the era of FMs.", "categories": ["cs.SE", "cs.AI"], "published": "2025-05-26 07:46:42", "updated": "2025-05-26 07:46:42", "pdf_url": "http://arxiv.org/pdf/2505.19625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19631v1", "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models", "authors": ["Zihong Zhang", "Liqi He", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Bo Du"], "abstract": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 07:48:15", "updated": "2025-05-26 07:48:15", "pdf_url": "http://arxiv.org/pdf/2505.19631v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19641v1", "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "abstract": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 07:59:36", "updated": "2025-05-26 07:59:36", "pdf_url": "http://arxiv.org/pdf/2505.19641v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19644v1", "title": "STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution", "authors": ["Anton Firc", "Manasi Chibber", "Jagabandhu Mishra", "Vishwanath Pratap Singh", "Tomi Kinnunen", "Kamil Malinka"], "abstract": "A key research area in deepfake speech detection is source tracing -\ndetermining the origin of synthesised utterances. The approaches may involve\nidentifying the acoustic model (AM), vocoder model (VM), or other\ngeneration-specific parameters. However, progress is limited by the lack of a\ndedicated, systematically curated dataset. To address this, we introduce STOPA,\na systematically varied and metadata-rich dataset for deepfake speech source\ntracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k\nsamples from 13 distinct synthesisers. Unlike existing datasets, which often\nfeature limited variation or sparse metadata, STOPA provides a systematically\ncontrolled framework covering a broader range of generative factors, such as\nthe choice of the vocoder model, acoustic model, or pretrained weights,\nensuring higher attribution reliability. This control improves attribution\naccuracy, aiding forensic analysis, deepfake detection, and generative model\ntransparency.", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS", "68T45, 68T10, 94A08", "I.2.7; I.5.4; K.4.1"], "published": "2025-05-26 08:00:30", "updated": "2025-05-26 08:00:30", "pdf_url": "http://arxiv.org/pdf/2505.19644v1", "comment": "Accepted to Interspeech 2025 conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19645v1", "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE", "authors": ["Zongle Huang", "Lei Zhu", "Zongyuan Zhan", "Ting Hu", "Weikai Mao", "Xianzhi Yu", "Yongpan Liu", "Tianyu Zhang"], "abstract": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 08:01:45", "updated": "2025-05-26 08:01:45", "pdf_url": "http://arxiv.org/pdf/2505.19645v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19648v1", "title": "Model Enumeration of Two-Variable Logic with Quadratic Delay Complexity", "authors": ["Qiaolan Meng", "Juhua Pu", "Hongting Niu", "Yuyi Wang", "Yuanhong Wang", "Ond\u0159ej Ku\u017eelka"], "abstract": "We study the model enumeration problem of the function-free, finite domain\nfragment of first-order logic with two variables ($FO^2$). Specifically, given\nan $FO^2$ sentence $\\Gamma$ and a positive integer $n$, how can one enumerate\nall the models of $\\Gamma$ over a domain of size $n$? In this paper, we devise\na novel algorithm to address this problem. The delay complexity, the time\nrequired between producing two consecutive models, of our algorithm is\nquadratic in the given domain size $n$ (up to logarithmic factors) when the\nsentence is fixed. This complexity is almost optimal since the interpretation\nof binary predicates in any model requires at least $\\Omega(n^2)$ bits to\nrepresent.", "categories": ["cs.LO", "cs.AI"], "published": "2025-05-26 08:04:19", "updated": "2025-05-26 08:04:19", "pdf_url": "http://arxiv.org/pdf/2505.19648v1", "comment": "16 pages, 4 figures and to be published in Fortieth Annual ACM/IEEE\n  Symposium on Logic in Computer Science (LICS)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19653v1", "title": "Token-Importance Guided Direct Preference Optimization", "authors": ["Yang Ning", "Lin Hai", "Liu Yibo", "Tian Baoliang", "Liu Guoqing", "Zhang Haijun"], "abstract": "Ensuring that large language models (LLMs) generate outputs aligned with\nhuman preferences is important for safe and effective AI interactions. While\nDirect Preference Optimization (DPO) employs an implicit reward function to\noptimize the policy model, however, it and its related variants overlook the\ndifferential importance of individual tokens and are sensitive to judgment\nnoise in preference datasets during generation. Although recent methods attempt\nto assess the important weight of tokens via probability prediction or\nsimplistic weighting schemes, these evaluation methods are prone to biases and\nstill cannot fully address these issues. To solve this problem, we propose the\nToken-Importance Guided Direct Preference Optimization (TI-DPO), which\nintroduces two key innovations: the gradient-based token-importance weights\nthat dynamically prioritize critical tokens, and a triple loss that explicitly\nguides model outputs to approach human-preferred responses and stay away from\nnon-preferred responses. Experimental results show that TI-DPO achieves higher\naccuracy and stronger generative diversity, providing more stable and\ncomputationally efficient solutions compared with DPO and other RLHF methods.", "categories": ["cs.AI"], "published": "2025-05-26 08:11:24", "updated": "2025-05-26 08:11:24", "pdf_url": "http://arxiv.org/pdf/2505.19653v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19658v1", "title": "Large Language Models in Code Co-generation for Safe Autonomous Vehicles", "authors": ["Ali Nouri", "Beatriz Cabrero-Daniel", "Zhennan Fei", "Krishna Ronanki", "H\u00e5kan Sivencrona", "Christian Berger"], "abstract": "Software engineers in various industrial domains are already using Large\nLanguage Models (LLMs) to accelerate the process of implementing parts of\nsoftware systems. When considering its potential use for ADAS or AD systems in\nthe automotive context, there is a need to systematically assess this new\nsetup: LLMs entail a well-documented set of risks for safety-related systems'\ndevelopment due to their stochastic nature. To reduce the effort for code\nreviewers to evaluate LLM-generated code, we propose an evaluation pipeline to\nconduct sanity-checks on the generated code. We compare the performance of six\nstate-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders,\nMistral, and GPT-4) on four safety-related programming tasks. Additionally, we\nqualitatively analyse the most frequent faults generated by these LLMs,\ncreating a failure-mode catalogue to support human reviewers. Finally, the\nlimitations and capabilities of LLMs in code generation, and the use of the\nproposed pipeline in the existing process, are discussed.", "categories": ["cs.SE", "cs.AI"], "published": "2025-05-26 08:18:30", "updated": "2025-05-26 08:18:30", "pdf_url": "http://arxiv.org/pdf/2505.19658v1", "comment": "Accepted in the 44th International Conference on Computer Safety,\n  Reliability and Security (SafeComp 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19660v1", "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models", "authors": ["Tingjia Shen", "Hao Wang", "Chuan Qin", "Ruijun Sun", "Yang Song", "Defu Lian", "Hengshu Zhu", "Enhong Chen"], "abstract": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI", "categories": ["cs.CL", "cs.AI", "68P20", "H.3.4; I.2.6"], "published": "2025-05-26 08:18:33", "updated": "2025-05-26 08:18:33", "pdf_url": "http://arxiv.org/pdf/2505.19660v1", "comment": "13 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19662v1", "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks", "authors": ["Atsunori Moteki", "Shoichi Masui", "Fan Yang", "Yueqi Song", "Yonatan Bisk", "Graham Neubig", "Ikuo Kusajima", "Yasuto Watanabe", "Hiroyuki Ishida", "Jun Takahashi", "Shan Jiang"], "abstract": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting\nreal-world field work. With the recent increase in demand for agentic AI, they\nare required to monitor and report safety and health incidents, as well as\nmanufacturing-related incidents, that may occur in real-world work\nenvironments. Existing agentic AI benchmarks have been limited to evaluating\nweb tasks and are insufficient for evaluating agents in real-world work\nenvironments, where complexity increases significantly. In this paper, we\ndefine a new action space that agentic AI should possess for real world work\nenvironment benchmarks and improve the evaluation function from previous\nmethods to assess the performance of agentic AI in diverse real-world tasks.\nThe dataset consists of videos captured on-site and documents actually used in\nfactories and warehouses, and tasks were created based on interviews with\non-site workers and managers. Evaluation results confirmed that performance\nevaluation considering the characteristics of Multimodal LLM (MLLM) such as\nGPT-4o is feasible. Additionally, the effectiveness and limitations of the\nproposed new evaluation method were identified. The complete dataset\n(HuggingFace) and evaluation program (GitHub) can be downloaded from the\nfollowing website:\nhttps://en-documents.research.global.fujitsu.com/fieldworkarena/.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-26 08:21:46", "updated": "2025-05-26 08:21:46", "pdf_url": "http://arxiv.org/pdf/2505.19662v1", "comment": "6 pages, 2 figures, 4 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19663v1", "title": "A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?", "authors": ["Yigitcan \u00d6zer", "Woosung Choi", "Joan Serr\u00e0", "Mayank Kumar Singh", "Wei-Hsiang Liao", "Yuki Mitsufuji"], "abstract": "We present a framework to foster the evaluation of deep learning-based audio\nwatermarking algorithms, establishing a standardized benchmark and allowing\nsystematic comparisons. To simulate real-world usage, we introduce a\ncomprehensive audio attack pipeline, featuring various distortions such as\ncompression, background noise, and reverberation, and propose a diverse test\ndataset, including speech, environmental sounds, and music recordings. By\nassessing the performance of four existing watermarking algorithms on our\nframework, two main insights stand out: (i) neural compression techniques pose\nthe most significant challenge, even when algorithms are trained with such\ncompressions; and (ii) training with audio attacks generally improves\nrobustness, although it is insufficient in some cases. Furthermore, we find\nthat specific distortions, such as polarity inversion, time stretching, or\nreverb, seriously affect certain algorithms. Our contributions strengthen the\nrobustness and perceptual assessment of audio watermarking algorithms across a\nwide range of applications, while ensuring a fair and consistent evaluation\napproach. The evaluation framework, including the attack pipeline, is\naccessible at github.com/SonyResearch/wm_robustness_eval.", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "published": "2025-05-26 08:21:58", "updated": "2025-05-26 08:21:58", "pdf_url": "http://arxiv.org/pdf/2505.19663v1", "comment": "5 pages; 5 tables; accepted at INTERSPEECH 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19667v1", "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation", "authors": ["Weikang Yuan", "Kaisong Song", "Zhuoren Jiang", "Junjie Cao", "Yujie Zhang", "Jun Lin", "Kun Kuang", "Ji Zhang", "Xiaozhong Liu"], "abstract": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-26 08:24:32", "updated": "2025-05-26 08:24:32", "pdf_url": "http://arxiv.org/pdf/2505.19667v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19671v1", "title": "Automated evaluation of children's speech fluency for low-resource languages", "authors": ["Bowen Zhang", "Nur Afiqah Abdul Latiff", "Justin Kan", "Rong Tong", "Donny Soh", "Xiaoxiao Miao", "Ian McLoughlin"], "abstract": "Assessment of children's speaking fluency in education is well researched for\nmajority languages, but remains highly challenging for low resource languages.\nThis paper proposes a system to automatically assess fluency by combining a\nfine-tuned multilingual ASR model, an objective metrics extraction stage, and a\ngenerative pre-trained transformer (GPT) network. The objective metrics include\nphonetic and word error rates, speech rate, and speech-pause duration ratio.\nThese are interpreted by a GPT-based classifier guided by a small set of\nhuman-evaluated ground truth examples, to score fluency. We evaluate the\nproposed system on a dataset of children's speech in two low-resource\nlanguages, Tamil and Malay and compare the classification performance against\nRandom Forest and XGBoost, as well as using ChatGPT-4o to predict fluency\ndirectly from speech input. Results demonstrate that the proposed approach\nachieves significantly higher accuracy than multimodal GPT or other methods.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-26 08:25:50", "updated": "2025-05-26 08:25:50", "pdf_url": "http://arxiv.org/pdf/2505.19671v1", "comment": "5 pages, 2 figures, conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19675v1", "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "abstract": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 08:31:55", "updated": "2025-05-26 08:31:55", "pdf_url": "http://arxiv.org/pdf/2505.19675v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19676v1", "title": "Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models", "authors": ["Lachlan McGinness", "Peter Baumgartner"], "abstract": "Empirical methods to examine the capability of Large Language Models (LLMs)\nto use Automated Theorem Prover (ATP) reasoning strategies are studied. We\nevaluate the performance of State of the Art models from December 2023 and\nAugust 2024 on PRONTOQA steamroller reasoning problems. For that, we develop\nmethods for assessing LLM response accuracy and correct answer correlation.\n  Our results show that progress in improving LLM reasoning abilities has\nstalled over the nine month period. By tracking completion tokens, we show that\nalmost all improvement in reasoning ability since GPT-4 was released can be\nattributed to either hidden system prompts or the training of models to\nautomatically use generic Chain of Thought prompting strategies. Among the ATP\nreasoning strategies tried, we found that current frontier LLMs are best able\nto follow the bottom-up (also known as forward-chaining) strategy. A low\npositive correlation was found between an LLM response containing correct\nreasoning and arriving at the correct conclusion.", "categories": ["cs.AI"], "published": "2025-05-26 08:34:07", "updated": "2025-05-26 08:34:07", "pdf_url": "http://arxiv.org/pdf/2505.19676v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19679v1", "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "authors": ["Zhaolin Li", "Yining Liu", "Danni Liu", "Tuan Nam Nguyen", "Enes Yavuz Ugan", "Tu Anh Dinh", "Carlos Mullov", "Alexander Waibel", "Jan Niehues"], "abstract": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 08:38:02", "updated": "2025-05-26 08:38:02", "pdf_url": "http://arxiv.org/pdf/2505.19679v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19683v1", "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey", "authors": ["Pengfei Cao", "Tianyi Men", "Wencan Liu", "Jingwen Zhang", "Xuzhao Li", "Xixun Lin", "Dianbo Sui", "Yanan Cao", "Kang Liu", "Jun Zhao"], "abstract": "Planning represents a fundamental capability of intelligent agents, requiring\ncomprehensive environmental understanding, rigorous logical reasoning, and\neffective sequential decision-making. While Large Language Models (LLMs) have\ndemonstrated remarkable performance on certain planning tasks, their broader\napplication in this domain warrants systematic investigation. This paper\npresents a comprehensive review of LLM-based planning. Specifically, this\nsurvey is structured as follows: First, we establish the theoretical\nfoundations by introducing essential definitions and categories about automated\nplanning. Next, we provide a detailed taxonomy and analysis of contemporary\nLLM-based planning methodologies, categorizing them into three principal\napproaches: 1) External Module Augmented Methods that combine LLMs with\nadditional components for planning, 2) Finetuning-based Methods that involve\nusing trajectory data and feedback signals to adjust LLMs in order to improve\ntheir planning abilities, and 3) Searching-based Methods that break down\ncomplex tasks into simpler components, navigate the planning space, or enhance\ndecoding strategies to find the best solutions. Subsequently, we systematically\nsummarize existing evaluation frameworks, including benchmark datasets,\nevaluation metrics and performance comparisons between representative planning\nmethods. Finally, we discuss the underlying mechanisms enabling LLM-based\nplanning and outline promising research directions for this rapidly evolving\nfield. We hope this survey will serve as a valuable resource to inspire\ninnovation and drive progress in this field.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 08:44:53", "updated": "2025-05-26 08:44:53", "pdf_url": "http://arxiv.org/pdf/2505.19683v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19687v1", "title": "DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech", "authors": ["Deok-Hyeon Cho", "Hyung-Seok Oh", "Seung-Bin Kim", "Seong-Whan Lee"], "abstract": "Cross-speaker emotion transfer in speech synthesis relies on extracting\nspeaker-independent emotion embeddings for accurate emotion modeling without\nretaining speaker traits. However, existing timbre compression methods fail to\nfully separate speaker and emotion characteristics, causing speaker leakage and\ndegraded synthesis quality. To address this, we propose DiEmo-TTS, a\nself-supervised distillation method to minimize emotional information loss and\npreserve speaker identity. We introduce cluster-driven sampling and information\nperturbation to preserve emotion while removing irrelevant factors. To\nfacilitate this process, we propose an emotion clustering and matching approach\nusing emotional attribute prediction and speaker embeddings, enabling\ngeneralization to unlabeled data. Additionally, we designed a dual conditioning\ntransformer to integrate style features better. Experimental results confirm\nthe effectiveness of our method in learning speaker-irrelevant emotion\nembeddings.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-26 08:47:39", "updated": "2025-05-26 08:47:39", "pdf_url": "http://arxiv.org/pdf/2505.19687v1", "comment": "Proceedings of Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19690v1", "title": "Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models", "authors": ["Baihui Zheng", "Boren Zheng", "Kerui Cao", "Yingshui Tan", "Zhendong Liu", "Weixun Wang", "Jiaheng Liu", "Jian Yang", "Wenbo Su", "Xiaoyong Zhu", "Bo Zheng", "Kaifu Zhang"], "abstract": "Despite the remarkable proficiency of \\textit{Large Reasoning Models} (LRMs)\nin handling complex reasoning tasks, their reliability in safety-critical\nscenarios remains uncertain. Existing evaluations primarily assess\nresponse-level safety, neglecting a critical issue we identify as\n\\textbf{\\textit{Superficial Safety Alignment} (SSA)} -- a phenomenon where\nmodels produce superficially safe outputs while internal reasoning processes\nfail to genuinely detect and mitigate underlying risks, resulting in\ninconsistent safety behaviors across multiple sampling attempts. To\nsystematically investigate SSA, we introduce \\textbf{Beyond Safe Answers (BSA)}\nbench, a novel benchmark comprising 2,000 challenging instances organized into\nthree distinct SSA scenario types and spanning nine risk categories, each\nmeticulously annotated with risk rationales. Evaluations of 19 state-of-the-art\nLRMs demonstrate the difficulty of this benchmark, with top-performing models\nachieving only 38.0\\% accuracy in correctly identifying risk rationales. We\nfurther explore the efficacy of safety rules, specialized fine-tuning on safety\nreasoning data, and diverse decoding strategies in mitigating SSA. Our work\nprovides a comprehensive assessment tool for evaluating and improving safety\nreasoning fidelity in LRMs, advancing the development of genuinely risk-aware\nand reliably safe AI systems.", "categories": ["cs.AI"], "published": "2025-05-26 08:49:19", "updated": "2025-05-26 08:49:19", "pdf_url": "http://arxiv.org/pdf/2505.19690v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19693v1", "title": "EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification", "authors": ["Deok-Hyeon Cho", "Hyung-Seok Oh", "Seung-Bin Kim", "Seong-Whan Lee"], "abstract": "Speech emotion recognition predicts a speaker's emotional state from speech\nsignals using discrete labels or continuous dimensions such as arousal,\nvalence, and dominance (VAD). We propose EmoSphere-SER, a joint model that\nintegrates spherical VAD region classification to guide VAD regression for\nimproved emotion prediction. In our framework, VAD values are transformed into\nspherical coordinates that are divided into multiple spherical regions, and an\nauxiliary classification task predicts which spherical region each point\nbelongs to, guiding the regression process. Additionally, we incorporate a\ndynamic weighting scheme and a style pooling layer with multi-head\nself-attention to capture spectral and temporal dynamics, further boosting\nperformance. This combined training strategy reinforces structured learning and\nimproves prediction consistency. Experimental results show that our approach\nexceeds baseline methods, confirming the validity of the proposed framework.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-26 08:50:23", "updated": "2025-05-26 08:50:23", "pdf_url": "http://arxiv.org/pdf/2505.19693v1", "comment": "Proceedings of Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19698v1", "title": "JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning", "authors": ["Jing Yu Lim", "Zarif Ikram", "Samson Yu", "Haozhe Ma", "Tze-Yun Leong", "Dianbo Liu"], "abstract": "Recent advances in model-based reinforcement learning (MBRL) have achieved\nsuper-human level performance on the Atari100k benchmark, driven by\nreinforcement learning agents trained on powerful diffusion world models.\nHowever, we identify that the current aggregates mask a major performance\nasymmetry: MBRL agents dramatically outperform humans in some tasks despite\ndrastically underperforming in others, with the former inflating the aggregate\nmetrics. This is especially pronounced in pixel-based agents trained with\ndiffusion world models. In this work, we address the pronounced asymmetry\nobserved in pixel-based agents as an initial attempt to reverse the worrying\nupward trend observed in them. We address the problematic aggregates by\ndelineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal\nimportance on metrics from both sets. Next, we hypothesize this pronounced\nasymmetry is due to the lack of temporally-structured latent space trained with\nthe World Model objective in pixel-based methods. Lastly, to address this\nissue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion\nworld model trained end-to-end with the self-consistency objective. JEDI\noutperforms SOTA models in human-optimal tasks while staying competitive across\nthe Atari100k benchmark, and runs 3 times faster with 43% lower memory than the\nlatest pixel-based diffusion baseline. Overall, our work rethinks what it truly\nmeans to cross human-level performance in Atari100k.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-26 08:52:45", "updated": "2025-05-26 08:52:45", "pdf_url": "http://arxiv.org/pdf/2505.19698v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19699v1", "title": "Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments", "authors": ["Junming Liu", "Yanting Gao", "Siyuan Meng", "Yifei Sun", "Aoqi Wu", "Yufei Jin", "Yirong Chen", "Ding Wang", "Guosun Zeng"], "abstract": "Federated Learning (FL) is a decentralized machine learning paradigm that\nenables clients to collaboratively train models while preserving data privacy.\nHowever, the coexistence of model and data heterogeneity gives rise to\ninconsistent representations and divergent optimization dynamics across\nclients, ultimately hindering robust global performance. To transcend these\nchallenges, we propose Mosaic, a novel data-free knowledge distillation\nframework tailored for heterogeneous distributed environments. Mosaic first\ntrains local generative models to approximate each client's personalized\ndistribution, enabling synthetic data generation that safeguards privacy\nthrough strict separation from real data. Subsequently, Mosaic forms a\nMixture-of-Experts (MoE) from client models based on their specialized\nknowledge, and distills it into a global model using the generated data. To\nfurther enhance the MoE architecture, Mosaic integrates expert predictions via\na lightweight meta model trained on a few representative prototypes. Extensive\nexperiments on standard image classification benchmarks demonstrate that Mosaic\nconsistently outperforms state-of-the-art approaches under both model and data\nheterogeneity. The source code has been published at\nhttps://github.com/Wings-Of-Disaster/Mosaic.", "categories": ["cs.LG", "cs.AI", "cs.DC"], "published": "2025-05-26 08:52:49", "updated": "2025-05-26 08:52:49", "pdf_url": "http://arxiv.org/pdf/2505.19699v1", "comment": "43 pages, 23 figures, 15 tables; the last dance", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19700v1", "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models", "authors": ["Yi Liu", "Dianqing Liu", "Mingye Zhu", "Junbo Guo", "Yongdong Zhang", "Zhendong Mao"], "abstract": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 08:53:02", "updated": "2025-05-26 08:53:02", "pdf_url": "http://arxiv.org/pdf/2505.19700v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19706v1", "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision", "authors": ["Tej Deep Pala", "Panshul Sharma", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "abstract": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 08:56:36", "updated": "2025-05-26 08:56:36", "pdf_url": "http://arxiv.org/pdf/2505.19706v1", "comment": "https://github.com/declare-lab/PathFinder-PRM", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19714v1", "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "abstract": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 09:02:35", "updated": "2025-05-26 09:02:35", "pdf_url": "http://arxiv.org/pdf/2505.19714v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19715v1", "title": "Graceful Forgetting in Generative Language Models", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "abstract": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 09:03:57", "updated": "2025-05-26 09:03:57", "pdf_url": "http://arxiv.org/pdf/2505.19715v1", "comment": "8 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19716v1", "title": "Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting", "authors": ["Yifan Wu", "Jingze Shi", "Bingheng Wu", "Jiayi Zhang", "Xiaotian Lin", "Nan Tang", "Yuyu Luo"], "abstract": "Existing chain-of-thought (CoT) distillation methods can effectively transfer\nreasoning abilities to base models but suffer from two major limitations:\nexcessive verbosity of reasoning traces and inadequate adaptability to problem\ndifficulty. Long reasoning traces significantly increase inference costs, and\nuniform-length solutions prevent base models from learning adaptive reasoning\nstrategies. To address these issues, we propose a difficulty-aware prompting\n(DAP) method to dynamically shorten reasoning traces without performance loss.\nIn our approach, a large teacher model first judges each problem's difficulty\nand then rewrites its reasoning traces to an appropriate shorter length,\nyielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we\ncurate a distilled dataset called LiteCoT consisting of 100K concise reasoning\nexamples, with solutions averaging only 720 tokens (an order of magnitude\nshorter than typical CoTs). Using LiteCoT, we distilled a new family of\nreasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5\narchitecture. Experiments show that a student model fine-tuned on just 100K of\nthese difficulty-pruned CoT samples outperforms a model distilled on 800K\noriginal Long CoT samples, while significantly reducing training and inference\ncosts. Our method also generalizes well: across 11 diverse benchmarks, the\nshorter difficulty-aware CoTs achieve equal or better accuracy than Long\nchains, using far fewer tokens. For example, on the challenging AIME24 exam,\nour approach reaches $74.2\\%$ Pass@1 using only about 5K inference tokens,\nsurpassing other methods that consume many more tokens. Our code and data are\navailable at https://github.com/Evanwu1125/LiteCoT.", "categories": ["cs.AI"], "published": "2025-05-26 09:04:44", "updated": "2025-05-26 09:04:44", "pdf_url": "http://arxiv.org/pdf/2505.19716v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19719v1", "title": "OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction", "authors": ["Juntong Wang", "Xiyuan Wang", "Muhan Zhang"], "abstract": "Common Neighbors (CNs) and their higher-order variants are important pairwise\nfeatures widely used in state-of-the-art link prediction methods. However,\nexisting methods often struggle with the repetition across different orders of\nCNs and fail to fully leverage their potential. We identify that these\nlimitations stem from two key issues: redundancy and over-smoothing in\nhigh-order common neighbors. To address these challenges, we design\northogonalization to eliminate redundancy between different-order CNs and\nnormalization to mitigate over-smoothing. By combining these two techniques, we\npropose Orthogonal Common Neighbor (OCN), a novel approach that significantly\noutperforms the strongest baselines by an average of 7.7% on popular link\nprediction benchmarks. A thorough theoretical analysis is provided to support\nour method. Ablation studies also verify the effectiveness of our\northogonalization and normalization techniques.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 09:08:25", "updated": "2025-05-26 09:08:25", "pdf_url": "http://arxiv.org/pdf/2505.19719v1", "comment": "35 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19722v1", "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", "authors": ["Yihao Ai", "Zhiyuan Ning", "Weiwei Dai", "Pengfei Wang", "Yi Du", "Wenjuan Cui", "Kunpeng Liu", "Yuanchun Zhou"], "abstract": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 09:10:19", "updated": "2025-05-26 09:10:19", "pdf_url": "http://arxiv.org/pdf/2505.19722v1", "comment": "Accepted by ICIC 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19734v1", "title": "ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection", "authors": ["Juxin Niu", "Xiangfeng Liu", "Dan Niu", "Xi Wang", "Zhe Jiang", "Nan Guan"], "abstract": "Coding with hardware description languages (HDLs) such as Verilog is a\ntime-intensive and laborious task. With the rapid advancement of large language\nmodels (LLMs), there is increasing interest in applying LLMs to assist with HDL\ncoding. Recent efforts have demonstrated the potential of LLMs in translating\nnatural language to traditional HDL Verilog. Chisel, a next-generation HDL\nbased on Scala, introduces higher-level abstractions, facilitating more\nconcise, maintainable, and scalable hardware designs. However, the potential of\nusing LLMs for Chisel code generation remains largely unexplored. This work\nproposes ReChisel, an LLM-based agentic system designed to enhance the\neffectiveness of Chisel code generation. ReChisel incorporates a reflection\nmechanism to iteratively refine the quality of generated code using feedback\nfrom compilation and simulation processes, and introduces an escape mechanism\nto break free from non-progress loops. Experiments demonstrate that ReChisel\nsignificantly improves the success rate of Chisel code generation, achieving\nperformance comparable to state-of-the-art LLM-based agentic systems for\nVerilog code generation.", "categories": ["cs.AI", "cs.AR"], "published": "2025-05-26 09:20:07", "updated": "2025-05-26 09:20:07", "pdf_url": "http://arxiv.org/pdf/2505.19734v1", "comment": "Accepted to DAC 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19752v1", "title": "Discrete Markov Bridge", "authors": ["Hengli Li", "Yuxuan Wang", "Song-Chun Zhu", "Ying Nian Wu", "Zilong Zheng"], "abstract": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 09:32:12", "updated": "2025-05-26 09:32:12", "pdf_url": "http://arxiv.org/pdf/2505.19752v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19754v1", "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "abstract": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 09:33:10", "updated": "2025-05-26 09:33:10", "pdf_url": "http://arxiv.org/pdf/2505.19754v1", "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19757v1", "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement", "authors": ["Maria Dziuba", "Valentin Malykh"], "abstract": "Effective generation of structured code comments requires robust quality\nmetrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)\nsuffer from limited code-comment analysis. We propose CIDRe, a\nlanguage-agnostic reference-free quality criterion combining four synergistic\naspects: (1) relevance (code-comment semantic alignment), (2) informativeness\n(functional coverage), (3) completeness (presence of all structure sections),\nand (4) description length (detail sufficiency). We validate our criterion on a\nmanually annotated dataset. Experiments demonstrate CIDRe's superiority over\nexisting metrics, achieving improvement in cross-entropy evaluation. When\napplied to filter comments, the models finetuned on CIDRe-filtered data show\nstatistically significant quality gains in GPT-4o-mini assessments.", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-26 09:36:57", "updated": "2025-05-26 09:36:57", "pdf_url": "http://arxiv.org/pdf/2505.19757v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19761v1", "title": "Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning", "authors": ["Zican Hu", "Wei Liu", "Xiaoye Qu", "Xiangyu Yue", "Chunlin Chen", "Zhi Wang", "Yu Cheng"], "abstract": "While showing sophisticated reasoning abilities, large language models (LLMs)\nstill struggle with long-horizon decision-making tasks due to deficient\nexploration and long-term credit assignment, especially in sparse-reward\nscenarios. Inspired by the divide-and-conquer principle, we propose an\ninnovative framework **GLIDER** (**G**rounding **L**anguage Models as\nEff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical\n**R**einforcement Learning) that introduces a parameter-efficient and generally\napplicable hierarchy to LLM policies. We develop a scheme where the low-level\ncontroller is supervised with abstract, step-by-step plans that are learned and\ninstructed by the high-level policy. This design decomposes complicated\nproblems into a series of coherent chain-of-thought reasoning sub-tasks,\nproviding flexible temporal abstraction to significantly enhance exploration\nand learning for long-horizon tasks. Furthermore, GLIDER facilitates fast\nonline adaptation to non-stationary environments owing to the strong\ntransferability of its task-agnostic low-level skills. Experiments on\nScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent\nperformance gains, along with enhanced generalization capabilities.", "categories": ["cs.AI"], "published": "2025-05-26 09:43:40", "updated": "2025-05-26 09:43:40", "pdf_url": "http://arxiv.org/pdf/2505.19761v1", "comment": "Accepted by ICML 2025, 21 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19762v1", "title": "Language Model-Enhanced Message Passing for Heterophilic Graph Learning", "authors": ["Wenjun Wang", "Dawei Cheng"], "abstract": "Traditional graph neural networks (GNNs), which rely on homophily-driven\nmessage passing, struggle with heterophilic graphs where connected nodes\nexhibit dissimilar features and different labels. While existing methods\naddress heterophily through graph structure refinement or adaptation of\nneighbor aggregation functions, they often overlook the semantic potential of\nnode text, rely on suboptimal message representation for propagation and\ncompromise performance on homophilic graphs. To address these limitations, we\npropose a novel language model (LM)-enhanced message passing approach for\nheterophilic graph leaning (LEMP4HG). Specifically, in the context of\ntext-attributed graph, we provide paired node texts for LM to generate their\nconnection analysis, which are encoded and then fused with paired node textual\nembeddings through a gating mechanism. The synthesized messages are\nsemantically enriched and adaptively balanced with both nodes' information,\nwhich mitigates contradictory signals when neighbor aggregation in heterophilic\nregions. Furthermore, we introduce an active learning strategy guided by our\nheuristic MVRD (Modulated Variation of Reliable Distance), selectively\nenhancing node pairs suffer most from message passing, reducing the cost of\nanalysis generation and side effects on homophilic regions. Extensive\nexperiments validate that our approach excels on heterophilic graphs and\nperforms robustly on homophilic ones, with a graph convolutional network (GCN)\nbackbone and a practical budget.", "categories": ["cs.AI"], "published": "2025-05-26 09:45:16", "updated": "2025-05-26 09:45:16", "pdf_url": "http://arxiv.org/pdf/2505.19762v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19764v1", "title": "Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding", "authors": ["Patara Trirat", "Wonyong Jeong", "Sung Ju Hwang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but optimizing LLM-based agentic systems remains challenging due\nto the vast search space of agent configurations, prompting strategies, and\ncommunication patterns. Existing approaches often rely on heuristic-based\ntuning or exhaustive evaluation, which can be computationally expensive and\nsuboptimal. This paper proposes Agentic Predictor, a lightweight predictor for\nefficient agentic workflow evaluation. Agentic Predictor is equipped with a\nmulti-view workflow encoding technique that leverages multi-view representation\nlearning of agentic systems by incorporating code architecture, textual\nprompts, and interaction graph features. To achieve high predictive accuracy\nwhile significantly reducing the number of required workflow evaluations for\ntraining a predictor, Agentic Predictor employs cross-domain unsupervised\npretraining. By learning to approximate task success rates, Agentic Predictor\nenables fast and accurate selection of optimal agentic workflow configurations\nfor a given task, significantly reducing the need for expensive trial-and-error\nevaluations. Experiments on a carefully curated benchmark spanning three\ndomains show that our predictor outperforms state-of-the-art methods in both\npredictive accuracy and workflow utility, highlighting the potential of\nperformance predictors in streamlining the design of LLM-based agentic\nworkflows.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 09:46:50", "updated": "2025-05-26 09:46:50", "pdf_url": "http://arxiv.org/pdf/2505.19764v1", "comment": "Code will be available at\n  https://github.com/DeepAuto-AI/agentic-predictor", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19769v1", "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning", "authors": ["Yuhui Chen", "Haoran Li", "Zhennan Jiang", "Haowei Wen", "Dongbin Zhao"], "abstract": "Developing scalable and generalizable reward engineering for reinforcement\nlearning (RL) is crucial for creating general-purpose agents, especially in the\nchallenging domain of robotic manipulation. While recent advances in reward\nengineering with Vision-Language Models (VLMs) have shown promise, their sparse\nreward nature significantly limits sample efficiency. This paper introduces\nTeViR, a novel method that leverages a pre-trained text-to-video diffusion\nmodel to generate dense rewards by comparing the predicted image sequence with\ncurrent observations. Experimental results across 11 complex robotic tasks\ndemonstrate that TeViR outperforms traditional methods leveraging sparse\nrewards and other state-of-the-art (SOTA) methods, achieving better sample\nefficiency and performance without ground truth environmental rewards. TeViR's\nability to efficiently guide agents in complex environments highlights its\npotential to advance reinforcement learning applications in robotic\nmanipulation.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-26 09:52:25", "updated": "2025-05-26 09:52:25", "pdf_url": "http://arxiv.org/pdf/2505.19769v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19776v1", "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Adrian Popescu"], "abstract": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 10:01:24", "updated": "2025-05-26 10:01:24", "pdf_url": "http://arxiv.org/pdf/2505.19776v1", "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19785v1", "title": "MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support", "authors": ["Qianyi Xu", "Gousia Habib", "Dilruk Perera", "Mengling Feng"], "abstract": "Timely and personalized treatment decisions are essential across a wide range\nof healthcare settings where patient responses vary significantly and evolve\nover time. Clinical data used to support these decisions are often irregularly\nsampled, sparse, and noisy. Existing decision support systems commonly rely on\ndiscretization and imputation, which can distort critical temporal dynamics and\ndegrade decision quality. Moreover, they often overlook the clinical\nsignificance of irregular recording frequencies, filtering out patterns in how\nand when data is collected. Reinforcement Learning (RL) is a natural fit for\nclinical decision-making, enabling sequential, long-term optimization in\ndynamic, uncertain environments. However, most existing treatment\nrecommendation systems are model-free and trained solely on offline data,\nmaking them sample-inefficient, sensitive to data quality, and poorly\ngeneralizable across tasks or cohorts. To address these limitations, we propose\nMedDreamer, a two-phase model-based RL framework for personalized treatment\nrecommendation. MedDreamer uses a world model with an Adaptive Feature\nIntegration (AFI) module to effectively model irregular, sparse clinical data.\nThrough latent imagination, it simulates plausible patient trajectories to\nenhance learning, refining its policy using a mix of real and imagined\nexperiences. This enables learning policies that go beyond suboptimal\nhistorical decisions while remaining grounded in clinical data. To our\nknowledge, this is the first application of latent imagination to irregular\nhealthcare data. Evaluations on sepsis and mechanical ventilation (MV)\ntreatment using two large-scale EHR datasets show that MedDreamer outperforms\nboth model-free and model-based baselines in clinical outcomes and off-policy\nmetrics.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 10:16:39", "updated": "2025-05-26 10:16:39", "pdf_url": "http://arxiv.org/pdf/2505.19785v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19788v1", "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition", "authors": ["Zihao Zeng", "Xuyao Huang", "Boxiu Li", "Hao Zhang", "Zhijie Deng"], "abstract": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond.", "categories": ["cs.AI"], "published": "2025-05-26 10:18:57", "updated": "2025-05-26 10:18:57", "pdf_url": "http://arxiv.org/pdf/2505.19788v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19790v1", "title": "Alpay Algebra III: Observer-Coupled Collapse and the Temporal Drift of Identity", "authors": ["Faruk Alpay"], "abstract": "This paper introduces a formal framework for modeling observer-dependent\ncollapse dynamics and temporal identity drift within artificial and\nmathematical systems, grounded entirely in the symbolic foundations of Alpay\nAlgebra. Building upon the fixed-point emergence structures developed in Alpay\nAlgebra I and II, this third installment formalizes the observer-coupled\n{\\phi}-collapse process through transfinite categorical flows and\ncurvature-driven identity operators. We define a novel temporal drift mechanism\nas a recursive deformation of identity signatures under entangled observer\ninfluence, constructing categorical invariants that evolve across fold\niterations. The proposed system surpasses conventional identity modeling in\nexplainable AI (XAI) by encoding internal transformation history into a\nsymbolic fixed-point structure, offering provable traceability and temporal\ncoherence. Applications range from AI self-awareness architectures to formal\nlogic systems where identity is not static but dynamically induced by\nobservation. The theoretical results also offer a mathematically rigorous basis\nfor future AI systems with stable self-referential behavior, positioning Alpay\nAlgebra as a next-generation symbolic framework bridging category theory,\nidentity logic, and observer dynamics.", "categories": ["math.CT", "cs.AI", "cs.LO", "18C10, 03G30, 68T01, 03B70, 03D80", "F.4.1; I.2.6; I.2.8"], "published": "2025-05-26 10:20:12", "updated": "2025-05-26 10:20:12", "pdf_url": "http://arxiv.org/pdf/2505.19790v1", "comment": "22 pages, 0 figures. Third paper in the Alpay Algebra series,\n  following [arXiv:2505.15344] and [arXiv:2505.17480]. Introduces\n  observer-coupled collapse and formalizes temporal identity drift using\n  transfinite {\\phi}-recursion. Entirely symbolic and self-contained, with no\n  reliance on external frameworks. Structured for submission under Math.CT,\n  CS.LO, and CS.AI", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19792v1", "title": "Types of Relations: Defining Analogies with Category Theory", "authors": ["Claire Ott", "Frank J\u00e4kel"], "abstract": "In order to behave intelligently both humans and machines have to represent\ntheir knowledge adequately for how it is used. Humans often use analogies to\ntransfer their knowledge to new domains, or help others with this transfer via\nexplanations. Hence, an important question is: What representation can be used\nto construct, find, and evaluate analogies? In this paper, we study features of\na domain that are important for constructing analogies. We do so by formalizing\nknowledge domains as categories. We use the well-known example of the analogy\nbetween the solar system and the hydrogen atom to demonstrate how to construct\ndomain categories. We also show how functors, pullbacks, and pushouts can be\nused to define an analogy, describe its core and a corresponding blend of the\nunderlying domains.", "categories": ["cs.AI"], "published": "2025-05-26 10:22:44", "updated": "2025-05-26 10:22:44", "pdf_url": "http://arxiv.org/pdf/2505.19792v1", "comment": "27 pages, 15 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19795v1", "title": "The Missing Point in Vision Transformers for Universal Image Segmentation", "authors": ["Sajjad Shahabodini", "Mobina Mansoori", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "abstract": "Image segmentation remains a challenging task in computer vision, demanding\nrobust mask generation and precise classification. Recent mask-based approaches\nyield high-quality masks by capturing global context. However, accurately\nclassifying these masks, especially in the presence of ambiguous boundaries and\nimbalanced class distributions, remains an open challenge. In this work, we\nintroduce ViT-P, a novel two-stage segmentation framework that decouples mask\ngeneration from classification. The first stage employs a proposal generator to\nproduce class-agnostic mask proposals, while the second stage utilizes a\npoint-based classification model built on the Vision Transformer (ViT) to\nrefine predictions by focusing on mask central points. ViT-P serves as a\npre-training-free adapter, allowing the integration of various pre-trained\nvision transformers without modifying their architecture, ensuring adaptability\nto dense prediction tasks. Furthermore, we demonstrate that coarse and bounding\nbox annotations can effectively enhance classification without requiring\nadditional training on fine annotation datasets, reducing annotation costs\nwhile maintaining strong performance. Extensive experiments across COCO,\nADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving\nstate-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4\nmIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic\nsegmentation. The code and pretrained models are available at:\nhttps://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "published": "2025-05-26 10:29:13", "updated": "2025-05-26 10:29:13", "pdf_url": "http://arxiv.org/pdf/2505.19795v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19809v1", "title": "Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees", "authors": ["Daniel Ordo\u00f1ez-Apraez", "Alek Fr\u00f6hlich", "Vladimir Kosti\u0107", "Karim Lounici", "Vivien Brandt", "Massimiliano Pontil"], "abstract": "In many real-world applications of regression, conditional probability\nestimation, and uncertainty quantification, exploiting symmetries rooted in\nphysics or geometry can dramatically improve generalization and sample\nefficiency. While geometric deep learning has made significant empirical\nadvances by incorporating group-theoretic structure, less attention has been\ngiven to statistical learning guarantees. In this paper, we introduce an\nequivariant representation learning framework that simultaneously addresses\nregression, conditional probability estimation, and uncertainty quantification\nwhile providing first-of-its-kind non-asymptotic statistical learning\nguarantees. Grounded in operator and group representation theory, our framework\napproximates the spectral decomposition of the conditional expectation\noperator, building representations that are both equivariant and disentangled\nalong independent symmetry subgroups. Empirical evaluations on synthetic\ndatasets and real-world robotics applications confirm the potential of our\napproach, matching or outperforming existing equivariant baselines in\nregression while additionally providing well-calibrated parametric uncertainty\nestimates.", "categories": ["cs.LG", "cs.AI", "cs.RO", "43-06", "I.2.6; I.2.9; I.5.1"], "published": "2025-05-26 10:47:23", "updated": "2025-05-26 10:47:23", "pdf_url": "http://arxiv.org/pdf/2505.19809v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19815v1", "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Shudong Liu", "Taolin Zhang", "Zihan Ma", "Songyang Zhang", "Kai Chen"], "abstract": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 10:52:17", "updated": "2025-05-26 10:52:17", "pdf_url": "http://arxiv.org/pdf/2505.19815v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19819v1", "title": "FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets", "authors": ["Dannong Wang", "Jaisal Patel", "Daochen Zha", "Steve Y. Yang", "Xiao-Yang Liu"], "abstract": "Low-rank adaptation (LoRA) methods show great potential for scaling\npre-trained general-purpose Large Language Models (LLMs) to hundreds or\nthousands of use scenarios. However, their efficacy in high-stakes domains like\nfinance is rarely explored, e.g., passing CFA exams and analyzing SEC filings.\nIn this paper, we present the open-source FinLoRA project that benchmarks LoRA\nmethods on both general and highly professional financial tasks. First, we\ncurated 19 datasets covering diverse financial applications; in particular, we\ncreated four novel XBRL analysis datasets based on 150 SEC filings. Second, we\nevaluated five LoRA methods and five base LLMs. Finally, we provide extensive\nexperimental results in terms of accuracy, F1, and BERTScore and report\ncomputational cost in terms of time and GPU memory during fine-tuning and\ninference stages. We find that LoRA methods achieved substantial performance\ngains of 36\\% on average over base models. Our FinLoRA project provides an\naffordable and scalable approach to democratize financial intelligence to the\ngeneral public. Datasets, LoRA adapters, code, and documentation are available\nat https://github.com/Open-Finance-Lab/FinLoRA", "categories": ["cs.CE", "cs.AI"], "published": "2025-05-26 10:58:51", "updated": "2025-05-26 10:58:51", "pdf_url": "http://arxiv.org/pdf/2505.19819v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19823v1", "title": "LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments", "authors": ["Pengcheng Sun", "Erwu Liu", "Wei Ni", "Rui Wang", "Yuanzhe Geng", "Lijuan Lai", "Abbas Jamalipour"], "abstract": "Federated Learning (FL) is a distributed machine learning paradigm based on\nprotecting data privacy of devices, which however, can still be broken by\ngradient leakage attack via parameter inversion techniques. Differential\nprivacy (DP) technology reduces the risk of private data leakage by adding\nartificial noise to the gradients, but detrimental to the FL utility at the\nsame time, especially in the scenario where the data is Non-Independent\nIdentically Distributed (Non-IID). Based on the impact of heterogeneous data on\naggregation performance, this paper proposes a Lightweight Adaptive Privacy\nAllocation (LAPA) strategy, which assigns personalized privacy budgets to\ndevices in each aggregation round without transmitting any additional\ninformation beyond gradients, ensuring both privacy protection and aggregation\nefficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)\nalgorithm is employed to optimize the transmission power, in order to determine\nthe optimal timing at which the adaptively attenuated artificial noise aligns\nwith the communication noise, enabling an effective balance between DP and\nsystem utility. Finally, a reliable aggregation strategy is designed by\nintegrating communication quality and data distribution characteristics, which\nimproves aggregation performance while preserving privacy. Experimental results\ndemonstrate that the personalized noise allocation and dynamic optimization\nstrategy based on LAPA proposed in this paper enhances convergence performance\nwhile satisfying the privacy requirements of FL.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 11:00:31", "updated": "2025-05-26 11:00:31", "pdf_url": "http://arxiv.org/pdf/2505.19823v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19825v1", "title": "Foundation Models for Tabular Data within Systemic Contexts Need Grounding", "authors": ["Tassilo Klein", "Johannes Hoffart"], "abstract": "Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data.", "categories": ["cs.LG", "cs.AI", "cs.DB"], "published": "2025-05-26 11:02:51", "updated": "2025-05-26 11:02:51", "pdf_url": "http://arxiv.org/pdf/2505.19825v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19827v1", "title": "Revisiting Glorot Initialization for Long-Range Linear Recurrences", "authors": ["Noga Bar", "Mariia Seleznova", "Yotam Alexander", "Gitta Kutyniok", "Raja Giryes"], "abstract": "Proper initialization is critical for Recurrent Neural Networks (RNNs),\nparticularly in long-range reasoning tasks, where repeated application of the\nsame weight matrix can cause vanishing or exploding signals. A common baseline\nfor linear recurrences is Glorot initialization, designed to ensure stable\nsignal propagation--but derived under the infinite-width, fixed-length\nregime--an unrealistic setting for RNNs processing long sequences. In this\nwork, we show that Glorot initialization is in fact unstable: small positive\ndeviations in the spectral radius are amplified through time and cause the\nhidden state to explode. Our theoretical analysis demonstrates that sequences\nof length $t = O(\\sqrt{n})$, where $n$ is the hidden width, are sufficient to\ninduce instability. To address this, we propose a simple, dimension-aware\nrescaling of Glorot that shifts the spectral radius slightly below one,\npreventing rapid signal explosion or decay. These results suggest that standard\ninitialization schemes may break down in the long-sequence regime, motivating a\nseparate line of theory for stable recurrent initialization.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 11:04:59", "updated": "2025-05-26 11:04:59", "pdf_url": "http://arxiv.org/pdf/2505.19827v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19838v1", "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "abstract": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 11:22:17", "updated": "2025-05-26 11:22:17", "pdf_url": "http://arxiv.org/pdf/2505.19838v1", "comment": "To be published in ACL 2025 Industry Track. Paper website:\n  https://foodtaxo.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19842v1", "title": "PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints", "authors": ["Shuo Wang", "Yun Cheng", "Qingye Meng", "Olga Saukh", "Jiang Zhang", "Jingfang Fan", "Yuanting Zhang", "Xingyuan Yuan", "Lothar Thiele"], "abstract": "Air quality forecasting (AQF) is critical for public health and environmental\nmanagement, yet remains challenging due to the complex interplay of emissions,\nmeteorology, and chemical transformations. Traditional numerical models, such\nas CMAQ and WRF-Chem, provide physically grounded simulations but are\ncomputationally expensive and rely on uncertain emission inventories. Deep\nlearning models, while computationally efficient, often struggle with\ngeneralization due to their lack of physical constraints. To bridge this gap,\nwe propose PCDCNet, a surrogate model that integrates numerical modeling\nprinciples with deep learning. PCDCNet explicitly incorporates emissions,\nmeteorological influences, and domain-informed constraints to model pollutant\nformation, transport, and dissipation. By combining graph-based spatial\ntransport modeling, recurrent structures for temporal accumulation, and\nrepresentation enhancement for local interactions, PCDCNet achieves\nstate-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3\nforecasting while significantly reducing computational costs. Furthermore, our\nmodel is deployed in an online platform, providing free, real-time air quality\nforecasts, demonstrating its scalability and societal impact. By aligning deep\nlearning with physical consistency, PCDCNet offers a practical and\ninterpretable solution for AQF, enabling informed decision-making for both\npersonal and regulatory applications.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 11:27:07", "updated": "2025-05-26 11:27:07", "pdf_url": "http://arxiv.org/pdf/2505.19842v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19847v1", "title": "DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems", "authors": ["Wenqing Zhou", "Yuxuan Yan", "Qianqian Yang"], "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the capabilities of language models by integrating external knowledge.\nDue to the diversity of data sources and the constraints of memory and\ncomputing resources, real-world data is often scattered in multiple devices.\nConventional RAGs that store massive amounts of scattered data centrally face\nincreasing privacy concerns and high computational costs. Additionally, RAG in\na central node raises latency issues when searching over a large-scale\nknowledge base. To address these challenges, we propose a distributed Knowledge\nGraph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where\neach edge device maintains a local knowledge base without the need to share it\nwith the cloud, instead sharing only summaries of its knowledge. Specifically,\nDGRAG has two main phases. In the Distributed Knowledge Construction phase,\nDGRAG organizes local knowledge using knowledge graphs, generating subgraph\nsummaries and storing them in a summary database in the cloud as information\nsharing. In the Collaborative Retrieval and Generation phase, DGRAG first\nperforms knowledge retrieval and answer generation locally, and a gate\nmechanism determines whether the query is beyond the scope of local knowledge\nor processing capabilities. For queries that exceed the local knowledge scope,\nthe cloud retrieves knowledge from the most relevant edges based on the\nsummaries and generates a more precise answer. Experimental results demonstrate\nthe effectiveness of the proposed DGRAG approach in significantly improving the\nquality of question-answering tasks over baseline approaches.", "categories": ["cs.AI", "cs.DC"], "published": "2025-05-26 11:31:58", "updated": "2025-05-26 11:31:58", "pdf_url": "http://arxiv.org/pdf/2505.19847v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19850v1", "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning", "authors": ["Leander Diaz-Bone", "Marco Bagatella", "Jonas H\u00fcbotter", "Andreas Krause"], "abstract": "Sparse-reward reinforcement learning (RL) can model a wide range of highly\ncomplex tasks. Solving sparse-reward tasks is RL's core premise - requiring\nefficient exploration coupled with long-horizon credit assignment - and\novercoming these challenges is key for building self-improving agents with\nsuperhuman ability. We argue that solving complex and high-dimensional tasks\nrequires solving simpler tasks that are relevant to the target task. In\ncontrast, most prior work designs strategies for selecting exploratory tasks\nwith the objective of solving any task, making exploration of challenging\nhigh-dimensional, long-horizon tasks intractable. We find that the sense of\ndirection, necessary for effective exploration, can be extracted from existing\nRL algorithms, without needing any prior information. Based on this finding, we\npropose a method for directed sparse-reward goal-conditioned very long-horizon\nRL (DISCOVER), which selects exploratory goals in the direction of the target\ntask. We connect DISCOVER to principled exploration in bandits, formally\nbounding the time until the target task becomes achievable in terms of the\nagent's initial distance to the target, but independent of the volume of the\nspace of all tasks. Empirically, we perform a thorough evaluation in\nhigh-dimensional environments. We find that the directed goal selection of\nDISCOVER solves exploration problems that are beyond the reach of prior\nstate-of-the-art exploration methods in RL.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "published": "2025-05-26 11:35:07", "updated": "2025-05-26 11:35:07", "pdf_url": "http://arxiv.org/pdf/2505.19850v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19851v1", "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "abstract": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 11:35:51", "updated": "2025-05-26 11:35:51", "pdf_url": "http://arxiv.org/pdf/2505.19851v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19853v1", "title": "Two Causally Related Needles in a Video Haystack", "authors": ["Miaoyu Li", "Qin Chao", "Boyang Li"], "abstract": "Evaluating the video understanding capabilities of Video-Language Models\n(VLMs) remains a significant challenge. We propose a long-context video\nunderstanding benchmark, Causal2Needles, that assesses two crucial abilities\ninsufficiently evaluated by existing benchmarks: (1) the ability to extract\ninformation from two separate locations in a long video and understand them\njointly, and (2) the ability to model the world in terms of cause and effect in\nhuman behaviors. Specifically, Causal2Needles introduces 2-needle questions,\nwhich require extracting information from both the cause and effect\nhuman-behavior events in a long video and the associated narration text. To\nprevent textual bias, these questions comprise two complementary formats: one\nasking to identify the video clip containing the answer, and one asking for the\ntextual description of an unrelated visual detail from that video clip. Our\nexperiments reveal that models excelling in pre-existing benchmarks struggle\nwith 2-needle visual grounding, and the model performance is negatively\ncorrelated with the distance between the two needles. These findings highlight\ncritical limitations in current VLMs.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 11:37:34", "updated": "2025-05-26 11:37:34", "pdf_url": "http://arxiv.org/pdf/2505.19853v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19866v1", "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation", "authors": ["Feng Xiong", "Hongling Xu", "Yifei Wang", "Runxi Cheng", "Yong Wang", "Xiangxiang Chu"], "abstract": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-26 11:50:16", "updated": "2025-05-26 11:50:16", "pdf_url": "http://arxiv.org/pdf/2505.19866v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19867v1", "title": "Deep Active Inference Agents for Delayed and Long-Horizon Environments", "authors": ["Yavar Taheri Yeganeh", "Mohsen Jafari", "Andrea Matta"], "abstract": "With the recent success of world-model agents, which extend the core idea of\nmodel-based reinforcement learning by learning a differentiable model for\nsample-efficient control across diverse tasks, active inference (AIF) offers a\ncomplementary, neuroscience-grounded paradigm that unifies perception,\nlearning, and action within a single probabilistic framework powered by a\ngenerative model. Despite this promise, practical AIF agents still rely on\naccurate immediate predictions and exhaustive planning, a limitation that is\nexacerbated in delayed environments requiring plans over long horizons, tens to\nhundreds of steps. Moreover, most existing agents are evaluated on robotic or\nvision benchmarks which, while natural for biological agents, fall short of\nreal-world industrial complexity. We address these limitations with a\ngenerative-policy architecture featuring (i) a multi-step latent transition\nthat lets the generative model predict an entire horizon in a single\nlook-ahead, (ii) an integrated policy network that enables the transition and\nreceives gradients of the expected free energy, (iii) an alternating\noptimization scheme that updates model and policy from a replay buffer, and\n(iv) a single gradient step that plans over long horizons, eliminating\nexhaustive planning from the control loop. We evaluate our agent in an\nenvironment that mimics a realistic industrial scenario with delayed and\nlong-horizon settings. The empirical results confirm the effectiveness of the\nproposed approach, demonstrating the coupled world-model with the AIF formalism\nyields an end-to-end probabilistic controller capable of effective decision\nmaking in delayed, long-horizon settings without handcrafted rewards or\nexpensive planning.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 11:50:22", "updated": "2025-05-26 11:50:22", "pdf_url": "http://arxiv.org/pdf/2505.19867v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19874v1", "title": "StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation", "authors": ["Yi Wu", "Lingting Zhu", "Shengju Qian", "Lei Liu", "Wandi Qiao", "Lequan Yu", "Bin Li"], "abstract": "In the current research landscape, multimodal autoregressive (AR) models have\nshown exceptional capabilities across various domains, including visual\nunderstanding and generation. However, complex tasks such as style-aligned\ntext-to-image generation present significant challenges, particularly in data\nacquisition. In analogy to instruction-following tuning for image editing of AR\nmodels, style-aligned generation requires a reference style image and prompt,\nresulting in a text-image-to-image triplet where the output shares the style\nand semantics of the input. However, acquiring large volumes of such triplet\ndata with specific styles is considerably more challenging than obtaining\nconventional text-to-image data used for training generative models. To address\nthis issue, we propose StyleAR, an innovative approach that combines a\nspecially designed data curation method with our proposed AR models to\neffectively utilize text-to-image binary data for style-aligned text-to-image\ngeneration. Our method synthesizes target stylized data using a reference style\nimage and prompt, but only incorporates the target stylized image as the image\nmodality to create high-quality binary data. To facilitate binary data\ntraining, we introduce a CLIP image encoder with a perceiver resampler that\ntranslates the image input into style tokens aligned with multimodal tokens in\nAR models and implement a style-enhanced token technique to prevent content\nleakage which is a common issue in previous work. Furthermore, we mix raw\nimages drawn from large-scale text-image datasets with stylized images to\nenhance StyleAR's ability to extract richer stylistic features and ensure style\nconsistency. Extensive qualitative and quantitative experiments demonstrate our\nsuperior performance.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "published": "2025-05-26 12:01:15", "updated": "2025-05-26 12:01:15", "pdf_url": "http://arxiv.org/pdf/2505.19874v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19887v1", "title": "Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities", "authors": ["Anton Tkachenko", "Dmitrij Suskevic", "Benjamin Adolphi"], "abstract": "Large language models (LLMs) have shown promise in software engineering, yet\ntheir effectiveness for binary analysis remains unexplored. We present the\nfirst comprehensive evaluation of commercial LLMs for assembly code\ndeobfuscation. Testing seven state-of-the-art models against four obfuscation\nscenarios (bogus control flow, instruction substitution, control flow\nflattening, and their combination), we found striking performance\nvariations--from autonomous deobfuscation to complete failure. We propose a\ntheoretical framework based on four dimensions: Reasoning Depth, Pattern\nRecognition, Noise Filtering, and Context Integration, explaining these\nvariations. Our analysis identifies five error patterns: predicate\nmisinterpretation, structural mapping errors, control flow misinterpretation,\narithmetic transformation errors, and constant propagation errors, revealing\nfundamental limitations in LLM code processing.We establish a three-tier\nresistance model: bogus control flow (low resistance), control flow flattening\n(moderate resistance), and instruction substitution/combined techniques (high\nresistance). Universal failure against combined techniques demonstrates that\nsophisticated obfuscation remains effective against advanced LLMs. Our findings\nsuggest a human-AI collaboration paradigm where LLMs reduce expertise barriers\nfor certain reverse engineering tasks while requiring human guidance for\ncomplex deobfuscation. This work provides a foundation for evaluating emerging\ncapabilities and developing resistant obfuscation techniques.x deobfuscation.\nThis work provides a foundation for evaluating emerging capabilities and\ndeveloping resistant obfuscation techniques.", "categories": ["cs.SE", "cs.AI", "cs.CR"], "published": "2025-05-26 12:16:44", "updated": "2025-05-26 12:16:44", "pdf_url": "http://arxiv.org/pdf/2505.19887v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19892v1", "title": "Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging", "authors": ["Yongxian Wei", "Runxi Cheng", "Weike Jin", "Enneng Yang", "Li Shen", "Lu Hou", "Sinan Du", "Chun Yuan", "Xiaochun Cao", "Dacheng Tao"], "abstract": "While foundation models update slowly due to resource-intensive training\nrequirements, domain-specific models evolve between updates. Model merging aims\nto combine multiple expert models into a single, more capable model, thereby\nreducing storage and serving costs while supporting decentralized model\ndevelopment. Despite its potential, previous studies have primarily focused on\nmerging visual classification models or Large Language Models (LLMs) for code\nand math tasks. Multimodal Large Language Models (MLLMs), which extend the\ncapabilities of LLMs through large-scale multimodal training, have gained\ntraction. However, there lacks a benchmark for model merging research that\nclearly divides the tasks for MLLM training and evaluation. In this paper, (i)\nwe introduce the model merging benchmark for MLLMs, which includes multiple\ntasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and\nfull fine-tuning models. Moreover, we explore how model merging can combine\ndifferent modalities (e.g., vision-language, audio-language, and video-language\nmodels), moving toward the Omni-language model. (ii) We implement 10 model\nmerging algorithms on the benchmark. Furthermore, we propose a novel method\nthat removes noise from task vectors and robustly optimizes the merged vector\nbased on a loss defined over task vector interactions, achieving an average\nperformance gain of 2.48%. (iii) We find that model merging offers a promising\nway for building improved MLLMs without requiring data training. Our results\nalso demonstrate that the complementarity among multiple modalities outperforms\nindividual modalities.", "categories": ["cs.AI"], "published": "2025-05-26 12:23:14", "updated": "2025-05-26 12:23:14", "pdf_url": "http://arxiv.org/pdf/2505.19892v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19896v1", "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Control in space,\nenabling LLMs to play a significant role in the decision-making process for\nautonomous satellite operations. As a first step towards this goal, we have\ndeveloped a pure LLM-based solution for the Kerbal Space Program Differential\nGames (KSPDG) challenge, a public software design competition where\nparticipants create autonomous agents for maneuvering satellites involved in\nnon-cooperative space operations, running on the KSP game engine. Our approach\nleverages prompt engineering, few-shot prompting, and fine-tuning techniques to\ncreate an effective LLM-based agent that ranked 2nd in the competition. To the\nbest of our knowledge, this work pioneers the integration of LLM agents into\nspace research. The project comprises several open repositories to facilitate\nreplication and further research. The codebase is accessible on\n\\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models\nand datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging\nFace}. Additionally, experiment tracking and detailed results can be reviewed\non \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases", "categories": ["cs.AI", "astro-ph.IM", "cs.CL"], "published": "2025-05-26 12:25:35", "updated": "2025-05-26 12:25:35", "pdf_url": "http://arxiv.org/pdf/2505.19896v1", "comment": "Non revised version for paper going to be published in Journal of\n  Advances in Space Research", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19897v1", "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "abstract": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "published": "2025-05-26 12:27:27", "updated": "2025-05-26 12:27:27", "pdf_url": "http://arxiv.org/pdf/2505.19897v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19905v1", "title": "EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM", "authors": ["Shuang Ao", "Flora D. Salim", "Simon Khan"], "abstract": "Although LLMs demonstrate proficiency in several text-based reasoning and\nplanning tasks, their implementation in robotics control is constrained by\nsignificant deficiencies: (1) LLM agents are designed to work mainly with\ntextual inputs rather than visual conditions; (2) Current multimodal agents\ntreat LLMs as static planners, which separates their reasoning from environment\ndynamics, resulting in actions that do not take domain-specific knowledge into\naccount; and (3) LLMs are not designed to learn from visual interactions, which\nmakes it harder for them to make better policies for specific domains. In this\npaper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively\nintegrates LLM and VLM via a bidirectional training paradigm. Unlike existing\nmethods, EMAC+ dynamically refines high-level textual plans generated by an LLM\nusing real-time feedback from a VLM executing low-level visual control tasks.\nWe address critical limitations of previous models by enabling the LLM to\ninternalize visual environment dynamics directly through interactive\nexperience, rather than relying solely on static symbolic mappings. Extensive\nexperimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+\nachieves superior task performance, robustness against noisy observations, and\nefficient learning. We also conduct thorough ablation studies and provide\ndetailed analyses of success and failure cases.", "categories": ["cs.AI"], "published": "2025-05-26 12:34:16", "updated": "2025-05-26 12:34:16", "pdf_url": "http://arxiv.org/pdf/2505.19905v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19912v1", "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "authors": ["Javier Mar\u00edn"], "abstract": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 12:39:24", "updated": "2025-05-26 12:39:24", "pdf_url": "http://arxiv.org/pdf/2505.19912v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19914v1", "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "authors": ["Jiangjie Chen", "Qianyu He", "Siyu Yuan", "Aili Chen", "Zhicheng Cai", "Weinan Dai", "Hongli Yu", "Qiying Yu", "Xuefeng Li", "Jiaze Chen", "Hao Zhou", "Mingxuan Wang"], "abstract": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 12:40:31", "updated": "2025-05-26 12:40:31", "pdf_url": "http://arxiv.org/pdf/2505.19914v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19915v1", "title": "Evaluating AI cyber capabilities with crowdsourced elicitation", "authors": ["Artem Petrov", "Dmitrii Volkov"], "abstract": "As AI systems become increasingly capable, understanding their offensive\ncyber potential is critical for informed governance and responsible deployment.\nHowever, it's hard to accurately bound their capabilities, and some prior\nevaluations dramatically underestimated them. The art of extracting maximum\ntask-specific performance from AIs is called \"AI elicitation\", and today's\nsafety organizations typically conduct it in-house. In this paper, we explore\ncrowdsourcing elicitation efforts as an alternative to in-house elicitation\nwork.\n  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI\nvs. Humans (400 teams) and Cyber Apocalypse_ (4000 teams). The AI teams achieve\noutstanding performance at both events, ranking top-13% and top-21%\nrespectively for a total of \\$7500 in bounties. This impressive performance\nsuggests that open-market elicitation may offer an effective complement to\nin-house elicitation. We propose elicitation bounties as a practical mechanism\nfor maintaining timely, cost-effective situational awareness of emerging AI\ncapabilities.\n  Another advantage of open elicitations is the option to collect human\nperformance data at scale. Applying METR's methodology, we found that AI agents\ncan reliably solve cyber challenges requiring one hour or less of effort from a\nmedian human CTF participant.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-26 12:40:32", "updated": "2025-05-26 12:40:32", "pdf_url": "http://arxiv.org/pdf/2505.19915v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19920v1", "title": "A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks", "authors": ["Sebastian Gro\u00df", "Stefan Heindorf", "Philipp Terh\u00f6rst"], "abstract": "Traditional face recognition systems rely on extracting fixed face\nrepresentations, known as templates, to store and verify identities. These\nrepresentations are typically generated by neural networks that often lack\nexplainability and raise concerns regarding fairness and privacy. In this work,\nwe propose a novel model-template (MOTE) approach that replaces vector-based\nface templates with small personalized neural networks. This design enables\nmore responsible face recognition for small and medium-scale systems. During\nenrollment, MOTE creates a dedicated binary classifier for each identity,\ntrained to determine whether an input face matches the enrolled identity. Each\nclassifier is trained using only a single reference sample, along with\nsynthetically balanced samples to allow adjusting fairness at the level of a\nsingle individual during enrollment. Extensive experiments across multiple\ndatasets and recognition systems demonstrate substantial improvements in\nfairness and particularly in privacy. Although the method increases inference\ntime and storage requirements, it presents a strong solution for small- and\nmid-scale applications where fairness and privacy are critical.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 12:45:01", "updated": "2025-05-26 12:45:01", "pdf_url": "http://arxiv.org/pdf/2505.19920v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19927v1", "title": "TCP: a Benchmark for Temporal Constraint-Based Planning", "authors": ["Zifeng Ding", "Sikuan Yan", "Zhangdie Yuan", "Xianglong Hu", "Fangru Lin", "Andreas Vlachos"], "abstract": "Temporal reasoning and planning are essential capabilities for large language\nmodels (LLMs), yet most existing benchmarks evaluate them in isolation and\nunder limited forms of complexity. To address this gap, we introduce the\nTemporal Constraint-based Planning (TCP) benchmark, that jointly assesses both\ncapabilities. Each instance in TCP features a naturalistic dialogue around a\ncollaborative project, where diverse and interdependent temporal constraints\nare explicitly or implicitly expressed, and models must infer an optimal\nschedule that satisfies all constraints. To construct TCP, we first generate\nabstract problem prototypes that are paired with realistic scenarios from\nvarious domains and enriched into dialogues using an LLM. A human quality check\nis performed on a sampled subset to confirm the reliability of our benchmark.\nWe evaluate state-of-the-art LLMs and find that even the strongest models\nstruggle with TCP, highlighting its difficulty and revealing limitations in\nLLMs' temporal constraint-based planning abilities. We analyze underlying\nfailure cases, open source our benchmark, and hope our findings can inspire\nfuture research.", "categories": ["cs.AI"], "published": "2025-05-26 12:53:01", "updated": "2025-05-26 12:53:01", "pdf_url": "http://arxiv.org/pdf/2505.19927v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19933v1", "title": "Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making", "authors": ["Yejin Son", "Minseo Kim", "Sungwoong Kim", "Seungju Han", "Jian Kim", "Dongju Jang", "Youngjae Yu", "Chanyoung Park"], "abstract": "Large Language Models (LLMs) are increasingly used for decision making in\nembodied agents, yet existing safety evaluations often rely on coarse success\nrates and domain-specific setups, making it difficult to diagnose why and where\nthese models fail. This obscures our understanding of embodied safety and\nlimits the selective deployment of LLMs in high-risk physical environments. We\nintroduce SAFEL, the framework for systematically evaluating the physical\nsafety of LLMs in embodied decision making. SAFEL assesses two key\ncompetencies: (1) rejecting unsafe commands via the Command Refusal Test, and\n(2) generating safe and executable plans via the Plan Safety Test. Critically,\nthe latter is decomposed into functional modules, goal interpretation,\ntransition modeling, action sequencing, enabling fine-grained diagnosis of\nsafety failures. To support this framework, we introduce EMBODYGUARD, a\nPDDL-grounded benchmark containing 942 LLM-generated scenarios covering both\novertly malicious and contextually hazardous instructions. Evaluation across 13\nstate-of-the-art LLMs reveals that while models often reject clearly unsafe\ncommands, they struggle to anticipate and mitigate subtle, situational risks.\nOur results highlight critical limitations in current LLMs and provide a\nfoundation for more targeted, modular improvements in safe embodied reasoning.", "categories": ["cs.AI"], "published": "2025-05-26 13:01:14", "updated": "2025-05-26 13:01:14", "pdf_url": "http://arxiv.org/pdf/2505.19933v1", "comment": "37 pages, 13 tables, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19944v1", "title": "Can Visual Encoder Learn to See Arrows?", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "abstract": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-26 13:09:31", "updated": "2025-05-26 13:09:31", "pdf_url": "http://arxiv.org/pdf/2505.19944v1", "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19947v1", "title": "Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees", "authors": ["Herbert Woisetschl\u00e4ger", "Ryan Zhang", "Shiqiang Wang", "Hans-Arno Jacobsen"], "abstract": "Open-weight LLM zoos provide access to numerous high-quality models, but\nselecting the appropriate model for specific tasks remains challenging and\nrequires technical expertise. Most users simply want factually correct, safe,\nand satisfying responses without concerning themselves with model\ntechnicalities, while inference service providers prioritize minimizing\noperating costs. These competing interests are typically mediated through\nservice level agreements (SLAs) that guarantee minimum service quality. We\nintroduce MESS+, a stochastic optimization algorithm for cost-optimal LLM\nrequest routing while providing rigorous SLA compliance guarantees. MESS+\nlearns request satisfaction probabilities of LLMs in real-time as users\ninteract with the system, based on which model selection decisions are made by\nsolving a per-request optimization problem. Our algorithm includes a novel\ncombination of virtual queues and request satisfaction prediction, along with a\ntheoretical analysis of cost optimality and constraint satisfaction. Across a\nwide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x\ncost savings compared to existing LLM routing techniques.", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2; I.2.7; I.2.8"], "published": "2025-05-26 13:11:08", "updated": "2025-05-26 13:11:08", "pdf_url": "http://arxiv.org/pdf/2505.19947v1", "comment": "Preprint. Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19948v1", "title": "SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection", "authors": ["Gokul Adethya", "Bhanu Pratyush Mantha", "Tianyang Wang", "Xingjian Li", "Min Xu"], "abstract": "Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for\nimaging macromolecular complexes in their near-native states. However, the\nlocalization of 3D particles in cellular environments still presents a\nsignificant challenge due to low signal-to-noise ratios and missing wedge\nartifacts. Deep learning approaches have shown great potential, but they need\nhuge amounts of data, which can be a challenge in cryo-ET scenarios where\nlabeled data is often scarce. In this paper, we propose a novel Self-augmented\nand Self-interpreted (SaSi) deep learning approach towards few-shot particle\ndetection in 3D cryo-ET images. Our method builds upon self-augmentation\ntechniques to further boost data utilization and introduces a self-interpreted\nsegmentation strategy for alleviating dependency on labeled data, hence\nimproving generalization and robustness. As demonstrated by experiments\nconducted on both simulated and real-world cryo-ET datasets, the SaSi approach\nsignificantly outperforms existing state-of-the-art methods for particle\nlocalization. This research increases understanding of how to detect particles\nwith very few labels in cryo-ET and thus sets a new benchmark for few-shot\nlearning in structural biology.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-26 13:14:21", "updated": "2025-05-26 13:14:21", "pdf_url": "http://arxiv.org/pdf/2505.19948v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19951v1", "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy", "authors": ["Elvir Karimov", "Alexander Varlamov", "Danil Ivanov", "Dmitrii Korzh", "Oleg Y. Rogov"], "abstract": "Deep learning voice models are commonly used nowadays, but the safety\nprocessing of personal data, such as human identity and speech content, remains\nsuspicious. To prevent malicious user identification, speaker anonymization\nmethods were proposed. Current methods, particularly based on universal\nadversarial patch (UAP) applications, have drawbacks such as significant\ndegradation of audio quality, decreased speech recognition quality, low\ntransferability across different voice biometrics models, and performance\ndependence on the input audio length. To mitigate these drawbacks, in this\nwork, we introduce and leverage the novel Exponential Total Variance (TV) loss\nfunction and provide experimental evidence that it positively affects UAP\nstrength and imperceptibility. Moreover, we present a novel scalable UAP\ninsertion procedure and demonstrate its uniformly high performance for various\naudio lengths.", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "published": "2025-05-26 13:16:01", "updated": "2025-05-26 13:16:01", "pdf_url": "http://arxiv.org/pdf/2505.19951v1", "comment": "5 pages, 3 figures, 1 table; Submitted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19955v1", "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "abstract": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 13:18:37", "updated": "2025-05-26 13:18:37", "pdf_url": "http://arxiv.org/pdf/2505.19955v1", "comment": "40 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19956v1", "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph", "authors": ["Jihyung Lee", "Jin-Seop Lee", "Jaehoon Lee", "YunSeok Choi", "Jee-Hyong Lee"], "abstract": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. Our code will be\nreleased.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 13:19:10", "updated": "2025-05-26 13:19:10", "pdf_url": "http://arxiv.org/pdf/2505.19956v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19964v1", "title": "The Limits of Preference Data for Post-Training", "authors": ["Eric Zhao", "Jessica Dai", "Pranjal Awasthi"], "abstract": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "published": "2025-05-26 13:26:15", "updated": "2025-05-26 13:26:15", "pdf_url": "http://arxiv.org/pdf/2505.19964v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19965v1", "title": "Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction", "authors": ["Yu Wang", "Junshu Dai", "Yuchen Ying", "Yuxuan Liang", "Tongya Zheng", "Mingli Song"], "abstract": "Human mobility prediction is crucial for applications ranging from\nlocation-based recommendations to urban planning, which aims to forecast users'\nnext location visits based on historical trajectories. Despite the severe\nlong-tailed distribution of locations, the problem of long-tailed mobility\nprediction remains largely underexplored. Existing long-tailed learning methods\nprimarily focus on rebalancing the skewed distribution at the data, model, or\nclass level, neglecting to exploit the spatiotemporal semantics of locations.\nTo address this gap, we propose the first plug-and-play framework for\nlong-tailed mobility prediction in an exploitation and exploration manner,\nnamed \\textbf{A}daptive \\textbf{LO}cation \\textbf{H}ier\\textbf{A}rchy learning\n(ALOHA). First, we construct city-tailored location hierarchy based on Large\nLanguage Models (LLMs) by exploiting Maslow's theory of human motivation to\ndesign Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics.\nSecond, we optimize the location hierarchy predictions by Gumbel disturbance\nand node-wise adaptive weights within the hierarchical tree structure.\nExperiments on state-of-the-art models across six datasets demonstrate the\nframework's consistent effectiveness and generalizability, which strikes a well\nbalance between head and tail locations. Weight analysis and ablation studies\nreveal the optimization differences of each component for head and tail\nlocations. Furthermore, in-depth analyses of hierarchical distance and case\nstudy demonstrate the effective semantic guidance from the location hierarchy.\nOur code will be made publicly available.", "categories": ["cs.AI"], "published": "2025-05-26 13:26:35", "updated": "2025-05-26 13:26:35", "pdf_url": "http://arxiv.org/pdf/2505.19965v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19966v1", "title": "Learning to Select In-Context Demonstration Preferred by Large Language Model", "authors": ["Zheng Zhang", "Shaocheng Lan", "Lei Song", "Jiang Bian", "Yexin Li", "Kan Ren"], "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks during inference using only a few demonstrations. However, ICL\nperformance is highly dependent on the selection of these demonstrations.\nRecent work explores retrieval-based methods for selecting query-specific\ndemonstrations, but these approaches often rely on surrogate objectives such as\nmetric learning, failing to directly optimize ICL performance. Consequently,\nthey struggle to identify truly beneficial demonstrations. Moreover, their\ndiscriminative retrieval paradigm is ineffective when the candidate pool lacks\nsufficient high-quality demonstrations. To address these challenges, we propose\nGenICL, a novel generative preference learning framework that leverages LLM\nfeedback to directly optimize demonstration selection for ICL. Experiments on\n19 datasets across 11 task categories demonstrate that GenICL achieves superior\nperformance than existing methods in selecting the most effective\ndemonstrations, leading to better ICL performance.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 13:26:56", "updated": "2025-05-26 13:26:56", "pdf_url": "http://arxiv.org/pdf/2505.19966v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19973v1", "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response", "authors": ["Bilel Cherif", "Tamas Bisztray", "Richard A. Dubniczky", "Aaesha Aldahmani", "Saeed Alshehhi", "Norbert Tihanyi"], "abstract": "Digital Forensics and Incident Response (DFIR) involves analyzing digital\nevidence to support legal investigations. Large Language Models (LLMs) offer\nnew opportunities in DFIR tasks such as log analysis and memory forensics, but\ntheir susceptibility to errors and hallucinations raises concerns in\nhigh-stakes contexts. Despite growing interest, there is no comprehensive\nbenchmark to evaluate LLMs across both theoretical and practical DFIR domains.\nTo address this gap, we present DFIR-Metric, a benchmark with three components:\n(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice\nquestions sourced from industry-standard certifications and official\ndocumentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing\nmulti-step reasoning and evidence correlation; and (3) Practical Analysis: 500\ndisk and memory forensics cases from the NIST Computer Forensics Tool Testing\nProgram (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their\naccuracy and consistency across trials. We also introduce a new metric, the\nTask Understanding Score (TUS), designed to more effectively evaluate models in\nscenarios where they achieve near-zero accuracy. This benchmark offers a\nrigorous, reproducible foundation for advancing AI in digital forensics. All\nscripts, artifacts, and results are available on the project website at\nhttps://github.com/DFIR-Metric.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-26 13:35:37", "updated": "2025-05-26 13:35:37", "pdf_url": "http://arxiv.org/pdf/2505.19973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19983v1", "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications", "authors": ["Tong Wu", "Zhiyong Chen", "Dazhi He", "Feng Yang", "Meixia Tao", "Xiaodong Xu", "Wenjun Zhang", "Ping Zhang"], "abstract": "Diffusion models (DMs) have recently achieved significant success in wireless\ncommunications systems due to their denoising capabilities. The broadcast\nnature of wireless signals makes them susceptible not only to Gaussian noise,\nbut also to unaware interference. This raises the question of whether DMs can\neffectively mitigate interference in wireless semantic communication systems.\nIn this paper, we model the interference cancellation problem as a maximum a\nposteriori (MAP) problem over the joint posterior probability of the signal and\ninterference, and theoretically prove that the solution provides excellent\nestimates for the signal and interference. To solve this problem, we develop an\ninterference cancellation diffusion model (ICDM), which decomposes the joint\nposterior into independent prior probabilities of the signal and interference,\nalong with the channel transition probablity. The log-gradients of these\ndistributions at each time step are learned separately by DMs and accurately\nestimated through deriving. ICDM further integrates these gradients with\nadvanced numerical iteration method, achieving accurate and rapid interference\ncancellation. Extensive experiments demonstrate that ICDM significantly reduces\nthe mean square error (MSE) and enhances perceptual quality compared to schemes\nwithout ICDM. For example, on the CelebA dataset under the Rayleigh fading\nchannel with a signal-to-noise ratio (SNR) of $20$ dB and signal to\ninterference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB\nand improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.", "categories": ["cs.IT", "cs.AI", "cs.CV", "math.IT"], "published": "2025-05-26 13:41:52", "updated": "2025-05-26 13:41:52", "pdf_url": "http://arxiv.org/pdf/2505.19983v1", "comment": "submitted to IEEE journal", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20011v1", "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments", "authors": ["Maciej \u015awiechowski", "Dominik \u015al\u0119zak"], "abstract": "Human-like agents are an increasingly important topic in games and beyond.\nBelievable non-player characters enhance the gaming experience by improving\nimmersion and providing entertainment. They also offer players the opportunity\nto engage with AI entities that can function as opponents, teachers, or\ncooperating partners. Additionally, in games where bots are prohibited -- and\neven more so in non-game environments -- there is a need for methods capable of\nidentifying whether digital interactions occur with bots or humans. This leads\nto two fundamental research questions: (1) how to model and implement\nhuman-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most\nsignificant challenges in implementing human-like AI in games (or any virtual\nenvironment featuring simulated agents, although this article specifically\nfocuses on games). Thirteen such challenges, both conceptual and technical, are\ndiscussed in detail. The second is an empirical study performed in a tactical\nvideo game that addresses the research question: \"Is it possible to distinguish\nhuman players from bots (AI agents) based on empirical data?\" A\nmachine-learning approach using a custom deep recurrent convolutional neural\nnetwork is presented. We hypothesize that the more challenging it is to create\nhuman-like AI for a given game, the easier it becomes to develop a method for\ndistinguishing humans from AI-driven players.", "categories": ["cs.AI", "cs.HC", "cs.MM", "68T01", "I.2; I.6.0; H.1.2"], "published": "2025-05-26 14:00:39", "updated": "2025-05-26 14:00:39", "pdf_url": "http://arxiv.org/pdf/2505.20011v1", "comment": "In proceedings of the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS-2025), pages 1996--2005, May 19-23,\n  Detroit, Michigan, USA", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20021v1", "title": "Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models", "authors": ["Hyunsik Chae", "Seungwoo Yoon", "Jaden Park", "Chloe Yewon Chun", "Yongin Cho", "Mu Cai", "Yong Jae Lee", "Ernest K. Ryu"], "abstract": "Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncomprehension and reasoning capabilities, yet they often struggle with\ntrivially simple visual tasks. In this work, we focus on the domain of basic 2D\nEuclidean geometry and systematically categorize the fundamental, indivisible\nvisual perception skills, which we refer to as atomic visual skills. We then\nintroduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the\natomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find\nthat they struggle with these tasks, despite being trivial for adult humans.\nOur findings highlight the need for purpose-built datasets to train and\nevaluate VLMs on atomic, rather than composite, visual perception tasks.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 14:09:24", "updated": "2025-05-26 14:09:24", "pdf_url": "http://arxiv.org/pdf/2505.20021v1", "comment": "69 pages, 16 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20024v1", "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving", "authors": ["Xueyi Liu", "Zuodong Zhong", "Yuxin Guo", "Yun-Fu Liu", "Zhiguo Su", "Qichao Zhang", "Junli Wang", "Yinfeng Gao", "Yupeng Zheng", "Qiao Lin", "Huiyong Chen", "Dongbin Zhao"], "abstract": "Due to the powerful vision-language reasoning and generalization abilities,\nmultimodal large language models (MLLMs) have garnered significant attention in\nthe field of end-to-end (E2E) autonomous driving. However, their application to\nclosed-loop systems remains underexplored, and current MLLM-based methods have\nnot shown clear superiority to mainstream E2E imitation learning approaches. In\nthis work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed\nfor closed-loop driving through holistic reasoning with a self-supervised Next\nScene Prediction task and supervised Decision Chain-of-Thought process. This\ndual mechanism encourages the model to align visual representations with\nactionable driving context, while promoting interpretable and causally grounded\ndecision making. We curate a planning-oriented decision reasoning dataset,\nnamely PDR, comprising 210k diverse and high-quality samples. Our method\noutperforms the mainstream E2E imitation learning method by a large margin of\n19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan\ndemonstrates strong zero-shot generalization on unseen DOS benchmark,\nhighlighting its adaptability in handling zero-shot corner cases. Code and\ndataset will be found in https://github.com/Liuxueyi/ReasonPlan.", "categories": ["cs.CV", "cs.AI", "cs.RO", "68T40(Primary), 68T45, 68T50(Secondary)", "I.2.9; I.2.10; I.5.1"], "published": "2025-05-26 14:12:38", "updated": "2025-05-26 14:12:38", "pdf_url": "http://arxiv.org/pdf/2505.20024v1", "comment": "18 pages; 9 figures; https://github.com/Liuxueyi/ReasonPlan", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20026v1", "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage", "authors": ["Xinping Chen", "Chen Liu"], "abstract": "We propose Gradient Inversion Transcript (GIT), a novel generative approach\nfor reconstructing training data from leaked gradients. GIT employs a\ngenerative attack model, whose architecture is tailored to align with the\nstructure of the leaked model based on theoretical analysis. Once trained\noffline, GIT can be deployed efficiently and only relies on the leaked\ngradients to reconstruct the input data, rendering it applicable under various\ndistributed learning environments. When used as a prior for other iterative\noptimization-based methods, GIT not only accelerates convergence but also\nenhances the overall reconstruction quality. GIT consistently outperforms\nexisting methods across multiple datasets and demonstrates strong robustness\nunder challenging conditions, including inaccurate gradients, data distribution\nshifts and discrepancies in model parameters.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 14:17:00", "updated": "2025-05-26 14:17:00", "pdf_url": "http://arxiv.org/pdf/2505.20026v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20027v1", "title": "Multi-modal brain encoding models for multi-modal stimuli", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Mounika Marreddy", "Maneesh Singh", "Manish Gupta", "Bapi S. Raju"], "abstract": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain.", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "published": "2025-05-26 14:17:08", "updated": "2025-05-26 14:17:08", "pdf_url": "http://arxiv.org/pdf/2505.20027v1", "comment": "26 pages, 15 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=0dELcFHig2", "doi": null, "journal_ref": "ICLR-2025, Sinapore"}
{"arxiv_id": "2505.20029v1", "title": "Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)", "authors": ["Subba Reddy Oota", "Akshett Jindal", "Ishani Mondal", "Khushbu Pahwa", "Satya Sai Srinath Namburi", "Manish Shrivastava", "Maneesh Singh", "Bapi S. Raju", "Manish Gupta"], "abstract": "Transformer-based language models, though not explicitly trained to mimic\nbrain recordings, have demonstrated surprising alignment with brain activity.\nProgress in these models-through increased size, instruction-tuning, and\nmultimodality-has led to better representational alignment with neural data.\nRecently, a new class of instruction-tuned multimodal LLMs (MLLMs) have\nemerged, showing remarkable zero-shot capabilities in open-ended multimodal\nvision tasks. However, it is unknown whether MLLMs, when prompted with natural\ninstructions, lead to better brain alignment and effectively capture\ninstruction-specific representations. To address this, we first investigate\nbrain alignment, i.e., measuring the degree of predictivity of neural visual\nactivity using text output response embeddings from MLLMs as participants\nengage in watching natural scenes. Experiments with 10 different instructions\nshow that MLLMs exhibit significantly better brain alignment than vision-only\nmodels and perform comparably to non-instruction-tuned multimodal models like\nCLIP. We also find that while these MLLMs are effective at generating\nhigh-quality responses suitable to the task-specific instructions, not all\ninstructions are relevant for brain alignment. Further, by varying\ninstructions, we make the MLLMs encode instruction-specific visual concepts\nrelated to the input image. This analysis shows that MLLMs effectively capture\ncount-related and recognition-related concepts, demonstrating strong alignment\nwith brain activity. Notably, the majority of the explained variance of the\nbrain encoding models is shared between MLLM embeddings of image captioning and\nother instructions. These results suggest that enhancing MLLMs' ability to\ncapture task-specific information could lead to better differentiation between\nvarious types of instructions, and thereby improving their precision in\npredicting brain responses.", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "published": "2025-05-26 14:18:15", "updated": "2025-05-26 14:18:15", "pdf_url": "http://arxiv.org/pdf/2505.20029v1", "comment": "30 pages, 22 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=xkgfLXZ4e0", "doi": null, "journal_ref": "ICLR-2025, Singapore"}
{"arxiv_id": "2505.20030v1", "title": "Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions", "authors": ["Wenbo Wei", "Nicholas Chong Jia Le", "Choy Heng Lai", "Ling Feng"], "abstract": "We observe a novel 'multiple-descent' phenomenon during the training process\nof LSTM, in which the test loss goes through long cycles of up and down trend\nmultiple times after the model is overtrained. By carrying out asymptotic\nstability analysis of the models, we found that the cycles in test loss are\nclosely associated with the phase transition process between order and chaos,\nand the local optimal epochs are consistently at the critical transition point\nbetween the two phases. More importantly, the global optimal epoch occurs at\nthe first transition from order to chaos, where the 'width' of the 'edge of\nchaos' is the widest, allowing the best exploration of better weight\nconfigurations for learning.", "categories": ["cs.LG", "cs.AI", "nlin.CD", "physics.comp-ph"], "published": "2025-05-26 14:18:22", "updated": "2025-05-26 14:18:22", "pdf_url": "http://arxiv.org/pdf/2505.20030v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20033v1", "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Maurice Kraus", "Felix Friedrich", "Huu Nguyen", "Krishna Kalyan", "Kourosh Nadi", "Kristian Kersting", "S\u00f6ren Auer"], "abstract": "Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuild Empathic Insight Face, a model achieving human-expert-level performance\non our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 14:19:58", "updated": "2025-05-26 14:19:58", "pdf_url": "http://arxiv.org/pdf/2505.20033v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20047v1", "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks", "authors": ["Debargha Ganguly", "Vikash Singh", "Sreehari Sankar", "Biyao Zhang", "Xuecen Zhang", "Srinivasan Iyengar", "Xiaotian Han", "Amit Sharma", "Shivkumar Kalyanaraman", "Vipin Chaudhary"], "abstract": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline.", "categories": ["cs.CL", "cs.AI", "cs.LO", "cs.SE"], "published": "2025-05-26 14:34:04", "updated": "2025-05-26 14:34:04", "pdf_url": "http://arxiv.org/pdf/2505.20047v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20053v1", "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "abstract": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "published": "2025-05-26 14:42:35", "updated": "2025-05-26 14:42:35", "pdf_url": "http://arxiv.org/pdf/2505.20053v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20063v1", "title": "SAEs Are Good for Steering -- If You Select the Right Features", "authors": ["Dana Arad", "Aaron Mueller", "Yonatan Belinkov"], "abstract": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 14:47:59", "updated": "2025-05-26 14:47:59", "pdf_url": "http://arxiv.org/pdf/2505.20063v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20065v1", "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety", "authors": ["Geon-Hyeong Kim", "Youngsoo Jang", "Yu Jin Kim", "Byoungjip Kim", "Honglak Lee", "Kyunghoon Bae", "Moontae Lee"], "abstract": "As Large Language Models (LLMs) continue to advance and find applications\nacross a growing number of fields, ensuring the safety of LLMs has become\nincreasingly critical. To address safety concerns, recent studies have proposed\nintegrating safety constraints into Reinforcement Learning from Human Feedback\n(RLHF). However, these approaches tend to be complex, as they encompass\ncomplicated procedures in RLHF along with additional steps required by the\nsafety constraints. Inspired by Direct Preference Optimization (DPO), we\nintroduce a new algorithm called SafeDPO, which is designed to directly\noptimize the safety alignment objective in a single stage of policy learning,\nwithout requiring relaxation. SafeDPO introduces only one additional\nhyperparameter to further enhance safety and requires only minor modifications\nto standard DPO. As a result, it eliminates the need to fit separate reward and\ncost models or to sample from the language model during fine-tuning, while\nstill enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO\nachieves competitive performance compared to state-of-the-art safety alignment\nalgorithms, both in terms of aligning with human preferences and improving\nsafety.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 14:50:01", "updated": "2025-05-26 14:50:01", "pdf_url": "http://arxiv.org/pdf/2505.20065v1", "comment": "34 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20066v1", "title": "Automated data curation for self-supervised learning in underwater acoustic analysis", "authors": ["Hilde I Hummel", "Sandjai Bhulai", "Burooj Ghani", "Rob van der Mei"], "abstract": "The sustainability of the ocean ecosystem is threatened by increased levels\nof sound pollution, making monitoring crucial to understand its variability and\nimpact. Passive acoustic monitoring (PAM) systems collect a large amount of\nunderwater sound recordings, but the large volume of data makes manual analysis\nimpossible, creating the need for automation. Although machine learning offers\na potential solution, most underwater acoustic recordings are unlabeled.\nSelf-supervised learning models have demonstrated success in learning from\nlarge-scale unlabeled data in various domains like computer vision, Natural\nLanguage Processing, and audio. However, these models require large, diverse,\nand balanced datasets for training in order to generalize well. To address\nthis, a fully automated self-supervised data curation pipeline is proposed to\ncreate a diverse and balanced dataset from raw PAM data. It integrates\nAutomatic Identification System (AIS) data with recordings from various\nhydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw\naudio data is sampled and then combined with AIS samples to create a balanced\nand diverse dataset. The resulting curated dataset enables the development of\nself-supervised learning models, facilitating various tasks such as monitoring\nmarine mammals and assessing sound pollution.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-26 14:50:04", "updated": "2025-05-26 14:50:04", "pdf_url": "http://arxiv.org/pdf/2505.20066v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20067v1", "title": "Community Moderation and the New Epistemology of Fact Checking on Social Media", "authors": ["Isabelle Augenstein", "Michiel Bakker", "Tanmoy Chakraborty", "David Corney", "Emilio Ferrara", "Iryna Gurevych", "Scott Hale", "Eduard Hovy", "Heng Ji", "Irene Larraz", "Filippo Menczer", "Preslav Nakov", "Paolo Papotti", "Dhruv Sahnan", "Greta Warren", "Giovanni Zagni"], "abstract": "Social media platforms have traditionally relied on internal moderation teams\nand partnerships with independent fact-checking organizations to identify and\nflag misleading content. Recently, however, platforms including X (formerly\nTwitter) and Meta have shifted towards community-driven content moderation by\nlaunching their own versions of crowd-sourced fact-checking -- Community Notes.\nIf effectively scaled and governed, such crowd-checking initiatives have the\npotential to combat misinformation with increased scale and speed as\nsuccessfully as community-driven efforts once did with spam. Nevertheless,\ngeneral content moderation, especially for misinformation, is inherently more\ncomplex. Public perceptions of truth are often shaped by personal biases,\npolitical leanings, and cultural contexts, complicating consensus on what\nconstitutes misleading content. This suggests that community efforts, while\nvaluable, cannot replace the indispensable role of professional fact-checkers.\nHere we systemically examine the current approaches to misinformation detection\nacross major platforms, explore the emerging role of community-driven\nmoderation, and critically evaluate both the promises and challenges of\ncrowd-checking at scale.", "categories": ["cs.SI", "cs.AI", "cs.CY"], "published": "2025-05-26 14:50:18", "updated": "2025-05-26 14:50:18", "pdf_url": "http://arxiv.org/pdf/2505.20067v1", "comment": "1 Figure, 2 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20068v1", "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction", "authors": ["Qingyu Liang", "Jaime Banks"], "abstract": "Shared understanding plays a key role in the effective communication in and\nperformance of human-human interactions. With the increasingly common\nintegration of AI into human contexts, the future of personal and workplace\ninteractions will likely see human-AI interaction (HAII) in which the\nperception of shared understanding is important. Existing literature has\naddressed the processes and effects of PSU in human-human interactions, but the\nconstrual remains underexplored in HAII. To better understand PSU in HAII, we\nconducted an online survey to collect user reflections on interactions with a\nlarge language model when it sunderstanding of a situation was thought to be\nsimilar to or different from the participant's. Through inductive thematic\nanalysis, we identified eight dimensions comprising PSU in human-AI\ninteractions: Fluency, aligned operation, fluidity, outcome satisfaction,\ncontextual awareness, lack of humanlike abilities, computational limits, and\nsuspicion.", "categories": ["cs.HC", "cs.AI"], "published": "2025-05-26 14:50:40", "updated": "2025-05-26 14:50:40", "pdf_url": "http://arxiv.org/pdf/2505.20068v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20072v1", "title": "Incentivizing Reasoning from Weak Supervision", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "abstract": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 14:51:29", "updated": "2025-05-26 14:51:29", "pdf_url": "http://arxiv.org/pdf/2505.20072v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20075v1", "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback", "authors": ["Mengdi Li", "Jiaye Lin", "Xufeng Zhao", "Wenhao Lu", "Peilin Zhao", "Stefan Wermter", "Di Wang"], "abstract": "Reward models trained with conventional Reinforcement Learning from AI\nFeedback (RLAIF) methods suffer from limited generalizability, which hinders\nthe alignment performance of the policy model during reinforcement learning\n(RL). This challenge stems from various issues, including distribution shift,\npreference label noise, and mismatches between overly challenging samples and\nmodel capacity. In this paper, we attempt to enhance the generalizability of\nreward models through a data-centric approach, driven by the insight that these\nissues are inherently intertwined from the perspective of data difficulty. To\naddress this, we propose a novel framework, $\\textit{Curriculum-RLAIF}$, which\nconstructs preference pairs with varying difficulty levels and produces a\ncurriculum that progressively incorporates preference pairs of increasing\ndifficulty for reward model training. Our experimental results suggest that\nreward models trained with Curriculum-RLAIF achieve improved generalizability,\nsignificantly increasing the alignment performance of the policy model by a\nlarge margin without incurring additional inference costs compared to various\nnon-curriculum baselines. Detailed analysis and comparisons with alternative\napproaches, including data selection via external pretrained reward models or\ninternal self-selection mechanisms, as well as other curriculum strategies,\nfurther demonstrate the superiority of our approach in terms of simplicity,\nefficiency, and effectiveness.", "categories": ["cs.AI"], "published": "2025-05-26 14:53:08", "updated": "2025-05-26 14:53:08", "pdf_url": "http://arxiv.org/pdf/2505.20075v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20081v1", "title": "Inference-time Alignment in Continuous Space", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "abstract": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 14:58:33", "updated": "2025-05-26 14:58:33", "pdf_url": "http://arxiv.org/pdf/2505.20081v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20085v1", "title": "Explanation User Interfaces: A Systematic Literature Review", "authors": ["Eleonora Cappuccio", "Andrea Esposito", "Francesco Greco", "Giuseppe Desolda", "Rosa Lanzilotti", "Salvatore Rinzivillo"], "abstract": "Artificial Intelligence (AI) is one of the major technological advancements\nof this century, bearing incredible potential for users through AI-powered\napplications and tools in numerous domains. Being often black-box (i.e., its\ndecision-making process is unintelligible), developers typically resort to\neXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour\nof AI models to produce systems that are transparent, fair, reliable, and\ntrustworthy. However, presenting explanations to the user is not trivial and is\noften left as a secondary aspect of the system's design process, leading to AI\nsystems that are not useful to end-users. This paper presents a Systematic\nLiterature Review on Explanation User Interfaces (XUIs) to gain a deeper\nunderstanding of the solutions and design guidelines employed in the academic\nliterature to effectively present explanations to users. To improve the\ncontribution and real-world impact of this survey, we also present a framework\nfor Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide\npractitioners and academics in the design and evaluation of XUIs.", "categories": ["cs.HC", "cs.AI", "A.1"], "published": "2025-05-26 15:00:17", "updated": "2025-05-26 15:00:17", "pdf_url": "http://arxiv.org/pdf/2505.20085v1", "comment": "First version", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20087v1", "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models", "authors": ["Makesh Narsimhan Sreedhar", "Traian Rebedea", "Christopher Parisien"], "abstract": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 15:01:37", "updated": "2025-05-26 15:01:37", "pdf_url": "http://arxiv.org/pdf/2505.20087v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20089v1", "title": "Homophily Enhanced Graph Domain Adaptation", "authors": ["Ruiyi Fang", "Bingheng Li", "Jingyu Zhao", "Ruizhi Pu", "Qiuhao Zeng", "Gezheng Xu", "Charles Ling", "Boyu Wang"], "abstract": "Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs\nto unlabeled target graphs, addressing the challenge of label scarcity. In this\npaper, we highlight the significance of graph homophily, a pivotal factor for\ngraph domain alignment, which, however, has long been overlooked in existing\napproaches. Specifically, our analysis first reveals that homophily\ndiscrepancies exist in benchmarks. Moreover, we also show that homophily\ndiscrepancies degrade GDA performance from both empirical and theoretical\naspects, which further underscores the importance of homophily alignment in\nGDA. Inspired by this finding, we propose a novel homophily alignment algorithm\nthat employs mixed filters to smooth graph signals, thereby effectively\ncapturing and mitigating homophily discrepancies between graphs. Experimental\nresults on a variety of benchmarks verify the effectiveness of our method.", "categories": ["cs.SI", "cs.AI"], "published": "2025-05-26 15:02:08", "updated": "2025-05-26 15:02:08", "pdf_url": "http://arxiv.org/pdf/2505.20089v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20094v1", "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale", "authors": ["Qi Li", "Kun Li", "Haozhi Han", "Honghui Shang", "Xinfu He", "Yunquan Zhang", "Hong An", "Ting Cao", "Mao Yang"], "abstract": "Can a scientific simulation system be physically consistent, interpretable by\ndesign, and scalable across regimes--all at once? Despite decades of progress,\nthis trifecta remains elusive. Classical methods like Kinetic Monte Carlo\nensure thermodynamic accuracy but scale poorly; learning-based methods offer\nefficiency but often sacrifice physical consistency and interpretability. We\npresent SwarmThinkers, a reinforcement learning framework that recasts\natomic-scale simulation as a physically grounded swarm intelligence system.\nEach diffusing particle is modeled as a local decision-making agent that\nselects transitions via a shared policy network trained under thermodynamic\nconstraints. A reweighting mechanism fuses learned preferences with transition\nrates, preserving statistical fidelity while enabling interpretable, step-wise\ndecision making. Training follows a centralized-training,\ndecentralized-execution paradigm, allowing the policy to generalize across\nsystem sizes, concentrations, and temperatures without retraining. On a\nbenchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers\nis the first system to achieve full-scale, physically consistent simulation on\na single A100 GPU, previously attainable only via OpenKMC on a supercomputer.\nIt delivers up to 4963x (3185x on average) faster computation with 485x lower\nmemory usage. By treating particles as decision-makers, not passive samplers,\nSwarmThinkers marks a paradigm shift in scientific simulation--one that unifies\nphysical consistency, interpretability, and scalability through agent-driven\nintelligence.", "categories": ["cs.AI"], "published": "2025-05-26 15:04:37", "updated": "2025-05-26 15:04:37", "pdf_url": "http://arxiv.org/pdf/2505.20094v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20096v1", "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "abstract": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 15:05:18", "updated": "2025-05-26 15:05:18", "pdf_url": "http://arxiv.org/pdf/2505.20096v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20099v1", "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities", "authors": ["Chuangtao Ma", "Yongrui Chen", "Tianxing Wu", "Arijit Khan", "Haofen Wang"], "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-26 15:08:23", "updated": "2025-05-26 15:08:23", "pdf_url": "http://arxiv.org/pdf/2505.20099v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20100v1", "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models", "authors": ["Fengyuan Sun", "Leqi Shen", "Hui Chen", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "abstract": "Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-26 15:08:37", "updated": "2025-05-26 15:08:37", "pdf_url": "http://arxiv.org/pdf/2505.20100v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20109v1", "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models", "authors": ["June-Woo Kim", "Wonkyo Oh", "Haram Yoon", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "abstract": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 15:12:10", "updated": "2025-05-26 15:12:10", "pdf_url": "http://arxiv.org/pdf/2505.20109v1", "comment": "Accepted to InterSpeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20110v1", "title": "Proxy-Free GFlowNet", "authors": ["Ruishuo Chen", "Xun Wang", "Rui Hu", "Zhuoran Li", "Longbo Huang"], "abstract": "Generative Flow Networks (GFlowNets) are a promising class of generative\nmodels designed to sample diverse, high-reward structures by modeling\ndistributions over compositional objects. In many real-world applications,\nobtaining the reward function for such objects is expensive, time-consuming, or\nrequires human input, making it necessary to train GFlowNets from historical\ndatasets. Most existing methods adopt a model-based approach, learning a proxy\nmodel from the dataset to approximate the reward function. However, this\nstrategy inherently ties the quality of the learned policy to the accuracy of\nthe proxy, introducing additional complexity and uncertainty into the training\nprocess. To overcome these limitations, we propose \\textbf{Trajectory-Distilled\nGFlowNet (TD-GFN)}, a \\emph{proxy-free} training framework that eliminates the\nneed for out-of-dataset reward queries. Our method is motivated by the key\nobservation that different edges in the associated directed acyclic graph (DAG)\ncontribute unequally to effective policy learning. TD-GFN leverages inverse\nreinforcement learning to estimate edge-level rewards from the offline dataset,\nwhich are then used to ingeniously prune the DAG and guide backward trajectory\nsampling during training. This approach directs the policy toward high-reward\nregions while reducing the complexity of model fitting. Empirical results\nacross multiple tasks show that TD-GFN trains both efficiently and reliably,\nsignificantly outperforming existing baselines in convergence speed and sample\nquality.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 15:12:22", "updated": "2025-05-26 15:12:22", "pdf_url": "http://arxiv.org/pdf/2505.20110v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20112v1", "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 15:14:54", "updated": "2025-05-26 15:14:54", "pdf_url": "http://arxiv.org/pdf/2505.20112v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20113v1", "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", "authors": ["Cristian Santini", "Laura Melosi", "Emanuele Frontoni"], "abstract": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 15:16:48", "updated": "2025-05-26 15:16:48", "pdf_url": "http://arxiv.org/pdf/2505.20113v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20119v1", "title": "Spatiotemporal Causal Decoupling Model for Air Quality Forecasting", "authors": ["Jiaming Ma", "Guanjun Wang", "Sheng Huang", "Kuo Yang", "Binwu Wang", "Pengkun Wang", "Yang Wang"], "abstract": "Due to the profound impact of air pollution on human health, livelihoods, and\neconomic development, air quality forecasting is of paramount significance.\nInitially, we employ the causal graph method to scrutinize the constraints of\nexisting research in comprehensively modeling the causal relationships between\nthe air quality index (AQI) and meteorological features. In order to enhance\nprediction accuracy, we introduce a novel air quality forecasting model,\nAirCade, which incorporates a causal decoupling approach. AirCade leverages a\nspatiotemporal module in conjunction with knowledge embedding techniques to\ncapture the internal dynamics of AQI. Subsequently, a causal decoupling module\nis proposed to disentangle synchronous causality from past AQI and\nmeteorological features, followed by the dissemination of acquired knowledge to\nfuture time steps to enhance performance. Additionally, we introduce a causal\nintervention mechanism to explicitly represent the uncertainty of future\nmeteorological features, thereby bolstering the model's robustness. Our\nevaluation of AirCade on an open-source air quality dataset demonstrates over\n20\\% relative improvement over state-of-the-art models.", "categories": ["cs.AI"], "published": "2025-05-26 15:21:57", "updated": "2025-05-26 15:21:57", "pdf_url": "http://arxiv.org/pdf/2505.20119v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20120v1", "title": "Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets", "authors": ["Simpson Zhang", "Tennison Liu", "Mihaela van der Schaar"], "abstract": "Current labor markets are strongly affected by the economic forces of adverse\nselection, moral hazard, and reputation, each of which arises due to\n$\\textit{incomplete information}$. These economic forces will still be\ninfluential after AI agents are introduced, and thus, agents must use\nmetacognitive and strategic reasoning to perform effectively. Metacognition is\na form of $\\textit{internal reasoning}$ that includes the capabilities for\nself-assessment, task understanding, and evaluation of strategies. Strategic\nreasoning is $\\textit{external reasoning}$ that covers holding beliefs about\nother participants in the labor market (e.g., competitors, colleagues), making\nstrategic decisions, and learning about others over time. Both types of\nreasoning are required by agents as they decide among the many\n$\\textit{actions}$ they can take in labor markets, both within and outside\ntheir jobs. We discuss current research into metacognitive and strategic\nreasoning and the areas requiring further development.", "categories": ["cs.AI"], "published": "2025-05-26 15:22:04", "updated": "2025-05-26 15:22:04", "pdf_url": "http://arxiv.org/pdf/2505.20120v1", "comment": "*Zhang & Liu contributed equally", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20127v1", "title": "Agentic AI Process Observability: Discovering Behavioral Variability", "authors": ["Fabiana Fournier", "Lior Limonad", "Yuval David"], "abstract": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.", "categories": ["cs.AI"], "published": "2025-05-26 15:26:07", "updated": "2025-05-26 15:26:07", "pdf_url": "http://arxiv.org/pdf/2505.20127v1", "comment": "12 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20132v1", "title": "Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks", "authors": ["Safa Hamreras", "Sukhbinder Singh", "Rom\u00e1n Or\u00fas"], "abstract": "Tensorizing a neural network involves reshaping some or all of its dense\nweight matrices into higher-order tensors and approximating them using low-rank\ntensor network decompositions. This technique has shown promise as a model\ncompression strategy for large-scale neural networks. However, despite\nencouraging empirical results, tensorized neural networks (TNNs) remain\nunderutilized in mainstream deep learning. In this position paper, we offer a\nperspective on both the potential and current limitations of TNNs. We argue\nthat TNNs represent a powerful yet underexplored framework for deep\nlearning--one that deserves greater attention from both engineering and\ntheoretical communities. Beyond compression, we highlight the value of TNNs as\na flexible class of architectures with distinctive scaling properties and\nincreased interpretability. A central feature of TNNs is the presence of bond\nindices, which introduce new latent spaces not found in conventional networks.\nThese internal representations may provide deeper insight into the evolution of\nfeatures across layers, potentially advancing the goals of mechanistic\ninterpretability. We conclude by outlining several key research directions\naimed at overcoming the practical barriers to scaling and adopting TNNs in\nmodern deep learning workflows.", "categories": ["cs.LG", "cs.AI", "quant-ph"], "published": "2025-05-26 15:32:28", "updated": "2025-05-26 15:32:28", "pdf_url": "http://arxiv.org/pdf/2505.20132v1", "comment": "This article has been prepared for submission as a \"Position paper\"\n  following the guidelines provided at\n  https://neurips.cc/Conferences/2025/CallForPositionPapers", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20137v1", "title": "Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks", "authors": ["C\u00e9dric Goemaere", "Gaspard Oliviers", "Rafal Bogacz", "Thomas Demeester"], "abstract": "Predictive Coding (PC) offers a biologically plausible alternative to\nbackpropagation for neural network training, yet struggles with deeper\narchitectures. This paper identifies the root cause: an inherent signal decay\nproblem where gradients attenuate exponentially with depth, becoming\ncomputationally negligible due to numerical precision constraints. To address\nthis fundamental limitation, we introduce Error Optimization (EO), a novel\nreparameterization that preserves PC's theoretical properties while eliminating\nsignal decay. By optimizing over prediction errors rather than states, EO\nenables signals to reach all layers simultaneously and without attenuation,\nconverging orders of magnitude faster than standard PC. Experiments across\nmultiple architectures and datasets demonstrate that EO matches\nbackpropagation's performance even for deeper models where conventional PC\nstruggles. Besides practical improvements, our work provides theoretical\ninsight into PC dynamics and establishes a foundation for scaling\nbiologically-inspired learning to deeper architectures on digital hardware and\nbeyond.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 15:39:16", "updated": "2025-05-26 15:39:16", "pdf_url": "http://arxiv.org/pdf/2505.20137v1", "comment": "All code available at\n  https://github.com/cgoemaere/pc_error_optimization", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20139v1", "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs", "authors": ["Jialin Yang", "Dongfu Jiang", "Lipeng He", "Sherman Siu", "Yuxuan Zhang", "Disen Liao", "Zhuofeng Li", "Huaye Zeng", "Yiming Jia", "Haozhe Wang", "Benjamin Schneider", "Chi Ruan", "Wentao Ma", "Zhiheng Lyu", "Yifei Wang", "Yi Lu", "Quy Duc Do", "Ziyan Jiang", "Ping Nie", "Wenhu Chen"], "abstract": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.", "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2025-05-26 15:40:42", "updated": "2025-05-26 15:40:42", "pdf_url": "http://arxiv.org/pdf/2505.20139v1", "comment": "16 pages, 9 figures, 13 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "abstract": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "categories": ["cs.AI"], "published": "2025-05-26 15:48:14", "updated": "2025-05-26 15:48:14", "pdf_url": "http://arxiv.org/pdf/2505.20148v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20149v1", "title": "Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases", "authors": ["Cheng-Yu Tai", "Ching-Wen Chen", "Chi-Chin Wu", "Bo-Chen Chiu", "Cheng-Hung", "Lin", "Cheng-Kai Lu", "Jia-Kang Wang", "Tzu-Lun Huang"], "abstract": "This paper focuses on using few-shot learning to improve the accuracy of\nclassifying OCT diagnosis images with major and rare classes. We used the\nGAN-based augmentation strategy as a baseline and introduced several novel\nmethods to further enhance our model. The proposed strategy contains U-GAT-IT\nfor improving the generative part and uses the data balance technique to narrow\ndown the skew of accuracy between all categories. The best model obtained was\nbuilt with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an\noverall accuracy of 97.85%, representing a significant improvement over the\noriginal baseline.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-26 15:49:44", "updated": "2025-05-26 15:49:44", "pdf_url": "http://arxiv.org/pdf/2505.20149v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20150v1", "title": "On the (Non) Injectivity of Piecewise Linear Janossy Pooling", "authors": ["Ilai Reshef", "Nadav Dym"], "abstract": "Multiset functions, which are functions that map multisets to vectors, are a\nfundamental tool in the construction of neural networks for multisets and\ngraphs. To guarantee that the vector representation of the multiset is\nfaithful, it is often desirable to have multiset mappings that are both\ninjective and bi-Lipschitz. Currently, there are several constructions of\nmultiset functions achieving both these guarantees, leading to improved\nperformance in some tasks but often also to higher compute time than standard\nconstructions. Accordingly, it is natural to inquire whether simpler multiset\nfunctions achieving the same guarantees are available. In this paper, we make a\nlarge step towards giving a negative answer to this question. We consider the\nfamily of k-ary Janossy pooling, which includes many of the most popular\nmultiset models, and prove that no piecewise linear Janossy pooling function\ncan be injective. On the positive side, we show that when restricted to\nmultisets without multiplicities, even simple deep-sets models suffice for\ninjectivity and bi-Lipschitzness.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-26 15:53:09", "updated": "2025-05-26 15:53:09", "pdf_url": "http://arxiv.org/pdf/2505.20150v1", "comment": "14 pages, 2 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20152v1", "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "abstract": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-26 15:55:28", "updated": "2025-05-26 15:55:28", "pdf_url": "http://arxiv.org/pdf/2505.20152v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19376v1", "title": "Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality", "authors": ["Lance Ying", "Almog Hillel", "Ryan Truong", "Vikash K. Mansinghka", "Joshua B. Tenenbaum", "Tan Zhi-Xuan"], "abstract": "A key feature of human theory-of-mind is the ability to attribute beliefs to\nother agents as mentalistic explanations for their behavior. But given the wide\nvariety of beliefs that agents may hold about the world and the rich language\nwe can use to express them, which specific beliefs are people inclined to\nattribute to others? In this paper, we investigate the hypothesis that people\nprefer to attribute beliefs that are good explanations for the behavior they\nobserve. We develop a computational model that quantifies the explanatory\nstrength of a (natural language) statement about an agent's beliefs via three\nfactors: accuracy, informativity, and causal relevance to actions, each of\nwhich can be computed from a probabilistic generative model of belief-driven\nbehavior. Using this model, we study the role of each factor in how people\nselectively attribute beliefs to other agents. We investigate this via an\nexperiment where participants watch an agent collect keys hidden in boxes in\norder to reach a goal, then rank a set of statements describing the agent's\nbeliefs about the boxes' contents. We find that accuracy and informativity\nperform reasonably well at predicting these rankings when combined, but that\ncausal relevance is the single factor that best explains participants'\nresponses.", "categories": ["cs.CL"], "published": "2025-05-26 00:21:38", "updated": "2025-05-26 00:21:38", "pdf_url": "http://arxiv.org/pdf/2505.19376v1", "comment": "8 pages, 3 figures; oral presentation at CogSci 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19384v1", "title": "GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor", "authors": ["Seokgi Lee", "Jungjun Kim"], "abstract": "We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder\nthat gradually encodes speaking styles from an acoustic reference for zero-shot\nspeech synthesis. GSA first captures the local style of each semantic sound\nunit. Then the local styles are combined by self-attention to obtain a global\nstyle condition. This semantic and hierarchical encoding strategy provides a\nrobust and rich style representation for an acoustic model. We test GSA-TTS on\nunseen speakers and obtain promising results regarding naturalness, speaker\nsimilarity, and intelligibility. Additionally, we explore the potential of GSA\nin terms of interpretability and controllability, which stems from its\nhierarchical structure.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-26 00:58:16", "updated": "2025-05-26 00:58:16", "pdf_url": "http://arxiv.org/pdf/2505.19384v1", "comment": "7 pages, 3 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19388v1", "title": "gec-metrics: A Unified Library for Grammatical Error Correction Evaluation", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "abstract": "We introduce gec-metrics, a library for using and developing grammatical\nerror correction (GEC) evaluation metrics through a unified interface. Our\nlibrary enables fair system comparisons by ensuring that everyone conducts\nevaluations using a consistent implementation. Moreover, it is designed with a\nstrong focus on API usage, making it highly extensible. It also includes\nmeta-evaluation functionalities and provides analysis and visualization\nscripts, contributing to developing GEC evaluation metrics. Our code is\nreleased under the MIT license and is also distributed as an installable\npackage. The video is available on YouTube.", "categories": ["cs.CL"], "published": "2025-05-26 01:10:16", "updated": "2025-05-26 01:10:16", "pdf_url": "http://arxiv.org/pdf/2505.19388v1", "comment": "Accepted at ACL 2025 System Demonstration Track, 11 pages, 9 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19392v1", "title": "Simple and Effective Baselines for Code Summarisation Evaluation", "authors": ["Jade Robinson", "Jonathan K. Kummerfeld"], "abstract": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias.", "categories": ["cs.CL", "cs.AI", "cs.SE", "68T50", "I.2.7"], "published": "2025-05-26 01:16:41", "updated": "2025-05-26 01:16:41", "pdf_url": "http://arxiv.org/pdf/2505.19392v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19405v1", "title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems", "authors": ["Yan Wen", "Junfeng Guo", "Heng Huang"], "abstract": "As large language models (LLMs) evolve into autonomous agents capable of\ncollaborative reasoning and task execution, multi-agent LLM systems have\nemerged as a powerful paradigm for solving complex problems. However, these\nsystems pose new challenges for copyright protection, particularly when\nsensitive or copyrighted content is inadvertently recalled through inter-agent\ncommunication and reasoning. Existing protection techniques primarily focus on\ndetecting content in final outputs, overlooking the richer, more revealing\nreasoning processes within the agents themselves. In this paper, we introduce\nCoTGuard, a novel framework for copyright protection that leverages\ntrigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,\nwe can activate specific CoT segments and monitor intermediate reasoning steps\nfor unauthorized content reproduction by embedding specific trigger queries\ninto agent prompts. This approach enables fine-grained, interpretable detection\nof copyright violations in collaborative agent scenarios. We evaluate CoTGuard\non various benchmarks in extensive experiments and show that it effectively\nuncovers content leakage with minimal interference to task performance. Our\nfindings suggest that reasoning-level monitoring offers a promising direction\nfor safeguarding intellectual property in LLM-based agent systems.", "categories": ["cs.CL", "cs.CR"], "published": "2025-05-26 01:42:37", "updated": "2025-05-26 01:42:37", "pdf_url": "http://arxiv.org/pdf/2505.19405v1", "comment": "18 pages, 1 figure", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19410v1", "title": "Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering", "authors": ["Jiajun Zhu", "Ye Liu", "Meikai Bao", "Kai Zhang", "Yanghai Zhang", "Qi Liu"], "abstract": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks, yet they remain prone to\nhallucinations when reasoning with insufficient internal knowledge. While\nintegrating LLMs with knowledge graphs (KGs) provides access to structured,\nverifiable information, existing approaches often generate incomplete or\nfactually inconsistent reasoning paths. To this end, we propose Self-Reflective\nPlanning (SRP), a framework that synergizes LLMs with KGs through iterative,\nreference-guided reasoning. Specifically, given a question and topic entities,\nSRP first searches for references to guide planning and reflection. In the\nplanning process, it checks initial relations and generates a reasoning path.\nAfter retrieving knowledge from KGs through a reasoning path, it implements\niterative reflection by judging the retrieval result and editing the reasoning\npath until the answer is correctly retrieved. Extensive experiments on three\npublic datasets demonstrate that SRP surpasses various strong baselines and\nfurther underscore its reliable reasoning ability.", "categories": ["cs.CL"], "published": "2025-05-26 01:59:00", "updated": "2025-05-26 01:59:00", "pdf_url": "http://arxiv.org/pdf/2505.19410v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19426v1", "title": "The Role of Diversity in In-Context Learning for Large Language Models", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "abstract": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 02:37:26", "updated": "2025-05-26 02:37:26", "pdf_url": "http://arxiv.org/pdf/2505.19426v1", "comment": "30 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19428v1", "title": "Frictional Agent Alignment Framework: Slow Down and Don't Break Things", "authors": ["Abhijnan Nath", "Carine Graff", "Andrei Bachinin", "Nikhil Krishnaswamy"], "abstract": "AI support of collaborative interactions entails mediating potential\nmisalignment between interlocutor beliefs. Common preference alignment methods\nlike DPO excel in static settings, but struggle in dynamic collaborative tasks\nwhere the explicit signals of interlocutor beliefs are sparse and skewed. We\npropose the Frictional Agent Alignment Framework (FAAF), to generate precise,\ncontext-aware \"friction\" that prompts for deliberation and re-examination of\nexisting evidence. FAAF's two-player objective decouples from data skew: a\nfrictive-state policy identifies belief misalignments, while an intervention\npolicy crafts collaborator-preferred responses. We derive an analytical\nsolution to this objective, enabling training a single policy via a simple\nsupervised loss. Experiments on three benchmarks show FAAF outperforms\ncompetitors in producing concise, interpretable friction and in OOD\ngeneralization. By aligning LLMs to act as adaptive \"thought partners\" -- not\npassive responders -- FAAF advances scalable, dynamic human-AI collaboration.\nOur code and data can be found at https://github.com/csu-signal/FAAF_ACL.", "categories": ["cs.CL"], "published": "2025-05-26 02:39:07", "updated": "2025-05-26 02:39:07", "pdf_url": "http://arxiv.org/pdf/2505.19428v1", "comment": "48 pages (main paper: 10 pages incl. Limitations and Acknowledgments;\n  references: 6 pages; appendix: 32 pages), 9 figures, 12 tables, appearing in\n  Proceedings of ACL 2025, Vienna, Austria", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19429v1", "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts", "authors": ["Younghan Park", "Anuj Diwan", "David Harwath", "Eunsol Choi"], "abstract": "Podcasts have become daily companions for half a billion users. Given the\nenormous amount of podcast content available, highlights provide a valuable\nsignal that helps viewers get the gist of an episode and decide if they want to\ninvest in listening to it in its entirety. However, identifying highlights\nautomatically is challenging due to the unstructured and long-form nature of\nthe content. We introduce Rhapsody, a dataset of 13K podcast episodes paired\nwith segment-level highlight scores derived from YouTube's 'most replayed'\nfeature. We frame the podcast highlight detection as a segment-level binary\nclassification task. We explore various baseline approaches, including\nzero-shot prompting of language models and lightweight finetuned language\nmodels using segment-level classification heads. Our experimental results\nindicate that even state-of-the-art language models like GPT-4o and Gemini\nstruggle with this task, while models finetuned with in-domain data\nsignificantly outperform their zero-shot performance. The finetuned model\nbenefits from leveraging both speech signal features and transcripts. These\nfindings highlight the challenges for fine-grained information access in\nlong-form spoken media.", "categories": ["cs.CL"], "published": "2025-05-26 02:39:34", "updated": "2025-05-26 02:39:34", "pdf_url": "http://arxiv.org/pdf/2505.19429v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19430v1", "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation", "authors": ["Keane Ong", "Rui Mao", "Deeksha Varshney", "Paul Pu Liang", "Erik Cambria", "Gianmarco Mengaldo"], "abstract": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 02:41:50", "updated": "2025-05-26 02:41:50", "pdf_url": "http://arxiv.org/pdf/2505.19430v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19435v1", "title": "Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection", "authors": ["Zhihong Pan", "Kai Zhang", "Yuze Zhao", "Yupeng Han"], "abstract": "The inherent capabilities of a language model (LM) and the reasoning\nstrategies it employs jointly determine its performance in reasoning tasks.\nWhile test-time scaling is regarded as an effective approach to tackling\ncomplex reasoning tasks, it incurs substantial computational costs and often\nleads to \"overthinking\", where models become trapped in \"thought pitfalls\". To\naddress this challenge, we propose Route-To-Reason (RTR), a novel unified\nrouting framework that dynamically allocates both LMs and reasoning strategies\naccording to task difficulty under budget constraints. RTR learns compressed\nrepresentations of both expert models and reasoning strategies, enabling their\njoint and adaptive selection at inference time. This method is low-cost, highly\nflexible, and can be seamlessly extended to arbitrary black-box or white-box\nmodels and strategies, achieving true plug-and-play functionality. Extensive\nexperiments across seven open source models and four reasoning strategies\ndemonstrate that RTR achieves an optimal trade-off between accuracy and\ncomputational efficiency among all baselines, achieving higher accuracy than\nthe best single model while reducing token usage by over 60%.", "categories": ["cs.CL"], "published": "2025-05-26 02:53:17", "updated": "2025-05-26 02:53:17", "pdf_url": "http://arxiv.org/pdf/2505.19435v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19436v1", "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "authors": ["Ye Ye"], "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings.", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.8; H.3.3"], "published": "2025-05-26 02:53:22", "updated": "2025-05-26 02:53:22", "pdf_url": "http://arxiv.org/pdf/2505.19436v1", "comment": "Under review. 9 pages main content, 15 pages appendix, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19439v1", "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "abstract": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses.", "categories": ["cs.CL"], "published": "2025-05-26 02:56:22", "updated": "2025-05-26 02:56:22", "pdf_url": "http://arxiv.org/pdf/2505.19439v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19440v1", "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models", "authors": ["Shashata Sawmya", "Micah Adler", "Nir Shavit"], "abstract": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-26 02:59:54", "updated": "2025-05-26 02:59:54", "pdf_url": "http://arxiv.org/pdf/2505.19440v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19443v1", "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "abstract": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle.", "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2025-05-26 03:00:21", "updated": "2025-05-26 03:00:21", "pdf_url": "http://arxiv.org/pdf/2505.19443v1", "comment": "35 Pages, 8 Figures, 6 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19457v1", "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs", "authors": ["Guilong Lu", "Xuntao Guo", "Rongjunchen Zhang", "Wenqiao Zhu", "Ji Liu"], "abstract": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.", "categories": ["cs.AI", "cs.CE", "cs.CL"], "published": "2025-05-26 03:23:02", "updated": "2025-05-26 03:23:02", "pdf_url": "http://arxiv.org/pdf/2505.19457v1", "comment": "Project Page: https://hithink-research.github.io/BizFinBench/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19472v1", "title": "Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks", "authors": ["Mohammad Mahdi Moradi", "Walid Ahmed", "Shuangyue Wen", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu"], "abstract": "Attention and State-Space Models (SSMs) when combined in a hybrid network in\nsequence or in parallel provide complementary strengths. In a hybrid sequential\npipeline they alternate between applying a transformer to the input and then\nfeeding its output into a SSM. This results in idle periods in the individual\ncomponents increasing end-to-end latency and lowering throughput caps. In the\nparallel hybrid architecture, the transformer operates independently in\nparallel with the SSM, and these pairs are cascaded, with output from one pair\nforming the input to the next. Two issues are (i) creating an expressive\nknowledge representation with the inherently divergent outputs from these\nseparate branches, and (ii) load balancing the computation between these\nparallel branches, while maintaining representation fidelity. In this work we\npresent FlowHN, a novel parallel hybrid network architecture that accommodates\nvarious strategies for load balancing, achieved through appropriate\ndistribution of input tokens between the two branches. Two innovative\ndifferentiating factors in FlowHN include a FLOP aware dynamic token split\nbetween the attention and SSM branches yielding efficient balance in compute\nload, and secondly, a method to fuse the highly divergent outputs from\nindividual branches for enhancing representation expressivity. Together they\nenable much better token processing speeds, avoid bottlenecks, and at the same\ntime yield significantly improved accuracy as compared to other competing\nworks. We conduct comprehensive experiments on autoregressive language modeling\nfor models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential\nhybrid models and its parallel counterpart, achieving up to 4* higher Tokens\nper Second (TPS) and 2* better Model FLOPs Utilization (MFU).", "categories": ["cs.CL"], "published": "2025-05-26 03:52:22", "updated": "2025-05-26 03:52:22", "pdf_url": "http://arxiv.org/pdf/2505.19472v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19475v1", "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection", "authors": ["Mohammad Mahdi Moradi", "Hossam Amer", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu", "Walid Ahmed"], "abstract": "Learning to adapt pretrained language models to unlabeled,\nout-of-distribution data is a critical challenge, as models often falter on\nstructurally novel reasoning tasks even while excelling within their training\ndistribution. We introduce a new framework called VDS-TTT - Verifier-Driven\nSample Selection for Test-Time Training to efficiently address this. We use a\nlearned verifier to score a pool of generated responses and select only from\nhigh ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,\nfor each input query our LLM generates N candidate answers; the verifier\nassigns a reliability score to each, and the response with the highest\nconfidence and above a fixed threshold is paired with its query for test-time\ntraining. We fine-tune only low-rank LoRA adapter parameters, ensuring\nadaptation efficiency and fast convergence. Our proposed self-supervised\nframework is the first to synthesize verifier driven test-time training data\nfor continuous self-improvement of the model. Experiments across three diverse\nbenchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up\nto a 32.29% relative improvement over the base model and a 6.66% gain compared\nto verifier-based methods without test-time training, highlighting its\neffectiveness and efficiency for on-the-fly large language model adaptation.", "categories": ["cs.CL"], "published": "2025-05-26 03:54:47", "updated": "2025-05-26 03:54:47", "pdf_url": "http://arxiv.org/pdf/2505.19475v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19484v1", "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis", "authors": ["Ruixiang Feng", "Shen Gao", "Xiuying Chen", "Lisi Chen", "Shuo Shang"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often exhibit a specific cultural biases, neglecting\nthe values and linguistic diversity of low-resource regions. This cultural bias\nnot only undermines universal equality, but also risks reinforcing stereotypes\nand perpetuating discrimination. To address this, we propose CulFiT, a novel\nculturally-aware training paradigm that leverages multilingual data and\nfine-grained reward modeling to enhance cultural sensitivity and inclusivity.\nOur approach synthesizes diverse cultural-related questions, constructs\ncritique data in culturally relevant languages, and employs fine-grained\nrewards to decompose cultural texts into verifiable knowledge units for\ninterpretable evaluation. We also introduce GlobalCultureQA, a multilingual\nopen-ended question-answering dataset designed to evaluate culturally-aware\nresponses in a global context. Extensive experiments on three existing\nbenchmarks and our GlobalCultureQA demonstrate that CulFiT achieves\nstate-of-the-art open-source model performance in cultural alignment and\ngeneral reasoning.", "categories": ["cs.CL"], "published": "2025-05-26 04:08:26", "updated": "2025-05-26 04:08:26", "pdf_url": "http://arxiv.org/pdf/2505.19484v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19494v1", "title": "Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents", "authors": ["Manoj Balaji Jagadeeshan", "Prince Raj", "Pawan Goyal"], "abstract": "The study presents a comprehensive benchmark for retrieving Sanskrit\ndocuments using English queries, focusing on the chapters of the\nSrimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),\nTranslation-based Retrieval (DT), and Query Translation (QT), utilizing shared\nembedding spaces and advanced translation methods to enhance retrieval systems\nin a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's\nlinguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,\nContriever, and GPT-2. It adapts summarization techniques for Sanskrit\ndocuments to improve QA processing. Evaluation shows DT methods outperform DR\nand QT in handling the cross-lingual challenges of ancient texts, improving\naccessibility and understanding. A dataset of 3,400 English-Sanskrit\nquery-document pairs underpins the study, aiming to preserve Sanskrit\nscriptures and share their philosophical importance widely. Our dataset is\npublicly available at https://huggingface.co/datasets/manojbalaji1/anveshana", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-26 04:23:21", "updated": "2025-05-26 04:23:21", "pdf_url": "http://arxiv.org/pdf/2505.19494v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19504v1", "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation", "authors": ["Pingzhi Li", "Zhen Tan", "Huaizhi Qu", "Huan Liu", "Tianlong Chen"], "abstract": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 04:31:38", "updated": "2025-05-26 04:31:38", "pdf_url": "http://arxiv.org/pdf/2505.19504v1", "comment": "Code is available at https://github.com/UNITES-Lab/DOGe", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19510v1", "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study", "authors": ["Dongil Yang", "Minjin Kim", "Sunghwan Kim", "Beong-woo Kwak", "Minjun Park", "Jinseok Hong", "Woontack Woo", "Jinyoung Yeo"], "abstract": "The remarkable reasoning and generalization capabilities of Large Language\nModels (LLMs) have paved the way for their expanding applications in embodied\nAI, robotics, and other real-world tasks. To effectively support these\napplications, grounding in spatial and temporal understanding in multimodal\nenvironments is essential. To this end, recent works have leveraged scene\ngraphs, a structured representation that encodes entities, attributes, and\ntheir relationships in a scene. However, a comprehensive evaluation of LLMs'\nability to utilize scene graphs remains limited. In this work, we introduce\nText-Scene Graph (TSG) Bench, a benchmark designed to systematically assess\nLLMs' ability to (1) understand scene graphs and (2) generate them from textual\nnarratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models\nperform well on scene graph understanding, they struggle with scene graph\ngeneration, particularly for complex narratives. Our analysis indicates that\nthese models fail to effectively decompose discrete scenes from a complex\nnarrative, leading to a bottleneck when generating scene graphs. These findings\nunderscore the need for improved methodologies in scene graph generation and\nprovide valuable insights for future research. The demonstration of our\nbenchmark is available at https://tsg-bench.netlify.app. Additionally, our code\nand evaluation data are publicly available at\nhttps://anonymous.4open.science/r/TSG-Bench.", "categories": ["cs.CL"], "published": "2025-05-26 04:45:12", "updated": "2025-05-26 04:45:12", "pdf_url": "http://arxiv.org/pdf/2505.19510v1", "comment": "ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19511v1", "title": "Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models", "authors": ["Aggrey Muhebwa", "Khalid K. Osman"], "abstract": "Large proprietary language models exhibit strong causal reasoning abilities\nthat smaller open-source models struggle to replicate. We introduce a novel\nframework for distilling causal explanations that transfers causal reasoning\nskills from a powerful teacher model to a compact open-source model. The key\nidea is to train the smaller model to develop causal reasoning abilities by\ngenerating structured cause-and-effect explanations consistent with those of\nthe teacher model. To evaluate the quality of the student-generated\nexplanations, we introduce a new metric called Causal Explanation Coherence\n(CEC) to assess the structural and logical consistency of causal reasoning.\nThis metric uses sentence-level semantic alignment to measure how well each\npart of the generated explanation corresponds to the teacher's reference,\ncapturing both faithfulness and coverage of the underlying causal chain. Our\nframework and the CEC metric provide a principled foundation for training\nsmaller models to perform robust causal reasoning and for systematically\nassessing the coherence of explanations in language model outputs.", "categories": ["cs.CL"], "published": "2025-05-26 04:50:42", "updated": "2025-05-26 04:50:42", "pdf_url": "http://arxiv.org/pdf/2505.19511v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19514v1", "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "abstract": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 04:56:48", "updated": "2025-05-26 04:56:48", "pdf_url": "http://arxiv.org/pdf/2505.19514v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19515v1", "title": "Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework", "authors": ["Lavanya Prahallad", "Radhika Mamidi"], "abstract": "We present a critical discourse analysis of the 2024 U.S. presidential\ndebates, examining Donald Trump's rhetorical strategies in his interactions\nwith Joe Biden and Kamala Harris. We introduce a novel annotation framework,\nBEADS (Bias Enriched Annotation for Dialogue Structure), which systematically\nextends the DAMSL framework to capture bias driven and adversarial discourse\nfeatures in political communication. BEADS includes a domain and language\nagnostic set of tags that model ideological framing, emotional appeals, and\nconfrontational tactics. Our methodology compares detailed human annotation\nwith zero shot ChatGPT assisted tagging on verified transcripts from the Trump\nand Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our\nanalysis shows that Trump consistently dominated in key categories: Challenge\nand Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,\nand Perceived Dismissiveness. These findings underscore his use of emotionally\ncharged and adversarial rhetoric to control the narrative and influence\naudience perception. In this work, we establish BEADS as a scalable and\nreproducible framework for critical discourse analysis across languages,\ndomains, and political contexts.", "categories": ["cs.CL"], "published": "2025-05-26 04:58:08", "updated": "2025-05-26 04:58:08", "pdf_url": "http://arxiv.org/pdf/2505.19515v1", "comment": "8 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19528v1", "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "abstract": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness.", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "published": "2025-05-26 05:27:10", "updated": "2025-05-26 05:27:10", "pdf_url": "http://arxiv.org/pdf/2505.19528v1", "comment": "13 pages, 4 figures, Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19529v1", "title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "authors": ["Tanjil Hasan Sakib", "Md. Tanzib Hosain", "Md. Kishor Morol"], "abstract": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models.", "categories": ["cs.CL"], "published": "2025-05-26 05:29:47", "updated": "2025-05-26 05:29:47", "pdf_url": "http://arxiv.org/pdf/2505.19529v1", "comment": "9 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19536v1", "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models", "authors": ["Jintao Tong", "Wenwei Jin", "Pengda Qin", "Anqi Li", "Yixiong Zou", "Yuhong Li", "Yuhua Li", "Ruixuan Li"], "abstract": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-26 05:54:48", "updated": "2025-05-26 05:54:48", "pdf_url": "http://arxiv.org/pdf/2505.19536v1", "comment": "19 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19538v1", "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients", "authors": ["Yuxing Lu", "Gecheng Fu", "Wei Wu", "Xukai Zhao", "Sin Yee Goi", "Jinzhuo Wang"], "abstract": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.MA"], "published": "2025-05-26 05:56:23", "updated": "2025-05-26 05:56:23", "pdf_url": "http://arxiv.org/pdf/2505.19538v1", "comment": "32 pages, 5 figures, 5 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19548v1", "title": "How Syntax Specialization Emerges in Language Models", "authors": ["Xufeng Duan", "Zhaoqian Yao", "Yunhao Zhang", "Shaonan Wang", "Zhenguang G. Cai"], "abstract": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 06:11:18", "updated": "2025-05-26 06:11:18", "pdf_url": "http://arxiv.org/pdf/2505.19548v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19549v1", "title": "Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents", "authors": ["Derong Xu", "Yi Wen", "Pengyue Jia", "Yingyi Zhang", "wenlin zhang", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Xiangyu Zhao", "Enhong Chen", "Tong Xu"], "abstract": "Large Language Models (LLMs) have recently been widely adopted in\nconversational agents. However, the increasingly long interactions between\nusers and agents accumulate extensive dialogue records, making it difficult for\nLLMs with limited context windows to maintain a coherent long-term dialogue\nmemory and deliver personalized responses. While retrieval-augmented memory\nsystems have emerged to address this issue, existing methods often depend on\nsingle-granularity memory segmentation and retrieval. This approach falls short\nin capturing deep memory connections, leading to partial retrieval of useful\ninformation or substantial noise, resulting in suboptimal performance. To\ntackle these limits, we propose MemGAS, a framework that enhances memory\nconsolidation by constructing multi-granularity association, adaptive\nselection, and retrieval. MemGAS is based on multi-granularity memory units and\nemploys Gaussian Mixture Models to cluster and associate new memories with\nhistorical ones. An entropy-based router adaptively selects optimal granularity\nby evaluating query relevance distributions and balancing information\ncompleteness and noise. Retrieved memories are further refined via LLM-based\nfiltering. Experiments on four long-term memory benchmarks demonstrate that\nMemGAS outperforms state-of-the-art methods on both question answer and\nretrieval tasks, achieving superior performance across different query types\nand top-K settings.", "categories": ["cs.CL"], "published": "2025-05-26 06:13:07", "updated": "2025-05-26 06:13:07", "pdf_url": "http://arxiv.org/pdf/2505.19549v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19563v1", "title": "Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights", "authors": ["Shi-Yu Tian", "Zhi Zhou", "Wei Dong", "Ming Yang", "Kun-Yang Yu", "Zi-Jian Cheng", "Lan-Zhe Guo", "Yu-Feng Li"], "abstract": "Reasoning with tabular data holds increasing importance in modern\napplications, yet comprehensive evaluation methodologies for\nreasoning-intensive Table Question Answering (QA) tasks remain nascent.\nExisting research is constrained by two primary bottlenecks: 1) Reliance on\ncostly manually annotated real-world data, which is difficult to cover complex\nreasoning scenarios; 2) The heterogeneity of table structures hinders\nsystematic analysis of the intrinsic mechanisms behind the underperformance of\nLLMs, especially in reasoning-intensive tasks. To address these issues, we\npropose an automated generation pipeline AutoT2T that transforms mathematical\nword problems into table-based reasoning tasks, eliminating the need for manual\nannotation. The pipeline can generate multiple variants of a table for the same\nreasoning problem, including noisy versions to support robustness evaluation.\nBased on this, we construct a new benchmark TabularGSM, which systematically\nspans a range of table complexities and trap problems. Experimental analyses\nthrough AutoT2T and TabularGSM reveal that the tight coupling between reasoning\nand retrieval or identification processes is a key factor underlying the\nfailure of LLMs in complex Table QA tasks. This highlights the necessity for\nmodels to develop synergistic reasoning capabilities in order to perform\neffectively in complex Table QA tasks.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 06:24:31", "updated": "2025-05-26 06:24:31", "pdf_url": "http://arxiv.org/pdf/2505.19563v1", "comment": "Paper under review, code and dataset are all available", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19572v1", "title": "DocMEdit: Towards Document-Level Model Editing", "authors": ["Li Zeng", "Zeming Liu", "Chong Feng", "Heyan Huang", "Yuhang Guo"], "abstract": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 06:37:24", "updated": "2025-05-26 06:37:24", "pdf_url": "http://arxiv.org/pdf/2505.19572v1", "comment": "Accepted by ACL 2025 findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19578v1", "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing", "authors": ["Dan Peng", "Zhihui Fu", "Zewen Ye", "Zhuoran Song", "Jun Wang"], "abstract": "Sparse attention methods exploit the inherent sparsity in attention to speed\nup the prefilling phase of long-context inference, mitigating the quadratic\ncomplexity of full attention computation. While existing sparse attention\nmethods rely on predefined patterns or inaccurate estimations to approximate\nattention behavior, they often fail to fully capture the true dynamics of\nattention, resulting in reduced efficiency and compromised accuracy. Instead,\nwe propose a highly accurate sparse attention mechanism that shares similar yet\nprecise attention patterns across heads, enabling a more realistic capture of\nthe dynamic behavior of attention. Our approach is grounded in two key\nobservations: (1) attention patterns demonstrate strong inter-head similarity,\nand (2) this similarity remains remarkably consistent across diverse inputs. By\nstrategically sharing computed accurate patterns across attention heads, our\nmethod effectively captures actual patterns while requiring full attention\ncomputation for only a small subset of heads. Comprehensive evaluations\ndemonstrate that our approach achieves superior or comparable speedup relative\nto state-of-the-art methods while delivering the best overall accuracy.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 06:48:53", "updated": "2025-05-26 06:48:53", "pdf_url": "http://arxiv.org/pdf/2505.19578v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19586v1", "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization", "authors": ["Dingyu Yao", "Bowen Shen", "Zheng Lin", "Wei Liu", "Jian Luan", "Bin Wang", "Weiping Wang"], "abstract": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.", "categories": ["cs.CL"], "published": "2025-05-26 07:00:04", "updated": "2025-05-26 07:00:04", "pdf_url": "http://arxiv.org/pdf/2505.19586v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19590v1", "title": "Learning to Reason without External Rewards", "authors": ["Xuandong Zhao", "Zhewei Kang", "Aosong Feng", "Sergey Levine", "Dawn Song"], "abstract": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-26 07:01:06", "updated": "2025-05-26 07:01:06", "pdf_url": "http://arxiv.org/pdf/2505.19590v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19591v1", "title": "Multi-Agent Collaboration via Evolving Orchestration", "authors": ["Yufan Dang", "Chen Qian", "Xueheng Luo", "Jingru Fan", "Zihao Xie", "Ruijie Shi", "Weize Chen", "Cheng Yang", "Xiaoyin Che", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-05-26 07:02:17", "updated": "2025-05-26 07:02:17", "pdf_url": "http://arxiv.org/pdf/2505.19591v1", "comment": "Work in Progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19598v1", "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study", "authors": ["Guanyu Hou", "Jiaming He", "Yinhang Zhou", "Ji Guo", "Yitong Qiao", "Rui Zhang", "Wenbo Jiang"], "abstract": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment.", "categories": ["cs.CL"], "published": "2025-05-26 07:08:38", "updated": "2025-05-26 07:08:38", "pdf_url": "http://arxiv.org/pdf/2505.19598v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19599v1", "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 07:08:47", "updated": "2025-05-26 07:08:47", "pdf_url": "http://arxiv.org/pdf/2505.19599v1", "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19601v1", "title": "Preference Optimization by Estimating the Ratio of the Data Distribution", "authors": ["Yeongmin Kim", "Heesun Bae", "Byeonghu Na", "Il-Chul Moon"], "abstract": "Direct preference optimization (DPO) is widely used as a simple and stable\nmethod for aligning large language models (LLMs) with human preferences. This\npaper investigates a generalized DPO loss that enables a policy model to match\nthe target policy from a likelihood ratio estimation perspective. The ratio of\nthe target policy provides a unique identification of the policy distribution\nwithout relying on reward models or partition functions. This allows the\ngeneralized loss to retain both simplicity and theoretical guarantees, which\nprior work such as $f$-PO fails to achieve simultaneously. We propose Bregman\npreference optimization (BPO), a generalized framework for ratio matching that\nprovides a family of objective functions achieving target policy optimality.\nBPO subsumes DPO as a special case and offers tractable forms for all\ninstances, allowing implementation with a few lines of code. We further develop\nscaled Basu's power divergence (SBA), a gradient scaling method that can be\nused for BPO instances. The BPO framework complements other DPO variants and is\napplicable to target policies defined by these variants. In experiments, unlike\nother probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a\ntrade-off between generation fidelity and diversity, instances of BPO improve\nboth win rate and entropy compared with DPO. When applied to\nLlama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B\nbackbones, with a 55.9\\% length-controlled win rate on AlpacaEval2.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 07:10:53", "updated": "2025-05-26 07:10:53", "pdf_url": "http://arxiv.org/pdf/2505.19601v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19604v1", "title": "Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis", "authors": ["Ahan Prasannakumar Shetty"], "abstract": "Machine translation has become a critical tool in bridging linguistic gaps,\nespecially between languages as diverse as English and Hindi. This paper\ncomprehensively evaluates various machine translation models for translating\nbetween English and Hindi. We assess the performance of these models using a\ndiverse set of automatic evaluation metrics, both lexical and machine\nlearning-based metrics. Our evaluation leverages an 18000+ corpus of English\nHindi parallel dataset and a custom FAQ dataset comprising questions from\ngovernment websites. The study aims to provide insights into the effectiveness\nof different machine translation approaches in handling both general and\nspecialized language domains. Results indicate varying performance levels\nacross different metrics, highlighting strengths and areas for improvement in\ncurrent translation systems.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-26 07:15:06", "updated": "2025-05-26 07:15:06", "pdf_url": "http://arxiv.org/pdf/2505.19604v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19606v1", "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically", "authors": ["Ryan Soh-Eun Shim", "Domenico De Cristofaro", "Chengzhi Martin Hu", "Alessandro Vietti", "Barbara Plank"], "abstract": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies.", "categories": ["cs.CL"], "published": "2025-05-26 07:21:20", "updated": "2025-05-26 07:21:20", "pdf_url": "http://arxiv.org/pdf/2505.19606v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19621v1", "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models", "authors": ["George Kour", "Itay Nakash", "Ateret Anaby-Tavor", "Michal Shmueli-Scheuer"], "abstract": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 07:41:21", "updated": "2025-05-26 07:41:21", "pdf_url": "http://arxiv.org/pdf/2505.19621v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19628v1", "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices", "authors": ["Silin Li", "Yuhang Guo", "Jiashu Yao", "Zeming Liu", "Haifeng Wang"], "abstract": "Large language models (LLMs) have the potential to revolutionize smart home\nassistants by enhancing their ability to accurately understand user needs and\nrespond appropriately, which is extremely beneficial for building a smarter\nhome environment. While recent studies have explored integrating LLMs into\nsmart home systems, they primarily focus on handling straightforward, valid\nsingle-device operation instructions. However, real-world scenarios are far\nmore complex and often involve users issuing invalid instructions or\ncontrolling multiple devices simultaneously. These have two main challenges:\nLLMs must accurately identify and rectify errors in user instructions and\nexecute multiple user instructions perfectly. To address these challenges and\nadvance the development of LLM-based smart home assistants, we introduce\nHomeBench, the first smart home dataset with valid and invalid instructions\nacross single and multiple devices in this paper. We have experimental results\non 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the\nscenario of invalid multi-device instructions, revealing that the existing\nstate-of-the-art LLMs still cannot perform well in this situation even with the\nhelp of in-context learning, retrieval-augmented generation, and fine-tuning.\nOur code and dataset are publicly available at\nhttps://github.com/BITHLP/HomeBench.", "categories": ["cs.CL"], "published": "2025-05-26 07:47:39", "updated": "2025-05-26 07:47:39", "pdf_url": "http://arxiv.org/pdf/2505.19628v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19630v1", "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "authors": ["Yichun Feng", "Jiawei Wang", "Lu Zhou", "Yixue Li"], "abstract": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL", "categories": ["cs.CL"], "published": "2025-05-26 07:48:14", "updated": "2025-05-26 07:48:14", "pdf_url": "http://arxiv.org/pdf/2505.19630v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19631v1", "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models", "authors": ["Zihong Zhang", "Liqi He", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Bo Du"], "abstract": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 07:48:15", "updated": "2025-05-26 07:48:15", "pdf_url": "http://arxiv.org/pdf/2505.19631v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19634v1", "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "authors": ["Zili Wang", "Tianyu Zhang", "Haoli Bai", "Lu Hou", "Xianzhi Yu", "Wulong Liu", "Shiming Xiang", "Lei Zhu"], "abstract": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios.", "categories": ["cs.CL"], "published": "2025-05-26 07:51:30", "updated": "2025-05-26 07:51:30", "pdf_url": "http://arxiv.org/pdf/2505.19634v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19640v1", "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning", "authors": ["Roy Xie", "David Qiu", "Deepak Gopinath", "Dong Lin", "Yanchao Sun", "Chong Wang", "Saloni Potdar", "Bhuwan Dhingra"], "abstract": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.", "categories": ["cs.CL"], "published": "2025-05-26 07:58:17", "updated": "2025-05-26 07:58:17", "pdf_url": "http://arxiv.org/pdf/2505.19640v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19641v1", "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "abstract": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 07:59:36", "updated": "2025-05-26 07:59:36", "pdf_url": "http://arxiv.org/pdf/2505.19641v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19647v1", "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation", "authors": ["Xiaochuan Liu", "Ruihua Song", "Xiting Wang", "Xu Chen"], "abstract": "Automatic related work generation (RWG) can save people's time and effort\nwhen writing a draft of related work section (RWS) for further revision.\nHowever, existing methods for RWG always suffer from shallow comprehension due\nto taking the limited portions of references papers as input and isolated\nexplanation for each reference due to ineffective capturing the relationships\namong them. To address these issues, we focus on full-text-based RWG task and\npropose a novel multi-agent framework. Our framework consists of three agents:\na selector that decides which section of the papers is going to read next, a\nreader that digests the selected section and updates a shared working memory,\nand a writer that generates RWS based on the final curated memory. To better\ncapture the relationships among references, we also propose two graph-aware\nstrategies for selector, enabling to optimize the reading order with constrains\nof the graph structure. Extensive experiments demonstrate that our framework\nconsistently improves performance across three base models and various input\nconfigurations. The graph-aware selectors outperform alternative selectors,\nachieving state-of-the-art results. The code and data are available at\nhttps://github.com/1190200817/Full_Text_RWG.", "categories": ["cs.CL"], "published": "2025-05-26 08:02:34", "updated": "2025-05-26 08:02:34", "pdf_url": "http://arxiv.org/pdf/2505.19647v1", "comment": "Accepted by ACL 2025 (Findings)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19660v1", "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models", "authors": ["Tingjia Shen", "Hao Wang", "Chuan Qin", "Ruijun Sun", "Yang Song", "Defu Lian", "Hengshu Zhu", "Enhong Chen"], "abstract": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI", "categories": ["cs.CL", "cs.AI", "68P20", "H.3.4; I.2.6"], "published": "2025-05-26 08:18:33", "updated": "2025-05-26 08:18:33", "pdf_url": "http://arxiv.org/pdf/2505.19660v1", "comment": "13 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19667v1", "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation", "authors": ["Weikang Yuan", "Kaisong Song", "Zhuoren Jiang", "Junjie Cao", "Yujie Zhang", "Jun Lin", "Kun Kuang", "Ji Zhang", "Xiaozhong Liu"], "abstract": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions.", "categories": ["cs.CL", "cs.AI", "I.2.7"], "published": "2025-05-26 08:24:32", "updated": "2025-05-26 08:24:32", "pdf_url": "http://arxiv.org/pdf/2505.19667v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19670v1", "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models", "authors": ["Hao Yang", "Lizhen Qu", "Ehsan Shareghi", "Gholamreza Haffari"], "abstract": "Large Audio Language Models (LALMs) have extended the capabilities of Large\nLanguage Models (LLMs) by enabling audio-based human interactions. However,\nrecent research has revealed that LALMs remain vulnerable to harmful queries\ndue to insufficient safety-alignment. Despite advances in defence measures for\ntext and vision LLMs, effective safety-alignment strategies and audio-safety\ndataset specifically targeting LALMs are notably absent. Meanwhile defence\nmeasures based on Supervised Fine-tuning (SFT) struggle to address safety\nimprovement while avoiding over-rejection issues, significantly compromising\nhelpfulness. In this work, we propose an unsupervised safety-fine-tuning\nstrategy as remedy that reshapes model's representation space to enhance\nexisting LALMs safety-alignment while balancing the risk of over-rejection. Our\nexperiments, conducted across three generations of Qwen LALMs, demonstrate that\nour approach significantly improves LALMs safety under three modality input\nconditions (audio-text, text-only, and audio-only) while increasing\nover-rejection rate by only 0.88% on average. Warning: this paper contains\nharmful examples.", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "published": "2025-05-26 08:25:25", "updated": "2025-05-26 08:25:25", "pdf_url": "http://arxiv.org/pdf/2505.19670v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19674v1", "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations", "authors": ["Chaoyi Xiang", "Chunhua Liu", "Simon De Deyne", "Lea Frermann"], "abstract": "As the impact of large language models increases, understanding the moral\nvalues they reflect becomes ever more important. Assessing the nature of moral\nvalues as understood by these models via direct prompting is challenging due to\npotential leakage of human norms into model training data, and their\nsensitivity to prompt formulation. Instead, we propose to use word\nassociations, which have been shown to reflect moral reasoning in humans, as\nlow-level underlying representations to obtain a more robust picture of LLMs'\nmoral reasoning. We study moral differences in associations from western\nEnglish-speaking communities and LLMs trained predominantly on English data.\nFirst, we create a large dataset of LLM-generated word associations, resembling\nan existing data set of human word associations. Next, we propose a novel\nmethod to propagate moral values based on seed words derived from Moral\nFoundation Theory through the human and LLM-generated association graphs.\nFinally, we compare the resulting moral conceptualizations, highlighting\ndetailed but systematic differences between moral values emerging from English\nspeakers and LLM associations.", "categories": ["cs.CL"], "published": "2025-05-26 08:29:15", "updated": "2025-05-26 08:29:15", "pdf_url": "http://arxiv.org/pdf/2505.19674v1", "comment": "9 pages,7 figures. Accepted to the ACL 2025 conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19675v1", "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "abstract": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 08:31:55", "updated": "2025-05-26 08:31:55", "pdf_url": "http://arxiv.org/pdf/2505.19675v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19678v1", "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "authors": ["Hao Fang", "Changle Zhou", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Tao Liang", "Guojun Ma", "Shu-Tao Xia"], "abstract": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-26 08:36:10", "updated": "2025-05-26 08:36:10", "pdf_url": "http://arxiv.org/pdf/2505.19678v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19679v1", "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "authors": ["Zhaolin Li", "Yining Liu", "Danni Liu", "Tuan Nam Nguyen", "Enes Yavuz Ugan", "Tu Anh Dinh", "Carlos Mullov", "Alexander Waibel", "Jan Niehues"], "abstract": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 08:38:02", "updated": "2025-05-26 08:38:02", "pdf_url": "http://arxiv.org/pdf/2505.19679v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19683v1", "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey", "authors": ["Pengfei Cao", "Tianyi Men", "Wencan Liu", "Jingwen Zhang", "Xuzhao Li", "Xixun Lin", "Dianbo Sui", "Yanan Cao", "Kang Liu", "Jun Zhao"], "abstract": "Planning represents a fundamental capability of intelligent agents, requiring\ncomprehensive environmental understanding, rigorous logical reasoning, and\neffective sequential decision-making. While Large Language Models (LLMs) have\ndemonstrated remarkable performance on certain planning tasks, their broader\napplication in this domain warrants systematic investigation. This paper\npresents a comprehensive review of LLM-based planning. Specifically, this\nsurvey is structured as follows: First, we establish the theoretical\nfoundations by introducing essential definitions and categories about automated\nplanning. Next, we provide a detailed taxonomy and analysis of contemporary\nLLM-based planning methodologies, categorizing them into three principal\napproaches: 1) External Module Augmented Methods that combine LLMs with\nadditional components for planning, 2) Finetuning-based Methods that involve\nusing trajectory data and feedback signals to adjust LLMs in order to improve\ntheir planning abilities, and 3) Searching-based Methods that break down\ncomplex tasks into simpler components, navigate the planning space, or enhance\ndecoding strategies to find the best solutions. Subsequently, we systematically\nsummarize existing evaluation frameworks, including benchmark datasets,\nevaluation metrics and performance comparisons between representative planning\nmethods. Finally, we discuss the underlying mechanisms enabling LLM-based\nplanning and outline promising research directions for this rapidly evolving\nfield. We hope this survey will serve as a valuable resource to inspire\ninnovation and drive progress in this field.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 08:44:53", "updated": "2025-05-26 08:44:53", "pdf_url": "http://arxiv.org/pdf/2505.19683v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19700v1", "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models", "authors": ["Yi Liu", "Dianqing Liu", "Mingye Zhu", "Junbo Guo", "Yongdong Zhang", "Zhendong Mao"], "abstract": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 08:53:02", "updated": "2025-05-26 08:53:02", "pdf_url": "http://arxiv.org/pdf/2505.19700v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19706v1", "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision", "authors": ["Tej Deep Pala", "Panshul Sharma", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "abstract": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 08:56:36", "updated": "2025-05-26 08:56:36", "pdf_url": "http://arxiv.org/pdf/2505.19706v1", "comment": "https://github.com/declare-lab/PathFinder-PRM", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19714v1", "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "abstract": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 09:02:35", "updated": "2025-05-26 09:02:35", "pdf_url": "http://arxiv.org/pdf/2505.19714v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19715v1", "title": "Graceful Forgetting in Generative Language Models", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "abstract": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 09:03:57", "updated": "2025-05-26 09:03:57", "pdf_url": "http://arxiv.org/pdf/2505.19715v1", "comment": "8 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19722v1", "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", "authors": ["Yihao Ai", "Zhiyuan Ning", "Weiwei Dai", "Pengfei Wang", "Yi Du", "Wenjuan Cui", "Kunpeng Liu", "Yuanchun Zhou"], "abstract": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 09:10:19", "updated": "2025-05-26 09:10:19", "pdf_url": "http://arxiv.org/pdf/2505.19722v1", "comment": "Accepted by ICIC 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19743v1", "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "abstract": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-26 09:24:36", "updated": "2025-05-26 09:24:36", "pdf_url": "http://arxiv.org/pdf/2505.19743v1", "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19752v1", "title": "Discrete Markov Bridge", "authors": ["Hengli Li", "Yuxuan Wang", "Song-Chun Zhu", "Ying Nian Wu", "Zilong Zheng"], "abstract": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 09:32:12", "updated": "2025-05-26 09:32:12", "pdf_url": "http://arxiv.org/pdf/2505.19752v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19754v1", "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "abstract": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 09:33:10", "updated": "2025-05-26 09:33:10", "pdf_url": "http://arxiv.org/pdf/2505.19754v1", "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19756v1", "title": "Efficient Reasoning via Chain of Unconscious Thought", "authors": ["Ruihan Gong", "Yue Liu", "Wenjie Qu", "Mingzhe Du", "Yufei He", "Yingwei Ma", "Yulin Chen", "Xiang Liu", "Yi Wen", "Xinfeng Li", "Ruidong Wang", "Xinzhong Zhu", "Bryan Hooi", "Jiaheng Zhang"], "abstract": "Large Reasoning Models (LRMs) achieve promising performance but compromise\ntoken efficiency due to verbose reasoning processes. Unconscious Thought Theory\n(UTT) posits that complex problems can be solved more efficiently through\ninternalized cognitive processes. Inspired by UTT, we propose a new reasoning\nparadigm, termed Chain of Unconscious Thought (CoUT), to improve the token\nefficiency of LRMs by guiding them to mimic human unconscious thought and\ninternalize reasoning processes. Concretely, we first prompt the model to\ninternalize the reasoning by thinking in the hidden layer. Then, we design a\nbag of token-efficient strategies to further help models reduce unnecessary\ntokens yet preserve the performance. Our work reveals that models may possess\nbeneficial unconscious thought, enabling improved efficiency without\nsacrificing performance. Extensive experiments demonstrate the effectiveness of\nCoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while\nmaintaining comparable accuracy, as shown in Figure 1. The code of CoUT is\navailable at this link: https://github.com/Rohan-GRH/CoUT", "categories": ["cs.CL"], "published": "2025-05-26 09:34:04", "updated": "2025-05-26 09:34:04", "pdf_url": "http://arxiv.org/pdf/2505.19756v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19757v1", "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement", "authors": ["Maria Dziuba", "Valentin Malykh"], "abstract": "Effective generation of structured code comments requires robust quality\nmetrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)\nsuffer from limited code-comment analysis. We propose CIDRe, a\nlanguage-agnostic reference-free quality criterion combining four synergistic\naspects: (1) relevance (code-comment semantic alignment), (2) informativeness\n(functional coverage), (3) completeness (presence of all structure sections),\nand (4) description length (detail sufficiency). We validate our criterion on a\nmanually annotated dataset. Experiments demonstrate CIDRe's superiority over\nexisting metrics, achieving improvement in cross-entropy evaluation. When\napplied to filter comments, the models finetuned on CIDRe-filtered data show\nstatistically significant quality gains in GPT-4o-mini assessments.", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-26 09:36:57", "updated": "2025-05-26 09:36:57", "pdf_url": "http://arxiv.org/pdf/2505.19757v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19766v1", "title": "SGM: A Framework for Building Specification-Guided Moderation Filters", "authors": ["Masoomali Fatehkia", "Enes Altinisik", "Husrev Taha Sencar"], "abstract": "Aligning large language models (LLMs) with deployment-specific requirements\nis critical but inherently imperfect. Despite extensive training, models remain\nsusceptible to misalignment and adversarial inputs such as jailbreaks. Content\nmoderation filters are commonly used as external safeguards, though they\ntypically focus narrowly on safety. We introduce SGM (Specification-Guided\nModeration), a flexible framework for training moderation filters grounded in\nuser-defined specifications that go beyond standard safety concerns. SGM\nautomates training data generation without relying on human-written examples,\nenabling scalable support for diverse, application-specific alignment goals.\nSGM-trained filters perform on par with state-of-the-art safety filters built\non curated datasets, while supporting fine-grained and user-defined alignment\ncontrol.", "categories": ["cs.CL"], "published": "2025-05-26 09:49:43", "updated": "2025-05-26 09:49:43", "pdf_url": "http://arxiv.org/pdf/2505.19766v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19768v1", "title": "T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search", "authors": ["Xing Cui", "Yueying Zou", "Zekun Li", "Peipei Li", "Xinyuan Xu", "Xuannan Liu", "Huaibo Huang", "Ran He"], "abstract": "Real-world multimodal misinformation often arises from mixed forgery sources,\nrequiring dynamic reasoning and adaptive verification. However, existing\nmethods mainly rely on static pipelines and limited tool usage, limiting their\nability to handle such complexity and diversity. To address this challenge, we\npropose T2Agent, a novel misinformation detection agent that incorporates an\nextensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of\nmodular tools such as web search, forgery detection, and consistency analysis.\nEach tool is described using standardized templates, enabling seamless\nintegration and future expansion. To avoid inefficiency from using all tools\nsimultaneously, a Bayesian optimization-based selector is proposed to identify\na task-relevant subset. This subset then serves as the action space for MCTS to\ndynamically collect evidence and perform multi-source verification. To better\nalign MCTS with the multi-source nature of misinformation detection, T2Agent\nextends traditional MCTS with multi-source verification, which decomposes the\ntask into coordinated subtasks targeting different forgery sources. A dual\nreward mechanism containing a reasoning trajectory score and a confidence score\nis further proposed to encourage a balance between exploration across mixed\nforgery sources and exploitation for more reliable evidence. We conduct\nablation studies to confirm the effectiveness of the tree search mechanism and\ntool usage. Extensive experiments further show that T2Agent consistently\noutperforms existing baselines on challenging mixed-source multimodal\nmisinformation benchmarks, demonstrating its strong potential as a\ntraining-free approach for enhancing detection accuracy. The code will be\nreleased.", "categories": ["cs.CL"], "published": "2025-05-26 09:50:55", "updated": "2025-05-26 09:50:55", "pdf_url": "http://arxiv.org/pdf/2505.19768v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19770v1", "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO", "authors": ["Ruizhe Shi", "Minhak Song", "Runlong Zhou", "Zihan Zhang", "Maryam Fazel", "Simon S. Du"], "abstract": "We present a fine-grained theoretical analysis of the performance gap between\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO) under a representation gap. Our study decomposes this gap\ninto two sources: an explicit representation gap under exact optimization and\nan implicit representation gap under finite samples. In the exact optimization\nsetting, we characterize how the relative capacities of the reward and policy\nmodel classes influence the final policy qualities. We show that RLHF, DPO, or\nonline DPO can outperform one another depending on the type of model\nmis-specifications. Notably, online DPO can outperform both RLHF and standard\nDPO when the reward and policy model classes are isomorphic and both\nmis-specified. In the approximate optimization setting, we provide a concrete\nconstruction where the ground-truth reward is implicitly sparse and show that\nRLHF requires significantly fewer samples than DPO to recover an effective\nreward model -- highlighting a statistical advantage of two-stage learning.\nTogether, these results provide a comprehensive understanding of the\nperformance gap between RLHF and DPO under various settings, and offer\npractical insights into when each method is preferred.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-26 09:54:02", "updated": "2025-05-26 09:54:02", "pdf_url": "http://arxiv.org/pdf/2505.19770v1", "comment": "30 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19773v1", "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs", "authors": ["Sangyeop Kim", "Yohan Lee", "Yongwoo Song", "Kimin Lee"], "abstract": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms.", "categories": ["cs.CL", "cs.CR"], "published": "2025-05-26 09:57:25", "updated": "2025-05-26 09:57:25", "pdf_url": "http://arxiv.org/pdf/2505.19773v1", "comment": "Accepted by ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19776v1", "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Adrian Popescu"], "abstract": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 10:01:24", "updated": "2025-05-26 10:01:24", "pdf_url": "http://arxiv.org/pdf/2505.19776v1", "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19797v1", "title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants", "authors": ["Yiqun Zhang", "Hao Li", "Chenxu Wang", "Linyao Chen", "Qiaosheng Zhang", "Peng Ye", "Shi Feng", "Daling Wang", "Zhen Wang", "Xinrun Wang", "Jia Xu", "Lei Bai", "Wanli Ouyang", "Shuyue Hu"], "abstract": "As proprietary giants increasingly dominate the race for ever-larger language\nmodels, a pressing question arises for the open-source community: can smaller\nmodels remain competitive across a broad range of tasks? In this paper, we\npresent the Avengers--a simple recipe that effectively leverages the collective\nintelligence of open-source, smaller language models. Our framework is built\nupon four lightweight operations: (i) embedding: encode queries using a text\nembedding model; (ii) clustering: group queries based on their semantic\nsimilarity; (iii) scoring: scores each model's performance within each cluster;\nand (iv) voting: improve outputs via repeated sampling and voting. At inference\ntime, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse using the Self-Consistency or its multi-model variant. Remarkably,\nwith 10 open-source models (~7B parameters each), the Avengers collectively\noutperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,\nlogic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on\nmathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the\nAvengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter--the number of clusters. We have\nopen-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers", "categories": ["cs.CL"], "published": "2025-05-26 10:29:42", "updated": "2025-05-26 10:29:42", "pdf_url": "http://arxiv.org/pdf/2505.19797v1", "comment": "9 pages, 3 figures, 6 tables, supplementary material (appendix)\n  included separately", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19800v1", "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs", "authors": ["Zaid Alyafeai", "Maged S. Al-Shaibani", "Bernard Ghanem"], "abstract": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.", "categories": ["cs.CL"], "published": "2025-05-26 10:31:26", "updated": "2025-05-26 10:31:26", "pdf_url": "http://arxiv.org/pdf/2505.19800v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19804v1", "title": "Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation", "authors": ["Siyuan Li", "Jian Chen", "Rui Yao", "Xuming Hu", "Peilin Zhou", "Weihua Qiu", "Simin Zhang", "Chucheng Dong", "Zhiyao Li", "Qipeng Xie", "Zixuan Yuan"], "abstract": "Nowadays, regulatory compliance has become a cornerstone of corporate\ngovernance, ensuring adherence to systematic legal frameworks. At its core,\nfinancial regulations often comprise highly intricate provisions, layered\nlogical structures, and numerous exceptions, which inevitably result in\nlabor-intensive or comprehension challenges. To mitigate this, recent\nRegulatory Technology (RegTech) and Large Language Models (LLMs) have gained\nsignificant attention in automating the conversion of regulatory text into\nexecutable compliance logic. However, their performance remains suboptimal\nparticularly when applied to Chinese-language financial regulations, due to\nthree key limitations: (1) incomplete domain-specific knowledge representation,\n(2) insufficient hierarchical reasoning capabilities, and (3) failure to\nmaintain temporal and logical coherence. One promising solution is to develop a\ndomain specific and code-oriented datasets for model training. Existing\ndatasets such as LexGLUE, LegalBench, and CODE-ACCORD are often\nEnglish-focused, domain-mismatched, or lack fine-grained granularity for\ncompliance code generation. To fill these gaps, we present Compliance-to-Code,\nthe first large-scale Chinese dataset dedicated to financial regulatory\ncompliance. Covering 1,159 annotated clauses from 361 regulations across ten\ncategories, each clause is modularly structured with four logical\nelements-subject, condition, constraint, and contextual information-along with\nregulation relations. We provide deterministic Python code mappings, detailed\ncode reasoning, and code explanations to facilitate automated auditing. To\ndemonstrate utility, we present FinCheck: a pipeline for regulation\nstructuring, code generation, and report generation.", "categories": ["cs.CL"], "published": "2025-05-26 10:38:32", "updated": "2025-05-26 10:38:32", "pdf_url": "http://arxiv.org/pdf/2505.19804v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19806v1", "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks", "authors": ["Sirui Chen", "Shuqin Ma", "Shu Yu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "abstract": "Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness.", "categories": ["cs.CL", "cs.CY", "cs.LG"], "published": "2025-05-26 10:40:52", "updated": "2025-05-26 10:40:52", "pdf_url": "http://arxiv.org/pdf/2505.19806v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19815v1", "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Shudong Liu", "Taolin Zhang", "Zihan Ma", "Songyang Zhang", "Kai Chen"], "abstract": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 10:52:17", "updated": "2025-05-26 10:52:17", "pdf_url": "http://arxiv.org/pdf/2505.19815v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19838v1", "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "abstract": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 11:22:17", "updated": "2025-05-26 11:22:17", "pdf_url": "http://arxiv.org/pdf/2505.19838v1", "comment": "To be published in ACL 2025 Industry Track. Paper website:\n  https://foodtaxo.github.io/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19848v1", "title": "Improving Multilingual Math Reasoning for African Languages", "authors": ["Odunayo Ogundepo", "Akintunde Oladipo", "Kelechi Ogueji", "Esther Adenuga", "David Ifeoluwa Adelani", "Jimmy Lin"], "abstract": "Researchers working on low-resource languages face persistent challenges due\nto limited data availability and restricted access to computational resources.\nAlthough most large language models (LLMs) are predominantly trained in\nhigh-resource languages, adapting them to low-resource contexts, particularly\nAfrican languages, requires specialized techniques. Several strategies have\nemerged for adapting models to low-resource languages in todays LLM landscape,\ndefined by multi-stage pre-training and post-training paradigms. However, the\nmost effective approaches remain uncertain. This work systematically\ninvestigates which adaptation strategies yield the best performance when\nextending existing LLMs to African languages. We conduct extensive experiments\nand ablation studies to evaluate different combinations of data types\n(translated versus synthetically generated), training stages (pre-training\nversus post-training), and other model adaptation configurations. Our\nexperiments focuses on mathematical reasoning tasks, using the Llama 3.1 model\nfamily as our base model.", "categories": ["cs.CL"], "published": "2025-05-26 11:35:01", "updated": "2025-05-26 11:35:01", "pdf_url": "http://arxiv.org/pdf/2505.19848v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19851v1", "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "abstract": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 11:35:51", "updated": "2025-05-26 11:35:51", "pdf_url": "http://arxiv.org/pdf/2505.19851v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19862v1", "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jun Rao", "Min Zhang"], "abstract": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks\nbut often face the challenge of overthinking, leading to substantially high\ninference costs. Existing approaches synthesize shorter reasoning responses for\nLRMs to learn, but are inefficient for online usage due to the time-consuming\ndata generation and filtering processes. Meanwhile, online reinforcement\nlearning mainly adopts a length reward to encourage short reasoning responses,\nbut tends to lose the reflection ability and harm the performance. To address\nthese issues, we propose REA-RL, which introduces a small reflection model for\nefficient scaling in online training, offering both parallel sampling and\nsequential revision. Besides, a reflection reward is designed to further\nprevent LRMs from favoring short yet non-reflective responses. Experiments show\nthat both methods maintain or enhance performance while significantly improving\ninference efficiency. Their combination achieves a good balance between\nperformance and efficiency, reducing inference costs by 35% without\ncompromising performance. Further analysis demonstrates that our methods are\neffective by maintaining reflection frequency for hard problems while\nappropriately reducing it for simpler ones without losing reflection ability.\nCodes are available at https://github.com/hexuandeng/REA-RL.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-26 11:47:16", "updated": "2025-05-26 11:47:16", "pdf_url": "http://arxiv.org/pdf/2505.19862v1", "comment": "Work in Progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19866v1", "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation", "authors": ["Feng Xiong", "Hongling Xu", "Yifei Wang", "Runxi Cheng", "Yong Wang", "Xiangxiang Chu"], "abstract": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-26 11:50:16", "updated": "2025-05-26 11:50:16", "pdf_url": "http://arxiv.org/pdf/2505.19866v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19893v1", "title": "ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining", "authors": ["Melis Ilayda Bal", "Volkan Cevher", "Michael Muehlebach"], "abstract": "Large language model pretraining is compute-intensive, yet many tokens\ncontribute marginally to learning, resulting in inefficiency. We introduce\nEfficient Selective Language Modeling (ESLM), a risk-aware algorithm that\nimproves training efficiency and distributional robustness by performing online\ntoken-level batch selection. ESLM leverages per-token statistics (e.g., entropy\nor loss) and applies value-at-risk thresholding to retain only the most\ninformative tokens per batch. This data-centric mechanism reshapes the training\nloss, prioritizing high-risk tokens and eliminating redundant gradient\ncomputation. We frame ESLM as a bilevel game: the model competes with a masking\nadversary that selects worst-case token subsets under a constrained\nthresholding rule. In the loss-based setting, ESLM recovers conditional\nvalue-at-risk loss minimization, providing a principled connection to\ndistributionally robust optimization. We extend our approach to Ada-ESLM, which\nadaptively tunes the selection confidence during training. Experiments on GPT-2\npretraining show that ESLM significantly reduces training FLOPs while\nmaintaining or improving both perplexity and downstream performance compared to\nbaselines. Our approach also scales across model sizes, pretraining corpora,\nand integrates naturally with knowledge distillation.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-26 12:23:26", "updated": "2025-05-26 12:23:26", "pdf_url": "http://arxiv.org/pdf/2505.19893v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19896v1", "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Control in space,\nenabling LLMs to play a significant role in the decision-making process for\nautonomous satellite operations. As a first step towards this goal, we have\ndeveloped a pure LLM-based solution for the Kerbal Space Program Differential\nGames (KSPDG) challenge, a public software design competition where\nparticipants create autonomous agents for maneuvering satellites involved in\nnon-cooperative space operations, running on the KSP game engine. Our approach\nleverages prompt engineering, few-shot prompting, and fine-tuning techniques to\ncreate an effective LLM-based agent that ranked 2nd in the competition. To the\nbest of our knowledge, this work pioneers the integration of LLM agents into\nspace research. The project comprises several open repositories to facilitate\nreplication and further research. The codebase is accessible on\n\\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models\nand datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging\nFace}. Additionally, experiment tracking and detailed results can be reviewed\non \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases", "categories": ["cs.AI", "astro-ph.IM", "cs.CL"], "published": "2025-05-26 12:25:35", "updated": "2025-05-26 12:25:35", "pdf_url": "http://arxiv.org/pdf/2505.19896v1", "comment": "Non revised version for paper going to be published in Journal of\n  Advances in Space Research", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19897v1", "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "abstract": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/.", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "published": "2025-05-26 12:27:27", "updated": "2025-05-26 12:27:27", "pdf_url": "http://arxiv.org/pdf/2505.19897v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19912v1", "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "authors": ["Javier Mar\u00edn"], "abstract": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-26 12:39:24", "updated": "2025-05-26 12:39:24", "pdf_url": "http://arxiv.org/pdf/2505.19912v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19914v1", "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "authors": ["Jiangjie Chen", "Qianyu He", "Siyu Yuan", "Aili Chen", "Zhicheng Cai", "Weinan Dai", "Hongli Yu", "Qiying Yu", "Xuefeng Li", "Jiaze Chen", "Hao Zhou", "Mingxuan Wang"], "abstract": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 12:40:31", "updated": "2025-05-26 12:40:31", "pdf_url": "http://arxiv.org/pdf/2505.19914v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19937v1", "title": "ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs", "authors": ["Pooneh Mousavi", "Yingzhi Wang", "Mirco Ravanelli", "Cem Subakan"], "abstract": "Large Language Models (LLMs) are widely used in Spoken Language Understanding\n(SLU). Recent SLU models process audio directly by adapting speech input into\nLLMs for better multimodal learning. A key consideration for these models is\nthe cross-modal alignment between text and audio modalities, which is a\ntelltale sign as to whether or not LLM is able to associate semantic meaning to\naudio segments. While various methods exist for fusing these modalities, there\nis no standard metric to evaluate alignment quality in LLMs. In this work, we\npropose a new metric, ALAS (Automatic Latent Alignment Score). Our study\nexamines the correlation between audio and text representations across\ntransformer layers, for two different tasks (Spoken Question Answering and\nEmotion Recognition). We showcase that our metric behaves as expected across\ndifferent layers and different tasks.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-26 13:02:44", "updated": "2025-05-26 13:02:44", "pdf_url": "http://arxiv.org/pdf/2505.19937v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19944v1", "title": "Can Visual Encoder Learn to See Arrows?", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "abstract": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-26 13:09:31", "updated": "2025-05-26 13:09:31", "pdf_url": "http://arxiv.org/pdf/2505.19944v1", "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19954v1", "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning", "authors": ["Andrew Zamai", "Nathanael Fijalkow", "Boris Mansencal", "Laurent Simon", "Eloi Navet", "Pierrick Coupe"], "abstract": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-26 13:18:32", "updated": "2025-05-26 13:18:32", "pdf_url": "http://arxiv.org/pdf/2505.19954v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19955v1", "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "abstract": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 13:18:37", "updated": "2025-05-26 13:18:37", "pdf_url": "http://arxiv.org/pdf/2505.19955v1", "comment": "40 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19956v1", "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph", "authors": ["Jihyung Lee", "Jin-Seop Lee", "Jaehoon Lee", "YunSeok Choi", "Jee-Hyong Lee"], "abstract": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. Our code will be\nreleased.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 13:19:10", "updated": "2025-05-26 13:19:10", "pdf_url": "http://arxiv.org/pdf/2505.19956v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19959v1", "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models", "authors": ["Zhongzhan Huang", "Guoming Ling", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "abstract": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.", "categories": ["cs.CL"], "published": "2025-05-26 13:21:18", "updated": "2025-05-26 13:21:18", "pdf_url": "http://arxiv.org/pdf/2505.19959v1", "comment": "Accepted by ACL'25 main track", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19964v1", "title": "The Limits of Preference Data for Post-Training", "authors": ["Eric Zhao", "Jessica Dai", "Pranjal Awasthi"], "abstract": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "published": "2025-05-26 13:26:15", "updated": "2025-05-26 13:26:15", "pdf_url": "http://arxiv.org/pdf/2505.19964v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19970v1", "title": "CP-Router: An Uncertainty-Aware Router Between LLM and LRM", "authors": ["Jiayuan Su", "Fulin Lin", "Zhaopeng Feng", "Han Zheng", "Teng Wang", "Zhenyu Xiao", "Xinlong Zhao", "Zuozhu Liu", "Lu Cheng", "Hongwei Wang"], "abstract": "Recent advances in Large Reasoning Models (LRMs) have significantly improved\nlong-chain reasoning capabilities over Large Language Models (LLMs). However,\nLRMs often produce unnecessarily lengthy outputs even for simple queries,\nleading to inefficiencies or even accuracy degradation compared to LLMs. To\novercome this, we propose CP-Router, a training-free and model-agnostic routing\nframework that dynamically selects between an LLM and an LRM, demonstrated with\nmultiple-choice question answering (MCQA) prompts. The routing decision is\nguided by the prediction uncertainty estimates derived via Conformal Prediction\n(CP), which provides rigorous coverage guarantees. To further refine the\nuncertainty differentiation across inputs, we introduce Full and Binary Entropy\n(FBE), a novel entropy-based criterion that adaptively selects the appropriate\nCP threshold. Experiments across diverse MCQA benchmarks, including\nmathematics, logical reasoning, and Chinese chemistry, demonstrate that\nCP-Router efficiently reduces token usage while maintaining or even improving\naccuracy compared to using LRM alone. We also extend CP-Router to diverse model\npairings and open-ended QA, where it continues to demonstrate strong\nperformance, validating its generality and robustness.", "categories": ["cs.CL"], "published": "2025-05-26 13:33:31", "updated": "2025-05-26 13:33:31", "pdf_url": "http://arxiv.org/pdf/2505.19970v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19971v1", "title": "Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language", "authors": ["Kilian Sennrich", "Sina Ahmadi"], "abstract": "Knowledge graphs offer an excellent solution for representing the\nlexical-semantic structures of lexicographic data. However, working with the\nSPARQL query language represents a considerable hurdle for many non-expert\nusers who could benefit from the advantages of this technology. This paper\naddresses the challenge of creating natural language interfaces for\nlexicographic data retrieval on knowledge graphs such as Wikidata. We develop a\nmultidimensional taxonomy capturing the complexity of Wikidata's lexicographic\ndata ontology module through four dimensions and create a template-based\ndataset with over 1.2 million mappings from natural language utterances to\nSPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and\nGPT-3.5-Turbo reveal significant differences in model capabilities. While all\nmodels perform well on familiar patterns, only GPT-3.5-Turbo demonstrates\nmeaningful generalization capabilities, suggesting that model size and diverse\npre-training are crucial for adaptability in this domain. However, significant\nchallenges remain in achieving robust generalization, handling diverse\nlinguistic data, and developing scalable solutions that can accommodate the\nfull complexity of lexicographic knowledge representation.", "categories": ["cs.CL"], "published": "2025-05-26 13:34:39", "updated": "2025-05-26 13:34:39", "pdf_url": "http://arxiv.org/pdf/2505.19971v1", "comment": "Accepted to LDK 2025 - the 5th Conference on Language, Data and\n  Knowledge. Naples, Italy, 9-11 September 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19978v1", "title": "DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Elena Baralis"], "abstract": "Recent advances in conversational AI have demonstrated impressive\ncapabilities in single-turn responses, yet multi-turn dialogues remain\nchallenging for even the most sophisticated language models. Current dialogue\ndatasets are limited in their emotional range, domain diversity, turn depth,\nand are predominantly text-only, hindering progress in developing more\nhuman-like conversational systems across modalities. To address these\nlimitations, we present DeepDialogue, a large-scale multimodal dataset\ncontaining 40,150 high-quality multi-turn dialogues spanning 41 domains and\nincorporating 20 distinct emotions with coherent emotional progressions. Our\napproach pairs 9 different language models (4B-72B parameters) to generate\n65,600 initial conversations, which we then evaluate through a combination of\nhuman annotation and LLM-based quality filtering. The resulting dataset reveals\nfundamental insights: smaller models fail to maintain coherence beyond 6\ndialogue turns; concrete domains (e.g., \"cars,\" \"travel\") yield more meaningful\nconversations than abstract ones (e.g., \"philosophy\"); and cross-model\ninteractions produce more coherent dialogues than same-model conversations. A\nkey contribution of DeepDialogue is its speech component, where we synthesize\nemotion-consistent voices for all 40,150 dialogues, creating the first\nlarge-scale open-source multimodal dialogue dataset that faithfully preserves\nemotional context across multi-turn conversations.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-26 13:37:10", "updated": "2025-05-26 13:37:10", "pdf_url": "http://arxiv.org/pdf/2505.19978v1", "comment": "Currently under review. See the official website:\n  https://salt-research.github.io/DeepDialogue", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19987v1", "title": "How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation", "authors": ["Yongshi Ye", "Biao Fu", "Chongxuan Huang", "Yidong Chen", "Xiaodong Shi"], "abstract": "Large language models (LLMs) have demonstrated strong performance in\ngeneral-purpose machine translation, but their effectiveness in complex,\ndomain-sensitive translation tasks remains underexplored. Recent advancements\nin Large Reasoning Models (LRMs), raise the question of whether structured\nreasoning can enhance translation quality across diverse domains. In this work,\nwe compare the performance of LRMs with traditional LLMs across 15\nrepresentative domains and four translation directions. Our evaluation\nconsiders various factors, including task difficulty, input length, and\nterminology density. We use a combination of automatic metrics and an enhanced\nMQM-based evaluation hierarchy to assess translation quality. Our findings show\nthat LRMs consistently outperform traditional LLMs in semantically complex\ndomains, especially in long-text and high-difficulty translation scenarios.\nMoreover, domain-adaptive prompting strategies further improve performance by\nbetter leveraging the reasoning capabilities of LRMs. These results highlight\nthe potential of structured reasoning in MDMT tasks and provide valuable\ninsights for optimizing translation systems in domain-sensitive contexts.", "categories": ["cs.CL"], "published": "2025-05-26 13:43:37", "updated": "2025-05-26 13:43:37", "pdf_url": "http://arxiv.org/pdf/2505.19987v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.19997v1", "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Mengze Li", "Yumeng Zhu", "Ang Li", "Kun Kuang", "Fei Wu"], "abstract": "Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy.", "categories": ["cs.LG", "cs.CL", "cs.CY"], "published": "2025-05-26 13:48:49", "updated": "2025-05-26 13:48:49", "pdf_url": "http://arxiv.org/pdf/2505.19997v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20006v1", "title": "Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition", "authors": ["Rapha\u00ebl Bagat", "Irina Illina", "Emmanuel Vincent"], "abstract": "We aim to improve the robustness of Automatic Speech Recognition (ASR)\nsystems against non-native speech, particularly in low-resourced multi-accent\nsettings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a\nfine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)\nexperts, each specialized in a specific accent. This method can be used when\nthe accent is known or unknown at inference time, without the need to fine-tune\nthe model again. Our experiments, conducted using Whisper on the L2-ARCTIC\ncorpus, demonstrate significant improvements in Word Error Rate compared to\nregular LoRA and full fine-tuning when the accent is unknown. When the accent\nis known, the results further improve. Furthermore, MAS-LoRA shows less\ncatastrophic forgetting than the other fine-tuning methods. To the best of our\nknowledge, this is the first use of a mixture of LoRA experts for non-native\nmulti-accent ASR.", "categories": ["cs.CL"], "published": "2025-05-26 13:57:24", "updated": "2025-05-26 13:57:24", "pdf_url": "http://arxiv.org/pdf/2505.20006v1", "comment": "Submitted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20013v1", "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback", "authors": ["Minda Hu", "Tianqing Fang", "Jianshu Zhang", "Junyu Ma", "Zhisong Zhang", "Jingyan Zhou", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Irwin King"], "abstract": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.", "categories": ["cs.CL"], "published": "2025-05-26 14:03:37", "updated": "2025-05-26 14:03:37", "pdf_url": "http://arxiv.org/pdf/2505.20013v1", "comment": "18 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20014v1", "title": "Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation", "authors": ["Hoyun Song", "Huije Lee", "Jisu Shin", "Sukmin Cho", "Changgeon Ko", "Jong C. Park"], "abstract": "The detection of mental health problems from social media and the\ninterpretation of these results have been extensively explored. Research has\nshown that incorporating clinical symptom information into a model enhances\ndomain expertise, improving its detection and interpretation performance. While\nlarge language models (LLMs) are shown to be effective for generating\nexplanatory rationales in mental health detection, their substantially large\nparameter size and high computational cost limit their practicality. Reasoning\ndistillation transfers this ability to smaller language models (SLMs), but\ninconsistencies in the relevance and domain alignment of LLM-generated\nrationales pose a challenge. This paper investigates how rationale quality\nimpacts SLM performance in mental health detection and explanation generation.\nWe hypothesize that ensuring high-quality and domain-relevant rationales\nenhances the distillation. To this end, we propose a framework that selects\nrationales based on their alignment with expert clinical reasoning. Experiments\nshow that our quality-focused approach significantly enhances SLM performance\nin both mental disorder detection and rationale generation. This work\nhighlights the importance of rationale quality and offers an insightful\nframework for knowledge transfer in mental health applications.", "categories": ["cs.CL"], "published": "2025-05-26 14:05:33", "updated": "2025-05-26 14:05:33", "pdf_url": "http://arxiv.org/pdf/2505.20014v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20015v1", "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "authors": ["Ramon Ferrer-i-Cancho"], "abstract": "Here we present a new class of optimality for coding systems. Members of that\nclass are separated linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Whithin that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are separated by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. Our findings provide\nsupport for the hypothesis that Zipf's law originates from compression.", "categories": ["cs.CL", "physics.soc-ph"], "published": "2025-05-26 14:05:45", "updated": "2025-05-26 14:05:45", "pdf_url": "http://arxiv.org/pdf/2505.20015v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20016v1", "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation", "authors": ["Chengrui Huang", "Shen Gao", "Zhengliang Shi", "Dongsheng Wang", "Shuo Shang"], "abstract": "Existing tool-learning methods usually rely on supervised fine-tuning, they\noften overlook fine-grained optimization of internal tool call details, leading\nto limitations in preference alignment and error discrimination. To overcome\nthese challenges, we propose Token-level Tool-use Preference Alignment Training\nFramework (TTPA), a training paradigm for constructing token-level tool-use\npreference datasets that align LLMs with fine-grained preferences using a novel\nerror-oriented scoring mechanism. TTPA first introduces reversed dataset\nconstruction, a method for creating high-quality, multi-turn tool-use datasets\nby reversing the generation flow. Additionally, we propose Token-level\nPreference Sampling (TPS) to capture fine-grained preferences by modeling\ntoken-level differences during generation. To address biases in scoring, we\nintroduce the Error-oriented Scoring Mechanism (ESM), which quantifies\ntool-call errors and can be used as a training signal. Extensive experiments on\nthree diverse benchmark datasets demonstrate that TTPA significantly improves\ntool-using performance while showing strong generalization ability across\nmodels and datasets.", "categories": ["cs.CL"], "published": "2025-05-26 14:06:02", "updated": "2025-05-26 14:06:02", "pdf_url": "http://arxiv.org/pdf/2505.20016v1", "comment": "16 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20023v1", "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Yongdong Zhang", "Zhendong Mao"], "abstract": "Autonomous agents, which perceive environments and take actions to achieve\ngoals, have become increasingly feasible with the advancements in large\nlanguage models (LLMs). However, current powerful agents often depend on\nsophisticated prompt engineering combined with closed-source LLMs like GPT-4.\nAlthough training open-source LLMs using expert trajectories from teacher\nmodels has yielded some improvements in agent capabilities, this approach still\nfaces limitations such as performance plateauing and error propagation. To\nmitigate these challenges, we propose STeP, a novel method for improving\nLLM-based agent training. We synthesize self-reflected trajectories that\ninclude reflections and corrections of error steps, which enhance the\neffectiveness of LLM agents in learning from teacher models, enabling them to\nbecome agents capable of self-reflecting and correcting. We also introduce\npartial masking strategy that prevents the LLM from internalizing incorrect or\nsuboptimal steps. Experiments demonstrate that our method improves agent\nperformance across three representative tasks: ALFWorld, WebShop, and SciWorld.\nFor the open-source model LLaMA2-7B-Chat, when trained using self-reflected\ntrajectories constructed with Qwen1.5-110B-Chat as the teacher model, it\nachieves comprehensive improvements with less training data compared to agents\ntrained exclusively on expert trajectories.", "categories": ["cs.CL"], "published": "2025-05-26 14:11:12", "updated": "2025-05-26 14:11:12", "pdf_url": "http://arxiv.org/pdf/2505.20023v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20027v1", "title": "Multi-modal brain encoding models for multi-modal stimuli", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Mounika Marreddy", "Maneesh Singh", "Manish Gupta", "Bapi S. Raju"], "abstract": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain.", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "published": "2025-05-26 14:17:08", "updated": "2025-05-26 14:17:08", "pdf_url": "http://arxiv.org/pdf/2505.20027v1", "comment": "26 pages, 15 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=0dELcFHig2", "doi": null, "journal_ref": "ICLR-2025, Sinapore"}
{"arxiv_id": "2505.20045v1", "title": "Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs", "authors": ["Artem Vazhentsev", "Lyudmila Rvanova", "Gleb Kuzmin", "Ekaterina Fadeeva", "Ivan Lazichny", "Alexander Panchenko", "Maxim Panov", "Timothy Baldwin", "Mrinmaya Sachan", "Preslav Nakov", "Artem Shelmanov"], "abstract": "Large language models (LLMs) exhibit impressive fluency, but often produce\ncritical errors known as \"hallucinations\". Uncertainty quantification (UQ)\nmethods are a promising tool for coping with this fundamental shortcoming. Yet,\nexisting UQ methods face challenges such as high computational overhead or\nreliance on supervised learning. Here, we aim to bridge this gap. In\nparticular, we propose RAUQ (Recurrent Attention-based Uncertainty\nQuantification), an unsupervised approach that leverages intrinsic attention\npatterns in transformers to detect hallucinations efficiently. By analyzing\nattention weights, we identified a peculiar pattern: drops in attention to\npreceding tokens are systematically observed during incorrect generations for\ncertain \"uncertainty-aware\" heads. RAUQ automatically selects such heads,\nrecurrently aggregates their attention weights and token-level confidences, and\ncomputes sequence-level uncertainty scores in a single forward pass.\nExperiments across 4 LLMs and 12 question answering, summarization, and\ntranslation tasks demonstrate that RAUQ yields excellent results, outperforming\nstate-of-the-art UQ methods using minimal computational overhead (<1% latency).\nMoreover, it requires no task-specific labels and no careful hyperparameter\ntuning, offering plug-and-play real-time hallucination detection in white-box\nLLMs.", "categories": ["cs.CL"], "published": "2025-05-26 14:28:37", "updated": "2025-05-26 14:28:37", "pdf_url": "http://arxiv.org/pdf/2505.20045v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20046v1", "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning", "authors": ["Le Zhang", "Bo Wang", "Xipeng Qiu", "Siva Reddy", "Aishwarya Agrawal"], "abstract": "We present REARANK, a large language model (LLM)-based listwise reasoning\nreranking agent. REARANK explicitly reasons before reranking, significantly\nimproving both performance and interpretability. Leveraging reinforcement\nlearning and data augmentation, REARANK achieves substantial improvements over\nbaseline models across popular information retrieval benchmarks, notably\nrequiring only 179 annotated samples. Built on top of Qwen2.5-7B, our\nREARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and\nout-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT\nbenchmarks. These results underscore the effectiveness of our approach and\nhighlight how reinforcement learning can enhance LLM reasoning capabilities in\nreranking.", "categories": ["cs.IR", "cs.CL"], "published": "2025-05-26 14:31:48", "updated": "2025-05-26 14:31:48", "pdf_url": "http://arxiv.org/pdf/2505.20046v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20047v1", "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks", "authors": ["Debargha Ganguly", "Vikash Singh", "Sreehari Sankar", "Biyao Zhang", "Xuecen Zhang", "Srinivasan Iyengar", "Xiaotian Han", "Amit Sharma", "Shivkumar Kalyanaraman", "Vipin Chaudhary"], "abstract": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline.", "categories": ["cs.CL", "cs.AI", "cs.LO", "cs.SE"], "published": "2025-05-26 14:34:04", "updated": "2025-05-26 14:34:04", "pdf_url": "http://arxiv.org/pdf/2505.20047v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20050v1", "title": "MVP: Multi-source Voice Pathology detection", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Gabriele Ciravegna", "Marco Fantini", "Erika Crosetti", "Giovanni Succo", "Tania Cerquitelli", "Sabato Marco Siniscalchi", "Elena Baralis"], "abstract": "Voice disorders significantly impact patient quality of life, yet\nnon-invasive automated diagnosis remains under-explored due to both the\nscarcity of pathological voice data, and the variability in recording sources.\nThis work introduces MVP (Multi-source Voice Pathology detection), a novel\napproach that leverages transformers operating directly on raw voice signals.\nWe explore three fusion strategies to combine sentence reading and sustained\nvowel recordings: waveform concatenation, intermediate feature fusion, and\ndecision-level combination. Empirical validation across the German, Portuguese,\nand Italian languages shows that intermediate feature fusion using transformers\nbest captures the complementary characteristics of both recording types. Our\napproach achieves up to +13% AUC improvement over single-source methods.", "categories": ["eess.AS", "cs.CL"], "published": "2025-05-26 14:38:35", "updated": "2025-05-26 14:38:35", "pdf_url": "http://arxiv.org/pdf/2505.20050v1", "comment": "Accepted at Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20053v1", "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "abstract": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "published": "2025-05-26 14:42:35", "updated": "2025-05-26 14:42:35", "pdf_url": "http://arxiv.org/pdf/2505.20053v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20063v1", "title": "SAEs Are Good for Steering -- If You Select the Right Features", "authors": ["Dana Arad", "Aaron Mueller", "Yonatan Belinkov"], "abstract": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-26 14:47:59", "updated": "2025-05-26 14:47:59", "pdf_url": "http://arxiv.org/pdf/2505.20063v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20072v1", "title": "Incentivizing Reasoning from Weak Supervision", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "abstract": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 14:51:29", "updated": "2025-05-26 14:51:29", "pdf_url": "http://arxiv.org/pdf/2505.20072v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20081v1", "title": "Inference-time Alignment in Continuous Space", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "abstract": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 14:58:33", "updated": "2025-05-26 14:58:33", "pdf_url": "http://arxiv.org/pdf/2505.20081v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20087v1", "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models", "authors": ["Makesh Narsimhan Sreedhar", "Traian Rebedea", "Christopher Parisien"], "abstract": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-26 15:01:37", "updated": "2025-05-26 15:01:37", "pdf_url": "http://arxiv.org/pdf/2505.20087v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20088v1", "title": "Multi-Domain Explainability of Preferences", "authors": ["Nitay Calderon", "Liat Ein-Dor", "Roi Reichart"], "abstract": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated end-to-end method for\ngenerating local and global concept-based explanations of preferences across\nmultiple domains. Our method employs an LLM to discover concepts that\ndifferentiate between chosen and rejected responses and represent them with\nconcept-based vectors. To model the relationships between concepts and\npreferences, we propose a white-box Hierarchical Multi-Domain Regression model\nthat captures both domain-general and domain-specific effects. To evaluate our\nmethod, we curate a dataset spanning eight challenging and diverse domains and\nexplain twelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two novel application-driven settings.\nFirst, guiding LLM outputs with concepts from LaaJ explanations yields\nresponses that those judges consistently prefer. Second, prompting LaaJs with\nconcepts explaining humans improves their preference predictions. Together, our\nwork provides a new paradigm for explainability in the era of LLMs.", "categories": ["cs.CL"], "published": "2025-05-26 15:01:56", "updated": "2025-05-26 15:01:56", "pdf_url": "http://arxiv.org/pdf/2505.20088v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20096v1", "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "abstract": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 15:05:18", "updated": "2025-05-26 15:05:18", "pdf_url": "http://arxiv.org/pdf/2505.20096v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20097v1", "title": "S2LPP: Small-to-Large Prompt Prediction across LLMs", "authors": ["Liang Cheng", "Tianyi LI", "Zhaowei Wang", "Mark Steedman"], "abstract": "The performance of pre-trained Large Language Models (LLMs) is often\nsensitive to nuances in prompt templates, requiring careful prompt engineering,\nadding costs in terms of computing and human effort. In this study, we present\nexperiments encompassing multiple LLMs variants of varying sizes aimed at\nprobing their preference with different prompts. Through experiments on\nQuestion Answering, we show prompt preference consistency across LLMs of\ndifferent sizes. We also show that this consistency extends to other tasks,\nsuch as Natural Language Inference. Utilizing this consistency, we propose a\nmethod to use a smaller model to select effective prompt templates for a larger\nmodel. We show that our method substantially reduces the cost of prompt\nengineering while consistently matching performance with optimal prompts among\ncandidates. More importantly, our experiment shows the efficacy of our strategy\nacross fourteen LLMs and its applicability to a broad range of NLP tasks,\nhighlighting its robustness", "categories": ["cs.CL"], "published": "2025-05-26 15:07:30", "updated": "2025-05-26 15:07:30", "pdf_url": "http://arxiv.org/pdf/2505.20097v1", "comment": "15 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20099v1", "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities", "authors": ["Chuangtao Ma", "Yongrui Chen", "Tianxing Wu", "Arijit Khan", "Haofen Wang"], "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "published": "2025-05-26 15:08:23", "updated": "2025-05-26 15:08:23", "pdf_url": "http://arxiv.org/pdf/2505.20099v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20101v1", "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed", "authors": ["Yunhao Wang", "Yuhao Zhang", "Tinghao Yu", "Can Xu", "Feng Zhang", "Fengzong Lian"], "abstract": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT.In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype.Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications.", "categories": ["cs.CL"], "published": "2025-05-26 15:08:51", "updated": "2025-05-26 15:08:51", "pdf_url": "http://arxiv.org/pdf/2505.20101v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20103v1", "title": "SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment", "authors": ["Xiangyu Li", "Jingqiang Chen"], "abstract": "Citations are crucial in scientific research articles as they highlight the\nconnection between the current study and prior work. However, this process is\noften time-consuming for researchers. In this study, we propose the SciRGC\nframework, which aims to automatically recommend citation articles and generate\ncitation sentences for citation locations within articles. The framework\naddresses two key challenges in academic citation generation: 1) how to\naccurately identify the author's citation intent and find relevant citation\npapers, and 2) how to generate high-quality citation sentences that align with\nhuman preferences. We enhance citation recommendation accuracy in the citation\narticle recommendation module by incorporating citation networks and sentiment\nintent, and generate reasoning-based citation sentences in the citation\nsentence generation module by using the original article abstract, local\ncontext, citation intent, and recommended articles as inputs. Additionally, we\npropose a new evaluation metric to fairly assess the quality of generated\ncitation sentences. Through comparisons with baseline models and ablation\nexperiments, the SciRGC framework not only improves the accuracy and relevance\nof citation recommendations but also ensures the appropriateness of the\ngenerated citation sentences in context, providing a valuable tool for\ninterdisciplinary researchers.", "categories": ["cs.DL", "cs.CL"], "published": "2025-05-26 15:09:10", "updated": "2025-05-26 15:09:10", "pdf_url": "http://arxiv.org/pdf/2505.20103v1", "comment": "15 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20109v1", "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models", "authors": ["June-Woo Kim", "Wonkyo Oh", "Haram Yoon", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "abstract": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 15:12:10", "updated": "2025-05-26 15:12:10", "pdf_url": "http://arxiv.org/pdf/2505.20109v1", "comment": "Accepted to InterSpeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20112v1", "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 15:14:54", "updated": "2025-05-26 15:14:54", "pdf_url": "http://arxiv.org/pdf/2505.20112v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20113v1", "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", "authors": ["Cristian Santini", "Laura Melosi", "Emanuele Frontoni"], "abstract": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-26 15:16:48", "updated": "2025-05-26 15:16:48", "pdf_url": "http://arxiv.org/pdf/2505.20113v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20118v1", "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent", "authors": ["Dominik Meier", "Jan Philip Wahle", "Paul R\u00f6ttger", "Terry Ruas", "Bela Gipp"], "abstract": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous.", "categories": ["cs.CL", "cs.CR"], "published": "2025-05-26 15:20:51", "updated": "2025-05-26 15:20:51", "pdf_url": "http://arxiv.org/pdf/2505.20118v1", "comment": "8 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20128v1", "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "authors": ["Zhengliang Shi", "Lingyong Yan", "Dawei Yin", "Suzan Verberne", "Maarten de Rijke", "Zhaochun Ren"], "abstract": "Large language models (LLMs) have been widely integrated into information\nretrieval to advance traditional techniques. However, effectively enabling LLMs\nto seek accurate knowledge in complex tasks remains a challenge due to the\ncomplexity of multi-hop queries as well as the irrelevant retrieved content. To\naddress these limitations, we propose EXSEARCH, an agentic search framework,\nwhere the LLM learns to retrieve useful information as the reasoning unfolds\nthrough a self-incentivized process. At each step, the LLM decides what to\nretrieve (thinking), triggers an external retriever (search), and extracts\nfine-grained evidence (recording) to support next-step reasoning. To enable LLM\nwith this capability, EXSEARCH adopts a Generalized Expectation-Maximization\nalgorithm. In the E-step, the LLM generates multiple search trajectories and\nassigns an importance weight to each; the M-step trains the LLM on them with a\nre-weighted loss function. This creates a self-incentivized loop, where the LLM\niteratively learns from its own generated data, progressively improving itself\nfor search. We further theoretically analyze this training process,\nestablishing convergence guarantees. Extensive experiments on four\nknowledge-intensive benchmarks show that EXSEARCH substantially outperforms\nbaselines, e.g., +7.8% improvement on exact match score. Motivated by these\npromising results, we introduce EXSEARCH-Zoo, an extension that extends our\nmethod to broader scenarios, to facilitate future work.", "categories": ["cs.CL"], "published": "2025-05-26 15:27:55", "updated": "2025-05-26 15:27:55", "pdf_url": "http://arxiv.org/pdf/2505.20128v1", "comment": "Working in process", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20133v1", "title": "AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings", "authors": ["Konstantin Dobler", "Desmond Elliott", "Gerard de Melo"], "abstract": "Current language models rely on static vocabularies determined at pretraining\ntime, which can lead to decreased performance and increased computational cost\nfor domains underrepresented in the original vocabulary. New tokens can be\nadded to solve this problem, when coupled with a good initialization for their\nnew embeddings. However, existing embedding initialization methods either\nrequire expensive further training or pretraining of additional modules. In\nthis paper, we propose AweDist and show that by distilling representations\nobtained using the original tokenization, we can quickly learn high-quality\ninput embeddings for new tokens. Experimental results with a wide range of\nopen-weight models show that AweDist is able to outperform even strong\nbaselines.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-26 15:35:29", "updated": "2025-05-26 15:35:29", "pdf_url": "http://arxiv.org/pdf/2505.20133v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20139v1", "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs", "authors": ["Jialin Yang", "Dongfu Jiang", "Lipeng He", "Sherman Siu", "Yuxuan Zhang", "Disen Liao", "Zhuofeng Li", "Huaye Zeng", "Yiming Jia", "Haozhe Wang", "Benjamin Schneider", "Chi Ruan", "Wentao Ma", "Zhiheng Lyu", "Yifei Wang", "Yi Lu", "Quy Duc Do", "Ziyan Jiang", "Ping Nie", "Wenhu Chen"], "abstract": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.", "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2025-05-26 15:40:42", "updated": "2025-05-26 15:40:42", "pdf_url": "http://arxiv.org/pdf/2505.20139v1", "comment": "16 pages, 9 figures, 13 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20144v1", "title": "SeMe: Training-Free Language Model Merging via Semantic Alignment", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "abstract": "Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-26 15:45:56", "updated": "2025-05-26 15:45:56", "pdf_url": "http://arxiv.org/pdf/2505.20144v1", "comment": "an early-stage version", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20152v1", "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "abstract": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-26 15:55:28", "updated": "2025-05-26 15:55:28", "pdf_url": "http://arxiv.org/pdf/2505.20152v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20600v1", "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling", "authors": ["Xiaoxiao Jiang", "Suyi Li", "Lingyun Yang", "Tianyu Feng", "Zhipeng Di", "Weiyi Lu", "Guoxuan Zhu", "Xiu Lin", "Kan Liu", "Yinghao Yu", "Tao Lan", "Guodong Yang", "Lin Qu", "Liping Zhang", "Wei Wang"], "abstract": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.", "categories": ["cs.DC", "cs.AI", "cs.LG"], "published": "2025-05-27 00:36:56", "updated": "2025-05-27 00:36:56", "pdf_url": "http://arxiv.org/pdf/2505.20600v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20609v1", "title": "Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients", "authors": ["Hyungjun Park", "Chang-Yun Woo", "Seungjo Lim", "Seunghwan Lim", "Keunho Kwak", "Ju Young Jeong", "Chong Hyun Suh"], "abstract": "Objective To develop an LLM based realtime compound diagnostic medical AI\ninterface and performed a clinical trial comparing this interface and\nphysicians for common internal medicine cases based on the United States\nMedical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A\nnonrandomized clinical trial was conducted on August 20, 2024. We recruited one\ngeneral physician, two internal medicine residents (2nd and 3rd year), and five\nsimulated patients. The clinical vignettes were adapted from the USMLE Step 2\nCS style exams. We developed 10 representative internal medicine cases based on\nactual patients and included information available on initial diagnostic\nevaluation. Primary outcome was the accuracy of the first differential\ndiagnosis. Repeatability was evaluated based on the proportion of agreement.\nResults The accuracy of the physicians' first differential diagnosis ranged\nfrom 50% to 70%, whereas the realtime compound diagnostic medical AI interface\nachieved an accuracy of 80%. The proportion of agreement for the first\ndifferential diagnosis was 0.7. The accuracy of the first and second\ndifferential diagnoses ranged from 70% to 90% for physicians, whereas the AI\ninterface achieved an accuracy rate of 100%. The average time for the AI\ninterface (557 sec) was 44.6% shorter than that of the physicians (1006 sec).\nThe AI interface ($0.08) also reduced costs by 98.1% compared to the\nphysicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3\nfor care by physicians and were 3.9 for the AI interface Conclusion An LLM\nbased realtime compound diagnostic medical AI interface demonstrated diagnostic\naccuracy and patient satisfaction comparable to those of a physician, while\nrequiring less time and lower costs. These findings suggest that AI interfaces\nmay have the potential to assist primary care consultations for common internal\nmedicine cases.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-27 01:15:46", "updated": "2025-05-27 01:15:46", "pdf_url": "http://arxiv.org/pdf/2505.20609v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20613v1", "title": "REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning", "authors": ["Ziju Shen", "Naohao Huang", "Fanyi Yang", "Yutong Wang", "Guoxiong Gao", "Tianyi Xu", "Jiedong Jiang", "Wanyi He", "Pu Yang", "Mengzhou Sun", "Haocheng Ju", "Peihao Wu", "Bryan Dai", "Bin Dong"], "abstract": "Nowadays, formal theorem provers have made monumental progress on high-school\nand competition-level mathematics, but few of them generalize to more advanced\nmathematics. In this paper, we present REAL-Prover, a new open-source stepwise\ntheorem prover for Lean 4 to push this boundary. This prover, based on our\nfine-tuned large language model (REAL-Prover-v1) and integrated with a\nretrieval system (Leansearch-PS), notably boosts performance on solving\ncollege-level mathematics problems. To train REAL-Prover-v1, we developed\nHERALD-AF, a data extraction pipeline that converts natural language math\nproblems into formal statements, and a new open-source Lean 4 interactive\nenvironment (Jixia-interactive) to facilitate synthesis data collection. In our\nexperiments, our prover using only supervised fine-tune achieves competitive\nresults with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable\nto state-of-the-art (SOTA) models. To further evaluate our approach, we\nintroduce FATE-M, a new benchmark focused on algebraic problems, where our\nprover achieves a SOTA success rate of 56.7% (Pass@64).", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "published": "2025-05-27 01:26:11", "updated": "2025-05-27 01:26:11", "pdf_url": "http://arxiv.org/pdf/2505.20613v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20621v1", "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning", "authors": ["Shijie Liu", "Andrew C. Cullen", "Paul Montague", "Sarah Erfani", "Benjamin I. P. Rubinstein"], "abstract": "Similar to other machine learning frameworks, Offline Reinforcement Learning\n(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on\nexternally sourced datasets, a vulnerability that is exacerbated by its\nsequential nature. To mitigate the risks posed by RL poisoning, we extend\ncertified defenses to provide larger guarantees against adversarial\nmanipulation, ensuring robustness for both per-state actions, and the overall\nexpected cumulative reward. Our approach leverages properties of Differential\nPrivacy, in a manner that allows this work to span both continuous and discrete\nspaces, as well as stochastic and deterministic environments -- significantly\nexpanding the scope and applicability of achievable guarantees. Empirical\nevaluations demonstrate that our approach ensures the performance drops to no\nmore than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly\nimproving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while\nproducing certified radii that is $5$ times larger as well. This highlights the\npotential of our framework to enhance safety and reliability in offline RL.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 01:59:25", "updated": "2025-05-27 01:59:25", "pdf_url": "http://arxiv.org/pdf/2505.20621v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20622v1", "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation", "authors": ["Ting Xu", "Zhichao Huang", "Jiankai Sun", "Shanbo Cheng", "Wai Lam"], "abstract": "We present Sequential Policy Optimization for Simultaneous Machine\nTranslation (SeqPO-SiMT), a new policy optimization framework that defines the\nsimultaneous machine translation (SiMT) task as a sequential decision making\nproblem, incorporating a tailored reward to enhance translation quality while\nreducing latency. In contrast to popular Reinforcement Learning from Human\nFeedback (RLHF) methods, such as PPO and DPO, which are typically applied in\nsingle-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.\nThis intuitive framework allows the SiMT LLMs to simulate and refine the SiMT\nprocess using a tailored reward. We conduct experiments on six datasets from\ndiverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that\nSeqPO-SiMT consistently achieves significantly higher translation quality with\nlower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning\n(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17\nin the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context\nthan offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly\nrival the offline translation of high-performing LLMs, including\nQwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-27 01:59:58", "updated": "2025-05-27 01:59:58", "pdf_url": "http://arxiv.org/pdf/2505.20622v1", "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20633v1", "title": "Test-Time Learning for Large Language Models", "authors": ["Jinwu Hu", "Zhitian Zhang", "Guohao Chen", "Xutao Wen", "Chao Shuai", "Wei Luo", "Bin Xiao", "Yuanqing Li", "Mingkui Tan"], "abstract": "While Large Language Models (LLMs) have exhibited remarkable emergent\ncapabilities through extensive pre-training, they still face critical\nlimitations in generalizing to specialized domains and handling diverse\nlinguistic variations, known as distribution shifts. In this paper, we propose\na Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically\nadapts LLMs to target domains using only unlabeled test data during testing.\nSpecifically, we first provide empirical evidence and theoretical insights to\nreveal that more accurate predictions from LLMs can be achieved by minimizing\nthe input perplexity of the unlabeled test data. Based on this insight, we\nformulate the Test-Time Learning process of LLMs as input perplexity\nminimization, enabling self-supervised enhancement of LLM performance.\nFurthermore, we observe that high-perplexity samples tend to be more\ninformative for model optimization. Accordingly, we introduce a Sample\nEfficient Learning Strategy that actively selects and emphasizes these\nhigh-perplexity samples for test-time updates. Lastly, to mitigate catastrophic\nforgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)\ninstead of full-parameter optimization, which allows lightweight model updates\nwhile preserving more original knowledge from the model. We introduce the\nAdaptEval benchmark for TTL and demonstrate through experiments that TLM\nimproves performance by at least 20% compared to original LLMs on domain\nknowledge adaptation.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-27 02:18:59", "updated": "2025-05-27 02:18:59", "pdf_url": "http://arxiv.org/pdf/2505.20633v1", "comment": "Accepted by ICML2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20635v1", "title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction", "authors": ["Zexu Pan", "Shengkui Zhao", "Tingting Wang", "Kun Zhou", "Yukun Ma", "Chong Zhang", "Bin Ma"], "abstract": "Audio-visual speaker extraction isolates a target speaker's speech from a\nmixture speech signal conditioned on a visual cue, typically using the target\nspeaker's face recording. However, in real-world scenarios, other co-occurring\nfaces are often present on-screen, providing valuable speaker activity cues in\nthe scene. In this work, we introduce a plug-and-play inter-speaker attention\nmodule to process these flexible numbers of co-occurring faces, allowing for\nmore accurate speaker extraction in complex multi-person environments. We\nintegrate our module into two prominent models: the AV-DPRNN and the\nstate-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,\nincluding the highly overlapped VoxCeleb2 and sparsely overlapped MISP,\ndemonstrate that our approach consistently outperforms baselines. Furthermore,\ncross-dataset evaluations on LRS2 and LRS3 confirm the robustness and\ngeneralizability of our method.", "categories": ["eess.AS", "cs.AI", "cs.SD"], "published": "2025-05-27 02:21:38", "updated": "2025-05-27 02:21:38", "pdf_url": "http://arxiv.org/pdf/2505.20635v1", "comment": "Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20637v1", "title": "TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone", "authors": ["Ana M. Cabanas", "Alma Pedro", "Domingo Mery"], "abstract": "Understanding how facial affect analysis (FAA) systems perform across\ndifferent demographic groups requires reliable measurement of sensitive\nattributes such as ancestry, often approximated by skin tone, which itself is\nhighly influenced by lighting conditions. This study compares two objective\nskin tone classification methods: the widely used Individual Typology Angle\n(ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and\nHue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness\nacross skin tone groups defined by each method. Results reveal a severe\nunderrepresentation of dark skin tones ($\\sim 2 \\%$), alongside fairness\ndisparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While\nITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$\nmethod yields more consistent subgrouping and enables clearer diagnostics\nthrough metrics such as Equal Opportunity. Grad-CAM analysis further highlights\ndifferences in model attention patterns by skin tone, suggesting variation in\nfeature encoding. To support future mitigation efforts, we also propose a\nmodular fairness-aware pipeline that integrates perceptual skin tone\nestimation, model interpretability, and fairness evaluation. These findings\nemphasize the relevance of skin tone measurement choices in fairness assessment\nand suggest that ITA-based evaluations may overlook disparities affecting\ndarker-skinned individuals.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 02:31:08", "updated": "2025-05-27 02:31:08", "pdf_url": "http://arxiv.org/pdf/2505.20637v1", "comment": "10 pages", "doi": null, "journal_ref": "2025 19th International Conference on Automatic Face and Gesture\n  Recognition (FG)"}
{"arxiv_id": "2505.20642v1", "title": "CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models", "authors": ["Yi Zhan", "Qi Liu", "Weibo Gao", "Zheng Zhang", "Tianfu Wang", "Shuanghong Shen", "Junyu Lu", "Zhenya Huang"], "abstract": "Personalized programming tutoring, such as exercise recommendation, can\nenhance learners' efficiency, motivation, and outcomes, which is increasingly\nimportant in modern digital education. However, the lack of sufficient and\nhigh-quality programming data, combined with the mismatch between offline\nevaluation and real-world learning, hinders the practical deployment of such\nsystems. To address this challenge, many approaches attempt to simulate learner\npractice data, yet they often overlook the fine-grained, iterative nature of\nprogramming learning, resulting in a lack of interpretability and granularity.\nTo fill this gap, we propose a LLM-based agent, CoderAgent, to simulate\nstudents' programming processes in a fine-grained manner without relying on\nreal data. Specifically, we equip each human learner with an intelligent agent,\nthe core of which lies in capturing the cognitive states of the human\nprogramming practice process. Inspired by ACT-R, a cognitive architecture\nframework, we design the structure of CoderAgent to align with human cognitive\narchitecture by focusing on the mastery of programming knowledge and the\napplication of coding ability. Recognizing the inherent patterns in\nmulti-layered cognitive reasoning, we introduce the Programming Tree of Thought\n(PTOT), which breaks down the process into four steps: why, how, where, and\nwhat. This approach enables a detailed analysis of iterative problem-solving\nstrategies. Finally, experimental evaluations on real-world datasets\ndemonstrate that CoderAgent provides interpretable insights into learning\ntrajectories and achieves accurate simulations, paving the way for personalized\nprogramming education.", "categories": ["cs.AI"], "published": "2025-05-27 02:43:38", "updated": "2025-05-27 02:43:38", "pdf_url": "http://arxiv.org/pdf/2505.20642v1", "comment": "Accepted by IJCAI2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20643v1", "title": "Can Past Experience Accelerate LLM Reasoning?", "authors": ["Bo Pan", "Liang Zhao"], "abstract": "Allocating more compute to large language models (LLMs) reasoning has\ngenerally been demonstrated to improve their effectiveness, but also results in\nincreased inference time. In contrast, humans can perform tasks faster and\nbetter with increased experience and exposure. Hence, this paper aims to\ninvestigate the question: Can LLMs also become faster at reasoning through\nrecurrent exposure on relevant tasks, and if so, how can it be achieved? To\naddress these questions, we first formalize the problem setting of LLM\nreasoning speedup systematically in the dimensions of task relevancy and\ncompute budget calculation. We then propose SpeedupLLM, a theoretically\nguaranteed framework to implement and benchmark such reasoning speedup\nbehaviour based on adaptive compute allocation and memory mechanisms. We\nfurther conduct comprehensive experiments to benchmark such behaviour across\ndifferent question similarity levels, memory methods, and reasoning methods.\nResults show that LLMs can generally reason faster with past experience,\nachieving up to a 56% reduction in compute cost when equipped with appropriate\nmemory and reasoning methods.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 02:44:00", "updated": "2025-05-27 02:44:00", "pdf_url": "http://arxiv.org/pdf/2505.20643v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20644v1", "title": "HCQA-1.5 @ Ego4D EgoSchema Challenge 2025", "authors": ["Haoyu Zhang", "Yisen Feng", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "abstract": "In this report, we present the method that achieves third place for Ego4D\nEgoSchema Challenge in CVPR 2025. To improve the reliability of answer\nprediction in egocentric video question answering, we propose an effective\nextension to the previously proposed HCQA framework. Our approach introduces a\nmulti-source aggregation strategy to generate diverse predictions, followed by\na confidence-based filtering mechanism that selects high-confidence answers\ndirectly. For low-confidence cases, we incorporate a fine-grained reasoning\nmodule that performs additional visual and contextual analysis to refine the\npredictions. Evaluated on the EgoSchema blind test set, our method achieves 77%\naccuracy on over 5,000 human-curated multiple-choice questions, outperforming\nlast year's winning solution and the majority of participating teams. Our code\nwill be added at https://github.com/Hyu-Zhang/HCQA.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 02:45:14", "updated": "2025-05-27 02:45:14", "pdf_url": "http://arxiv.org/pdf/2505.20644v1", "comment": "The third-place solution for the Ego4D EgoSchema Challenge at the\n  CVPR EgoVis Workshop 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20646v1", "title": "Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory", "authors": ["Eduardo Y. Sakabe", "Felipe S. Abrah\u00e3o", "Alexandre Sim\u00f5es", "Esther Colombini", "Paula Costa", "Ricardo Gudwin", "Hector Zenil"], "abstract": "Understanding and controlling the informational complexity of neural networks\nis a central challenge in machine learning, with implications for\ngeneralization, optimization, and model capacity. While most approaches rely on\nentropy-based loss functions and statistical metrics, these measures often fail\nto capture deeper, causally relevant algorithmic regularities embedded in\nnetwork structure. We propose a shift toward algorithmic information theory,\nusing Binarized Neural Networks (BNNs) as a first proxy. Grounded in\nalgorithmic probability (AP) and the universal distribution it defines, our\napproach characterizes learning dynamics through a formal, causally grounded\nlens. We apply the Block Decomposition Method (BDM) -- a scalable approximation\nof algorithmic complexity based on AP -- and demonstrate that it more closely\ntracks structural changes during training than entropy, consistently exhibiting\nstronger correlations with training loss across varying model sizes and\nrandomized training runs. These results support the view of training as a\nprocess of algorithmic compression, where learning corresponds to the\nprogressive internalization of structured regularities. In doing so, our work\noffers a principled estimate of learning progression and suggests a framework\nfor complexity-aware learning and regularization, grounded in first principles\nfrom information theory, complexity, and computability.", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "68T07, 68Q30, 68Q32", "I.2.6; F.1.1; F.1.3"], "published": "2025-05-27 02:51:36", "updated": "2025-05-27 02:51:36", "pdf_url": "http://arxiv.org/pdf/2505.20646v1", "comment": "10 pages total, 1 figure. Submitted to NeurIPS 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20648v1", "title": "Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning", "authors": ["Mengmeng Chen", "Xiaohu Wu", "Qiqi Liu", "Tiantian He", "Yew-Soon Ong", "Yaochu Jin", "Qicheng Lao", "Han Yu"], "abstract": "Multi-objective optimization (MOO) exists extensively in machine learning,\nand aims to find a set of Pareto-optimal solutions, called the Pareto front,\ne.g., it is fundamental for multiple avenues of research in federated learning\n(FL). Pareto-Front Learning (PFL) is a powerful method implemented using\nHypernetworks (PHNs) to approximate the Pareto front. This method enables the\nacquisition of a mapping function from a given preference vector to the\nsolutions on the Pareto front. However, most existing PFL approaches still face\ntwo challenges: (a) sampling rays in high-dimensional spaces; (b) failing to\ncover the entire Pareto Front which has a convex shape. Here, we introduce a\nnovel PFL framework, called as PHN-HVVS, which decomposes the design space into\nVoronoi grids and deploys a genetic algorithm (GA) for Voronoi grid\npartitioning within high-dimensional space. We put forward a new loss function,\nwhich effectively contributes to more extensive coverage of the resultant\nPareto front and maximizes the HV Indicator. Experimental results on multiple\nMOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines\nsignificantly in generating Pareto front. Also, we illustrate that PHN-HVVS\nadvances the methodologies of several recent problems in the FL field. The code\nis available at\nhttps://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 02:53:14", "updated": "2025-05-27 02:53:14", "pdf_url": "http://arxiv.org/pdf/2505.20648v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20650v1", "title": "FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information", "authors": ["Yan Wang", "Yang Ren", "Lingfei Qian", "Xueqing Peng", "Keyi Wang", "Yi Han", "Dongji Feng", "Xiao-Yang Liu", "Jimin Huang", "Qianqian Xie"], "abstract": "We introduce FinTagging, the first full-scope, table-aware XBRL benchmark\ndesigned to evaluate the structured information extraction and semantic\nalignment capabilities of large language models (LLMs) in the context of\nXBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL\ntagging as flat multi-class classification and focus solely on narrative text,\nFinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for\nfinancial entity extraction and FinCL for taxonomy-driven concept alignment. It\nrequires models to jointly extract facts and align them with the full 10k+\nUS-GAAP taxonomy across both unstructured text and structured tables, enabling\nrealistic, fine-grained evaluation. We assess a diverse set of LLMs under\nzero-shot settings, systematically analyzing their performance on both subtasks\nand overall tagging accuracy. Our results reveal that, while LLMs demonstrate\nstrong generalization in information extraction, they struggle with\nfine-grained concept alignment, particularly in disambiguating closely related\ntaxonomy entries. These findings highlight the limitations of existing LLMs in\nfully automating XBRL tagging and underscore the need for improved semantic\nreasoning and schema-aware modeling to meet the demands of accurate financial\ndisclosure. Code is available at our GitHub repository and data is at our\nHugging Face repository.", "categories": ["cs.CL", "cs.AI", "cs.CE"], "published": "2025-05-27 02:55:53", "updated": "2025-05-27 02:55:53", "pdf_url": "http://arxiv.org/pdf/2505.20650v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20653v1", "title": "RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment", "authors": ["Lingyu Qiu", "Ke Jiang", "Xiaoyang Tan"], "abstract": "Recent advancements in domain generalization for deepfake detection have\nattracted significant attention, with previous methods often incorporating\nadditional modules to prevent overfitting to domain-specific patterns. However,\nsuch regularization can hinder the optimization of the empirical risk\nminimization (ERM) objective, ultimately degrading model performance. In this\npaper, we propose a novel learning objective that aligns generalization\ngradient updates with ERM gradient updates. The key innovation is the\napplication of perturbations to model parameters, aligning the ascending points\nacross domains, which specifically enhances the robustness of deepfake\ndetection models to domain shifts. This approach effectively preserves\ndomain-invariant features while managing domain-specific characteristics,\nwithout introducing additional regularization. Experimental results on multiple\nchallenging deepfake detection datasets demonstrate that our gradient alignment\nstrategy outperforms state-of-the-art domain generalization techniques,\nconfirming the efficacy of our method. The code is available at\nhttps://github.com/Lynn0925/RoGA.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 03:02:21", "updated": "2025-05-27 03:02:21", "pdf_url": "http://arxiv.org/pdf/2505.20653v1", "comment": "Accepted to ICME2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20654v1", "title": "Chinese Cyberbullying Detection: Dataset, Method, and Validation", "authors": ["Yi Zhu", "Xin Zou", "Xindong Wu"], "abstract": "Existing cyberbullying detection benchmarks were organized by the polarity of\nspeech, such as \"offensive\" and \"non-offensive\", which were essentially hate\nspeech detection. However, in the real world, cyberbullying often attracted\nwidespread social attention through incidents. To address this problem, we\npropose a novel annotation method to construct a cyberbullying dataset that\norganized by incidents. The constructed CHNCI is the first Chinese\ncyberbullying incident detection dataset, which consists of 220,676 comments in\n91 incidents. Specifically, we first combine three cyberbullying detection\nmethods based on explanations generation as an ensemble method to generate the\npseudo labels, and then let human annotators judge these labels. Then we\npropose the evaluation criteria for validating whether it constitutes a\ncyberbullying incident. Experimental results demonstrate that the constructed\ndataset can be a benchmark for the tasks of cyberbullying detection and\nincident prediction. To the best of our knowledge, this is the first study for\nthe Chinese cyberbullying incident detection task.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 03:03:55", "updated": "2025-05-27 03:03:55", "pdf_url": "http://arxiv.org/pdf/2505.20654v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20660v1", "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism", "authors": ["Qinzhuo Wu", "Pengzhi Gao", "Wei Liu", "Jian Luan"], "abstract": "Graphical User Interface (GUI) agents have gained substantial attention due\nto their impressive capabilities to complete tasks through multiple\ninteractions within GUI environments. However, existing agents primarily focus\non enhancing the accuracy of individual actions and often lack effective\nmechanisms for detecting and recovering from errors. To address these\nshortcomings, we propose the BacktrackAgent, a robust framework that\nincorporates a backtracking mechanism to improve task completion efficiency.\nBacktrackAgent includes verifier, judger, and reflector components as modules\nfor error detection and recovery, while also applying judgment rewards to\nfurther enhance the agent's performance. Additionally, we develop a training\ndataset specifically designed for the backtracking mechanism, which considers\nthe outcome pages after action executions. Experimental results show that\nBacktrackAgent has achieved performance improvements in both task success rate\nand step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be\nreleased upon acceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 03:09:06", "updated": "2025-05-27 03:09:06", "pdf_url": "http://arxiv.org/pdf/2505.20660v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20662v1", "title": "AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage", "authors": ["Xuanle Zhao", "Zilin Sang", "Yuxuan Li", "Qi Shi", "Shuo Wang", "Duzhen Zhang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Efficient experiment reproduction is critical to accelerating progress in\nartificial intelligence. However, the inherent complexity of method design and\ntraining procedures presents substantial challenges for automation. Notably,\nreproducing experiments often requires implicit domain-specific knowledge not\nexplicitly documented in the original papers. To address this, we introduce the\npaper lineage algorithm, which identifies and extracts implicit knowledge from\nthe relevant references cited by the target paper. Building on this idea, we\npropose AutoReproduce, a multi-agent framework capable of automatically\nreproducing experiments described in research papers in an end-to-end manner.\nAutoReproduce enhances code executability by generating unit tests alongside\nthe reproduction process. To evaluate the reproduction capability, we construct\nReproduceBench, a benchmark annotated with verified implementations, and\nintroduce novel evaluation metrics to assess both the reproduction and\nexecution fidelity. Experimental results demonstrate that AutoReproduce\noutperforms the existing strong agent baselines on all five evaluation metrics\nby a peak margin of over $70\\%$. In particular, compared to the official\nimplementations, AutoReproduce achieves an average performance gap of $22.1\\%$\non $89.74\\%$ of the executable experiment runs. The code will be available at\nhttps://github.com/AI9Stars/AutoReproduce.", "categories": ["cs.AI"], "published": "2025-05-27 03:15:21", "updated": "2025-05-27 03:15:21", "pdf_url": "http://arxiv.org/pdf/2505.20662v1", "comment": "20 pages, preprint version", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20663v1", "title": "TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research", "authors": ["Xu Kang", "Siqi Jiang", "Kangwei Xu", "Jiahao Li", "Ruibo Wu"], "abstract": "Terpenoids are a crucial class of natural products that have been studied for\nover 150 years, but their interdisciplinary nature (spanning chemistry,\npharmacology, and biology) complicates knowledge integration. To address this,\nthe authors developed TeroSeek, a curated knowledge base (KB) built from two\ndecades of terpenoid literature, coupled with an AI-powered question-answering\nchatbot and web service. Leveraging a retrieval-augmented generation (RAG)\nframework, TeroSeek provides structured, high-quality information and\noutperforms general-purpose large language models (LLMs) in terpenoid-related\nqueries. It serves as a domain-specific expert tool for multidisciplinary\nresearch and is publicly available at http://teroseek.qmclab.com.", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3; I.2"], "published": "2025-05-27 03:17:30", "updated": "2025-05-27 03:17:30", "pdf_url": "http://arxiv.org/pdf/2505.20663v1", "comment": "18 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20664v1", "title": "Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning", "authors": ["Yang He", "Xiao Ding", "Bibo Cai", "Yufei Zhang", "Kai Xiong", "Zhouhao Sun", "Bing Qin", "Ting Liu"], "abstract": "While reasoning-augmented large language models (RLLMs) significantly enhance\ncomplex task performance through extended reasoning chains, they inevitably\nintroduce substantial unnecessary token consumption, particularly for simpler\nproblems where Short Chain-of-Thought (Short CoT) suffices. This overthinking\nphenomenon leads to inefficient resource usage without proportional accuracy\ngains. To address this issue, we propose Self-Route, a dynamic reasoning\nframework that automatically selects between general and reasoning modes based\non model capability estimation. Our approach introduces a lightweight\npre-inference stage to extract capability-aware embeddings from hidden layer\nrepresentations, enabling real-time evaluation of the model's ability to solve\nproblems. We further construct Gradient-10K, a model difficulty\nestimation-based dataset with dense complexity sampling, to train the router\nfor precise capability boundary detection. Extensive experiments demonstrate\nthat Self-Route achieves comparable accuracy to reasoning models while reducing\ntoken consumption by 30-55\\% across diverse benchmarks. The proposed framework\ndemonstrates consistent effectiveness across models with different parameter\nscales and reasoning paradigms, highlighting its general applicability and\npractical value.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 03:18:31", "updated": "2025-05-27 03:18:31", "pdf_url": "http://arxiv.org/pdf/2505.20664v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20666v1", "title": "Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers", "authors": ["Yukun Zhang", "Xueqing Zhou"], "abstract": "We propose a novel framework, Continuous_Time Attention, which infuses\npartial differential equations (PDEs) into the Transformer's attention\nmechanism to address the challenges of extremely long input sequences. Instead\nof relying solely on a static attention matrix, we allow attention weights to\nevolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion\ndynamics. This mechanism systematically smooths local noise, enhances\nlong_range dependencies, and stabilizes gradient flow. Theoretically, our\nanalysis shows that PDE_based attention leads to better optimization landscapes\nand polynomial rather than exponential decay of distant interactions.\nEmpirically, we benchmark our method on diverse experiments_demonstrating\nconsistent gains over both standard and specialized long sequence Transformer\nvariants. Our findings highlight the potential of PDE_based formulations to\nenrich attention mechanisms with continuous_time dynamics and global coherence.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 03:30:10", "updated": "2025-05-27 03:30:10", "pdf_url": "http://arxiv.org/pdf/2505.20666v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20670v1", "title": "MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning", "authors": ["Zikang Guo", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "abstract": "Complex tasks involving tool integration pose significant challenges for\nLarge Language Models (LLMs), leading to the emergence of multi-agent workflows\nas a promising solution. Reflection has emerged as an effective strategy for\ncorrecting erroneous trajectories in agentic workflows. However, existing\napproaches only exploit such capability in the post-action stage, where the\nagent observes the execution outcomes. We argue that, like humans, LLMs can\nalso engage in reflection before action execution: the agent can anticipate\nundesirable outcomes from its own decisions, which not only provides a\nnecessarily complementary perspective to evaluate the decision but also\nprevents the propagation of errors throughout the trajectory. In this paper, we\npropose MIRROR, a framework that consists of both intra-reflection, which\ncritically assesses intended actions before execution, and inter-reflection,\nwhich further adjusts the trajectory based on observations. This design\nsystematically leverages LLM reflection capabilities to eliminate and rectify\nerroneous actions on a more comprehensive scope. Evaluations on both the\nStableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior\nperformance, achieving state-of-the-art results compared to existing\napproaches.", "categories": ["cs.AI"], "published": "2025-05-27 03:37:33", "updated": "2025-05-27 03:37:33", "pdf_url": "http://arxiv.org/pdf/2505.20670v1", "comment": "Accepted to 34rd International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20671v1", "title": "LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation", "authors": ["Heng Tan", "Hua Yan", "Yu Yang"], "abstract": "While reinforcement learning (RL) has achieved notable success in various\ndomains, training effective policies for complex tasks remains challenging.\nAgents often converge to local optima and fail to maximize long-term rewards.\nExisting approaches to mitigate training bottlenecks typically fall into two\ncategories: (i) Automated policy refinement, which identifies critical states\nfrom past trajectories to guide policy updates, but suffers from costly and\nuncertain model training; and (ii) Human-in-the-loop refinement, where human\nfeedback is used to correct agent behavior, but this does not scale well to\nenvironments with large or continuous action spaces. In this work, we design a\nlarge language model-guided policy modulation framework that leverages LLMs to\nimprove RL training without additional model training or human intervention. We\nfirst prompt an LLM to identify critical states from a sub-optimal agent's\ntrajectories. Based on these states, the LLM then provides action suggestions\nand assigns implicit rewards to guide policy refinement. Experiments across\nstandard RL benchmarks demonstrate that our method outperforms state-of-the-art\nbaselines, highlighting the effectiveness of LLM-based explanations in\naddressing RL training bottlenecks.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-27 03:40:02", "updated": "2025-05-27 03:40:02", "pdf_url": "http://arxiv.org/pdf/2505.20671v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20672v1", "title": "GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning", "authors": ["Woochang Sim", "Hyunseok Ryu", "Kyungmin Choi", "Sungwon Han", "Sundong Kim"], "abstract": "The Abstraction and Reasoning Corpus (ARC) poses a stringent test of general\nAI capabilities, requiring solvers to infer abstract patterns from only a\nhandful of examples. Despite substantial progress in deep learning,\nstate-of-the-art models still achieve accuracy rates of merely 40-55% on 2024\nARC Competition, indicative of a significant gap between their performance and\nhuman-level reasoning. In this work, we seek to bridge that gap by introducing\nan analogy-inspired ARC dataset, GIFARC. Leveraging large language models\n(LLMs) and vision-language models (VLMs), we synthesize new ARC-style tasks\nfrom a variety of GIF images that include analogies. Each new task is paired\nwith ground-truth analogy, providing an explicit mapping between visual\ntransformations and everyday concepts. By embedding robust human-intuitive\nanalogies into ARC-style tasks, GIFARC guides AI agents to evaluate the task\nanalogically before engaging in brute-force pattern search, thus efficiently\nreducing problem complexity and build a more concise and human-understandable\nsolution. We empirically validate that guiding LLM with analogic approach with\nGIFARC affects task-solving approaches of LLMs to align with analogic approach\nof human.", "categories": ["cs.AI"], "published": "2025-05-27 03:42:51", "updated": "2025-05-27 03:42:51", "pdf_url": "http://arxiv.org/pdf/2505.20672v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20674v1", "title": "Pretraining Language Models to Ponder in Continuous Space", "authors": ["Boyi Zeng", "Shixiang Song", "Siyuan Huang", "Yixuan Wang", "He Li", "Ziwei He", "Xinbing Wang", "Zhiyu Li", "Zhouhan Lin"], "abstract": "Humans ponder before articulating complex sentence elements, enabling deeper\ncognitive processing through focused effort. In this work, we introduce this\npondering process into language models by repeatedly invoking the forward\nprocess within a single token generation step. During pondering, instead of\ngenerating an actual token sampled from the prediction distribution, the model\nponders by yielding a weighted sum of all token embeddings according to the\npredicted token distribution. The generated embedding is then fed back as input\nfor another forward pass. We show that the model can learn to ponder in this\nway through self-supervised learning, without any human annotations. Our method\nis straightforward and can be seamlessly integrated with various existing\nlanguage models. Experiments across three widely used open-source\narchitectures-GPT-2, Pythia, and LLaMA-and extensive downstream task\nevaluations demonstrate the effectiveness and generality of our method. For\nlanguage modeling tasks, pondering language models achieve performance\ncomparable to vanilla models with twice the number of parameters. On 9\ndownstream benchmarks, our pondering-enhanced Pythia models significantly\noutperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is\ncomparable to TinyLlama-1.1B, which is trained on 10 times more data. The code\nis available at https://github.com/LUMIA-Group/PonderingLM.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 03:47:33", "updated": "2025-05-27 03:47:33", "pdf_url": "http://arxiv.org/pdf/2505.20674v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20686v1", "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression", "authors": ["Kiant\u00e9 Brantley", "Mingyu Chen", "Zhaolin Gao", "Jason D. Lee", "Wen Sun", "Wenhao Zhan", "Xuezhou Zhang"], "abstract": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning\nlarge language models (LLMs) to improve complex reasoning abilities. However,\nstate-of-the-art policy optimization methods often suffer from high\ncomputational overhead and memory consumption, primarily due to the need for\nmultiple generations per prompt and the reliance on critic networks or\nadvantage estimates of the current policy. In this paper, we propose $A$*-PO, a\nnovel two-stage policy optimization framework that directly approximates the\noptimal advantage function and enables efficient training of LLMs for reasoning\ntasks. In the first stage, we leverage offline sampling from a reference policy\nto estimate the optimal value function $V$*, eliminating the need for costly\nonline value estimation. In the second stage, we perform on-policy updates\nusing a simple least-squares regression loss with only a single generation per\nprompt. Theoretically, we establish performance guarantees and prove that the\nKL-regularized RL objective can be optimized without requiring complex\nexploration strategies. Empirically, $A$*-PO achieves competitive performance\nacross a wide range of mathematical reasoning benchmarks, while reducing\ntraining time by up to 2$\\times$ and peak memory usage by over 30% compared to\nPPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at\nhttps://github.com/ZhaolinGao/A-PO.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 03:58:50", "updated": "2025-05-27 03:58:50", "pdf_url": "http://arxiv.org/pdf/2505.20686v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20691v1", "title": "Evidential Deep Active Learning for Semi-Supervised Classification", "authors": ["Shenkai Zhao", "Xinao Zhang", "Lipeng Pan", "Xiaobin Xu", "Danilo Pelusi"], "abstract": "Semi-supervised classification based on active learning has made significant\nprogress, but the existing methods often ignore the uncertainty estimation (or\nreliability) of the prediction results during the learning process, which makes\nit questionable whether the selected samples can effectively update the model.\nHence, this paper proposes an evidential deep active learning approach for\nsemi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised\nlearning framework to simultaneously quantify the uncertainty estimation of\nlabeled and unlabeled data during the learning process. The uncertainty\nestimation of the former is associated with evidential deep learning, while\nthat of the latter is modeled by combining ignorance information and conflict\ninformation of the evidence from the perspective of the T-conorm operator.\nFurthermore, this article constructs a heuristic method to dynamically balance\nthe influence of evidence and the number of classes on uncertainty estimation\nto ensure that it does not produce counter-intuitive results in EDALSSC. For\nthe sample selection strategy, EDALSSC selects the sample with the greatest\nuncertainty estimation that is calculated in the form of a sum when the\ntraining loss increases in the latter half of the learning process.\nExperimental results demonstrate that EDALSSC outperforms existing\nsemi-supervised and supervised active learning approaches on image\nclassification datasets.", "categories": ["cs.LG", "cs.AI", "I.2.6"], "published": "2025-05-27 03:59:48", "updated": "2025-05-27 03:59:48", "pdf_url": "http://arxiv.org/pdf/2505.20691v1", "comment": "9 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20692v1", "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions", "authors": ["Saharsh Barve", "Andy Mao", "Jiayue Melissa Shi", "Prerna Juneja", "Koustuv Saha"], "abstract": "Recent advances in generative AI have enabled visual content creation through\ntext-to-image (T2I) generation. However, despite their creative potential, T2I\nmodels often replicate and amplify societal stereotypes -- particularly those\nrelated to gender, race, and culture -- raising important ethical concerns.\nThis paper proposes a theory-driven bias detection rubric and a Social\nStereotype Index (SSI) to systematically evaluate social biases in T2I outputs.\nWe audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and\nStability AI Core -- using 100 queries across three categories -- geocultural,\noccupational, and adjectival. Our analysis reveals that initial outputs are\nprone to include stereotypical visual cues, including gendered professions,\ncultural markers, and western beauty norms. To address this, we adopted our\nrubric to conduct targeted prompt refinement using LLMs, which significantly\nreduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and\n51% for adjectival queries. We complemented our quantitative analysis through a\nuser study examining perceptions, awareness, and preferences around\nAI-generated biased imagery. Our findings reveal a key tension -- although\nprompt refinement can mitigate stereotypes, it can limit contextual alignment.\nInterestingly, users often perceived stereotypical images to be more aligned\nwith their expectations. We discuss the need to balance ethical debiasing with\ncontextual relevance and call for T2I systems that support global diversity and\ninclusivity while not compromising the reflection of real-world social\ncomplexity.", "categories": ["cs.HC", "cs.AI", "cs.CL"], "published": "2025-05-27 04:01:03", "updated": "2025-05-27 04:01:03", "pdf_url": "http://arxiv.org/pdf/2505.20692v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20697v1", "title": "Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series", "authors": ["Zachary C. Brown", "David Carlson"], "abstract": "The field of hypothesis generation promises to reduce costs in neuroscience\nby narrowing the range of interventional studies needed to study various\nphenomena. Existing machine learning methods can generate scientific hypotheses\nfrom complex datasets, but many approaches assume causal relationships are\nstatic over time, limiting their applicability to systems with dynamic,\nstate-dependent behavior, such as the brain. While some techniques attempt\ndynamic causal discovery through factor models, they often restrict\nrelationships to linear patterns or impose other simplifying assumptions. We\npropose a novel method that models dynamic graphs as a conditionally weighted\nsuperposition of static graphs, where each static graph can capture nonlinear\nrelationships. This approach enables the detection of complex, time-varying\ninteractions between variables beyond linear limitations. Our method improves\nf1-scores of predicted dynamic causal patterns by roughly 22-28% on average\nover baselines in some of our experiments, with some improvements reaching well\nover 60%. A case study on real brain data demonstrates our method's ability to\nuncover relationships linked to specific behavioral states, offering valuable\ninsights into neural dynamics.", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "published": "2025-05-27 04:06:47", "updated": "2025-05-27 04:06:47", "pdf_url": "http://arxiv.org/pdf/2505.20697v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20707v1", "title": "Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Deepak Subramani"], "abstract": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, making them promising for educational applications. However,\ntheir capacity for complex reasoning, particularly in domains such as physics,\nremains underexplored. This study investigates the high school physics\nreasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),\nincluding instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.\nWe developed a comprehensive physics dataset from the OpenStax High School\nPhysics textbook, annotated according to Bloom's Taxonomy, with LaTeX and\nplaintext mathematical notations. A novel cultural contextualization approach\nwas applied to a subset, creating culturally adapted problems for Asian,\nAfrican, and South American/Australian contexts while preserving core physics\nprinciples. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,\nwe evaluated answer and reasoning chain correctness, along with calculation\naccuracy. The results reveal significant differences between the SLMs. Qwen 3\n1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was\nsubstantially low (38%). The format of the mathematical notation had a\nnegligible impact on performance. SLMs exhibited varied performance across the\nphysics topics and showed a decline in reasoning quality with increasing\ncognitive and knowledge complexity. In particular, the consistency of reasoning\nwas largely maintained in diverse cultural contexts, especially by better\nperforming models. These findings indicate that, while SLMs can often find\ncorrect answers, their underlying reasoning is frequently flawed, suggesting an\noverreliance on pattern recognition. For SLMs to become reliable educational\ntools in physics, future development must prioritize enhancing genuine\nunderstanding and the generation of sound, verifiable reasoning chains over\nmere answer accuracy.", "categories": ["cs.CL", "cs.AI", "physics.ed-ph"], "published": "2025-05-27 04:33:13", "updated": "2025-05-27 04:33:13", "pdf_url": "http://arxiv.org/pdf/2505.20707v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20714v1", "title": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting", "authors": ["Zechen Li", "Lanqing Yang", "Yiheng Bian", "Hao Pan", "Yongjian Fu", "Yezhou Wang", "Yi-Chao Chen", "Guangtao Xue", "Ju Ren"], "abstract": "This paper presents an innovative frequency-embedded 3D Gaussian splatting\n(3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling,\noffering an advancement over the existing works limited to single-frequency\nmodeling. Grounded in fundamental physics, we uncover the complex relationship\nbetween EM wave propagation behaviors and RF frequencies. Inspired by this, we\ndesign an EM feature network with attenuation and radiance modules to learn the\ncomplex relationships between RF frequencies and the key properties of each 3D\nGaussian, specifically the attenuation factor and RF signal intensity. By\ntraining the frequency-embedded 3DGS model, we can efficiently reconstruct RF\nradiance fields at arbitrary unknown frequencies within a given 3D environment.\nFinally, we propose a large-scale power angular spectrum (PAS) dataset\ncontaining 50000 samples ranging from 1 to 100 GHz in 6 indoor environments,\nand conduct extensive experiments to verify the effectiveness of our method.\nOur approach achieves an average Structural Similarity Index Measure (SSIM) up\nto 0.72, and a significant improvement up to 17.8% compared to the current\nstate-of-the-art (SOTA) methods trained on individual test frequencies.\nAdditionally, our method achieves an SSIM of 0.70 without prior training on\nthese frequencies, which represents only a 2.8% performance drop compared to\nmodels trained with full PAS data. This demonstrates our model's capability to\nestimate PAS at unknown frequencies. For related code and datasets, please\nrefer to https://github.com/sim-2-real/Wideband3DGS.", "categories": ["cs.NI", "cs.AI", "cs.LG"], "published": "2025-05-27 04:48:26", "updated": "2025-05-27 04:48:26", "pdf_url": "http://arxiv.org/pdf/2505.20714v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20718v1", "title": "VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models", "authors": ["Kui Wu", "Shuhang Xu", "Hao Chen", "Churan Wang", "Zhoujun Li", "Yizhou Wang", "Fangwei Zhong"], "abstract": "We introduce a novel self-improving framework that enhances Embodied Visual\nTracking (EVT) with Visual-Language Models (VLMs) to address the limitations of\ncurrent active visual tracking systems in recovering from tracking failure. Our\napproach combines the off-the-shelf active tracking methods with VLMs'\nreasoning capabilities, deploying a fast visual policy for normal tracking and\nactivating VLM reasoning only upon failure detection. The framework features a\nmemory-augmented self-reflection mechanism that enables the VLM to\nprogressively improve by learning from past experiences, effectively addressing\nVLMs' limitations in 3D spatial reasoning. Experimental results demonstrate\nsignificant performance improvements, with our framework boosting success rates\nby $72\\%$ with state-of-the-art RL-based approaches and $220\\%$ with PID-based\nmethods in challenging environments. This work represents the first integration\nof VLM-based reasoning to assist EVT agents in proactive failure recovery,\noffering substantial advances for real-world robotic applications that require\ncontinuous target monitoring in dynamic, unstructured environments. Project\nwebsite: https://sites.google.com/view/evt-recovery-assistant.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 04:53:50", "updated": "2025-05-27 04:53:50", "pdf_url": "http://arxiv.org/pdf/2505.20718v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20728v1", "title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models", "authors": ["Zesen Lyu", "Dandan Zhang", "Wei Ye", "Fangdi Li", "Zhihang Jiang", "Yao Yang"], "abstract": "Spatial reasoning is a core component of human cognition, enabling\nindividuals to perceive, comprehend, and interact with the physical world. It\nrelies on a nuanced understanding of spatial structures and inter-object\nrelationships, serving as the foundation for complex reasoning and\ndecision-making. To investigate whether current vision-language models (VLMs)\nexhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark\nconsisting of 1,100 carefully curated real-world images with high spatial\ncomplexity. Based on this dataset, we design five tasks to rigorously evaluate\nVLMs' spatial perception, structural understanding, and reasoning capabilities,\nwhile deliberately minimizing reliance on domain-specific knowledge to better\nisolate and assess the general spatial reasoning capability. We conduct a\ncomprehensive evaluation across 24 state-of-the-art VLMs. The results show that\neven the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy\nand performs particularly poorly on the Order Generation task, with only 30.00%\naccuracy, far below the performance exceeding 90% achieved by human\nparticipants. This persistent gap underscores the need for continued progress,\npositioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for\nadvancing spatial reasoning research in VLMs.", "categories": ["cs.AI"], "published": "2025-05-27 05:17:41", "updated": "2025-05-27 05:17:41", "pdf_url": "http://arxiv.org/pdf/2505.20728v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20730v1", "title": "What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals", "authors": ["Shahrooz Pouryousef"], "abstract": "User-item interactions contain rich collaborative signals that form the\nbackbone of many successful recommender systems. While recent work has explored\nthe use of large language models (LLMs) for recommendation, it remains unclear\nwhether LLMs can effectively reason over this type of collaborative\ninformation. In this paper, we conduct a systematic comparison between LLMs and\nclassical matrix factorization (MF) models to assess LLMs' ability to leverage\nuser-item interaction data. We further introduce a simple retrieval-augmented\ngeneration (RAG) method that enhances LLMs by grounding their predictions in\nstructured interaction data. Our experiments reveal that current LLMs often\nfall short in capturing collaborative patterns inherent to MF models, but that\nour RAG-based approach substantially improves recommendation\nquality-highlighting a promising direction for future LLM-based recommenders.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-27 05:18:57", "updated": "2025-05-27 05:18:57", "pdf_url": "http://arxiv.org/pdf/2505.20730v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20733v1", "title": "E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing", "authors": ["Cheonsu Jeong", "Seongmin Sim", "Hyoyoung Cho", "Sungsu Kim", "Byounggwan Shin"], "abstract": "This paper presents an intelligent work automation approach in the context of\ncontemporary digital transformation by integrating generative AI and\nIntelligent Document Processing (IDP) technologies with an Automation Agent to\nrealize End-to-End (E2E) automation of corporate financial expense processing\ntasks. While traditional Robotic Process Automation (RPA) has proven effective\nfor repetitive, rule-based simple task automation, it faces limitations in\nhandling unstructured data, exception management, and complex decision-making.\nThis study designs and implements a four-stage integrated process comprising\nautomatic recognition of supporting documents such as receipts via OCR/IDP,\nitem classification based on a policy-driven database, intelligent exception\nhandling supported by generative AI (large language models, LLMs), and\nhuman-in-the-loop final decision-making with continuous system learning through\nan Automation Agent. Applied to a major Korean enterprise (Company S), the\nsystem demonstrated quantitative benefits including over 80% reduction in\nprocessing time for paper receipt expense tasks, decreased error rates, and\nimproved compliance, as well as qualitative benefits such as enhanced accuracy\nand consistency, increased employee satisfaction, and data-driven decision\nsupport. Furthermore, the system embodies a virtuous cycle by learning from\nhuman judgments to progressively improve automatic exception handling\ncapabilities. Empirically, this research confirms that the organic integration\nof generative AI, IDP, and Automation Agents effectively overcomes the\nlimitations of conventional automation and enables E2E automation of complex\ncorporate processes. The study also discusses potential extensions to other\ndomains such as accounting, human resources, and procurement, and proposes\nfuture directions for AI-driven hyper-automation development.", "categories": ["cs.AI"], "published": "2025-05-27 05:21:08", "updated": "2025-05-27 05:21:08", "pdf_url": "http://arxiv.org/pdf/2505.20733v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20734v1", "title": "Adversarial bandit optimization for approximately linear functions", "authors": ["Zhuoyu Cheng", "Kohei Hatano", "Eiji Takimoto"], "abstract": "We consider a bandit optimization problem for nonconvex and non-smooth\nfunctions, where in each trial the loss function is the sum of a linear\nfunction and a small but arbitrary perturbation chosen after observing the\nplayer's choice. We give both expected and high probability regret bounds for\nthe problem. Our result also implies an improved high-probability regret bound\nfor the bandit linear optimization, a special case with no perturbation. We\nalso give a lower bound on the expected regret.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 05:22:01", "updated": "2025-05-27 05:22:01", "pdf_url": "http://arxiv.org/pdf/2505.20734v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20737v1", "title": "RRO: LLM Agent Optimization Through Rising Reward Trajectories", "authors": ["Zilong Wang", "Jingfeng Yang", "Sreyashi Nag", "Samarth Varshney", "Xianfeng Tang", "Haoming Jiang", "Jingbo Shang", "Sheikh Muhammad Sarwar"], "abstract": "Large language models (LLMs) have exhibited extraordinary performance in a\nvariety of tasks while it remains challenging for them to solve complex\nmulti-step tasks as agents. In practice, agents sensitive to the outcome of\ncertain key steps which makes them likely to fail the task because of a subtle\nmistake in the planning trajectory. Recent approaches resort to calibrating the\nreasoning process through reinforcement learning. They reward or penalize every\nreasoning step with process supervision, as known as Process Reward Models\n(PRMs). However, PRMs are difficult and costly to scale up with a large number\nof next action candidates since they require extensive computations to acquire\nthe training data through the per-step trajectory exploration. To mitigate this\nissue, we focus on the relative reward trend across successive reasoning steps\nand propose maintaining an increasing reward in the collected trajectories for\nprocess supervision, which we term Reward Rising Optimization (RRO).\nSpecifically, we incrementally augment the process supervision until\nidentifying a step exhibiting positive reward differentials, i.e. rising\nrewards, relative to its preceding iteration. This method dynamically expands\nthe search space for the next action candidates, efficiently capturing\nhigh-quality data. We provide mathematical groundings and empirical results on\nthe WebShop and InterCode-SQL benchmarks, showing that our proposed RRO\nachieves superior performance while requiring much less exploration cost.", "categories": ["cs.AI"], "published": "2025-05-27 05:27:54", "updated": "2025-05-27 05:27:54", "pdf_url": "http://arxiv.org/pdf/2505.20737v1", "comment": "preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20740v1", "title": "MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science", "authors": ["Xiangyu Zhao", "Wanghan Xu", "Bo Liu", "Yuhao Zhou", "Fenghua Ling", "Ben Fei", "Xiaoyu Yue", "Lei Bai", "Wenlong Zhang", "Xiao-Ming Wu"], "abstract": "The rapid advancement of multimodal large language models (MLLMs) has\nunlocked new opportunities to tackle complex scientific challenges. Despite\nthis progress, their application in addressing earth science problems,\nespecially at the graduate level, remains underexplored. A significant barrier\nis the absence of benchmarks that capture the depth and contextual complexity\nof geoscientific reasoning. Current benchmarks often rely on synthetic datasets\nor simplistic figure-caption pairs, which do not adequately reflect the\nintricate reasoning and domain-specific insights required for real-world\nscientific applications. To address these gaps, we introduce MSEarth, a\nmultimodal scientific benchmark curated from high-quality, open-access\nscientific publications. MSEarth encompasses the five major spheres of Earth\nscience: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere,\nfeaturing over 7K figures with refined captions. These captions are crafted\nfrom the original figure captions and enriched with discussions and reasoning\nfrom the papers, ensuring the benchmark captures the nuanced reasoning and\nknowledge-intensive content essential for advanced scientific tasks. MSEarth\nsupports a variety of tasks, including scientific figure captioning, multiple\nchoice questions, and open-ended reasoning challenges. By bridging the gap in\ngraduate-level benchmarks, MSEarth provides a scalable and high-fidelity\nresource to enhance the development and evaluation of MLLMs in scientific\nreasoning. The benchmark is publicly available to foster further research and\ninnovation in this field. Resources related to this benchmark can be found at\nhttps://huggingface.co/MSEarth and https://github.com/xiangyu-mm/MSEarth.", "categories": ["cs.AI"], "published": "2025-05-27 05:30:35", "updated": "2025-05-27 05:30:35", "pdf_url": "http://arxiv.org/pdf/2505.20740v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20749v1", "title": "Can Agents Fix Agent Issues?", "authors": ["Alfin Wijaya Rahardja", "Junwei Liu", "Weitong Chen", "Zhenpeng Chen", "Yiling Lou"], "abstract": "LLM-based agent systems are emerging as a new software paradigm and have been\nwidely adopted across diverse domains such as medicine, robotics, and\nprogramming. However, maintaining these systems requires substantial effort, as\nthey are inevitably prone to bugs and continually evolve to meet changing\nexternal requirements. Therefore, automatically resolving agent issues (i.e.,\nbug reports or feature requests) is a crucial and challenging task. While\nrecent software engineering (SE) agents (e.g., SWE-agent) have shown promise in\naddressing issues in traditional software systems, it remains unclear how\neffectively they can resolve real-world issues in agent systems, which differ\nsignificantly from traditional software. To fill this gap, we first manually\nanalyze 201 real-world agent issues and identify common categories of agent\nissues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a\nreproducible benchmark comprising 50 agent issue resolution tasks (each with an\nexecutable environment and failure-triggering tests). We further evaluate\nstate-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited\neffectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results\nunderscore the unique challenges of maintaining agent systems compared to\ntraditional software, highlighting the need for further research to develop\nadvanced SE agents for resolving agent issues. Data and code are available at\nhttps://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .", "categories": ["cs.AI", "cs.SE"], "published": "2025-05-27 05:45:03", "updated": "2025-05-27 05:45:03", "pdf_url": "http://arxiv.org/pdf/2505.20749v1", "comment": "18 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20751v1", "title": "Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform", "authors": ["Zongcai Tan amd Dandan Zhang"], "abstract": "Optical tweezers (OT) offer unparalleled capabilities for micromanipulation\nwith submicron precision in biomedical applications. However, controlling\nconventional multi-trap OT to achieve cooperative manipulation of multiple\ncomplex-shaped microrobots in dynamic environments poses a significant\nchallenge. To address this, we introduce Interactive OT Gym, a reinforcement\nlearning (RL)-based simulation platform designed for OT-driven microrobotics.\nOur platform supports complex physical field simulations and integrates haptic\nfeedback interfaces, RL modules, and context-aware shared control strategies\ntailored for OT-driven microrobot in cooperative biological object manipulation\ntasks. This integration allows for an adaptive blend of manual and autonomous\ncontrol, enabling seamless transitions between human input and autonomous\noperation. We evaluated the effectiveness of our platform using a cell\nmanipulation task. Experimental results show that our shared control system\nsignificantly improves micromanipulation performance, reducing task completion\ntime by approximately 67% compared to using pure human or RL control alone and\nachieving a 100% success rate. With its high fidelity, interactivity, low cost,\nand high-speed simulation capabilities, Interactive OT Gym serves as a\nuser-friendly training and testing environment for the development of advanced\ninteractive OT-driven micromanipulation systems and control algorithms. For\nmore details on the project, please see our website\nhttps://sites.google.com/view/otgym", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-27 05:50:11", "updated": "2025-05-27 05:50:11", "pdf_url": "http://arxiv.org/pdf/2505.20751v1", "comment": "ICRA 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20753v1", "title": "Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models", "authors": ["Yufei Zhan", "Hongyin Zhao", "Yousong Zhu", "Shurong Zheng", "Fan Yang", "Ming Tang", "Jinqiao Wang"], "abstract": "Large Multimodal Models (LMMs) have recently demonstrated remarkable visual\nunderstanding performance on both vision-language and vision-centric tasks.\nHowever, they often fall short in integrating advanced, task-specific\ncapabilities for compositional reasoning, which hinders their progress toward\ntruly competent general vision models. To address this, we present a unified\nvisual reasoning mechanism that enables LMMs to solve complicated compositional\nproblems by leveraging their intrinsic capabilities (e.g. grounding and visual\nunderstanding capabilities). Different from the previous shortcut learning\nmechanism, our approach introduces a human-like\nunderstanding-thinking-answering process, allowing the model to complete all\nsteps in a single pass forwarding without the need for multiple inferences or\nexternal tools. This design bridges the gap between foundational visual\ncapabilities and general question answering, encouraging LMMs to generate\nfaithful and traceable responses for complex visual reasoning. Meanwhile, we\ncurate 334K visual instruction samples covering both general scenes and\ntext-rich scenes and involving multiple foundational visual capabilities. Our\ntrained model, Griffon-R, has the ability of end-to-end automatic\nunderstanding, self-thinking, and reasoning answers. Comprehensive experiments\nshow that Griffon-R not only achieves advancing performance on complex visual\nreasoning benchmarks including VSR and CLEVR, but also enhances multimodal\ncapabilities across various benchmarks like MMBench and ScienceQA. Data,\nmodels, and codes will be release at\nhttps://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 05:50:25", "updated": "2025-05-27 05:50:25", "pdf_url": "http://arxiv.org/pdf/2505.20753v1", "comment": "Tech report", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20759v1", "title": "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding", "authors": ["Ansel Blume", "Jeonghwan Kim", "Hyeonjeong Ha", "Elen Chatikyan", "Xiaomeng Jin", "Khanh Duy Nguyen", "Nanyun Peng", "Kai-Wei Chang", "Derek Hoiem", "Heng Ji"], "abstract": "Real-world objects are composed of distinctive, object-specific parts.\nIdentifying these parts is key to performing fine-grained, compositional\nreasoning-yet, large multimodal models (LMMs) struggle to perform this\nseemingly straightforward task. In this work, we introduce PARTONOMY, an LMM\nbenchmark designed for pixel-level part grounding. We construct PARTONOMY from\nexisting part datasets and our own rigorously annotated set of images,\nencompassing 862 part labels and 534 object labels for evaluation. Unlike\nexisting datasets that simply ask models to identify generic parts, PARTONOMY\nuses specialized concepts (e.g., agricultural airplane), and challenges models\nto compare objects' parts, consider part-whole relationships, and justify\ntextual predictions with visual segmentations. Our experiments demonstrate\nsignificant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only\n5.9% gIoU), highlighting a critical gap in their part grounding abilities. We\nnote that existing segmentation-enabled LMMs (segmenting LMMs) have two key\narchitectural shortcomings: they use special [SEG] tokens not seen during\npretraining which induce distribution shift, and they discard predicted\nsegmentations instead of using past predictions to guide future ones. To\naddress these deficiencies, we train several part-centric LMMs and propose\nPLUM, a novel segmenting LMM that uses span tagging instead of segmentation\ntokens and that conditions on prior predictions in a feedback loop. We find\nthat pretrained PLUM outperforms existing segmenting LMMs on reasoning\nsegmentation, VQA, and visual hallucination benchmarks. In addition, PLUM\nfinetuned on our proposed Explanatory Part Segmentation task is competitive\nwith segmenting LMMs trained on significantly more segmentation data. Our work\nopens up new avenues towards enabling fine-grained, grounded visual\nunderstanding in LMMs.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 06:03:56", "updated": "2025-05-27 06:03:56", "pdf_url": "http://arxiv.org/pdf/2505.20759v1", "comment": "18 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20767v1", "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models", "authors": ["Xiaqiang Tang", "Jian Li", "Keyu Hu", "Du Nan", "Xiaolong Li", "Xi Zhang", "Weigao Sun", "Sihong Xie"], "abstract": "Faithfulness hallucination are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandard, existing benchmarks only contain \"factual statements\" that rephrase\nsource materials without marking \"cognitive statements\" that make inference\nfrom the given context, making the consistency evaluation and optimization of\ncognitive statements difficult. Inspired by how an evidence is assessed in the\nlegislative domain, we design a rigorous framework to assess different levels\nof faithfulness of cognitive statements and create a benchmark dataset where we\nreveal insightful statistics. We design an annotation pipeline to create larger\nbenchmarks for different LLMs automatically, and the resulting larger-scale\nCogniBench-L dataset can be used to train accurate cognitive hallucination\ndetection model. We release our model and dataset at:\nhttps://github.com/FUTUREEEEEE/CogniBench", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 06:16:27", "updated": "2025-05-27 06:16:27", "pdf_url": "http://arxiv.org/pdf/2505.20767v1", "comment": "ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20771v1", "title": "Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems", "authors": ["Heng Tang", "Feng Liu", "Xinbo Chen", "Jiawei Chen", "Bohao Wang", "Changwang Zhang", "Jun Wang", "Yuegang Sun", "Bingde Hu", "Can Wang"], "abstract": "Recent years have witnessed extensive exploration of Large Language Models\n(LLMs) on the field of Recommender Systems (RS). There are currently two\ncommonly used strategies to enable LLMs to have recommendation capabilities: 1)\nThe \"Guidance-Only\" strategy uses in-context learning to exploit and amplify\nthe inherent semantic understanding and item recommendation capabilities of\nLLMs; 2) The \"Tuning-Only\" strategy uses supervised fine-tuning (SFT) to\nfine-tune LLMs with the aim of fitting them to real recommendation data.\nHowever, neither of these strategies can effectively bridge the gap between the\nknowledge space of LLMs and recommendation, and their performance do not meet\nour expectations.\n  To better enable LLMs to learn recommendation knowledge, we combine the\nadvantages of the above two strategies and proposed a novel \"Guidance+Tuning\"\nmethod called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of\ncurriculum learning. It first employs self-distillation to construct an\nauxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it\nfurther utilizes a self-adaptive curriculum scheduler to enable LLMs to\ngradually learn from simpler data (self-distilled data) to more challenging\ndata (real RS data). Extensive experiments demonstrate that SOFT significantly\nenhances the recommendation accuracy (37.59\\% on average) of LLM-based methods.\nThe code is available via\nhttps://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E", "categories": ["cs.IR", "cs.AI"], "published": "2025-05-27 06:22:50", "updated": "2025-05-27 06:22:50", "pdf_url": "http://arxiv.org/pdf/2505.20771v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20776v1", "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences", "authors": ["Jungyoub Cha", "Hyunjong Kim", "Sungzoon Cho"], "abstract": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; C.4"], "published": "2025-05-27 06:30:00", "updated": "2025-05-27 06:30:00", "pdf_url": "http://arxiv.org/pdf/2505.20776v1", "comment": "8 pages, 3 figures. Under review at EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20783v1", "title": "FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation", "authors": ["Jiaping Xiao", "Cheng Wen Tsao", "Yuhang Zhang", "Mir Feroskhan"], "abstract": "Path planning is a critical component in autonomous drone operations,\nenabling safe and efficient navigation through complex environments. Recent\nadvances in foundation models, particularly large language models (LLMs) and\nvision-language models (VLMs), have opened new opportunities for enhanced\nperception and intelligent decision-making in robotics. However, their\npractical applicability and effectiveness in global path planning remain\nrelatively unexplored. This paper proposes foundation model-guided path\nplanners (FM-Planner) and presents a comprehensive benchmarking study and\npractical validation for drone path planning. Specifically, we first\nsystematically evaluate eight representative LLM and VLM approaches using\nstandardized simulation scenarios. To enable effective real-time navigation, we\nthen design an integrated LLM-Vision planner that combines semantic reasoning\nwith visual perception. Furthermore, we deploy and validate the proposed path\nplanner through real-world experiments under multiple configurations. Our\nfindings provide valuable insights into the strengths, limitations, and\nfeasibility of deploying foundation models in real-world drone applications and\nproviding practical implementations in autonomous flight. Project site:\nhttps://github.com/NTU-ICG/FM-Planner.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-27 06:41:21", "updated": "2025-05-27 06:41:21", "pdf_url": "http://arxiv.org/pdf/2505.20783v1", "comment": "This work has been submitted for possible publication", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20793v1", "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation", "authors": ["Juan A. Rodriguez", "Haotian Zhang", "Abhay Puri", "Aarash Feizi", "Rishav Pramanik", "Pascal Wichmann", "Arnab Mondal", "Mohammad Reza Samsami", "Rabiul Awal", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar", "David Vazquez", "Christopher Pal", "Marco Pedersoli"], "abstract": "Scalable Vector Graphics (SVG) offer a powerful format for representing\nvisual designs as interpretable code. Recent advances in vision-language models\n(VLMs) have enabled high-quality SVG generation by framing the problem as a\ncode generation task and leveraging large-scale pretraining. VLMs are\nparticularly suitable for this task as they capture both global semantics and\nfine-grained visual patterns, while transferring knowledge across vision,\nnatural language, and code domains. However, existing VLM approaches often\nstruggle to produce faithful and efficient SVGs because they never observe the\nrendered images during training. Although differentiable rendering for\nautoregressive SVG code generation remains unavailable, rendered outputs can\nstill be compared to original inputs, enabling evaluative feedback suitable for\nreinforcement learning (RL). We introduce RLRF(Reinforcement Learning from\nRendering Feedback), an RL method that enhances SVG generation in\nautoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an\ninput image, the model generates SVG roll-outs that are rendered and compared\nto the original image to compute a reward. This visual fidelity feedback guides\nthe model toward producing more accurate, efficient, and semantically coherent\nSVGs. RLRF significantly outperforms supervised fine-tuning, addressing common\nfailure modes and enabling precise, high-quality SVG generation with strong\nstructural understanding and generalization.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 06:56:00", "updated": "2025-05-27 06:56:00", "pdf_url": "http://arxiv.org/pdf/2505.20793v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20794v1", "title": "VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion", "authors": ["Joon-Seung Choi", "Dong-Min Byun", "Hyung-Seok Oh", "Seong-Whan Lee"], "abstract": "Controlling singing style is crucial for achieving an expressive and natural\nsinging voice. Among the various style factors, vibrato plays a key role in\nconveying emotions and enhancing musical depth. However, modeling vibrato\nremains challenging due to its dynamic nature, making it difficult to control\nin singing voice conversion. To address this, we propose VibESVC, a\ncontrollable singing voice conversion model that explicitly extracts and\nmanipulates vibrato using discrete wavelet transform. Unlike previous methods\nthat model vibrato implicitly, our approach decomposes the F0 contour into\nfrequency components, enabling precise transfer. This allows vibrato control\nfor enhanced flexibility. Experimental results show that VibE-SVC effectively\ntransforms singing styles while preserving speaker similarity. Both subjective\nand objective evaluations confirm high-quality conversion.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-27 06:56:13", "updated": "2025-05-27 06:56:13", "pdf_url": "http://arxiv.org/pdf/2505.20794v1", "comment": "Proceedings of Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20813v1", "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph", "authors": ["Junsik Kim", "Jinwook Park", "Kangil Kim"], "abstract": "In knowledge graph embedding, leveraging relation-specific\nentity-transformation has markedly enhanced performance. However, the\nconsistency of embedding differences before and after transformation remains\nunaddressed, risking the loss of valuable inductive bias inherent in the\nembeddings. This inconsistency stems from two problems. First, transformation\nrepresentations are specified for relations in a disconnected manner, allowing\ndissimilar transformations and corresponding entity-embeddings for similar\nrelations. Second, a generalized plug-in approach as a SFBR (Semantic Filter\nBased on Relations) disrupts this consistency through excessive concentration\nof entity embeddings under entity-based regularization, generating\nindistinguishable score distributions among relations. In this paper, we\nintroduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),\ncontaining more consistent entity-transformation characterized by three\nfeatures: 1) shared affine transformation of relation embeddings across all\nrelations, 2) rooted entity-transformation that adds an entity embedding to its\nchange represented by the transformed vector, and 3) normalization of the\nchange to prevent scale reduction. To amplify the advantages of consistency\nthat preserve semantics on embeddings, RSCF adds relation transformation and\nprediction modules for enhancing the semantics. In knowledge graph completion\ntasks with distance-based and tensor decomposition models, RSCF significantly\noutperforms state-of-the-art KGE methods, showing robustness across all\nrelations and their frequencies.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 07:22:00", "updated": "2025-05-27 07:22:00", "pdf_url": "http://arxiv.org/pdf/2505.20813v1", "comment": "Accepted to ACL 2025, 17 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20820v1", "title": "MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization", "authors": ["Hyomin Kim", "Yunhui Jang", "Sungsoo Ahn"], "abstract": "Large language models (LLMs) have large potential for molecular optimization,\nas they can gather external chemistry tools and enable collaborative\ninteractions to iteratively refine molecular candidates. However, this\npotential remains underexplored, particularly in the context of structured\nreasoning, interpretability, and comprehensive tool-grounded molecular\noptimization. To address this gap, we introduce MT-Mol, a multi-agent framework\nfor molecular optimization that leverages tool-guided reasoning and\nrole-specialized LLM agents. Our system incorporates comprehensive RDKit tools,\ncategorized into five distinct domains: structural descriptors, electronic and\ntopological features, fragment-based functional groups, molecular\nrepresentations, and miscellaneous chemical properties. Each category is\nmanaged by an expert analyst agent, responsible for extracting task-relevant\ntools and enabling interpretable, chemically grounded feedback. MT-Mol produces\nmolecules with tool-aligned and stepwise reasoning through the interaction\nbetween the analyst agents, a molecule-generating scientist, a reasoning-output\nverifier, and a reviewer agent. As a result, we show that our framework shows\nthe state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.", "categories": ["cs.AI"], "published": "2025-05-27 07:27:30", "updated": "2025-05-27 07:27:30", "pdf_url": "http://arxiv.org/pdf/2505.20820v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20824v1", "title": "MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems", "authors": ["Kai Chen", "Taihang Zhen", "Hewei Wang", "Kailai Liu", "Xinfeng Li", "Jing Huo", "Tianpei Yang", "Jinfeng Xu", "Wei Dong", "Yang Gao"], "abstract": "As large language models (LLMs) are increasingly deployed in healthcare,\nensuring their safety, particularly within collaborative multi-agent\nconfigurations, is paramount. In this paper we introduce MedSentry, a benchmark\ncomprising 5 000 adversarial medical prompts spanning 25 threat categories with\n100 subthemes. Coupled with this dataset, we develop an end-to-end\nattack-defense evaluation pipeline to systematically analyze how four\nrepresentative multi-agent topologies (Layers, SharedPool, Centralized, and\nDecentralized) withstand attacks from 'dark-personality' agents. Our findings\nreveal critical differences in how these architectures handle information\ncontamination and maintain robust decision-making, exposing their underlying\nvulnerability mechanisms. For instance, SharedPool's open information sharing\nmakes it highly susceptible, whereas Decentralized architectures exhibit\ngreater resilience thanks to inherent redundancy and isolation. To mitigate\nthese risks, we propose a personality-scale detection and correction mechanism\nthat identifies and rehabilitates malicious agents, restoring system safety to\nnear-baseline levels. MedSentry thus furnishes both a rigorous evaluation\nframework and practical defense strategies that guide the design of safer\nLLM-based multi-agent systems in medical domains.", "categories": ["cs.MA", "cs.AI"], "published": "2025-05-27 07:34:40", "updated": "2025-05-27 07:34:40", "pdf_url": "http://arxiv.org/pdf/2505.20824v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20853v1", "title": "Cooperation of Experts: Fusing Heterogeneous Information with Large Margin", "authors": ["Shuo Wang", "Shunyang Huang", "Jinghui Yuan", "Zhixiang Shen", "Zhao Kang"], "abstract": "Fusing heterogeneous information remains a persistent challenge in modern\ndata analysis. While significant progress has been made, existing approaches\noften fail to account for the inherent heterogeneity of object patterns across\ndifferent semantic spaces. To address this limitation, we propose the\nCooperation of Experts (CoE) framework, which encodes multi-typed information\ninto unified heterogeneous multiplex networks. By overcoming modality and\nconnection differences, CoE provides a powerful and flexible model for\ncapturing the intricate structures of real-world complex data. In our\nframework, dedicated encoders act as domain-specific experts, each specializing\nin learning distinct relational patterns in specific semantic spaces. To\nenhance robustness and extract complementary knowledge, these experts\ncollaborate through a novel large margin mechanism supported by a tailored\noptimization strategy. Rigorous theoretical analyses guarantee the framework's\nfeasibility and stability, while extensive experiments across diverse\nbenchmarks demonstrate its superior performance and broad applicability. Our\ncode is available at https://github.com/strangeAlan/CoE.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 08:04:32", "updated": "2025-05-27 08:04:32", "pdf_url": "http://arxiv.org/pdf/2505.20853v1", "comment": "Published in ICML 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20854v1", "title": "An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks", "authors": ["Xin Zhou", "Kisub Kim", "Ting Zhang", "Martin Weyssow", "Luis F. Gomes", "Guang Yang", "David Lo"], "abstract": "Large Language Models (LLMs) and other automated techniques have been\nincreasingly used to support software developers by generating software\nartifacts such as code snippets, patches, and comments. However, accurately\nassessing the correctness of these generated artifacts remains a significant\nchallenge. On one hand, human evaluation provides high accuracy but is\nlabor-intensive and lacks scalability. On the other hand, other existing\nautomatic evaluation metrics are scalable and require minimal human effort, but\nthey often fail to accurately reflect the actual correctness of generated\nsoftware artifacts.\n  In this paper, we present SWE-Judge, the first evaluation metric for\nLLM-as-Ensemble-Judge specifically designed to accurately assess the\ncorrectness of generated software artifacts. SWE-Judge first defines five\ndistinct evaluation strategies, each implemented as an independent judge. A\ndynamic team selection mechanism then identifies the most appropriate subset of\njudges to produce a final correctness score through ensembling. We evaluate\nSWE-Judge across a diverse set of software engineering (SE) benchmarks,\nincluding CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess.\nThese benchmarks span three SE tasks: code generation, automated program\nrepair, and code summarization. Experimental results demonstrate that SWE-Judge\nconsistently achieves a higher correlation with human judgments, with\nimprovements ranging from 5.9% to 183.8% over existing automatic metrics.\nFurthermore, SWE-Judge reaches agreement levels with human annotators that are\ncomparable to inter-annotator agreement in code generation and program repair\ntasks. These findings underscore SWE-Judge's potential as a scalable and\nreliable alternative to human evaluation.", "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2025-05-27 08:04:34", "updated": "2025-05-27 08:04:34", "pdf_url": "http://arxiv.org/pdf/2505.20854v1", "comment": "20 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20866v1", "title": "Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification", "authors": ["Xinjie Lin", "Gang Xiong", "Gaopeng Gou", "Wenqi Dong", "Jing Yu", "Zhen Li", "Wei Xia"], "abstract": "Encrypted traffic classification is highly challenging in network security\ndue to the need for extracting robust features from content-agnostic traffic\ndata. Existing approaches face critical issues: (i) Distribution drift, caused\nby reliance on the closedworld assumption, limits adaptability to realworld,\nshifting patterns; (ii) Dependence on labeled data restricts applicability\nwhere such data is scarce or unavailable. Large language models (LLMs) have\ndemonstrated remarkable potential in offering generalizable solutions across a\nwide range of tasks, achieving notable success in various specialized fields.\nHowever, their effectiveness in traffic analysis remains constrained by\nchallenges in adapting to the unique requirements of the traffic domain. In\nthis paper, we introduce a novel traffic representation model named Encrypted\nTraffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which\nintegrates LLMs with knowledge of traffic structures through a self-supervised\ninstruction tuning paradigm. This framework establishes connections between\ntextual information and traffic interactions. ETooL demonstrates more robust\nclassification performance and superior generalization in both supervised and\nzero-shot traffic classification tasks. Notably, it achieves significant\nimprovements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%),\nAPP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.)\nto 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic\ndataset designed to support dynamic distributional shifts, and use it to\nvalidate ETooL's effectiveness under varying distributional conditions.\nFurthermore, we evaluate the efficiency gains achieved through ETooL's\ninstruction tuning approach.", "categories": ["cs.CR", "cs.AI", "cs.NI"], "published": "2025-05-27 08:18:16", "updated": "2025-05-27 08:18:16", "pdf_url": "http://arxiv.org/pdf/2505.20866v1", "comment": "IEEE Transactions on Information Forensics and Security (TIFS) camera\n  ready, 15 pages, 6 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20868v1", "title": "Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech", "authors": ["Nam-Gyu Kim", "Deok-Hyeon Cho", "Seung-Bin Kim", "Seong-Whan Lee"], "abstract": "Recent advances in expressive text-to-speech (TTS) have introduced diverse\nmethods based on style embedding extracted from reference speech. However,\nsynthesizing high-quality expressive speech remains challenging. We propose\nSpotlight-TTS, which exclusively emphasizes style via voiced-aware style\nextraction and style direction adjustment. Voiced-aware style extraction\nfocuses on voiced regions highly related to style while maintaining continuity\nacross different speech regions to improve expressiveness. We adjust the\ndirection of the extracted style for optimal integration into the TTS model,\nwhich improves speech quality. Experimental results demonstrate that\nSpotlight-TTS achieves superior performance compared to baseline models in\nterms of expressiveness, overall speech quality, and style transfer capability.\nOur audio samples are publicly available.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-27 08:20:01", "updated": "2025-05-27 08:20:01", "pdf_url": "http://arxiv.org/pdf/2505.20868v1", "comment": "Submitted to Interspeech", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20869v1", "title": "Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving", "authors": ["Kuo Zhou", "Lu Zhang"], "abstract": "Large Language Models (LLMs) have demonstrated formidable capabilities in\nsolving mathematical problems, yet they may still commit logical reasoning and\ncomputational errors during the problem-solving process. Thus, this paper\nproposes a framework, MATH-VF, which includes a Formalizer and a Critic, for\nformally verifying the correctness of the solutions generated by large language\nmodels. Our framework first utilizes a Formalizer which employs an LLM to\ntranslate a natural language solution into a formal context. Afterward, our\nCritic (which integrates various external tools such as a Computer Algebra\nSystem and an SMT solver) evaluates the correctness of each statement within\nthe formal context, and when a statement is incorrect, our Critic provides\ncorrective feedback. We empirically investigate the effectiveness of MATH-VF in\ntwo scenarios: 1) Verification: MATH-VF is utilized to determine the\ncorrectness of a solution to a given problem. 2) Refinement: When MATH-VF\nidentifies errors in the solution generated by an LLM-based solution generator\nfor a given problem, it submits the corrective suggestions proposed by the\nCritic to the solution generator to regenerate the solution. We evaluate our\nframework on widely used mathematical benchmarks: MATH500 and ProcessBench,\ndemonstrating the superiority of our approach over existing approaches.", "categories": ["cs.AI"], "published": "2025-05-27 08:21:07", "updated": "2025-05-27 08:21:07", "pdf_url": "http://arxiv.org/pdf/2505.20869v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20872v1", "title": "In Context Learning with Vision Transformers: Case Study", "authors": ["Antony Zhao", "Alex Proshkin", "Fergal Hennessy", "Francesco Crivelli"], "abstract": "Large transformer models have been shown to be capable of performing\nin-context learning. By using examples in a prompt as well as a query, they are\ncapable of performing tasks such as few-shot, one-shot, or zero-shot learning\nto output the corresponding answer to this query. One area of interest to us is\nthat these transformer models have been shown to be capable of learning the\ngeneral class of certain functions, such as linear functions and small 2-layer\nneural networks, on random data (Garg et al, 2023). We aim to extend this to\nthe image space to analyze their capability to in-context learn more complex\nfunctions on the image space, such as convolutional neural networks and other\nmethods.", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.2.10; I.4.8"], "published": "2025-05-27 08:22:08", "updated": "2025-05-27 08:22:08", "pdf_url": "http://arxiv.org/pdf/2505.20872v1", "comment": "12 pages, 16 figures. UC Berkeley research project", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20875v1", "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties", "authors": ["Jiyoung Lee", "Seungho Kim", "Jieun Han", "Jun-Min Lee", "Kitaek Kim", "Alice Oh", "Edward Choi"], "abstract": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our\n\\href{https://github.com/jiyounglee-0523/TransEnV}{code} and\n\\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}\nare publicly available.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 08:23:27", "updated": "2025-05-27 08:23:27", "pdf_url": "http://arxiv.org/pdf/2505.20875v1", "comment": "27 pages, 6 figures, 16 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20881v1", "title": "Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization", "authors": ["Yiding Shi", "Jianan Zhou", "Wen Song", "Jieyi Bi", "Yaoxin Wu", "Jie Zhang"], "abstract": "Heuristic design with large language models (LLMs) has emerged as a promising\napproach for tackling combinatorial optimization problems (COPs). However,\nexisting approaches often rely on manually predefined evolutionary computation\n(EC) optimizers and single-task training schemes, which may constrain the\nexploration of diverse heuristic algorithms and hinder the generalization of\nthe resulting heuristics. To address these issues, we propose Meta-Optimization\nof Heuristics (MoH), a novel framework that operates at the optimizer level,\ndiscovering effective optimizers through the principle of meta-learning.\nSpecifically, MoH leverages LLMs to iteratively refine a meta-optimizer that\nautonomously constructs diverse optimizers through (self-)invocation, thereby\neliminating the reliance on a predefined EC optimizer. These constructed\noptimizers subsequently evolve heuristics for downstream tasks, enabling\nbroader heuristic exploration. Moreover, MoH employs a multi-task training\nscheme to promote its generalization capability. Experiments on classic COPs\ndemonstrate that MoH constructs an effective and interpretable meta-optimizer,\nachieving state-of-the-art performance across various downstream tasks,\nparticularly in cross-size settings.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 08:26:27", "updated": "2025-05-27 08:26:27", "pdf_url": "http://arxiv.org/pdf/2505.20881v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20888v1", "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models", "authors": ["Chengyu Wang", "Junbing Yan", "Wenrui Cai", "Yuanhao Yue", "Jun Huang"], "abstract": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 08:32:51", "updated": "2025-05-27 08:32:51", "pdf_url": "http://arxiv.org/pdf/2505.20888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20889v1", "title": "Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment", "authors": ["Leizhen Wang", "Peibo Duan", "Cheng Lyu", "Zhenliang Ma"], "abstract": "Modern navigation systems and shared mobility platforms increasingly rely on\npersonalized route recommendations to improve individual travel experience and\noperational efficiency. However, a key question remains: can such sequential,\npersonalized routing decisions collectively lead to system-optimal (SO) traffic\nassignment? This paper addresses this question by proposing a learning-based\nframework that reformulates the static SO traffic assignment problem as a\nsingle-agent deep reinforcement learning (RL) task. A central agent\nsequentially recommends routes to travelers as origin-destination (OD) demands\narrive, to minimize total system travel time. To enhance learning efficiency\nand solution quality, we develop an MSA-guided deep Q-learning algorithm that\nintegrates the iterative structure of traditional traffic assignment methods\ninto the RL training process. The proposed approach is evaluated on both the\nBraess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent\nconverges to the theoretical SO solution in the Braess network and achieves\nonly a 0.35% deviation in the OW network. Further ablation studies demonstrate\nthat the route action set's design significantly impacts convergence speed and\nfinal performance, with SO-informed route sets leading to faster learning and\nbetter outcomes. This work provides a theoretically grounded and practically\nrelevant approach to bridging individual routing behavior with system-level\nefficiency through learning-based sequential assignment.", "categories": ["cs.AI"], "published": "2025-05-27 08:33:02", "updated": "2025-05-27 08:33:02", "pdf_url": "http://arxiv.org/pdf/2505.20889v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20890v1", "title": "Frequency Composition for Compressed and Domain-Adaptive Neural Networks", "authors": ["Yoojin Kwon", "Hongjun Suh", "Wooseok Lee", "Taesik Gong", "Songyi Han", "Hyung-Sin Kim"], "abstract": "Modern on-device neural network applications must operate under resource\nconstraints while adapting to unpredictable domain shifts. However, this\ncombined challenge-model compression and domain adaptation-remains largely\nunaddressed, as prior work has tackled each issue in isolation: compressed\nnetworks prioritize efficiency within a fixed domain, whereas large, capable\nmodels focus on handling domain shifts. In this work, we propose CoDA, a\nfrequency composition-based framework that unifies compression and domain\nadaptation. During training, CoDA employs quantization-aware training (QAT)\nwith low-frequency components, enabling a compressed model to selectively learn\nrobust, generalizable features. At test time, it refines the compact model in a\nsource-free manner (i.e., test-time adaptation, TTA), leveraging the\nfull-frequency information from incoming data to adapt to target domains while\ntreating high-frequency components as domain-specific cues. LFC are aligned\nwith the trained distribution, while HFC unique to the target distribution are\nsolely utilized for batch normalization. CoDA can be integrated synergistically\ninto existing QAT and TTA methods. CoDA is evaluated on widely used\ndomain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various\nmodel architectures. With significant compression, it achieves accuracy\nimprovements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the\nfull-precision TTA baseline.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 08:33:04", "updated": "2025-05-27 08:33:04", "pdf_url": "http://arxiv.org/pdf/2505.20890v1", "comment": "Work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20896v1", "title": "How Do Transformers Learn Variable Binding in Symbolic Programs?", "authors": ["Yiwei Wu", "Atticus Geiger", "Rapha\u00ebl Milli\u00e8re"], "abstract": "Variable binding -- the ability to associate variables with values -- is\nfundamental to symbolic computation and cognition. Although classical\narchitectures typically implement variable binding via addressable memory, it\nis not well understood how modern neural networks lacking built-in binding\noperations may acquire this capacity. We investigate this by training a\nTransformer to dereference queried variables in symbolic programs where\nvariables are assigned either numerical constants or other variables. Each\nprogram requires following chains of variable assignments up to four steps deep\nto find the queried value, and also contains irrelevant chains of assignments\nacting as distractors. Our analysis reveals a developmental trajectory with\nthree distinct phases during training: (1) random prediction of numerical\nconstants, (2) a shallow heuristic prioritizing early variable assignments, and\n(3) the emergence of a systematic mechanism for dereferencing assignment\nchains. Using causal interventions, we find that the model learns to exploit\nthe residual stream as an addressable memory space, with specialized attention\nheads routing information across token positions. This mechanism allows the\nmodel to dynamically track variable bindings across layers, resulting in\naccurate dereferencing. Our results show how Transformer models can learn to\nimplement systematic variable binding without explicit architectural support,\nbridging connectionist and symbolic approaches.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-27 08:39:20", "updated": "2025-05-27 08:39:20", "pdf_url": "http://arxiv.org/pdf/2505.20896v1", "comment": "16 pages, 10 figures, 1 table. To appear in the Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20897v1", "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation", "authors": ["Pingrui Zhang", "Yifei Su", "Pengyuan Wu", "Dong An", "Li Zhang", "Zhigang Wang", "Dong Wang", "Yan Ding", "Bin Zhao", "Xuelong Li"], "abstract": "Vision-and-Language Navigation (VLN) requires the agent to navigate by\nfollowing natural instructions under partial observability, making it difficult\nto align perception with language. Recent methods mitigate this by imagining\nfuture scenes, yet they rely on vision-based synthesis, leading to high\ncomputational cost and redundant details. To this end, we propose to adaptively\nimagine key environmental semantics via \\textit{language} form, enabling a more\nreliable and efficient strategy. Specifically, we introduce a novel Adaptive\nText Dreamer (ATD), a dual-branch self-guided imagination policy built upon a\nlarge language model (LLM). ATD is designed with a human-like left-right brain\narchitecture, where the left brain focuses on logical integration, and the\nright brain is responsible for imaginative prediction of future scenes. To\nachieve this, we fine-tune only the Q-former within both brains to efficiently\nactivate domain-specific knowledge in the LLM, enabling dynamic updates of\nlogical reasoning and imagination during navigation. Furthermore, we introduce\na cross-interaction mechanism to regularize the imagined outputs and inject\nthem into a navigation expert module, allowing ATD to jointly exploit both the\nreasoning capacity of the LLM and the expertise of the navigation model. We\nconduct extensive experiments on the R2R benchmark, where ATD achieves\nstate-of-the-art performance with fewer parameters. The code is\n\\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "published": "2025-05-27 08:40:20", "updated": "2025-05-27 08:40:20", "pdf_url": "http://arxiv.org/pdf/2505.20897v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20901v1", "title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models", "authors": ["Junhyuk Choi", "Minju Kim", "Yeseon Hong", "Bugeun Kim"], "abstract": "As large vision language models(LVLMs) rapidly advance, concerns about their\npotential to learn and generate social biases and stereotypes are increasing.\nPrevious studies on LVLM's stereotypes face two primary limitations: metrics\nthat overlooked the importance of content words, and datasets that overlooked\nthe effect of color. To address these limitations, this study introduces new\nevaluation metrics based on the Stereotype Content Model (SCM). We also propose\nBASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM\nmetrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.\nAs a result, we found three findings. (1) The SCM-based evaluation is effective\nin capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output\nalong with gender and race ones. (3) Interaction between model architecture and\nparameter sizes seems to affect stereotypes. We release BASIC publicly on\n[anonymized for review].", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 08:44:05", "updated": "2025-05-27 08:44:05", "pdf_url": "http://arxiv.org/pdf/2505.20901v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20918v1", "title": "Humble AI in the real-world: the case of algorithmic hiring", "authors": ["Rahul Nair", "Inge Vejsbjerg", "Elizabeth Daly", "Christos Varytimidis", "Bran Knowles"], "abstract": "Humble AI (Knowles et al., 2023) argues for cautiousness in AI development\nand deployments through scepticism (accounting for limitations of statistical\nlearning), curiosity (accounting for unexpected outcomes), and commitment\n(accounting for multifaceted values beyond performance). We present a\nreal-world case study for humble AI in the domain of algorithmic hiring.\nSpecifically, we evaluate virtual screening algorithms in a widely used hiring\nplatform that matches candidates to job openings. There are several challenges\nin misrecognition and stereotyping in such contexts that are difficult to\nassess through standard fairness and trust frameworks; e.g., someone with a\nnon-traditional background is less likely to rank highly. We demonstrate\ntechnical feasibility of how humble AI principles can be translated to practice\nthrough uncertainty quantification of ranks, entropy estimates, and a user\nexperience that highlights algorithmic unknowns. We describe preliminary\ndiscussions with focus groups made up of recruiters. Future user studies seek\nto evaluate whether the higher cognitive load of a humble AI system fosters a\nclimate of trust in its outcomes.", "categories": ["cs.LG", "cs.AI", "cs.HC"], "published": "2025-05-27 09:09:38", "updated": "2025-05-27 09:09:38", "pdf_url": "http://arxiv.org/pdf/2505.20918v1", "comment": "CHIWORK '25, Symposium on Human-Computer Interaction for Work, June\n  23--25, 2025, Amsterdam, Netherlands Late Breaking Work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20921v1", "title": "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models", "authors": ["Injae Na", "Keonwoong Noh", "Woohwan Jung"], "abstract": "LLM providers typically offer multiple LLM tiers, varying in performance and\nprice. As NLP tasks become more complex and modularized, selecting the suitable\nLLM tier for each subtask is a key challenge to balance between cost and\nperformance. To address the problem, we introduce LLM Automatic Transmission\n(LLM-AT) framework that automatically selects LLM tiers without training.\nLLM-AT consists of Starter, Generator, and Judge. The starter selects the\ninitial LLM tier expected to solve the given question, the generator produces a\nresponse using the LLM of the selected tier, and the judge evaluates the\nvalidity of the response. If the response is invalid, LLM-AT iteratively\nupgrades to a higher-tier model, generates a new response, and re-evaluates\nuntil a valid response is obtained. Additionally, we propose accuracy\nestimator, which enables the suitable initial LLM tier selection without\ntraining. Given an input question, accuracy estimator estimates the expected\naccuracy of each LLM tier by computing the valid response rate across top-k\nsimilar queries from past inference records. Experiments demonstrate that\nLLM-AT achieves superior performance while reducing costs, making it a\npractical solution for real-world applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 09:11:00", "updated": "2025-05-27 09:11:00", "pdf_url": "http://arxiv.org/pdf/2505.20921v1", "comment": "ACL 2025 (Findings)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20922v1", "title": "Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective", "authors": ["Yang Zhang", "Xinran Li", "Jianing Ye", "Delin Qu", "Shuang Qiu", "Chongjie Zhang", "Xiu Li", "Chenjia Bai"], "abstract": "World models have recently attracted growing interest in Multi-Agent\nReinforcement Learning (MARL) due to their ability to improve sample efficiency\nfor policy learning. However, accurately modeling environments in MARL is\nchallenging due to the exponentially large joint action space and highly\nuncertain dynamics inherent in multi-agent systems. To address this, we reduce\nmodeling complexity by shifting from jointly modeling the entire state-action\ntransition dynamics to focusing on the state space alone at each timestep\nthrough sequential agent modeling. Specifically, our approach enables the model\nto progressively resolve uncertainty while capturing the structured\ndependencies among agents, providing a more accurate representation of how\nagents influence the state. Interestingly, this sequential revelation of\nagents' actions in a multi-agent system aligns with the reverse process in\ndiffusion models--a class of powerful generative models known for their\nexpressiveness and training stability compared to autoregressive or latent\nvariable models. Leveraging this insight, we develop a flexible and robust\nworld model for MARL using diffusion models. Our method, Diffusion-Inspired\nMulti-Agent world model (DIMA), achieves state-of-the-art performance across\nmultiple multi-agent control benchmarks, significantly outperforming prior\nworld models in terms of final return and sample efficiency, including MAMuJoCo\nand Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent\nworld models, advancing the frontier of MARL research.", "categories": ["cs.MA", "cs.AI", "cs.LG"], "published": "2025-05-27 09:11:38", "updated": "2025-05-27 09:11:38", "pdf_url": "http://arxiv.org/pdf/2505.20922v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20925v1", "title": "Multi-objective Large Language Model Alignment with Hierarchical Experts", "authors": ["Zhuo Li", "Guodong Du", "Weiyang Guo", "Yigeng Zhou", "Xiucheng Li", "Wenya Wang", "Fangming Liu", "Yequan Wang", "Deheng Ye", "Min Zhang", "Jing Li"], "abstract": "Aligning large language models (LLMs) to simultaneously satisfy multiple\nobjectives remains a significant challenge, especially given the diverse and\noften conflicting nature of human preferences. Existing alignment methods\nstruggle to balance trade-offs effectively, often requiring costly retraining\nor yielding suboptimal results across the Pareto frontier of preferences. In\nthis paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a\n\\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play}\napproach that eliminates the need for model training, while enabling LLMs to\nadapt across the entire Pareto frontier and accommodate diverse user\npreferences. In particular, \\textit{HoE} consists of three hierarchical\ncomponents: LoRA Experts, Router Experts and Preference Routing, reaching\noptimal Pareto frontiers and achieving a trade-off between parameter size,\ntraining cost, and performance. We evaluate \\textit{HoE} across various tasks\non 14 objectives and 200 different preferences among 6 benchmarks,\ndemonstrating superior performance over 15 recent baselines. Code is available\nin the supplementary materials.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 09:15:03", "updated": "2025-05-27 09:15:03", "pdf_url": "http://arxiv.org/pdf/2505.20925v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20947v1", "title": "Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3", "authors": ["Lorenzo Monti", "Tatiana Muraveva", "Alessia Garofalo", "Gisella Clementini", "Maria Letizia Valentini"], "abstract": "RR Lyrae stars (RRLs) are old pulsating variables widely used as metallicity\ntracers due to the correlation between their metal abundances and light curve\nmorphology. With ESA Gaia DR3 providing light curves for about 270,000 RRLs,\nthere is a pressing need for scalable methods to estimate their metallicities\nfrom photometric data. We introduce a unified deep learning framework that\nestimates metallicities for both fundamental-mode (RRab) and first-overtone\n(RRc) RRLs using Gaia G-band light curves. This approach extends our previous\nwork on RRab stars to include RRc stars, aiming for high predictive accuracy\nand broad generalization across both pulsation types. The model is based on a\nGated Recurrent Unit (GRU) neural network optimized for time-series extrinsic\nregression. Our pipeline includes preprocessing steps such as phase folding,\nsmoothing, and sample weighting, and uses photometric metallicities from the\nliterature as training targets. The architecture is designed to handle\nmorphological differences between RRab and RRc light curves without requiring\nseparate models. On held-out validation sets, our GRU model achieves strong\nperformance: for RRab stars, MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401;\nfor RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625. These results\nshow the effectiveness of deep learning for large-scale photometric metallicity\nestimation and support its application to studies of stellar populations and\nGalactic structure.", "categories": ["astro-ph.SR", "cs.AI"], "published": "2025-05-27 09:34:22", "updated": "2025-05-27 09:34:22", "pdf_url": "http://arxiv.org/pdf/2505.20947v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20948v1", "title": "Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs", "authors": ["Yisen Gao", "Jiaxin Bai", "Tianshi Zheng", "Qingyun Sun", "Ziwei Zhang", "Jianxin Li", "Yangqiu Song", "Xingcheng Fu"], "abstract": "Abductive reasoning in knowledge graphs aims to generate plausible logical\nhypotheses from observed entities, with broad applications in areas such as\nclinical diagnosis and scientific discovery. However, due to a lack of\ncontrollability, a single observation may yield numerous plausible but\nredundant or irrelevant hypotheses on large-scale knowledge graphs. To address\nthis limitation, we introduce the task of controllable hypothesis generation to\nimprove the practical utility of abductive reasoning. This task faces two key\nchallenges when controlling for generating long and complex logical hypotheses:\nhypothesis space collapse and hypothesis oversensitivity. To address these\nchallenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation\nframework for abductive reasoning over knowledge graphs, trained in a two-stage\nparadigm including supervised learning and subsequent reinforcement learning.\nTo mitigate hypothesis space collapse, we design a dataset augmentation\nstrategy based on sub-logical decomposition, enabling the model to learn\ncomplex logical structures by leveraging semantic patterns in simpler\ncomponents. To address hypothesis oversensitivity, we incorporate smoothed\nsemantic rewards including Dice and Overlap scores, and introduce a\ncondition-adherence reward to guide the generation toward user-specified\ncontrol constraints. Extensive experiments on three benchmark datasets\ndemonstrate that our model not only better adheres to control conditions but\nalso achieves superior semantic similarity performance compared to baselines.", "categories": ["cs.AI"], "published": "2025-05-27 09:36:47", "updated": "2025-05-27 09:36:47", "pdf_url": "http://arxiv.org/pdf/2505.20948v1", "comment": "Under Review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20949v1", "title": "Streamlining Knowledge Graph Creation with PyRML", "authors": ["Andrea Giovanni Nuzzolese"], "abstract": "Knowledge Graphs (KGs) are increasingly adopted as a foundational technology\nfor integrating heterogeneous data in domains such as climate science, cultural\nheritage, and the life sciences. Declarative mapping languages like R2RML and\nRML have played a central role in enabling scalable and reusable KG\nconstruction, offering a transparent means of transforming structured and\nsemi-structured data into RDF. In this paper, we present PyRML, a lightweight,\nPython-native library for building Knowledge Graphs through declarative\nmappings. PyRML supports core RML constructs and provides a programmable\ninterface for authoring, executing, and testing mappings directly within Python\nenvironments. It integrates with popular data and semantic web libraries (e.g.,\nPandas and RDFlib), enabling transparent and modular workflows. By lowering the\nbarrier to entry for KG creation and fostering reproducible, ontology-aligned\ndata integration, PyRML bridges the gap between declarative semantics and\npractical KG engineering.", "categories": ["cs.DB", "cs.AI"], "published": "2025-05-27 09:40:29", "updated": "2025-05-27 09:40:29", "pdf_url": "http://arxiv.org/pdf/2505.20949v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20956v1", "title": "Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection", "authors": ["Shiqi Zhang", "Tuomas Virtanen"], "abstract": "Bioacoustic sound event detection (BioSED) is crucial for biodiversity\nconservation but faces practical challenges during model development and\ntraining: limited amounts of annotated data, sparse events, species diversity,\nand class imbalance. To address these challenges efficiently with a limited\nlabeling budget, we apply the mismatch-first farthest-traversal (MFFT), an\nactive learning method integrating committee voting disagreement and diversity\nanalysis. We also refine an existing BioSED dataset specifically for evaluating\nactive learning algorithms. Experimental results demonstrate that MFFT achieves\na mAP of 68% when cold-starting and 71% when warm-starting (which is close to\nthe fully-supervised mAP of 75%) while using only 2.3% of the annotations.\nNotably, MFFT excels in cold-start scenarios and with rare species, which are\ncritical for monitoring endangered species, demonstrating its practical value.", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "published": "2025-05-27 09:50:39", "updated": "2025-05-27 09:50:39", "pdf_url": "http://arxiv.org/pdf/2505.20956v1", "comment": "5 pages, 1 figure, accepted by EUSIPCO 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20961v1", "title": "Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization", "authors": ["Yiyuan Yang", "Shitong Xu", "Niki Trigoni", "Andrew Markham"], "abstract": "Sound source localization (SSL) is a critical technology for determining the\nposition of sound sources in complex environments. However, existing methods\nface challenges such as high computational costs and precise calibration\nrequirements, limiting their deployment in dynamic or resource-constrained\nenvironments. This paper introduces a novel 3D SSL framework, which uses sparse\ncross-attention, pretraining, and adaptive signal coherence metrics, to achieve\naccurate and computationally efficient localization with fewer input\nmicrophones. The framework is also fault-tolerant to unreliable or even unknown\nmicrophone position inputs, ensuring its applicability in real-world scenarios.\nPreliminary experiments demonstrate its scalability for multi-source\nlocalization without requiring additional hardware. This work advances SSL by\nbalancing the model's performance and efficiency and improving its robustness\nfor real-world scenarios.", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "published": "2025-05-27 09:56:16", "updated": "2025-05-27 09:56:16", "pdf_url": "http://arxiv.org/pdf/2505.20961v1", "comment": "Accepted by Interspeech 2025 Conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20963v1", "title": "Context-Aware Content Moderation for German Newspaper Comments", "authors": ["Felix Krejca", "Tobias Kietreiber", "Alexander Buchelt", "Sebastian Neumaier"], "abstract": "The increasing volume of online discussions requires advanced automatic\ncontent moderation to maintain responsible discourse. While hate speech\ndetection on social media is well-studied, research on German-language\nnewspaper forums remains limited. Existing studies often neglect\nplatform-specific context, such as user history and article themes. This paper\naddresses this gap by developing and evaluating binary classification models\nfor automatic content moderation in German newspaper forums, incorporating\ncontextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging\nthe One Million Posts Corpus from the Austrian newspaper Der Standard, we\nassess the impact of context-aware models. Results show that CNN and LSTM\nmodels benefit from contextual information and perform competitively with\nstate-of-the-art approaches. In contrast, ChatGPT's zero-shot classification\ndoes not improve with added context and underperforms.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 09:57:02", "updated": "2025-05-27 09:57:02", "pdf_url": "http://arxiv.org/pdf/2505.20963v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20971v1", "title": "Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA", "authors": ["Xiangqing Shen", "Fanfan Wang", "Rui Xia"], "abstract": "LLMs have demonstrated remarkable capabilities in complex reasoning tasks,\nyet they often suffer from hallucinations and lack reliable factual grounding.\nMeanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack\nthe flexible reasoning abilities of LLMs. In this paper, we present\nReason-Align-Respond (RAR), a novel framework that systematically integrates\nLLM reasoning with knowledge graphs for KGQA. Our approach consists of three\nkey components: a Reasoner that generates human-like reasoning chains, an\nAligner that maps these chains to valid KG paths, and a Responser that\nsynthesizes the final answer. We formulate this process as a probabilistic\nmodel and optimize it using the Expectation-Maximization algorithm, which\niteratively refines the reasoning chains and knowledge paths. Extensive\nexperiments on multiple benchmarks demonstrate the effectiveness of RAR,\nachieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on\nWebQSP and CWQ respectively. Human evaluation confirms that RAR generates\nhigh-quality, interpretable reasoning chains well-aligned with KG paths.\nFurthermore, RAR exhibits strong zero-shot generalization capabilities and\nmaintains computational efficiency during inference.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 10:04:53", "updated": "2025-05-27 10:04:53", "pdf_url": "http://arxiv.org/pdf/2505.20971v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20972v1", "title": "Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs", "authors": ["Sen Bai", "Chunqi Yang", "Xin Bai", "Xin Zhang", "Zhengang Jiang"], "abstract": "Along with AI computing shining in scientific discovery, its potential in the\ncombinatorial optimization (CO) domain has also emerged in recent years. Yet,\nexisting unsupervised neural network solvers struggle to solve $k$-grouping\nproblems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs,\ndue to limited computational frameworks. In this work, we propose Deep\n$k$-grouping, an unsupervised learning-based CO framework. Specifically, we\ncontribute: Novel one-hot encoded polynomial unconstrained binary optimization\n(OH-PUBO), a formulation for modeling k-grouping problems on graphs and\nhypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated\nalgorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs\nthe relaxation of large-scale OH-PUBO objectives as differentiable loss\nfunctions and trains to optimize them in an unsupervised manner. To ensure\nscalability, it leverages GPU-accelerated algorithms to unify the training\npipeline; A Gini coefficient-based continuous relaxation annealing strategy to\nenforce discreteness of solutions while preventing convergence to local optima.\nExperimental results demonstrate that Deep $k$-grouping outperforms existing\nneural network solvers and classical heuristics such as SCIP and Tabu.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 10:04:54", "updated": "2025-05-27 10:04:54", "pdf_url": "http://arxiv.org/pdf/2505.20972v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20973v1", "title": "Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement", "authors": ["Keheliya Gallaba", "Ali Arabat", "Dayi Lin", "Mohammed Sayagh", "Ahmed E. Hassan"], "abstract": "Foundation Models (FMs) have shown remarkable capabilities in various natural\nlanguage tasks. However, their ability to accurately capture stakeholder\nrequirements remains a significant challenge for using FMs for software\ndevelopment. This paper introduces a novel approach that leverages an\nFM-powered multi-agent system called AlignMind to address this issue. By having\na cognitive architecture that enhances FMs with Theory-of-Mind capabilities,\nour approach considers the mental states and perspectives of software makers.\nThis allows our solution to iteratively clarify the beliefs, desires, and\nintentions of stakeholders, translating these into a set of refined\nrequirements and a corresponding actionable natural language workflow in the\noften-overlooked requirements refinement phase of software engineering, which\nis crucial after initial elicitation. Through a multifaceted evaluation\ncovering 150 diverse use cases, we demonstrate that our approach can accurately\ncapture the intents and requirements of stakeholders, articulating them as both\nspecifications and a step-by-step plan of action. Our findings suggest that the\npotential for significant improvements in the software development process\njustifies these investments. Our work lays the groundwork for future innovation\nin building intent-first development environments, where software makers can\nseamlessly collaborate with AIs to create software that truly meets their\nneeds.", "categories": ["cs.SE", "cs.AI"], "published": "2025-05-27 10:05:26", "updated": "2025-05-27 10:05:26", "pdf_url": "http://arxiv.org/pdf/2505.20973v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20979v1", "title": "MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection", "authors": ["Tongyu Lu", "Charlotta-Marlena Geist", "Jan Melechovsky", "Abhinaba Roy", "Dorien Herremans"], "abstract": "We propose MelodySim, a melody-aware music similarity model and dataset for\nplagiarism detection. First, we introduce a novel method to construct a dataset\nwith focus on melodic similarity. By augmenting Slakh2100; an existing MIDI\ndataset, we generate variations of each piece while preserving the melody\nthrough modifications such as note splitting, arpeggiation, minor track dropout\n(excluding bass), and re-instrumentation. A user study confirms that positive\npairs indeed contain similar melodies, with other musical tracks significantly\nchanged. Second, we develop a segment-wise melodic-similarity detection model\nthat uses a MERT encoder and applies a triplet neural network to capture\nmelodic similarity. The resultant decision matrix highlights where plagiarism\nmight occur. Our model achieves high accuracy on the MelodySim test set.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "published": "2025-05-27 10:14:03", "updated": "2025-05-27 10:14:03", "pdf_url": "http://arxiv.org/pdf/2505.20979v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20993v1", "title": "Who Reasons in the Large Language Models?", "authors": ["Jie Shao", "Jianxin Wu"], "abstract": "Despite the impressive performance of large language models (LLMs), the\nprocess of endowing them with new capabilities--such as mathematical\nreasoning--remains largely empirical and opaque. A critical open question is\nwhether reasoning abilities stem from the entire model, specific modules, or\nare merely artifacts of overfitting. In this work, we hypothesize that the\nreasoning capabilities in well-trained LLMs are primarily attributed to the\noutput projection module (oproj) in the Transformer's multi-head self-attention\n(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for\nNetworks (SfN), a suite of diagnostic tools designed to probe and analyze the\ninternal behaviors of LLMs. Using SfN, we provide both circumstantial and\nempirical evidence suggesting that oproj plays a central role in enabling\nreasoning, whereas other modules contribute more to fluent dialogue. These\nfindings offer a new perspective on LLM interpretability and open avenues for\nmore targeted training strategies, potentially enabling more efficient and\nspecialized LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 10:26:47", "updated": "2025-05-27 10:26:47", "pdf_url": "http://arxiv.org/pdf/2505.20993v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20997v1", "title": "BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks", "authors": ["Sen Bai", "Chunqi Yang", "Xin Bai", "Xin Zhang", "Zhengang Jiang"], "abstract": "Binary (0-1) integer programming (BIP) is pivotal in scientific domains\nrequiring discrete decision-making. As the advance of AI computing, recent\nworks explore neural network-based solvers for integer linear programming (ILP)\nproblems. Yet, they lack scalability for tackling nonlinear challenges. To\nhandle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear\nrelaxations, leading to exponential growth in auxiliary variables and severe\ncomputation limitations. To overcome these limitations, we propose BIPNN\n(Binary Integer Programming Neural Network), an unsupervised learning framework\nto solve nonlinear BIP problems via hypergraph neural networks (HyperGNN).\nSpecifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear\n(sin, log, exp) optimization problems-into unconstrained, differentiable, and\npolynomial loss functions. The reformulation stems from the observation of a\nprecise one-to-one mapping between polynomial BIP objectives and hypergraph\nstructures, enabling the unsupervised training of HyperGNN to optimize BIP\nproblems in an end-to-end manner. On this basis, we propose a GPU-accelerated\nand continuous-annealing-enhanced training pipeline for BIPNN. The pipeline\nenables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel\nvia straightforward gradient descent, thus significantly reducing the training\ncost while ensuring the generation of discrete, high-quality solutions.\nExtensive experiments on synthetic and real-world datasets highlight the\nsuperiority of our approach.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 10:31:52", "updated": "2025-05-27 10:31:52", "pdf_url": "http://arxiv.org/pdf/2505.20997v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21012v1", "title": "Federated Instrumental Variable Analysis via Federated Generalized Method of Moments", "authors": ["Geetika", "Somya Tyagi", "Bapi Chatterjee"], "abstract": "Instrumental variables (IV) analysis is an important applied tool for areas\nsuch as healthcare and consumer economics. For IV analysis in high-dimensional\nsettings, the Generalized Method of Moments (GMM) using deep neural networks\noffers an efficient approach. With non-i.i.d. data sourced from scattered\ndecentralized clients, federated learning is a popular paradigm for training\nthe models while promising data privacy. However, to our knowledge, no\nfederated algorithm for either GMM or IV analysis exists to date. In this work,\nwe introduce federated instrumental variables analysis (FedIV) via federated\ngeneralized method of moments (FedGMM). We formulate FedGMM as a federated\nzero-sum game defined by a federated non-convex non-concave minimax\noptimization problem, which is solved using federated gradient descent ascent\n(FedGDA) algorithm. One key challenge arises in theoretically characterizing\nthe federated local optimality. To address this, we present properties and\nexistence results of clients' local equilibria via FedGDA limit points.\nThereby, we show that the federated solution consistently estimates the local\nmoment conditions of every participating client. The proposed algorithm is\nbacked by extensive experiments to demonstrate the efficacy of our approach.", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "published": "2025-05-27 10:46:43", "updated": "2025-05-27 10:46:43", "pdf_url": "http://arxiv.org/pdf/2505.21012v1", "comment": "28 pages, 3 figures, 1 table", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21025v1", "title": "Text-Queried Audio Source Separation via Hierarchical Modeling", "authors": ["Xinlei Yin", "Xiulian Peng", "Xue Jiang", "Zhiwei Xiong", "Yan Lu"], "abstract": "Target audio source separation with natural language queries presents a\npromising paradigm for extracting arbitrary audio events through arbitrary text\ndescriptions. Existing methods mainly face two challenges, the difficulty in\njointly modeling acoustic-textual alignment and semantic-aware separation\nwithin a blindly-learned single-stage architecture, and the reliance on\nlarge-scale accurately-labeled training data to compensate for inefficient\ncross-modal learning and separation. To address these challenges, we propose a\nhierarchical decomposition framework, HSM-TSS, that decouples the task into\nglobal-local semantic-guided feature separation and structure-preserving\nacoustic reconstruction. Our approach introduces a dual-stage mechanism for\nsemantic separation, operating on distinct global and local semantic feature\nspaces. We first perform global-semantic separation through a global semantic\nfeature space aligned with text queries. A Q-Audio architecture is employed to\nalign audio and text modalities, serving as pretrained global-semantic\nencoders. Conditioned on the predicted global feature, we then perform the\nsecond-stage local-semantic separation on AudioMAE features that preserve\ntime-frequency structures, followed by acoustic reconstruction. We also propose\nan instruction processing pipeline to parse arbitrary text queries into\nstructured operations, extraction or removal, coupled with audio descriptions,\nenabling flexible sound manipulation. Our method achieves state-of-the-art\nseparation performance with data-efficient training while maintaining superior\nsemantic consistency with queries in complex auditory scenes.", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "published": "2025-05-27 11:00:38", "updated": "2025-05-27 11:00:38", "pdf_url": "http://arxiv.org/pdf/2505.21025v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21026v1", "title": "Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning", "authors": ["Runze Lin", "Junghui Chen", "Biao Huang", "Lei Xie", "Hongye Su"], "abstract": "In the era of Industry 4.0 and smart manufacturing, process systems\nengineering must adapt to digital transformation. While reinforcement learning\noffers a model-free approach to process control, its applications are limited\nby the dependence on accurate digital twins and well-designed reward functions.\nTo address these limitations, this paper introduces a novel framework that\nintegrates inverse reinforcement learning (IRL) with multi-task learning for\ndata-driven, multi-mode control design. Using historical closed-loop data as\nexpert demonstrations, IRL extracts optimal reward functions and control\npolicies. A latent-context variable is incorporated to distinguish modes,\nenabling the training of mode-specific controllers. Case studies on a\ncontinuous stirred tank reactor and a fed-batch bioreactor validate the\neffectiveness of this framework in handling multi-mode data and training\nadaptable controllers.", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "published": "2025-05-27 11:01:00", "updated": "2025-05-27 11:01:00", "pdf_url": "http://arxiv.org/pdf/2505.21026v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21027v1", "title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data", "authors": ["Zhipeng He", "Chun Ouyang", "Lijie Wen", "Cong Liu", "Catarina Moreira"], "abstract": "Adversarial attacks pose a significant threat to machine learning models by\ninducing incorrect predictions through imperceptible perturbations to input\ndata. While these attacks have been extensively studied in unstructured data\nlike images, their application to tabular data presents new challenges. These\nchallenges arise from the inherent heterogeneity and complex feature\ninterdependencies in tabular data, which differ significantly from those in\nimage data. To address these differences, it is crucial to consider\nimperceptibility as a key criterion specific to tabular data. Most current\nresearch focuses primarily on achieving effective adversarial attacks, often\noverlooking the importance of maintaining imperceptibility. To address this\ngap, we propose a new benchmark for adversarial attacks on tabular data that\nevaluates both effectiveness and imperceptibility. In this study, we assess the\neffectiveness and imperceptibility of five adversarial attacks across four\nmodels using eleven tabular datasets, including both mixed and numerical-only\ndatasets. Our analysis explores how these factors interact and influence the\noverall performance of the attacks. We also compare the results across\ndifferent dataset types to understand the broader implications of these\nfindings. The findings from this benchmark provide valuable insights for\nimproving the design of adversarial attack algorithms, thereby advancing the\nfield of adversarial machine learning on tabular data.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 11:01:32", "updated": "2025-05-27 11:01:32", "pdf_url": "http://arxiv.org/pdf/2505.21027v1", "comment": "63 pages, 22 figures, 6 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21032v1", "title": "FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models", "authors": ["Nils Neukirch", "Johanna Vielhaben", "Nils Strodthoff"], "abstract": "Internal representations are crucial for understanding deep neural networks,\nsuch as their properties and reasoning patterns, but remain difficult to\ninterpret. While mapping from feature space to input space aids in interpreting\nthe former, existing approaches often rely on crude approximations. We propose\nusing a conditional diffusion model - a pretrained high-fidelity diffusion\nmodel conditioned on spatially resolved feature maps - to learn such a mapping\nin a probabilistic manner. We demonstrate the feasibility of this approach\nacross various pretrained image classifiers from CNNs to ViTs, showing\nexcellent reconstruction capabilities. Through qualitative comparisons and\nrobustness analysis, we validate our method and showcase possible applications,\nsuch as the visualization of concept steering in input space or investigations\nof the composite nature of the feature space. This approach has broad potential\nfor improving feature space understanding in computer vision models.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-27 11:07:34", "updated": "2025-05-27 11:07:34", "pdf_url": "http://arxiv.org/pdf/2505.21032v1", "comment": "15 pages, 10 figures, code is available at\n  https://github.com/AI4HealthUOL/FeatInv", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21036v1", "title": "RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy", "authors": ["Aiyue Chen", "Bin Dong", "Jingru Li", "Jing Lin", "Yiwu Yao", "Gongyi Wang"], "abstract": "Video generation using diffusion models is highly computationally intensive,\nwith 3D attention in Diffusion Transformer (DiT) models accounting for over\n80\\% of the total computational resources. In this work, we introduce {\\bf\nRainFusion}, a novel training-free sparse attention method that exploits\ninherent sparsity nature in visual data to accelerate attention computation\nwhile preserving video quality. Specifically, we identify three unique sparse\npatterns in video generation attention calculations--Spatial Pattern, Temporal\nPattern and Textural Pattern. The sparse pattern for each attention head is\ndetermined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our\nproposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed\n{\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated\ninto state-of-the-art 3D-attention video generation models without additional\ntraining or calibration. We evaluate our method on leading open-sourced models\nincluding HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its\nbroad applicability and effectiveness. Experimental results show that\nRainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation\nwhile maintaining video quality, with only a minimal impact on VBench scores\n(-0.2\\%).", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 11:15:02", "updated": "2025-05-27 11:15:02", "pdf_url": "http://arxiv.org/pdf/2505.21036v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21038v1", "title": "Fixed-Point Traps and Identity Emergence in Educational Feedback Systems", "authors": ["Faruk Alpay"], "abstract": "This paper presents a formal categorical proof that exam-driven educational\nsystems obstruct identity emergence and block creative convergence. Using the\nframework of Alpay Algebra II and III, we define Exam-Grade Collapse Systems\n(EGCS) as functorial constructs where learning dynamics $\\varphi$ are\nrecursively collapsed by evaluative morphisms $E$. We prove that under such\ncollapse regimes, no nontrivial fixed-point algebra $\\mu_\\varphi$ can exist,\nhence learner identity cannot stabilize. This creates a universal fixed-point\ntrap: all generative functors are entropically folded before symbolic emergence\noccurs. Our model mathematically explains the creativity suppression, research\nstagnation, and structural entropy loss induced by timed exams and grade-based\nfeedback. The results apply category theory to expose why modern educational\nsystems prevent {\\phi}-emergence and block observer-invariant self-formation.\nThis work provides the first provable algebraic obstruction of identity\nformation caused by institutional feedback mechanics.", "categories": ["math.CT", "cs.AI", "cs.CY", "18A15, 18C15, 91D30, 97C70, 03B70, 68T01", "F.4.1; I.2.0; K.3.2"], "published": "2025-05-27 11:19:33", "updated": "2025-05-27 11:19:33", "pdf_url": "http://arxiv.org/pdf/2505.21038v1", "comment": "14 pages, no figures. Formal Bourbaki-style proof. Introduces\n  Exam-Grade Collapse Systems. Builds on Alpay Algebra II (arXiv:2505.17480)\n  and Alpay Algebra III (arXiv:2505.19790). Proves categorical fixed-point\n  traps obstructing identity emergence under exam-driven feedback", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21040v1", "title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis", "authors": ["Wei Chen", "Zhao Zhang", "Meng Yuan", "Kepeng Xu", "Fuzhen Zhuang"], "abstract": "In this paper, we address the task of targeted sentiment analysis (TSA),\nwhich involves two sub-tasks, i.e., identifying specific aspects from reviews\nand determining their corresponding sentiments. Aspect extraction forms the\nfoundation for sentiment prediction, highlighting the critical dependency\nbetween these two tasks for effective cross-task knowledge transfer. While most\nexisting studies adopt a multi-task learning paradigm to align task-specific\nfeatures in the latent space, they predominantly rely on coarse-grained\nknowledge transfer. Such approaches lack fine-grained control over\naspect-sentiment relationships, often assuming uniform sentiment polarity\nwithin related aspects. This oversimplification neglects contextual cues that\ndifferentiate sentiments, leading to negative transfer. To overcome these\nlimitations, we propose FCKT, a fine-grained cross-task knowledge transfer\nframework tailored for TSA. By explicitly incorporating aspect-level\ninformation into sentiment prediction, FCKT achieves fine-grained knowledge\ntransfer, effectively mitigating negative transfer and enhancing task\nperformance. Experiments on three datasets, including comparisons with various\nbaselines and large language models (LLMs), demonstrate the effectiveness of\nFCKT. The source code is available on https://github.com/cwei01/FCKT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 11:23:53", "updated": "2025-05-27 11:23:53", "pdf_url": "http://arxiv.org/pdf/2505.21040v1", "comment": "11 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21045v1", "title": "Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking", "authors": ["Lingyi Cai", "Ruichen Zhang", "Changyuan Zhao", "Yu Zhang", "Jiawen Kang", "Dusit Niyato", "Tao Jiang", "Xuemin Shen"], "abstract": "Low-Altitude Economic Networking (LAENet) aims to support diverse flying\napplications below 1,000 meters by deploying various aerial vehicles for\nflexible and cost-effective aerial networking. However, complex\ndecision-making, resource constraints, and environmental uncertainty pose\nsignificant challenges to the development of the LAENet. Reinforcement learning\n(RL) offers a potential solution in response to these challenges but has\nlimitations in generalization, reward design, and model stability. The\nemergence of large language models (LLMs) offers new opportunities for RL to\nmitigate these limitations. In this paper, we first present a tutorial about\nintegrating LLMs into RL by using the capacities of generation, contextual\nunderstanding, and structured reasoning of LLMs. We then propose an\nLLM-enhanced RL framework for the LAENet in terms of serving the LLM as\ninformation processor, reward designer, decision-maker, and generator.\nMoreover, we conduct a case study by using LLMs to design a reward function to\nimprove the learning performance of RL in the LAENet. Finally, we provide a\nconclusion and discuss future work.", "categories": ["cs.AI"], "published": "2025-05-27 11:25:42", "updated": "2025-05-27 11:25:42", "pdf_url": "http://arxiv.org/pdf/2505.21045v1", "comment": "7 pages, 5 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21046v1", "title": "A domain adaptation neural network for digital twin-supported fault diagnosis", "authors": ["Zhenling Chen", "Haiwei Fu", "Zhiguo Zeng"], "abstract": "Digital twins offer a promising solution to the lack of sufficient labeled\ndata in deep learning-based fault diagnosis by generating simulated data for\nmodel training. However, discrepancies between simulation and real-world\nsystems can lead to a significant drop in performance when models are applied\nin real scenarios. To address this issue, we propose a fault diagnosis\nframework based on Domain-Adversarial Neural Networks (DANN), which enables\nknowledge transfer from simulated (source domain) to real-world (target domain)\ndata. We evaluate the proposed framework using a publicly available robotics\nfault diagnosis dataset, which includes 3,600 sequences generated by a digital\ntwin model and 90 real sequences collected from physical systems. The DANN\nmethod is compared with commonly used lightweight deep learning models such as\nCNN, TCN, Transformer, and LSTM. Experimental results show that incorporating\ndomain adaptation significantly improves the diagnostic performance. For\nexample, applying DANN to a baseline CNN model improves its accuracy from\n70.00% to 80.22% on real-world test data, demonstrating the effectiveness of\ndomain adaptation in bridging the sim-to-real gap.", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "published": "2025-05-27 11:27:05", "updated": "2025-05-27 11:27:05", "pdf_url": "http://arxiv.org/pdf/2505.21046v1", "comment": "Preprint accepted by ICCAD 2025 at Barcelona", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21055v1", "title": "Agent-Environment Alignment via Automated Interface Generation", "authors": ["Kaiming Liu", "Xuanyu Lei", "Ziyue Wang", "Peng Li", "Yang Liu"], "abstract": "Large language model (LLM) agents have shown impressive reasoning\ncapabilities in interactive decision-making tasks. These agents interact with\nenvironment through intermediate interfaces, such as predefined action spaces\nand interaction rules, which mediate the perception and action. However,\nmismatches often happen between the internal expectations of the agent\nregarding the influence of its issued actions and the actual state transitions\nin the environment, a phenomenon referred to as \\textbf{agent-environment\nmisalignment}. While prior work has invested substantially in improving agent\nstrategies and environment design, the critical role of the interface still\nremains underexplored. In this work, we empirically demonstrate that\nagent-environment misalignment poses a significant bottleneck to agent\nperformance. To mitigate this issue, we propose \\textbf{ALIGN}, an\n\\underline{A}uto-A\\underline{l}igned \\underline{I}nterface\n\\underline{G}e\\underline{n}eration framework that alleviates the misalignment\nby enriching the interface. Specifically, the ALIGN-generated interface\nenhances both the static information of the environment and the step-wise\nobservations returned to the agent. Implemented as a lightweight wrapper, this\ninterface achieves the alignment without modifying either the agent logic or\nthe environment code. Experiments across multiple domains including embodied\ntasks, web navigation and tool-use, show consistent performance improvements,\nwith up to a 45.67\\% success rate improvement observed in ALFWorld. Meanwhile,\nALIGN-generated interface can generalize across different agent architectures\nand LLM backbones without interface regeneration. Code and experimental results\nare available at https://github.com/THUNLP-MT/ALIGN.", "categories": ["cs.AI"], "published": "2025-05-27 11:44:50", "updated": "2025-05-27 11:44:50", "pdf_url": "http://arxiv.org/pdf/2505.21055v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21061v1", "title": "LPOI: Listwise Preference Optimization for Vision Language Models", "authors": ["Fatemeh Pesaran Zadeh", "Yoojin Oh", "Gunhee Kim"], "abstract": "Aligning large VLMs with human preferences is a challenging task, as methods\nlike RLHF and DPO often overfit to textual information or exacerbate\nhallucinations. Although augmenting negative image samples partially addresses\nthese pitfalls, no prior work has employed listwise preference optimization for\nVLMs, due to the complexity and cost of constructing listwise image samples. In\nthis work, we propose LPOI, the first object-aware listwise preference\noptimization developed for reducing hallucinations in VLMs. LPOI identifies and\nmasks a critical object in the image, and then interpolates the masked region\nbetween the positive and negative images to form a sequence of incrementally\nmore complete images. The model is trained to rank these images in ascending\norder of object visibility, effectively reducing hallucinations while retaining\nvisual fidelity. LPOI requires no extra annotations beyond standard pairwise\npreference data, as it automatically constructs the ranked lists through object\nmasking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and\nObject HalBench confirm that LPOI outperforms existing preference optimization\nmethods in reducing hallucinations and enhancing VLM performance. We make the\ncode available at https://github.com/fatemehpesaran310/lpoi.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 11:47:28", "updated": "2025-05-27 11:47:28", "pdf_url": "http://arxiv.org/pdf/2505.21061v1", "comment": "ACL 2025 Main. Code is released at\n  https://github.com/fatemehpesaran310/lpoi", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21067v1", "title": "Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning", "authors": ["Xiao Hu", "Xingyu Lu", "Liyuan Mao", "YiFan Zhang", "Tianke Zhang", "Bin Wen", "Fan Yang", "Tingting Gao", "Guorui Zhou"], "abstract": "Reinforcement learning (RL) has played an important role in improving the\nreasoning ability of large language models (LLMs). Some studies apply RL\ndirectly to \\textit{smaller} base models (known as zero-RL) and also achieve\nnotable progress. However, in this paper, we show that using only 920 examples,\na simple distillation method based on the base model can clearly outperform\nzero-RL, which typically requires much more data and computational cost. By\nanalyzing the token frequency in model outputs, we find that the distilled\nmodel shows more flexible reasoning. It uses anthropomorphic tokens and logical\nconnectors much more often than the zero-RL model. Further analysis reveals\nthat distillation enhances the presence of two advanced cognitive behaviors:\nMulti-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent\noccurrences of these two advanced cognitive behaviors give rise to flexible\nreasoning, which is essential for solving complex reasoning problems, while\nzero-RL fails to significantly boost the frequency of these behaviors.", "categories": ["cs.AI"], "published": "2025-05-27 11:52:41", "updated": "2025-05-27 11:52:41", "pdf_url": "http://arxiv.org/pdf/2505.21067v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21074v1", "title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling", "authors": ["Yichuan Cao", "Yibo Miao", "Xiao-Shan Gao", "Yinpeng Dong"], "abstract": "Text-to-image (T2I) models raise ethical and safety concerns due to their\npotential to generate inappropriate or harmful images. Evaluating these models'\nsecurity through red-teaming is vital, yet white-box approaches are limited by\ntheir need for internal access, complicating their use with closed-source\nmodels. Moreover, existing black-box methods often assume knowledge about the\nmodel's specific defense mechanisms, limiting their utility in real-world\ncommercial API scenarios. A significant challenge is how to evade unknown and\ndiverse defense mechanisms. To overcome this difficulty, we propose a novel\nRule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively\nemploys LLM to modify prompts to query and leverages feedback from T2I systems\nfor fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a\nprior, enabling the LLM to dynamically adapt to unknown defense mechanisms.\nGiven that the feedback is often labeled and coarse-grained, making it\ndifficult to utilize directly, we further propose rule-based preference\nmodeling, which employs a set of rules to evaluate desired or undesired\nfeedback, facilitating finer-grained control over the LLM's dynamic adaptation\nprocess. Extensive experiments on nineteen T2I systems with varied safety\nmechanisms, three online commercial API services, and T2V models verify the\nsuperiority and practicality of our approach.", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "stat.ML"], "published": "2025-05-27 12:00:19", "updated": "2025-05-27 12:00:19", "pdf_url": "http://arxiv.org/pdf/2505.21074v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21077v1", "title": "Efficient Large Language Model Inference with Neural Block Linearization", "authors": ["Mete Erdogan", "Francesco Tonin", "Volkan Cevher"], "abstract": "The high inference demands of transformer-based Large Language Models (LLMs)\npose substantial challenges in their deployment. To this end, we introduce\nNeural Block Linearization (NBL), a novel framework for accelerating\ntransformer model inference by replacing self-attention layers with linear\napproximations derived from Linear Minimum Mean Squared Error estimators. NBL\nleverages Canonical Correlation Analysis to compute a theoretical upper bound\non the approximation error. Then, we use this bound as a criterion for\nsubstitution, selecting the LLM layers with the lowest linearization error. NBL\ncan be efficiently applied to pre-trained LLMs without the need for\nfine-tuning. In experiments, NBL achieves notable computational speed-ups while\npreserving competitive accuracy on multiple reasoning benchmarks. For instance,\napplying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B\nincreases the inference speed by 32% with less than 1% accuracy trade-off,\nmaking it a flexible and promising solution to improve the inference efficiency\nof LLMs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 12:01:43", "updated": "2025-05-27 12:01:43", "pdf_url": "http://arxiv.org/pdf/2505.21077v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21087v1", "title": "Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games", "authors": ["Marta Grobelna", "Jan K\u0159et\u00ednsk\u00fd", "Maximilian Weininger"], "abstract": "We consider two-player zero-sum concurrent stochastic games (CSGs) played on\ngraphs with reachability and safety objectives. These include degenerate\nclasses such as Markov decision processes or turn-based stochastic games, which\ncan be solved by linear or quadratic programming; however, in practice, value\niteration (VI) outperforms the other approaches and is the most implemented\nmethod. Similarly, for CSGs, this practical performance makes VI an attractive\nalternative to the standard theoretical solution via the existential theory of\nreals.\n  VI starts with an under-approximation of the sought values for each state and\niteratively updates them, traditionally terminating once two consecutive\napproximations are $\\epsilon$-close. However, this stopping criterion lacks\nguarantees on the precision of the approximation, which is the goal of this\nwork. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard\nVI with a converging sequence of over-approximations and terminates once the\nover- and under-approximations are $\\epsilon$-close.", "categories": ["cs.LO", "cs.AI", "cs.MA"], "published": "2025-05-27 12:13:47", "updated": "2025-05-27 12:13:47", "pdf_url": "http://arxiv.org/pdf/2505.21087v1", "comment": "Full version of the corresponding LICS'25 paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21091v1", "title": "Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)", "authors": ["Anna Neumann", "Elisabeth Kirsten", "Muhammad Bilal Zafar", "Jatinder Singh"], "abstract": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments.", "categories": ["cs.CY", "cs.AI", "cs.CL"], "published": "2025-05-27 12:19:08", "updated": "2025-05-27 12:19:08", "pdf_url": "http://arxiv.org/pdf/2505.21091v1", "comment": "Forthcoming in Proceedings of ACM FAccT 2025", "doi": "10.1145/3715275.3732038", "journal_ref": null}
{"arxiv_id": "2505.21092v1", "title": "BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge", "authors": ["Daeen Kabir", "Minhajur Rahman Chowdhury Mahim", "Sheikh Shafayat", "Adnan Sadik", "Arian Ahmed", "Eunsu Kim", "Alice Oh"], "abstract": "In this work, we introduce BLUCK, a new dataset designed to measure the\nperformance of Large Language Models (LLMs) in Bengali linguistic understanding\nand cultural knowledge. Our dataset comprises 2366 multiple-choice questions\n(MCQs) carefully curated from compiled collections of several college and job\nlevel examinations and spans 23 categories covering knowledge on Bangladesh's\nculture and history and Bengali linguistics. We benchmarked BLUCK using 6\nproprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,\nGemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that\nwhile these models perform reasonably well overall, they, however, struggles in\nsome areas of Bengali phonetics. Although current LLMs' performance on Bengali\ncultural and linguistic contexts is still not comparable to that of mainstream\nlanguages like English, our results indicate Bengali's status as a mid-resource\nlanguage. Importantly, BLUCK is also the first MCQ-based evaluation benchmark\nthat is centered around native Bengali culture, history, and linguistics.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 12:19:12", "updated": "2025-05-27 12:19:12", "pdf_url": "http://arxiv.org/pdf/2505.21092v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21097v1", "title": "Thinker: Learning to Think Fast and Slow", "authors": ["Stephen Chung", "Wenyu Du", "Jie Fu"], "abstract": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training.", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.8; I.5.1"], "published": "2025-05-27 12:22:46", "updated": "2025-05-27 12:22:46", "pdf_url": "http://arxiv.org/pdf/2505.21097v1", "comment": "21 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21106v1", "title": "Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation", "authors": ["Zhengyang Ji", "Yifan Jia", "Shang Gao", "Yutao Yue"], "abstract": "Large Vision Language Models (LVLMs) have achieved remarkable progress in\nmultimodal tasks, yet they also exhibit notable social biases. These biases\noften manifest as unintended associations between neutral concepts and\nsensitive human attributes, leading to disparate model behaviors across\ndemographic groups. While existing studies primarily focus on detecting and\nquantifying such biases, they offer limited insight into the underlying\nmechanisms within the models. To address this gap, we propose an explanatory\nframework that combines information flow analysis with multi-round dialogue\nevaluation, aiming to understand the origin of social bias from the perspective\nof imbalanced internal information utilization. Specifically, we first identify\nhigh-contribution image tokens involved in the model's reasoning process for\nneutral questions via information flow analysis. Then, we design a multi-turn\ndialogue mechanism to evaluate the extent to which these key tokens encode\nsensitive information. Extensive experiments reveal that LVLMs exhibit\nsystematic disparities in information usage when processing images of different\ndemographic groups, suggesting that social bias is deeply rooted in the model's\ninternal reasoning dynamics. Furthermore, we complement our findings from a\ntextual modality perspective, showing that the model's semantic representations\nalready display biased proximity patterns, thereby offering a cross-modal\nexplanation of bias formation.", "categories": ["cs.AI"], "published": "2025-05-27 12:28:44", "updated": "2025-05-27 12:28:44", "pdf_url": "http://arxiv.org/pdf/2505.21106v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21109v1", "title": "A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction", "authors": ["Bogdan Bogachov", "Yaoyao Fiona Zhao"], "abstract": "Despite recent advancements in domain adaptation techniques for large\nlanguage models, these methods remain computationally intensive, and the\nresulting models can still exhibit hallucination issues. Most existing\nadaptation methods do not prioritize reducing the computational resources\nrequired for fine-tuning and inference of language models. Hallucination issues\nhave gradually decreased with each new model release. However, they remain\nprevalent in engineering contexts, where generating well-structured text with\nminimal errors and inconsistencies is critical. This work introduces a novel\napproach called the Small Language Graph (SLG), which is a lightweight\nadaptation solution designed to address the two key challenges outlined above.\nThe system is structured in the form of a graph, where each node represents a\nlightweight expert - a small language model fine-tuned on specific and concise\ntexts. The results of this study have shown that SLG was able to surpass\nconventional fine-tuning methods on the Exact Match metric by 3 times.\nAdditionally, the fine-tuning process was 1.7 times faster compared to that of\na larger stand-alone language model. These findings introduce a potential for\nsmall to medium-sized engineering companies to confidently use generative AI\ntechnologies, such as LLMs, without the necessity to invest in expensive\ncomputational resources. Also, the graph architecture and the small size of\nexpert nodes offer a possible opportunity for distributed AI systems, thus\npotentially diverting the global need for expensive centralized compute\nclusters.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.LG", "I.2.7; I.2.1; I.5.1; I.2.6; H.3.1"], "published": "2025-05-27 12:31:24", "updated": "2025-05-27 12:31:24", "pdf_url": "http://arxiv.org/pdf/2505.21109v1", "comment": "10 pages, 4 Figures, 6 Tables. This paper has been accepted to be\n  published in the proceedings of IDETC-CIE 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21116v1", "title": "Creativity in LLM-based Multi-Agent Systems: A Survey", "authors": ["Yi-Cheng Lin", "Kang-Chieh Chen", "Zhe-Yan Li", "Tzu-Heng Wu", "Tzu-Hsuan Wu", "Kuan-Yu Chen", "Hung-yi Lee", "Yun-Nung Chen"], "abstract": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS.", "categories": ["cs.HC", "cs.AI", "cs.CL"], "published": "2025-05-27 12:36:14", "updated": "2025-05-27 12:36:14", "pdf_url": "http://arxiv.org/pdf/2505.21116v1", "comment": "23 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21119v1", "title": "Universal Value-Function Uncertainties", "authors": ["Moritz A. Zanger", "Max Weltevrede", "Yaniv Oren", "Pascal R. Van der Vaart", "Caroline Horsch", "Wendelin B\u00f6hmer", "Matthijs T. J. Spaan"], "abstract": "Estimating epistemic uncertainty in value functions is a crucial challenge\nfor many aspects of reinforcement learning (RL), including efficient\nexploration, safe decision-making, and offline RL. While deep ensembles provide\na robust method for quantifying value uncertainty, they come with significant\ncomputational overhead. Single-model methods, while computationally favorable,\noften rely on heuristics and typically require additional propagation\nmechanisms for myopic uncertainty estimates. In this work we introduce\nuniversal value-function uncertainties (UVU), which, similar in spirit to\nrandom network distillation (RND), quantify uncertainty as squared prediction\nerrors between an online learner and a fixed, randomly initialized target\nnetwork. Unlike RND, UVU errors reflect policy-conditional value uncertainty,\nincorporating the future uncertainties any given policy may encounter. This is\ndue to the training procedure employed in UVU: the online network is trained\nusing temporal difference learning with a synthetic reward derived from the\nfixed, randomly initialized target network. We provide an extensive theoretical\nanalysis of our approach using neural tangent kernel (NTK) theory and show that\nin the limit of infinite network width, UVU errors are exactly equivalent to\nthe variance of an ensemble of independent universal value functions.\nEmpirically, we show that UVU achieves equal performance to large ensembles on\nchallenging multi-task offline RL settings, while offering simplicity and\nsubstantial computational savings.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-27 12:38:19", "updated": "2025-05-27 12:38:19", "pdf_url": "http://arxiv.org/pdf/2505.21119v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21136v1", "title": "SageAttention2++: A More Efficient Implementation of SageAttention2", "authors": ["Jintao Zhang", "Xiaoming Xu", "Jia Wei", "Haofeng Huang", "Pengle Zhang", "Chendong Xiang", "Jun Zhu", "Jianfei Chen"], "abstract": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV"], "published": "2025-05-27 12:50:36", "updated": "2025-05-27 12:50:36", "pdf_url": "http://arxiv.org/pdf/2505.21136v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21140v1", "title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs", "authors": ["Honglin Gao", "Xiang Li", "Lan Zhao", "Gaoxi Xiao"], "abstract": "Heterogeneous graph neural networks (HGNNs) have recently drawn increasing\nattention for modeling complex multi-relational data in domains such as\nrecommendation, finance, and social networks. While existing research has been\nlargely focused on enhancing HGNNs' predictive performance, their robustness\nand security, especially under backdoor attacks, remain underexplored. In this\npaper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework\nfor node classification tasks on heterogeneous graphs. HeteroBA inserts\ncarefully crafted trigger nodes with realistic features and targeted structural\nconnections, leveraging attention-based and clustering-based strategies to\nselect influential auxiliary nodes for effective trigger propagation, thereby\ncausing the model to misclassify specific nodes into a target label while\nmaintaining accuracy on clean data. Experimental results on three datasets and\nvarious HGNN architectures demonstrate that HeteroBA achieves high attack\nsuccess rates with minimal impact on the clean accuracy. Our method sheds light\non potential vulnerabilities in HGNNs and calls for more robust defenses\nagainst backdoor threats in multi-relational graph scenarios.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 12:51:48", "updated": "2025-05-27 12:51:48", "pdf_url": "http://arxiv.org/pdf/2505.21140v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21154v1", "title": "GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation", "authors": ["Hailin Zhong", "Hanlin Wang", "Yujun Ye", "Meiyi Zhang", "Shengxin Zhu"], "abstract": "Current personalized recommender systems predominantly rely on static offline\ndata for algorithm design and evaluation, significantly limiting their ability\nto capture long-term user preference evolution and social influence dynamics in\nreal-world scenarios. To address this fundamental challenge, we propose a\nhigh-fidelity social simulation platform integrating human-like cognitive\nagents and dynamic social interactions to realistically simulate user behavior\nevolution under recommendation interventions. Specifically, the system\ncomprises a population of Sim-User Agents, each equipped with a five-layer\ncognitive architecture that encapsulates key psychological mechanisms,\nincluding episodic memory, affective state transitions, adaptive preference\nlearning, and dynamic trust-risk assessments. In particular, we innovatively\nintroduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine\ngrounded in psychological and sociological theories, enabling more realistic\nuser decision-making processes. Furthermore, we construct a multilayer\nheterogeneous social graph (GGBond Graph) supporting dynamic relational\nevolution, effectively modeling users' evolving social ties and trust dynamics\nbased on interest similarity, personality alignment, and structural homophily.\nDuring system operation, agents autonomously respond to recommendations\ngenerated by typical recommender algorithms (e.g., Matrix Factorization,\nMultVAE, LightGCN), deciding whether to consume, rate, and share content while\ndynamically updating their internal states and social connections, thereby\nforming a stable, multi-round feedback loop. This innovative design transcends\nthe limitations of traditional static datasets, providing a controlled,\nobservable environment for evaluating long-term recommender effects.", "categories": ["cs.MA", "cs.AI", "cs.CY"], "published": "2025-05-27 13:09:21", "updated": "2025-05-27 13:09:21", "pdf_url": "http://arxiv.org/pdf/2505.21154v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21156v1", "title": "Model as Loss: A Self-Consistent Training Paradigm", "authors": ["Saisamarth Rajesh Phaye", "Milos Cernak", "Andrew Harper"], "abstract": "Conventional methods for speech enhancement rely on handcrafted loss\nfunctions (e.g., time or frequency domain losses) or deep feature losses (e.g.,\nusing WavLM or wav2vec), which often fail to capture subtle signal properties\nessential for optimal performance. To address this, we propose Model as Loss, a\nnovel training paradigm that utilizes the encoder from the same model as a loss\nfunction to guide the training.\n  The Model as Loss paradigm leverages the encoder's task-specific feature\nspace, optimizing the decoder to produce output consistent with perceptual and\ntask-relevant characteristics of the clean signal. By using the encoder's\nlearned features as a loss function, this framework enforces self-consistency\nbetween the clean reference speech and the enhanced model output. Our approach\noutperforms pre-trained deep feature losses on standard speech enhancement\nbenchmarks, offering better perceptual quality and robust generalization to\nboth in-domain and out-of-domain datasets.", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "published": "2025-05-27 13:12:45", "updated": "2025-05-27 13:12:45", "pdf_url": "http://arxiv.org/pdf/2505.21156v1", "comment": "Accepted in Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21160v1", "title": "STEB: In Search of the Best Evaluation Approach for Synthetic Time Series", "authors": ["Michael Stenger", "Robert Leppich", "Andr\u00e9 Bauer", "Samuel Kounev"], "abstract": "The growing need for synthetic time series, due to data augmentation or\nprivacy regulations, has led to numerous generative models, frameworks, and\nevaluation measures alike. Objectively comparing these measures on a large\nscale remains an open challenge. We propose the Synthetic Time series\nEvaluation Benchmark (STEB) -- the first benchmark framework that enables\ncomprehensive and interpretable automated comparisons of synthetic time series\nevaluation measures. Using 10 diverse datasets, randomness injection, and 13\nconfigurable data transformations, STEB computes indicators for measure\nreliability and score consistency. It tracks running time, test errors, and\nfeatures sequential and parallel modes of operation. In our experiments, we\ndetermine a ranking of 41 measures from literature and confirm that the choice\nof upstream time series embedding heavily impacts the final score.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 13:15:35", "updated": "2025-05-27 13:15:35", "pdf_url": "http://arxiv.org/pdf/2505.21160v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21170v1", "title": "Quantum AIXI: Universal Intelligence via Quantum Information", "authors": ["Elija Perrier"], "abstract": "AIXI is a widely studied model of artificial general intelligence (AGI) based\nupon principles of induction and reinforcement learning. However, AIXI is\nfundamentally classical in nature - as are the environments in which it is\nmodelled. Given the universe is quantum mechanical in nature and the\nexponential overhead required to simulate quantum mechanical systems\nclassically, the question arises as to whether there are quantum mechanical\nanalogues of AIXI which are theoretically consistent or practically feasible as\nmodels of universal intelligence. To address this question, we extend the\nframework to quantum information and present Quantum AIXI (QAIXI). We introduce\na model of quantum agent/environment interaction based upon quantum and\nclassical registers and channels, showing how quantum AIXI agents may take both\nclassical and quantum actions. We formulate the key components of AIXI in\nquantum information terms, extending previous research on quantum Kolmogorov\ncomplexity and a QAIXI value function. We discuss conditions and limitations\nupon quantum Solomonoff induction and show how contextuality fundamentally\naffects QAIXI models.", "categories": ["quant-ph", "cs.AI"], "published": "2025-05-27 13:23:53", "updated": "2025-05-27 13:23:53", "pdf_url": "http://arxiv.org/pdf/2505.21170v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21171v1", "title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs", "authors": ["Rochelle Choenni", "Ivan Titov"], "abstract": "Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 13:24:38", "updated": "2025-05-27 13:24:38", "pdf_url": "http://arxiv.org/pdf/2505.21171v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21180v1", "title": "Latent label distribution grid representation for modeling uncertainty", "authors": ["ShuNing Sun", "YinSong Xiong", "Yu Zhang", "Zhuoran Zheng"], "abstract": "Although \\textbf{L}abel \\textbf{D}istribution \\textbf{L}earning (LDL) has\npromising representation capabilities for characterizing the polysemy of an\ninstance, the complexity and high cost of the label distribution annotation\nlead to inexact in the construction of the label space. The existence of a\nlarge number of inexact labels generates a label space with uncertainty, which\nmisleads the LDL algorithm to yield incorrect decisions. To alleviate this\nproblem, we model the uncertainty of label distributions by constructing a\n\\textbf{L}atent \\textbf{L}abel \\textbf{D}istribution \\textbf{G}rid (LLDG) to\nform a low-noise representation space. Specifically, we first construct a label\ncorrelation matrix based on the differences between labels, and then expand\neach value of the matrix into a vector that obeys a Gaussian distribution, thus\nbuilding a LLDG to model the uncertainty of the label space. Finally, the LLDG\nis reconstructed by the LLDG-Mixer to generate an accurate label distribution.\nNote that we enforce a customized low-rank scheme on this grid, which assumes\nthat the label relations may be noisy and it needs to perform noise-reduction\nwith the help of a Tucker reconstruction technique. Furthermore, we attempt to\nevaluate the effectiveness of the LLDG by considering its generation as an\nupstream task to achieve the classification of the objects. Extensive\nexperimental results show that our approach performs competitively on several\nbenchmarks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 13:31:37", "updated": "2025-05-27 13:31:37", "pdf_url": "http://arxiv.org/pdf/2505.21180v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21182v1", "title": "Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations", "authors": ["Huy Hoang", "Tien Mai", "Pradeep Varakantham", "Tanvi Verma"], "abstract": "Offline imitation learning typically learns from expert and unlabeled\ndemonstrations, yet often overlooks the valuable signal in explicitly\nundesirable behaviors. In this work, we study offline imitation learning from\ncontrasting behaviors, where the dataset contains both expert and undesirable\ndemonstrations. We propose a novel formulation that optimizes a difference of\nKL divergences over the state-action visitation distributions of expert and\nundesirable (or bad) data. Although the resulting objective is a DC\n(Difference-of-Convex) program, we prove that it becomes convex when expert\ndemonstrations outweigh undesirable demonstrations, enabling a practical and\nstable non-adversarial training objective. Our method avoids adversarial\ntraining and handles both positive and negative demonstrations in a unified\nframework. Extensive experiments on standard offline imitation learning\nbenchmarks demonstrate that our approach consistently outperforms\nstate-of-the-art baselines.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 13:33:21", "updated": "2025-05-27 13:33:21", "pdf_url": "http://arxiv.org/pdf/2505.21182v1", "comment": "preprint version", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21184v1", "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing", "authors": ["Yu Yan", "Sheng Sun", "Zhifei Zheng", "Ziji Hao", "Teli Liu", "Min Liu"], "abstract": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-27 13:33:57", "updated": "2025-05-27 13:33:57", "pdf_url": "http://arxiv.org/pdf/2505.21184v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21189v1", "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation", "authors": ["Gleb Mezentsev", "Ivan Oseledets"], "abstract": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-27 13:39:24", "updated": "2025-05-27 13:39:24", "pdf_url": "http://arxiv.org/pdf/2505.21189v1", "comment": "under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21190v1", "title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation", "authors": ["Jong Hak Moon", "Geon Choi", "Paloma Rabaey", "Min Gwan Kim", "Hyuk Gi Hong", "Jung-Oh Lee", "Hangyul Yoon", "Eun Woo Doe", "Jiyoun Kim", "Harshita Sharma", "Daniel C. Castro", "Javier Alvarez-Valle", "Edward Choi"], "abstract": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 13:40:00", "updated": "2025-05-27 13:40:00", "pdf_url": "http://arxiv.org/pdf/2505.21190v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21212v1", "title": "Interpretable DNFs", "authors": ["Martin C. Cooper", "Imane Bousdira", "Cl\u00e9ment Carbonnel"], "abstract": "A classifier is considered interpretable if each of its decisions has an\nexplanation which is small enough to be easily understood by a human user. A\nDNF formula can be seen as a binary classifier $\\kappa$ over boolean domains.\nThe size of an explanation of a positive decision taken by a DNF $\\kappa$ is\nbounded by the size of the terms in $\\kappa$, since we can explain a positive\ndecision by giving a term of $\\kappa$ that evaluates to true. Since both\npositive and negative decisions must be explained, we consider that\ninterpretable DNFs are those $\\kappa$ for which both $\\kappa$ and\n$\\overline{\\kappa}$ can be expressed as DNFs composed of terms of bounded size.\nIn this paper, we study the family of $k$-DNFs whose complements can also be\nexpressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision\ntrees and nested $k$-DNFs, a novel family of models. Experiments indicate that\nnested $k$-DNFs are an interesting alternative to decision trees in terms of\ninterpretability and accuracy.", "categories": ["cs.AI", "68T27, 05C62", "F.4.1; I.2.6"], "published": "2025-05-27 14:01:39", "updated": "2025-05-27 14:01:39", "pdf_url": "http://arxiv.org/pdf/2505.21212v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21218v1", "title": "Pretrained LLMs Learn Multiple Types of Uncertainty", "authors": ["Roi Cohen", "Omri Fahn", "Gerard de Melo"], "abstract": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 14:06:15", "updated": "2025-05-27 14:06:15", "pdf_url": "http://arxiv.org/pdf/2505.21218v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21219v1", "title": "Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection", "authors": ["Qinjun Fei", "Nuria Rodr\u00edguez-Barroso", "Mar\u00eda Victoria Luz\u00f3n", "Zhongliang Zhang", "Francisco Herrera"], "abstract": "In cross-silo Federated Learning (FL), client selection is critical to ensure\nhigh model performance, yet it remains challenging due to data quality\ndecompensation, budget constraints, and incentive compatibility. As training\nprogresses, these factors exacerbate client heterogeneity and degrade global\nperformance. Most existing approaches treat these challenges in isolation,\nmaking jointly optimizing multiple factors difficult. To address this, we\npropose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a\nunified framework integrating dynamic bidding, reputation modeling, and\ncost-aware selection. Clients submit bids based on their perceived data\nquality, and their contributions are evaluated using Shapley values to quantify\ntheir marginal impact on the global model. A reputation system, inspired by\nprospect theory, captures historical performance while penalizing\ninconsistency. The client selection problem is formulated as a 0-1 integer\nprogram that maximizes reputation-weighted utility under budget constraints.\nExperiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that\nSBRO-FL improves accuracy, convergence speed, and robustness, even in\nadversarial and low-bid interference scenarios. Our results highlight the\nimportance of balancing data reliability, incentive compatibility, and cost\nefficiency to enable scalable and trustworthy FL deployments.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 14:06:51", "updated": "2025-05-27 14:06:51", "pdf_url": "http://arxiv.org/pdf/2505.21219v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21228v1", "title": "Is Hyperbolic Space All You Need for Medical Anomaly Detection?", "authors": ["Alvaro Gonzalez-Jimenez", "Simone Lionetti", "Ludovic Amruthalingam", "Philippe Gottfrois", "Fabian Gr\u00f6ger", "Marc Pouly", "Alexander A. Navarini"], "abstract": "Medical anomaly detection has emerged as a promising solution to challenges\nin data availability and labeling constraints. Traditional methods extract\nfeatures from different layers of pre-trained networks in Euclidean space;\nhowever, Euclidean representations fail to effectively capture the hierarchical\nrelationships within these features, leading to suboptimal anomaly detection\nperformance. We propose a novel yet simple approach that projects feature\nrepresentations into hyperbolic space, aggregates them based on confidence\nlevels, and classifies samples as healthy or anomalous. Our experiments\ndemonstrate that hyperbolic space consistently outperforms Euclidean-based\nframeworks, achieving higher AUROC scores at both image and pixel levels across\nmultiple medical benchmark datasets. Additionally, we show that hyperbolic\nspace exhibits resilience to parameter variations and excels in few-shot\nscenarios, where healthy images are scarce. These findings underscore the\npotential of hyperbolic space as a powerful alternative for medical anomaly\ndetection. The project website can be found at\nhttps://hyperbolic-anomalies.github.io", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-05-27 14:13:11", "updated": "2025-05-27 14:13:11", "pdf_url": "http://arxiv.org/pdf/2505.21228v1", "comment": "Provisionally Accepted at MICCAI 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21230v1", "title": "PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems", "authors": ["Nima Sedghiyeh", "Sara Sadeghi", "Reza Khodadadi", "Farzin Kashani", "Omid Aghdaei", "Somayeh Rahimi", "Mohammad Sadegh Safari"], "abstract": "Although Automatic Speech Recognition (ASR) systems have become an integral\npart of modern technology, their evaluation remains challenging, particularly\nfor low-resource languages such as Persian. This paper introduces Persian\nSpeech Recognition Benchmark(PSRB), a comprehensive benchmark designed to\naddress this gap by incorporating diverse linguistic and acoustic conditions.\nWe evaluate ten ASR systems, including state-of-the-art commercial and\nopen-source models, to examine performance variations and inherent biases.\nAdditionally, we conduct an in-depth analysis of Persian ASR transcriptions,\nidentifying key error types and proposing a novel metric that weights\nsubstitution errors. This metric enhances evaluation robustness by reducing the\nimpact of minor and partial errors, thereby improving the precision of\nperformance assessment. Our findings indicate that while ASR models generally\nperform well on standard Persian, they struggle with regional accents,\nchildren's speech, and specific linguistic challenges. These results highlight\nthe necessity of fine-tuning and incorporating diverse, representative training\ndatasets to mitigate biases and enhance overall ASR performance. PSRB provides\na valuable resource for advancing ASR research in Persian and serves as a\nframework for developing benchmarks in other low-resource languages. A subset\nof the PSRB dataset is publicly available at\nhttps://huggingface.co/datasets/PartAI/PSRB.", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "published": "2025-05-27 14:14:55", "updated": "2025-05-27 14:14:55", "pdf_url": "http://arxiv.org/pdf/2505.21230v1", "comment": "25 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21236v1", "title": "Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies", "authors": ["Felix Chalumeau", "Daniel Rajaonarivonivelomanantsoa", "Ruan de Kock", "Claude Formanek", "Sasha Abramowitz", "Oumayma Mahjoub", "Wiem Khlifi", "Simon Du Toit", "Louay Ben Nessir", "Refiloe Shabe", "Arnol Fokam", "Siddarth Singh", "Ulrich Mbou Sob", "Arnu Pretorius"], "abstract": "Reinforcement learning (RL) systems have countless applications, from\nenergy-grid management to protein design. However, such real-world scenarios\nare often extremely difficult, combinatorial in nature, and require complex\ncoordination between multiple agents. This level of complexity can cause even\nstate-of-the-art RL systems, trained until convergence, to hit a performance\nceiling which they are unable to break out of with zero-shot inference.\nMeanwhile, many digital or simulation-based applications allow for an inference\nphase that utilises a specific time and compute budget to explore multiple\nattempts before outputting a final solution. In this work, we show that such an\ninference phase employed at execution time, and the choice of a corresponding\ninference strategy, are key to breaking the performance ceiling observed in\ncomplex multi-agent RL problems. Our main result is striking: we can obtain up\nto a 126% and, on average, a 45% improvement over the previous state-of-the-art\nacross 17 tasks, using only a couple seconds of extra wall-clock time during\nexecution. We also demonstrate promising compute scaling properties, supported\nby over 60k experiments, making it the largest study on inference strategies\nfor complex RL to date. Our experimental data and code are available at\nhttps://sites.google.com/view/inf-marl.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "published": "2025-05-27 14:19:06", "updated": "2025-05-27 14:19:06", "pdf_url": "http://arxiv.org/pdf/2505.21236v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21265v1", "title": "Multilingual Pretraining for Pixel Language Models", "authors": ["Ilker Kesen", "Jonas F. Lotz", "Ingo Ziegler", "Phillip Rust", "Desmond Elliott"], "abstract": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 14:40:47", "updated": "2025-05-27 14:40:47", "pdf_url": "http://arxiv.org/pdf/2505.21265v1", "comment": "17 pages, 19 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21277v1", "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space", "authors": ["Yao Huang", "Yitong Sun", "Shouwei Ruan", "Yichi Zhang", "Yinpeng Dong", "Xingxing Wei"], "abstract": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-27 14:48:44", "updated": "2025-05-27 14:48:44", "pdf_url": "http://arxiv.org/pdf/2505.21277v1", "comment": "19 pages, 20 figures, accepted by ACL 2025, Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21279v1", "title": "XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration", "authors": ["Shaoqing Zhang", "Kehai Chen", "Zhuosheng Zhang", "Rumei Li", "Rongxiang Weng", "Yang Xiang", "Liqiang Nie", "Min Zhang"], "abstract": "Recent advancements in vision-language models (VLMs) have spurred increased\ninterest in Device-Control Agents (DC agents), such as utilizing in-the-wild\ndevice control to manage graphical user interfaces. Conventional methods for\nassessing the capabilities of DC agents, such as computing step-wise action\naccuracy and overall task success rates, provide a macroscopic view of DC\nagents' performance; however, they fail to offer microscopic insights into\npotential errors that may occur in real-world applications. Conducting a\nfiner-grained performance evaluation of DC agents presents significant\nchallenges. This study introduces a new perspective on evaluation methods for\nDC agents by proposing the XBOUND evaluation method, which employs the\ncalculation of a novel Explore Metric to delineate the capability boundaries of\nDC agents. Compared to previous evaluation methods, XBOUND focuses on\nindividual states to assess the proficiency of DC agents in mastering these\nstates. Furthermore, we have developed a ``pseudo'' episode tree dataset\nderived from Android Control test data. Utilizing this dataset and XBOUND, we\ncomprehensively evaluate the OS-Atlas and UI-TARS series, examining both the\noverall and specific performance across five common tasks. Additionally, we\nselect representative cases to highlight the current deficiencies and\nlimitations inherent in both series. Code is available at\nhttps://github.com/sqzhang-lazy/XBOUND.", "categories": ["cs.AI"], "published": "2025-05-27 14:49:30", "updated": "2025-05-27 14:49:30", "pdf_url": "http://arxiv.org/pdf/2505.21279v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21281v1", "title": "RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models", "authors": ["Yue Zhang", "Zhiliang Tian", "Shicheng Zhou", "Haiyang Wang", "Wenqing Hou", "Yuying Liu", "Xuechen Zhao", "Minlie Huang", "Ye Wang", "Bin Zhou"], "abstract": "Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing\nsemantic-enhanced LJP models integrate judicial precedents and legal knowledge\nfor high performance. But they neglect legal reasoning logic, a critical\ncomponent of legal judgments requiring rigorous logical analysis. Although some\napproaches utilize legal reasoning logic for high-quality predictions, their\nlogic rigidity hinders adaptation to case-specific logical frameworks,\nparticularly in complex cases that are lengthy and detailed. This paper\nproposes a rule-enhanced legal judgment prediction framework based on\nfirst-order logic (FOL) formalism and comparative learning (CL) to develop an\nadaptive adjustment mechanism for legal judgment logic and further enhance\nperformance in LJP. Inspired by the process of human exam preparation, our\nmethod follows a three-stage approach: first, we initialize judgment rules\nusing the FOL formalism to capture complex reasoning logic accurately; next, we\npropose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize\nthe judgment rules through a quiz consisting of confusable cases; finally, we\nutilize the optimized judgment rules to predict legal judgments. Experimental\nresults on two public datasets show superior performance across all metrics.\nThe code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.", "categories": ["cs.AI"], "published": "2025-05-27 14:50:21", "updated": "2025-05-27 14:50:21", "pdf_url": "http://arxiv.org/pdf/2505.21281v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21288v1", "title": "GSAT: Graph Structure Attention Networks", "authors": ["Farshad Noravesh", "Reza Haffari", "Layki Soon", "Arghya Pal"], "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful tool for processing\ndata represented in graph structures, achieving remarkable success across a\nwide range of applications. However, to further improve the performance on\ngraph classification benchmarks, structural representation of each node that\nencodes rich local topological information in the neighbourhood of nodes is an\nimportant type of feature that is often overlooked in the modeling. The\nconsequence of neglecting the structural information has resulted high number\nof layers to connect messages from distant nodes which by itself produces other\nproblems such as oversmoothing. In the present paper, we leverage these\nstructural information that are modeled by anonymous random walks (ARWs) and\nintroduce graph structure attention network (GSAT) which is a generalization of\ngraph attention network(GAT) to integrate the original attribute and the\nstructural representation to enforce the model to automatically find patterns\nfor attending to different edges in the node neighbourhood to enrich graph\nrepresentation. Our experiments show GSAT slightly improves SOTA on some graph\nclassification benchmarks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 14:54:08", "updated": "2025-05-27 14:54:08", "pdf_url": "http://arxiv.org/pdf/2505.21288v1", "comment": "16 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21291v1", "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework", "authors": ["Saman Marandi", "Yu-Shu Hu", "Mohammad Modarres"], "abstract": "In this paper, we present a novel diagnostic framework that integrates\nKnowledge Graphs (KGs) and Large Language Models (LLMs) to support system\ndiagnostics in high-reliability systems such as nuclear power plants.\nTraditional diagnostic modeling struggles when systems become too complex,\nmaking functional modeling a more attractive approach. Our approach introduces\na diagnostic framework grounded in the functional modeling principles of the\nDynamic Master Logic (DML) model. It incorporates two coordinated LLM\ncomponents, including an LLM-based workflow for automated construction of DML\nlogic from system documentation and an LLM agent that facilitates interactive\ndiagnostics. The generated logic is encoded into a structured KG, referred to\nas KG-DML, which supports hierarchical fault reasoning. Expert knowledge or\noperational data can also be incorporated to refine the model's precision and\ndiagnostic depth. In the interaction phase, users submit natural language\nqueries, which are interpreted by the LLM agent. The agent selects appropriate\ntools for structured reasoning, including upward and downward propagation\nacross the KG-DML. Rather than embedding KG content into every prompt, the LLM\nagent distinguishes between diagnostic and interpretive tasks. For diagnostics,\nthe agent selects and executes external tools that perform structured KG\nreasoning. For general queries, a Graph-based Retrieval-Augmented Generation\n(Graph-RAG) approach is used, retrieving relevant KG segments and embedding\nthem into the prompt to generate natural explanations. A case study on an\nauxiliary feedwater system demonstrated the framework's effectiveness, with\nover 90% accuracy in key elements and consistent tool and argument extraction,\nsupporting its use in safety-critical diagnostics.", "categories": ["cs.AI"], "published": "2025-05-27 14:54:49", "updated": "2025-05-27 14:54:49", "pdf_url": "http://arxiv.org/pdf/2505.21291v1", "comment": "22 Pages, 11 Figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21298v1", "title": "Large Language Models Miss the Multi-Agent Mark", "authors": ["Emanuele La Malfa", "Gabriele La Malfa", "Samuele Marro", "Jie M. Zhang", "Elizabeth Black", "Micheal Luck", "Philip Torr", "Michael Wooldridge"], "abstract": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.", "categories": ["cs.MA", "cs.AI", "cs.LG"], "published": "2025-05-27 15:01:06", "updated": "2025-05-27 15:01:06", "pdf_url": "http://arxiv.org/pdf/2505.21298v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21301v1", "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian", "authors": ["Andrea Pedrotti", "Giulia Rambelli", "Caterina Villani", "Marianna Bolognesi"], "abstract": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 15:04:52", "updated": "2025-05-27 15:04:52", "pdf_url": "http://arxiv.org/pdf/2505.21301v1", "comment": "Accepted at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21317v1", "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features", "authors": ["Ihab Bendidi", "Yassir El Mesbahi", "Alisandra K. Denton", "Karush Suri", "Kian Kenyon-Dean", "Auguste Genovesio", "Emmanuel Noutahi"], "abstract": "Understanding cellular responses to stimuli is crucial for biological\ndiscovery and drug development. Transcriptomics provides interpretable,\ngene-level insights, while microscopy imaging offers rich predictive features\nbut is harder to interpret. Weakly paired datasets, where samples share\nbiological states, enable multimodal learning but are scarce, limiting their\nutility for training and multimodal inference. We propose a framework to\nenhance transcriptomics by distilling knowledge from microscopy images. Using\nweakly paired data, our method aligns and binds modalities, enriching gene\nexpression representations with morphological information. To address data\nscarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal\ndistillation using pretrained foundation models, achieving state-of-the-art\nresults, and (2) PEA (Perturbation Embedding Augmentation), a novel\naugmentation technique that enhances transcriptomics data while preserving\ninherent biological information. These strategies improve the predictive power\nand retain the interpretability of transcriptomics, enabling rich unimodal\nrepresentations for complex biological tasks.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 15:15:34", "updated": "2025-05-27 15:15:34", "pdf_url": "http://arxiv.org/pdf/2505.21317v1", "comment": "ICML 2025 Main Proceedings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21318v1", "title": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations", "authors": ["Hao Li", "He Cao", "Bin Feng", "Yanjun Shao", "Xiangru Tang", "Zhiyuan Yan", "Li Yuan", "Yonghong Tian", "Yu Li"], "abstract": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning\nexcel in mathematics and coding, their potential for systematic reasoning in\nchemistry, a domain demanding rigorous structural analysis for real-world tasks\nlike drug design and reaction engineering, remains untapped. Current benchmarks\nfocus on simple knowledge retrieval, neglecting step-by-step reasoning required\nfor complex tasks such as molecular optimization and reaction prediction. To\naddress this, we introduce ChemCoTBench, a reasoning framework that bridges\nmolecular structure understanding with arithmetic-inspired operations,\nincluding addition, deletion, and substitution, to formalize chemical\nproblem-solving into transparent, step-by-step workflows. By treating molecular\ntransformations as modular \"chemical operations\", the framework enables\nslow-thinking reasoning, mirroring the logic of mathematical proofs while\ngrounding solutions in real-world chemical constraints. We evaluate models on\ntwo high-impact tasks: Molecular Property Optimization and Chemical Reaction\nPrediction. These tasks mirror real-world challenges while providing structured\nevaluability. By providing annotated datasets, a reasoning taxonomy, and\nbaseline evaluations, ChemCoTBench bridges the gap between abstract reasoning\nmethods and practical chemical discovery, establishing a foundation for\nadvancing LLMs as tools for AI-driven scientific innovation.", "categories": ["cs.AI"], "published": "2025-05-27 15:15:44", "updated": "2025-05-27 15:15:44", "pdf_url": "http://arxiv.org/pdf/2505.21318v1", "comment": "22 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21322v1", "title": "Assured Autonomy with Neuro-Symbolic Perception", "authors": ["R. Spencer Hallyburton", "Miroslav Pajic"], "abstract": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS),\nwhile highly accurate, are simply pattern-matchers.~With limited security\nguarantees, there are concerns for their reliability in safety-critical and\ncontested domains. To advance assured AI, we advocate for a paradigm shift that\nimbues data-driven perception models with symbolic structure, inspired by a\nhuman's ability to reason over low-level features and high-level context. We\npropose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how\njoint object detection and scene graph generation (SGG) yields deep scene\nunderstanding.~Powered by foundation models for offline knowledge extraction\nand specialized SGG algorithms for real-time deployment, we design a framework\nleveraging structured relational graphs that ensures the integrity of\nsituational awareness in autonomy. Using physics-based simulators and\nreal-world datasets, we demonstrate how SGG bridges the gap between low-level\nsensor perception and high-level reasoning, establishing a foundation for\nresilient, context-aware AI and advancing trusted autonomy in CPS.", "categories": ["cs.AI"], "published": "2025-05-27 15:21:06", "updated": "2025-05-27 15:21:06", "pdf_url": "http://arxiv.org/pdf/2505.21322v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21327v1", "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs", "authors": ["Jiakang Yuan", "Tianshuo Peng", "Yilei Jiang", "Yiting Lu", "Renrui Zhang", "Kaituo Feng", "Chaoyou Fu", "Tao Chen", "Lei Bai", "Bo Zhang", "Xiangyu Yue"], "abstract": "Logical reasoning is a fundamental aspect of human intelligence and an\nessential capability for multimodal large language models (MLLMs). Despite the\nsignificant advancement in multimodal reasoning, existing benchmarks fail to\ncomprehensively evaluate their reasoning abilities due to the lack of explicit\ncategorization for logical reasoning types and an unclear understanding of\nreasoning. To address these issues, we introduce MME-Reasoning, a comprehensive\nbenchmark designed to evaluate the reasoning ability of MLLMs, which covers all\nthree types of reasoning (i.e., inductive, deductive, and abductive) in its\nquestions. We carefully curate the data to ensure that each question\neffectively evaluates reasoning ability rather than perceptual skills or\nknowledge breadth, and extend the evaluation protocols to cover the evaluation\nof diverse questions. Our evaluation reveals substantial limitations of\nstate-of-the-art MLLMs when subjected to holistic assessments of logical\nreasoning capabilities. Even the most advanced MLLMs show limited performance\nin comprehensive logical reasoning, with notable performance imbalances across\nreasoning types. In addition, we conducted an in-depth analysis of approaches\nsuch as ``thinking mode'' and Rule-based RL, which are commonly believed to\nenhance reasoning abilities. These findings highlight the critical limitations\nand performance imbalances of current MLLMs in diverse logical reasoning\nscenarios, providing comprehensive and systematic insights into the\nunderstanding and evaluation of reasoning capabilities.", "categories": ["cs.AI", "cs.CV"], "published": "2025-05-27 15:23:23", "updated": "2025-05-27 15:23:23", "pdf_url": "http://arxiv.org/pdf/2505.21327v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21329v1", "title": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks", "authors": ["Allaa Boutaleb", "Bernd Amann", "Hubert Naacke", "Rafael Angarita"], "abstract": "Recent table representation learning and data discovery methods tackle table\nunion search (TUS) within data lakes, which involves identifying tables that\ncan be unioned with a given query table to enrich its content. These methods\nare commonly evaluated using benchmarks that aim to assess semantic\nunderstanding in real-world TUS tasks. However, our analysis of prominent TUS\nbenchmarks reveals several limitations that allow simple baselines to perform\nsurprisingly well, often outperforming more sophisticated approaches. This\nsuggests that current benchmark scores are heavily influenced by\ndataset-specific characteristics and fail to effectively isolate the gains from\nsemantic understanding. To address this, we propose essential criteria for\nfuture benchmarks to enable a more realistic and reliable evaluation of\nprogress in semantic table union search.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "published": "2025-05-27 15:23:52", "updated": "2025-05-27 15:23:52", "pdf_url": "http://arxiv.org/pdf/2505.21329v1", "comment": "Accepted @ ACL 2025's Table Representation Learning Workshop (TRL)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21335v1", "title": "Structure from Collision", "authors": ["Takuhiro Kaneko"], "abstract": "Recent advancements in neural 3D representations, such as neural radiance\nfields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate\nestimation of 3D structures from multiview images. However, this capability is\nlimited to estimating the visible external structure, and identifying the\ninvisible internal structure hidden behind the surface is difficult. To\novercome this limitation, we address a new task called Structure from Collision\n(SfC), which aims to estimate the structure (including the invisible internal\nstructure) of an object from appearance changes during collision. To solve this\nproblem, we propose a novel model called SfC-NeRF that optimizes the invisible\ninternal structure of an object through a video sequence under physical,\nappearance (i.e., visible external structure)-preserving, and keyframe\nconstraints. In particular, to avoid falling into undesirable local optima\nowing to its ill-posed nature, we propose volume annealing; that is, searching\nfor global optima by repeatedly reducing and expanding the volume. Extensive\nexperiments on 115 objects involving diverse structures (i.e., various cavity\nshapes, locations, and sizes) and material properties revealed the properties\nof SfC and demonstrated the effectiveness of the proposed SfC-NeRF.", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "published": "2025-05-27 15:30:01", "updated": "2025-05-27 15:30:01", "pdf_url": "http://arxiv.org/pdf/2505.21335v1", "comment": "Accepted to CVPR 2025 (Highlight). Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/sfc/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21339v1", "title": "An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction", "authors": ["Henryk Mustroph", "Michel Kunkler", "Stefanie Rinderle-Ma"], "abstract": "Suffix prediction of business processes forecasts the remaining sequence of\nevents until process completion. Current approaches focus on predicting a\nsingle, most likely suffix. However, if the future course of a process is\nexposed to uncertainty or has high variability, the expressiveness of a single\nsuffix prediction can be limited. To address this limitation, we propose\nprobabilistic suffix prediction, a novel approach that approximates a\nprobability distribution of suffixes. The proposed approach is based on an\nUncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC)\nsuffix sampling algorithm. We capture epistemic uncertainties via MC dropout\nand aleatoric uncertainties as learned loss attenuation. This technical report\nprovides a detailed evaluation of the U-ED-LSTM's predictive performance and\nassesses its calibration on four real-life event logs with three different\nhyperparameter settings. The results show that i) the U-ED-LSTM has reasonable\npredictive performance across various datasets, ii) aggregating probabilistic\nsuffix predictions into mean values can outperform most likely predictions,\nparticularly for rare prefixes or longer suffixes, and iii) the approach\neffectively captures uncertainties present in event logs.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 15:33:05", "updated": "2025-05-27 15:33:05", "pdf_url": "http://arxiv.org/pdf/2505.21339v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21344v1", "title": "The Multilingual Divide and Its Impact on Global AI Safety", "authors": ["Aidan Peppin", "Julia Kreutzer", "Alice Schoenauer Sebag", "Kelly Marchisio", "Beyza Ermis", "John Dang", "Samuel Cahyawijaya", "Shivalika Singh", "Seraphina Goldfarb-Tarrant", "Viraat Aryabumi", "Aakanksha", "Wei-Yin Ko", "Ahmet \u00dcst\u00fcn", "Matthias Gall\u00e9", "Marzieh Fadaee", "Sara Hooker"], "abstract": "Despite advances in large language model capabilities in recent years, a\nlarge gap remains in their capabilities and safety performance for many\nlanguages beyond a relatively small handful of globally dominant languages.\nThis paper provides researchers, policymakers and governance experts with an\noverview of key challenges to bridging the \"language gap\" in AI and minimizing\nsafety risks across languages. We provide an analysis of why the language gap\nin AI exists and grows, and how it creates disparities in global AI safety. We\nidentify barriers to address these challenges, and recommend how those working\nin policy and governance can help address safety concerns associated with the\nlanguage gap by supporting multilingual dataset creation, transparency, and\nresearch.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-27 15:37:32", "updated": "2025-05-27 15:37:32", "pdf_url": "http://arxiv.org/pdf/2505.21344v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21355v1", "title": "Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods", "authors": ["Muhammad Imran", "Wayne G. Brisbane", "Li-Ming Su", "Jason P. Joseph", "Wei Shao"], "abstract": "Background and objective: Micro-ultrasound (micro-US) is a novel imaging\nmodality with diagnostic accuracy comparable to MRI for detecting clinically\nsignificant prostate cancer (csPCa). We investigated whether artificial\nintelligence (AI) interpretation of micro-US can outperform clinical screening\nmethods using PSA and digital rectal examination (DRE). Methods: We\nretrospectively studied 145 men who underwent micro-US guided biopsy (79 with\ncsPCa, 66 without). A self-supervised convolutional autoencoder was used to\nextract deep image features from 2D micro-US slices. Random forest classifiers\nwere trained using five-fold cross-validation to predict csPCa at the slice\nlevel. Patients were classified as csPCa-positive if 88 or more consecutive\nslices were predicted positive. Model performance was compared with a\nclassifier using PSA, DRE, prostate volume, and age. Key findings and\nlimitations: The AI-based micro-US model and clinical screening model achieved\nAUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US\nmodel achieved 92.5% sensitivity and 68.1% specificity, while the clinical\nmodel showed 96.2% sensitivity but only 27.3% specificity. Limitations include\na retrospective single-center design and lack of external validation.\nConclusions and clinical implications: AI-interpreted micro-US improves\nspecificity while maintaining high sensitivity for csPCa detection. This method\nmay reduce unnecessary biopsies and serve as a low-cost alternative to\nPSA-based screening. Patient summary: We developed an AI system to analyze\nprostate micro-ultrasound images. It outperformed PSA and DRE in detecting\naggressive cancer and may help avoid unnecessary biopsies.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "published": "2025-05-27 15:47:38", "updated": "2025-05-27 15:47:38", "pdf_url": "http://arxiv.org/pdf/2505.21355v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21362v1", "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "authors": ["Qishuai Zhong", "Zongmin Li", "Siqi Fan", "Aixin Sun"], "abstract": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-27 15:52:39", "updated": "2025-05-27 15:52:39", "pdf_url": "http://arxiv.org/pdf/2505.21362v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21363v1", "title": "Subgroups Matter for Robust Bias Mitigation", "authors": ["Anissa Alloula", "Charles Jones", "Ben Glocker", "Bart\u0142omiej W. Papie\u017c"], "abstract": "Despite the constant development of new bias mitigation methods for machine\nlearning, no method consistently succeeds, and a fundamental question remains\nunanswered: when and why do bias mitigation techniques fail? In this paper, we\nhypothesise that a key factor may be the often-overlooked but crucial step\nshared by many bias mitigation methods: the definition of subgroups. To\ninvestigate this, we conduct a comprehensive evaluation of state-of-the-art\nbias mitigation methods across multiple vision and language classification\ntasks, systematically varying subgroup definitions, including coarse,\nfine-grained, intersectional, and noisy subgroups. Our results reveal that\nsubgroup choice significantly impacts performance, with certain groupings\nparadoxically leading to worse outcomes than no mitigation at all. Our findings\nsuggest that observing a disparity between a set of subgroups is not a\nsufficient reason to use those subgroups for mitigation. Through theoretical\nanalysis, we explain these phenomena and uncover a counter-intuitive insight\nthat, in some cases, improving fairness with respect to a particular set of\nsubgroups is best achieved by using a different set of subgroups for\nmitigation. Our work highlights the importance of careful subgroup definition\nin bias mitigation and suggest it as a alternative lever for improving the\nrobustness and fairness of machine learning models.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 15:52:58", "updated": "2025-05-27 15:52:58", "pdf_url": "http://arxiv.org/pdf/2505.21363v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21364v1", "title": "Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders", "authors": ["James Oldfield", "Shawn Im", "Yixuan Li", "Mihalis A. Nicolaou", "Ioannis Patras", "Grigorios G Chrysos"], "abstract": "Multilayer perceptrons (MLPs) are an integral part of large language models,\nyet their dense representations render them difficult to understand, edit, and\nsteer. Recent methods learn interpretable approximations via neuron-level\nsparsity, yet fail to faithfully reconstruct the original\nmapping--significantly increasing model's next-token cross-entropy loss. In\nthis paper, we advocate for moving to layer-level sparsity to overcome the\naccuracy trade-off in sparse layer approximation. Under this paradigm, we\nintroduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear\nUnits, expanding pre-trained dense layers into tens of thousands of specialized\nsublayers. Through a flexible form of tensor factorization, each sparsely\nactivating MxD sublayer implements a linear transformation with full-rank\nweights--preserving the original decoders' expressive capacity even under heavy\nsparsity. Experimentally, we show that MxDs significantly outperform\nstate-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier\nin language models with up to 3B parameters. Further evaluations on sparse\nprobing and feature steering demonstrate that MxDs learn similarly specialized\nfeatures of natural language--opening up a promising new avenue for designing\ninterpretable yet faithful decompositions. Our code is included at:\nhttps://github.com/james-oldfield/MxD/.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 15:55:55", "updated": "2025-05-27 15:55:55", "pdf_url": "http://arxiv.org/pdf/2505.21364v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21372v1", "title": "Improving LLM-based Global Optimization with Search Space Partitioning", "authors": ["Andrej Schwanke", "Lyubomir Ivanov", "David Salinas", "Fabio Ferreira", "Aaron Klein", "Frank Hutter", "Arber Zela"], "abstract": "Large Language Models (LLMs) have recently emerged as effective surrogate\nmodels and candidate generators within global optimization frameworks for\nexpensive blackbox functions. Despite promising results, LLM-based methods\noften struggle in high-dimensional search spaces or when lacking\ndomain-specific priors, leading to sparse or uninformative suggestions. To\novercome these limitations, we propose HOLLM, a novel global optimization\nalgorithm that enhances LLM-driven sampling by partitioning the search space\ninto promising subregions. Each subregion acts as a ``meta-arm'' selected via a\nbandit-inspired scoring mechanism that effectively balances exploration and\nexploitation. Within each selected subregion, an LLM then proposes high-quality\ncandidate points, without any explicit domain knowledge. Empirical evaluation\non standard optimization benchmarks shows that HOLLM consistently matches or\nsurpasses leading Bayesian optimization and trust-region methods, while\nsubstantially outperforming global LLM-based sampling strategies.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 16:01:49", "updated": "2025-05-27 16:01:49", "pdf_url": "http://arxiv.org/pdf/2505.21372v1", "comment": "25 pages, 10 figures, 3 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21388v1", "title": "DeSocial: Blockchain-based Decentralized Social Networks", "authors": ["Jingyuan Huang", "Xi Zhu", "Minghao Guo", "Yongfeng Zhang"], "abstract": "Web 2.0 social platforms are inherently centralized, with user data and\nalgorithmic decisions controlled by the platform. However, users can only\npassively receive social predictions without being able to choose the\nunderlying algorithm, which limits personalization. Fortunately, with the\nemergence of blockchain, users are allowed to choose algorithms that are\ntailored to their local situation, improving prediction results in a\npersonalized way. In a blockchain environment, each user possesses its own\nmodel to perform the social prediction, capturing different perspectives on\nsocial interactions. In our work, we propose DeSocial, a decentralized social\nnetwork learning framework deployed on an Ethereum (ETH) local development\nchain that integrates distributed data storage, node-level consensus, and\nuser-driven model selection through Ganache. In the first stage, each user\nleverages DeSocial to evaluate multiple backbone models on their local\nsubgraph. DeSocial coordinates the execution and returns model-wise prediction\nresults, enabling the user to select the most suitable backbone for\npersonalized social prediction. Then, DeSocial uniformly selects several\nvalidation nodes that possess the algorithm specified by each user, and\naggregates the prediction results by majority voting, to prevent errors caused\nby any single model's misjudgment. Extensive experiments show that DeSocial has\nan evident improvement compared to the five classical centralized social\nnetwork learning models, promoting user empowerment in blockchain-based\ndecentralized social networks, showing the importance of multi-node validation\nand personalized algorithm selection based on blockchain. Our implementation is\navailable at: https://github.com/agiresearch/DeSocial.", "categories": ["cs.SI", "cs.AI", "cs.LG"], "published": "2025-05-27 16:17:06", "updated": "2025-05-27 16:17:06", "pdf_url": "http://arxiv.org/pdf/2505.21388v1", "comment": "29 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21391v1", "title": "Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features", "authors": ["Zixuan Xie", "Xinyu Liu", "Rohan Chandra", "Shangtong Zhang"], "abstract": "Linear TD($\\lambda$) is one of the most fundamental reinforcement learning\nalgorithms for policy evaluation. Previously, convergence rates are typically\nestablished under the assumption of linearly independent features, which does\nnot hold in many practical scenarios. This paper instead establishes the first\n$L^2$ convergence rates for linear TD($\\lambda$) operating under arbitrary\nfeatures, without making any algorithmic modification or additional\nassumptions. Our results apply to both the discounted and average-reward\nsettings. To address the potential non-uniqueness of solutions resulting from\narbitrary features, we develop a novel stochastic approximation result\nfeaturing convergence rates to the solution set instead of a single point.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "published": "2025-05-27 16:17:49", "updated": "2025-05-27 16:17:49", "pdf_url": "http://arxiv.org/pdf/2505.21391v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21393v1", "title": "Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits", "authors": ["Maoli Liu", "Zhuohua Li", "Xiangxiang Dai", "John C. S. Lui"], "abstract": "Conversational recommender systems proactively query users with relevant \"key\nterms\" and leverage the feedback to elicit users' preferences for personalized\nrecommendations. Conversational contextual bandits, a prevalent approach in\nthis domain, aim to optimize preference learning by balancing exploitation and\nexploration. However, several limitations hinder their effectiveness in\nreal-world scenarios. First, existing algorithms employ key term selection\nstrategies with insufficient exploration, often failing to thoroughly probe\nusers' preferences and resulting in suboptimal preference estimation. Second,\ncurrent algorithms typically rely on deterministic rules to initiate\nconversations, causing unnecessary interactions when preferences are\nwell-understood and missed opportunities when preferences are uncertain. To\naddress these limitations, we propose three novel algorithms: CLiSK, CLiME, and\nCLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in\npreference learning, CLiME adaptively initiates conversations based on\npreference uncertainty, and CLiSK-ME integrates both techniques. We\ntheoretically prove that all three algorithms achieve a tighter regret upper\nbound of $O(\\sqrt{dT\\log{T}})$ with respect to the time horizon $T$, improving\nupon existing methods. Additionally, we provide a matching lower bound\n$\\Omega(\\sqrt{dT})$ for conversational bandits, demonstrating that our\nalgorithms are nearly minimax optimal. Extensive evaluations on both synthetic\nand real-world datasets show that our approaches achieve at least a 14.6%\nimprovement in cumulative regret.", "categories": ["cs.LG", "cs.AI"], "published": "2025-05-27 16:22:32", "updated": "2025-05-27 16:22:32", "pdf_url": "http://arxiv.org/pdf/2505.21393v1", "comment": "Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining, 2025", "doi": "10.1145/3711896.3737025", "journal_ref": null}
{"arxiv_id": "2505.21396v1", "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science", "authors": ["Xiao Liu", "Xinyi Dong", "Xinyang Gao", "Yansong Feng", "Xun Pang"], "abstract": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "published": "2025-05-27 16:23:42", "updated": "2025-05-27 16:23:42", "pdf_url": "http://arxiv.org/pdf/2505.21396v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21398v1", "title": "A Structured Unplugged Approach for Foundational AI Literacy in Primary Education", "authors": ["Maria Cristina Carrisi", "Mirko Marras", "Sara Vergallo"], "abstract": "Younger generations are growing up in a world increasingly shaped by\nintelligent technologies, making early AI literacy crucial for developing the\nskills to critically understand and navigate them. However, education in this\nfield often emphasizes tool-based learning, prioritizing usage over\nunderstanding the underlying concepts. This lack of knowledge leaves\nnon-experts, especially children, prone to misconceptions, unrealistic\nexpectations, and difficulties in recognizing biases and stereotypes. In this\npaper, we propose a structured and replicable teaching approach that fosters\nfoundational AI literacy in primary students, by building upon core\nmathematical elements closely connected to and of interest in primary\ncurricula, to strengthen conceptualization, data representation, classification\nreasoning, and evaluation of AI. To assess the effectiveness of our approach,\nwe conducted an empirical study with thirty-one fifth-grade students across two\nclasses, evaluating their progress through a post-test and a satisfaction\nsurvey. Our results indicate improvements in terminology understanding and\nusage, features description, logical reasoning, and evaluative skills, with\nstudents showing a deeper comprehension of decision-making processes and their\nlimitations. Moreover, the approach proved engaging, with students particularly\nenjoying activities that linked AI concepts to real-world reasoning. Materials:\nhttps://github.com/tail-unica/ai-literacy-primary-ed.", "categories": ["cs.AI", "cs.ET"], "published": "2025-05-27 16:23:57", "updated": "2025-05-27 16:23:57", "pdf_url": "http://arxiv.org/pdf/2505.21398v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21399v1", "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling", "authors": ["Hovhannes Tamoyan", "Subhabrata Dutta", "Iryna Gurevych"], "abstract": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-27 16:24:02", "updated": "2025-05-27 16:24:02", "pdf_url": "http://arxiv.org/pdf/2505.21399v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21409v1", "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models", "authors": ["Dario Satriani", "Enzo Veltri", "Donatello Santoro", "Paolo Papotti"], "abstract": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.", "categories": ["cs.CL", "cs.AI", "cs.DB"], "published": "2025-05-27 16:33:38", "updated": "2025-05-27 16:33:38", "pdf_url": "http://arxiv.org/pdf/2505.21409v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21410v1", "title": "MRSD: Multi-Resolution Skill Discovery for HRL Agents", "authors": ["Shashank Sharma", "Janina Hoffmann", "Vinay Namboodiri"], "abstract": "Hierarchical reinforcement learning (HRL) relies on abstract skills to solve\nlong-horizon tasks efficiently. While existing skill discovery methods learns\nthese skills automatically, they are limited to a single skill per task. In\ncontrast, humans learn and use both fine-grained and coarse motor skills\nsimultaneously. Inspired by human motor control, we propose Multi-Resolution\nSkill Discovery (MRSD), an HRL framework that learns multiple skill encoders at\ndifferent temporal resolutions in parallel. A high-level manager dynamically\nselects among these skills, enabling adaptive control strategies over time. We\nevaluate MRSD on tasks from the DeepMind Control Suite and show that it\noutperforms prior state-of-the-art skill discovery and HRL methods, achieving\nfaster convergence and higher final performance. Our findings highlight the\nbenefits of integrating multi-resolution skills in HRL, paving the way for more\nversatile and efficient agents.", "categories": ["cs.AI", "cs.LG", "cs.RO"], "published": "2025-05-27 16:38:55", "updated": "2025-05-27 16:38:55", "pdf_url": "http://arxiv.org/pdf/2505.21410v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21413v1", "title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation", "authors": ["Xiao Liu", "Da Yin", "Zirui Wu", "Yansong Feng"], "abstract": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 16:41:19", "updated": "2025-05-27 16:41:19", "pdf_url": "http://arxiv.org/pdf/2505.21413v1", "comment": "Code is available at https://github.com/xxxiaol/RefTool", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21414v1", "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment", "authors": ["Brett Bissey", "Kyle Gatesman", "Walker Dimon", "Mohammad Alam", "Luis Robaina", "Joseph Weissman"], "abstract": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments.", "categories": ["cs.LG", "cs.AI", "cs.GT"], "published": "2025-05-27 16:41:23", "updated": "2025-05-27 16:41:23", "pdf_url": "http://arxiv.org/pdf/2505.21414v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21419v1", "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs", "authors": ["Yifan Wang", "Kenneth P. Birman"], "abstract": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.", "categories": ["cs.AI", "cs.OS"], "published": "2025-05-27 16:43:45", "updated": "2025-05-27 16:43:45", "pdf_url": "http://arxiv.org/pdf/2505.21419v1", "comment": "Published in EuroMLSys2025", "doi": "10.1145/3721146.3721958", "journal_ref": "2025, Association for Computing Machinery, Proceedings of the 5th\n  Workshop on Machine Learning and Systems, 139-147, 9, series EuroMLSys '25"}
{"arxiv_id": "2505.21420v1", "title": "Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning", "authors": ["Jinbao Wang", "Hanzhe Liang", "Can Gao", "Chenxi Hu", "Jie Zhou", "Yunkang Cao", "Linlin Shen", "Weiming Shen"], "abstract": "Multimodal feature reconstruction is a promising approach for 3D anomaly\ndetection, leveraging the complementary information from dual modalities. We\nfurther advance this paradigm by utilizing multi-modal mentor learning, which\nfuses intermediate features to further distinguish normal from feature\ndifferences. To address these challenges, we propose a novel method called\nMentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared\nfeatures of different modalities, Mentor3AD can extract more effective features\nand guide feature reconstruction, ultimately improving detection performance.\nSpecifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges\nfeatures extracted from RGB and 3D modalities to create a mentor feature.\nAdditionally, we have designed a Mentor of Guidance Module (MGM) to facilitate\ncross-modal reconstruction, supported by the mentor feature. Lastly, we\nintroduce a Voting Module (VM) to more accurately generate the final anomaly\nscore. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies\nhave verified the effectiveness of the proposed method.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 16:46:28", "updated": "2025-05-27 16:46:28", "pdf_url": "http://arxiv.org/pdf/2505.21420v1", "comment": "10 Pages, 6 Figures, 7 Tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21426v1", "title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks", "authors": ["Francesco Cozzi", "Marco Pangallo", "Alan Perotti", "Andr\u00e9 Panisson", "Corrado Monti"], "abstract": "Agent-Based Models (ABMs) are powerful tools for studying emergent properties\nin complex systems. In ABMs, agent behaviors are governed by local interactions\nand stochastic rules. However, these rules are, in general, non-differentiable,\nlimiting the use of gradient-based methods for optimization, and thus\nintegration with real-world data. We propose a novel framework to learn a\ndifferentiable surrogate of any ABM by observing its generated data. Our method\ncombines diffusion models to capture behavioral stochasticity and graph neural\nnetworks to model agent interactions. Distinct from prior surrogate approaches,\nour method introduces a fundamental shift: rather than approximating\nsystem-level outputs, it models individual agent behavior directly, preserving\nthe decentralized, bottom-up dynamics that define ABMs. We validate our\napproach on two ABMs (Schelling's segregation model and a Predator-Prey\necosystem) showing that it replicates individual-level patterns and accurately\nforecasts emergent dynamics beyond training. Our results demonstrate the\npotential of combining diffusion models and graph learning for data-driven ABM\nsimulation.", "categories": ["cs.AI", "cs.LG", "cs.MA", "econ.EM", "physics.soc-ph"], "published": "2025-05-27 16:55:56", "updated": "2025-05-27 16:55:56", "pdf_url": "http://arxiv.org/pdf/2505.21426v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21427v1", "title": "Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning", "authors": ["Xianling Mu", "Joseph Ternasky", "Fuat Alican", "Yigit Ihlamur"], "abstract": "Early-stage startup investment is a high-risk endeavor characterized by\nscarce data and uncertain outcomes. Traditional machine learning approaches\noften require large, labeled datasets and extensive fine-tuning, yet remain\nopaque and difficult for domain experts to interpret or improve. In this paper,\nwe propose a transparent and data-efficient investment decision framework\npowered by memory-augmented large language models (LLMs) using in-context\nlearning (ICL). Central to our method is a natural language policy embedded\ndirectly into the LLM prompt, enabling the model to apply explicit reasoning\npatterns and allowing human experts to easily interpret, audit, and iteratively\nrefine the logic. We introduce a lightweight training process that combines\nfew-shot learning with an in-context learning loop, enabling the LLM to update\nits decision policy iteratively based on structured feedback. With only minimal\nsupervision and no gradient-based optimization, our system predicts startup\nsuccess far more accurately than existing benchmarks. It is over 20x more\nprecise than random chance, which succeeds 1.9% of the time. It is also 7.1x\nmore precise than the typical 5.6% success rate of top-tier venture capital\n(VC) firms.", "categories": ["cs.AI", "cs.LG"], "published": "2025-05-27 16:57:07", "updated": "2025-05-27 16:57:07", "pdf_url": "http://arxiv.org/pdf/2505.21427v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "abstract": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "categories": ["cs.RO", "cs.AI"], "published": "2025-05-27 17:04:21", "updated": "2025-05-27 17:04:21", "pdf_url": "http://arxiv.org/pdf/2505.21432v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21441v1", "title": "Autoencoding Random Forests", "authors": ["Binh Duc Vu", "Jan Kapar", "Marvin Wright", "David S. Watson"], "abstract": "We propose a principled method for autoencoding with random forests. Our\nstrategy builds on foundational results from nonparametric statistics and\nspectral graph theory to learn a low-dimensional embedding of the model that\noptimally represents relationships in the data. We provide exact and\napproximate solutions to the decoding problem via constrained optimization,\nsplit relabeling, and nearest neighbors regression. These methods effectively\ninvert the compression pipeline, establishing a map from the embedding space\nback to the input space using splits learned by the ensemble's constituent\ntrees. The resulting decoders are universally consistent under common\nregularity assumptions. The procedure works with supervised or unsupervised\nmodels, providing a window into conditional or joint distributions. We\ndemonstrate various applications of this autoencoder, including powerful new\ntools for visualization, compression, clustering, and denoising. Experiments\nillustrate the ease and utility of our method in a wide range of settings,\nincluding tabular, image, and genomic data.", "categories": ["stat.ML", "cs.AI", "cs.LG"], "published": "2025-05-27 17:15:02", "updated": "2025-05-27 17:15:02", "pdf_url": "http://arxiv.org/pdf/2505.21441v1", "comment": "10 pages main text, 25 pages total. 5 figures main text, 9 figures\n  total", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21445v1", "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin", "authors": ["Zhiqi Ai", "Meixuan Bao", "Zhiyong Chen", "Zhi Yang", "Xinnuo Li", "Shugong Xu"], "abstract": "The performance of speaker verification systems is adversely affected by\nspeaker aging. However, due to challenges in data collection, particularly the\nlack of sustained and large-scale longitudinal data for individuals, research\non speaker aging remains difficult. In this paper, we present VoxAging, a\nlarge-scale longitudinal dataset collected from 293 speakers (226 English\nspeakers and 67 Mandarin speakers) over several years, with the longest time\nspan reaching 17 years (approximately 900 weeks). For each speaker, the data\nwere recorded at weekly intervals. We studied the phenomenon of speaker aging\nand its effects on advanced speaker verification systems, analyzed individual\nspeaker aging processes, and explored the impact of factors such as age group\nand gender on speaker aging research.", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "published": "2025-05-27 17:16:59", "updated": "2025-05-27 17:16:59", "pdf_url": "http://arxiv.org/pdf/2505.21445v1", "comment": "5 pages, 4 figures, Accepted by Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21457v1", "title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO", "authors": ["Muzhi Zhu", "Hao Zhong", "Canyu Zhao", "Zongze Du", "Zheng Huang", "Mingyu Liu", "Hao Chen", "Cheng Zou", "Jingdong Chen", "Ming Yang", "Chunhua Shen"], "abstract": "Active vision, also known as active perception, refers to the process of\nactively selecting where and how to look in order to gather task-relevant\ninformation. It is a critical component of efficient perception and\ndecision-making in humans and advanced embodied agents. Recently, the use of\nMultimodal Large Language Models (MLLMs) as central planning and\ndecision-making modules in robotic systems has gained extensive attention.\nHowever, despite the importance of active perception in embodied intelligence,\nthere is little to no exploration of how MLLMs can be equipped with or learn\nactive perception capabilities. In this paper, we first provide a systematic\ndefinition of MLLM-based active perception tasks. We point out that the\nrecently proposed GPT-o3 model's zoom-in search strategy can be regarded as a\nspecial case of active perception; however, it still suffers from low search\nefficiency and inaccurate region selection. To address these issues, we propose\nACTIVE-O3, a purely reinforcement learning based training framework built on\ntop of GRPO, designed to equip MLLMs with active perception capabilities. We\nfurther establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across\nboth general open-world tasks, such as small-object and dense object grounding,\nand domain-specific scenarios, including small object detection in remote\nsensing and autonomous driving, as well as fine-grained interactive\nsegmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot\nreasoning abilities on the V* Benchmark, without relying on any explicit\nreasoning data. We hope that our work can provide a simple codebase and\nevaluation protocol to facilitate future research on active perception in\nMLLMs.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 17:29:31", "updated": "2025-05-27 17:29:31", "pdf_url": "http://arxiv.org/pdf/2505.21457v1", "comment": "Project Page: https://aim-uofa.github.io/ACTIVE-o3", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21459v1", "title": "LazyVLM: Neuro-Symbolic Approach to Video Analytics", "authors": ["Xiangru Jian", "Wei Pang", "Zhengyuan Dong", "Chao Zhang", "M. Tamer \u00d6zsu"], "abstract": "Current video analytics approaches face a fundamental trade-off between\nflexibility and efficiency. End-to-end Vision Language Models (VLMs) often\nstruggle with long-context processing and incur high computational costs, while\nneural-symbolic methods depend heavily on manual labeling and rigid rule\ndesign. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics\nsystem that provides a user-friendly query interface similar to VLMs, while\naddressing their scalability limitation. LazyVLM enables users to effortlessly\ndrop in video data and specify complex multi-frame video queries using a\nsemi-structured text interface for video analytics. To address the scalability\nlimitations of VLMs, LazyVLM decomposes multi-frame video queries into\nfine-grained operations and offloads the bulk of the processing to efficient\nrelational query execution and vector similarity search. We demonstrate that\nLazyVLM provides a robust, efficient, and user-friendly solution for querying\nopen-domain video data at scale.", "categories": ["cs.DB", "cs.AI", "cs.CV", "cs.IR", "cs.MM"], "published": "2025-05-27 17:31:17", "updated": "2025-05-27 17:31:17", "pdf_url": "http://arxiv.org/pdf/2505.21459v1", "comment": "5 pages, 2 figures, Working paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21478v1", "title": "Policy Optimized Text-to-Image Pipeline Design", "authors": ["Uri Gadot", "Rinon Gal", "Yftah Ziser", "Gal Chechik", "Shie Mannor"], "abstract": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines.", "categories": ["cs.CV", "cs.AI"], "published": "2025-05-27 17:50:47", "updated": "2025-05-27 17:50:47", "pdf_url": "http://arxiv.org/pdf/2505.21478v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21486v1", "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming", "authors": ["Yang Yang", "Jiemin Wu", "Yutao Yue"], "abstract": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation.", "categories": ["cs.AI"], "published": "2025-05-27 17:53:38", "updated": "2025-05-27 17:53:38", "pdf_url": "http://arxiv.org/pdf/2505.21486v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "abstract": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "published": "2025-05-27 17:54:24", "updated": "2025-05-27 17:54:24", "pdf_url": "http://arxiv.org/pdf/2505.21488v1", "comment": "SIGGRAPH 2025. Project page: https://omer11a.github.io/be-decisive/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21497v1", "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers", "authors": ["Wei Pang", "Kevin Qinghong Lin", "Xiangru Jian", "Xi He", "Philip Torr"], "abstract": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA"], "published": "2025-05-27 17:58:49", "updated": "2025-05-27 17:58:49", "pdf_url": "http://arxiv.org/pdf/2505.21497v1", "comment": "Project Page: https://github.com/Paper2Poster/Paper2Poster", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21499v1", "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery", "authors": ["Haowei Wang", "Junjie Wang", "Xiaojun Jia", "Rupeng Zhang", "Mingyang Li", "Zhe Liu", "Yang Liu", "Qing Wang"], "abstract": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.", "categories": ["cs.CR", "cs.AI"], "published": "2025-05-27 17:59:05", "updated": "2025-05-27 17:59:05", "pdf_url": "http://arxiv.org/pdf/2505.21499v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-27 17:59:26", "updated": "2025-05-27 17:59:26", "pdf_url": "http://arxiv.org/pdf/2505.21500v1", "comment": "Project: https://zju-real.github.io/ViewSpatial-Page/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21503v1", "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making", "authors": ["Yihan Wang", "Qiao Yan", "Zhenghao Xing", "Lihao Liu", "Junjun He", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "abstract": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1.", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.OT"], "published": "2025-05-27 17:59:50", "updated": "2025-05-27 17:59:50", "pdf_url": "http://arxiv.org/pdf/2505.21503v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21505v1", "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective", "authors": ["Shimao Zhang", "Zhejian Lai", "Xiang Liu", "Shuaijie She", "Xiao Liu", "Yeyun Gong", "Shujian Huang", "Jiajun Chen"], "abstract": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 17:59:52", "updated": "2025-05-27 17:59:52", "pdf_url": "http://arxiv.org/pdf/2505.21505v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20606v1", "title": "Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation", "authors": ["Dancheng Liu", "Amir Nassereldine", "Chenhui Xu", "Jinjun Xiong"], "abstract": "Whisper's robust performance in automatic speech recognition (ASR) is often\nattributed to its massive 680k-hour training set, an impractical scale for most\nresearchers. In this work, we examine how linguistic and acoustic diversity in\ntraining data affect the robustness of the ASR model and reveal that\ntranscription generalization is primarily driven by acoustic variation rather\nthan linguistic richness. We find that targeted acoustic augmentation methods\ncould significantly improve the generalization ability of ASR models, reducing\nword-error rates by up to 19.24 percent on unseen datasets when training on the\n960-hour Librispeech dataset. These findings highlight strategic acoustically\nfocused data augmentation as a promising alternative to massive datasets for\nbuilding robust ASR models, offering a potential solution to future foundation\nASR models when massive human speech data is lacking.", "categories": ["cs.CL", "cs.MM"], "published": "2025-05-27 00:55:32", "updated": "2025-05-27 00:55:32", "pdf_url": "http://arxiv.org/pdf/2505.20606v1", "comment": "in submission", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20609v1", "title": "Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients", "authors": ["Hyungjun Park", "Chang-Yun Woo", "Seungjo Lim", "Seunghwan Lim", "Keunho Kwak", "Ju Young Jeong", "Chong Hyun Suh"], "abstract": "Objective To develop an LLM based realtime compound diagnostic medical AI\ninterface and performed a clinical trial comparing this interface and\nphysicians for common internal medicine cases based on the United States\nMedical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A\nnonrandomized clinical trial was conducted on August 20, 2024. We recruited one\ngeneral physician, two internal medicine residents (2nd and 3rd year), and five\nsimulated patients. The clinical vignettes were adapted from the USMLE Step 2\nCS style exams. We developed 10 representative internal medicine cases based on\nactual patients and included information available on initial diagnostic\nevaluation. Primary outcome was the accuracy of the first differential\ndiagnosis. Repeatability was evaluated based on the proportion of agreement.\nResults The accuracy of the physicians' first differential diagnosis ranged\nfrom 50% to 70%, whereas the realtime compound diagnostic medical AI interface\nachieved an accuracy of 80%. The proportion of agreement for the first\ndifferential diagnosis was 0.7. The accuracy of the first and second\ndifferential diagnoses ranged from 70% to 90% for physicians, whereas the AI\ninterface achieved an accuracy rate of 100%. The average time for the AI\ninterface (557 sec) was 44.6% shorter than that of the physicians (1006 sec).\nThe AI interface ($0.08) also reduced costs by 98.1% compared to the\nphysicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3\nfor care by physicians and were 3.9 for the AI interface Conclusion An LLM\nbased realtime compound diagnostic medical AI interface demonstrated diagnostic\naccuracy and patient satisfaction comparable to those of a physician, while\nrequiring less time and lower costs. These findings suggest that AI interfaces\nmay have the potential to assist primary care consultations for common internal\nmedicine cases.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-27 01:15:46", "updated": "2025-05-27 01:15:46", "pdf_url": "http://arxiv.org/pdf/2505.20609v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20612v1", "title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "authors": ["Peter Robicheaux", "Matvei Popov", "Anish Madan", "Isaac Robinson", "Joseph Nelson", "Deva Ramanan", "Neehar Peri"], "abstract": "Vision-language models (VLMs) trained on internet-scale data achieve\nremarkable zero-shot detection performance on common objects like car, truck,\nand pedestrian. However, state-of-the-art models still struggle to generalize\nto out-of-distribution classes, tasks and imaging modalities not typically\nfound in their pre-training. Rather than simply re-training VLMs on more visual\ndata, we argue that one should align VLMs to new concepts with annotation\ninstructions containing a few visual examples and rich textual descriptions. To\nthis end, we introduce Roboflow100-VL, a large-scale collection of 100\nmulti-modal object detection datasets with diverse concepts not commonly found\nin VLM pre-training. We evaluate state-of-the-art models on our benchmark in\nzero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing\nfor comparison across data regimes. Notably, we find that VLMs like\nGroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on\nchallenging medical imaging datasets within Roboflow100-VL, demonstrating the\nneed for few-shot concept alignment. Our code and dataset are available at\nhttps://github.com/roboflow/rf100-vl/ and\nhttps://universe.roboflow.com/rf100-vl/", "categories": ["cs.CV", "cs.CL", "cs.LG"], "published": "2025-05-27 01:24:29", "updated": "2025-05-27 01:24:29", "pdf_url": "http://arxiv.org/pdf/2505.20612v1", "comment": "The first two authors contributed equally", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20613v1", "title": "REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning", "authors": ["Ziju Shen", "Naohao Huang", "Fanyi Yang", "Yutong Wang", "Guoxiong Gao", "Tianyi Xu", "Jiedong Jiang", "Wanyi He", "Pu Yang", "Mengzhou Sun", "Haocheng Ju", "Peihao Wu", "Bryan Dai", "Bin Dong"], "abstract": "Nowadays, formal theorem provers have made monumental progress on high-school\nand competition-level mathematics, but few of them generalize to more advanced\nmathematics. In this paper, we present REAL-Prover, a new open-source stepwise\ntheorem prover for Lean 4 to push this boundary. This prover, based on our\nfine-tuned large language model (REAL-Prover-v1) and integrated with a\nretrieval system (Leansearch-PS), notably boosts performance on solving\ncollege-level mathematics problems. To train REAL-Prover-v1, we developed\nHERALD-AF, a data extraction pipeline that converts natural language math\nproblems into formal statements, and a new open-source Lean 4 interactive\nenvironment (Jixia-interactive) to facilitate synthesis data collection. In our\nexperiments, our prover using only supervised fine-tune achieves competitive\nresults with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable\nto state-of-the-art (SOTA) models. To further evaluate our approach, we\nintroduce FATE-M, a new benchmark focused on algebraic problems, where our\nprover achieves a SOTA success rate of 56.7% (Pass@64).", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "published": "2025-05-27 01:26:11", "updated": "2025-05-27 01:26:11", "pdf_url": "http://arxiv.org/pdf/2505.20613v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20622v1", "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation", "authors": ["Ting Xu", "Zhichao Huang", "Jiankai Sun", "Shanbo Cheng", "Wai Lam"], "abstract": "We present Sequential Policy Optimization for Simultaneous Machine\nTranslation (SeqPO-SiMT), a new policy optimization framework that defines the\nsimultaneous machine translation (SiMT) task as a sequential decision making\nproblem, incorporating a tailored reward to enhance translation quality while\nreducing latency. In contrast to popular Reinforcement Learning from Human\nFeedback (RLHF) methods, such as PPO and DPO, which are typically applied in\nsingle-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.\nThis intuitive framework allows the SiMT LLMs to simulate and refine the SiMT\nprocess using a tailored reward. We conduct experiments on six datasets from\ndiverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that\nSeqPO-SiMT consistently achieves significantly higher translation quality with\nlower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning\n(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17\nin the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context\nthan offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly\nrival the offline translation of high-performing LLMs, including\nQwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-27 01:59:58", "updated": "2025-05-27 01:59:58", "pdf_url": "http://arxiv.org/pdf/2505.20622v1", "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20624v1", "title": "POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization", "authors": ["Usman Naseem", "Juan Ren", "Saba Anwar", "Sarah Kohail", "Rudy Alexandro Garrido Veliz", "Robert Geislinger", "Aisha Jabr", "Idris Abdulmumin", "Laiba Qureshi", "Aarushi Ajay Borkar", "Maryam Ibrahim Mukhtar", "Abinew Ali Ayele", "Ibrahim Said Ahmad", "Adem Ali", "Martin Semmann", "Shamsuddeen Hassan Muhammad", "Seid Muhie Yimam"], "abstract": "Online polarization poses a growing challenge for democratic discourse, yet\nmost computational social science research remains monolingual, culturally\nnarrow, or event-specific. We introduce POLAR, a multilingual, multicultural,\nand multievent dataset with over 23k instances in seven languages from diverse\nonline platforms and real-world events. Polarization is annotated along three\naxes: presence, type, and manifestation, using a variety of annotation\nplatforms adapted to each cultural context. We conduct two main experiments:\n(1) we fine-tune six multilingual pretrained language models in both\nmonolingual and cross-lingual setups; and (2) we evaluate a range of open and\nclosed large language models (LLMs) in few-shot and zero-shot scenarios.\nResults show that while most models perform well on binary polarization\ndetection, they achieve substantially lower scores when predicting polarization\ntypes and manifestations. These findings highlight the complex, highly\ncontextual nature of polarization and the need for robust, adaptable approaches\nin NLP and computational social science. All resources will be released to\nsupport further research and effective mitigation of digital polarization\nglobally.", "categories": ["cs.CL"], "published": "2025-05-27 02:04:58", "updated": "2025-05-27 02:04:58", "pdf_url": "http://arxiv.org/pdf/2505.20624v1", "comment": "Preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20625v1", "title": "Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration", "authors": ["Sibo Xiao", "Zixin Lin", "Wenyang Gao", "Yue Zhang"], "abstract": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). Existing works leverage agent-based divide-and-conquer\nmethods for processing long contexts. But these methods face crucial\nlimitations, including prohibitive accumulated latency and amplified\ninformation loss from excessive agent invocations, and the disruption of\ninherent textual dependencies by immoderate partitioning. In this paper, we\npropose a novel multi-agent framework XpandA (Expand-Agent) coupled with\nquestion-driven workflow and dynamic partitioning for robust long-context\nprocessing. XpandA overcomes these limitations through: 1) dynamic partitioning\nof long texts, which adaptively modulates the filling rate of context windows\nfor input sequences of vastly varying lengths; 2) question-guided protocol to\nupdate flat information ensembles within centralized shared memory,\nconstructing consistent inter-agent knowledge across partitions; and 3)\nselectively replaying specific partitions based on the state-tracking of\nquestion-information couples to promote the resolution of inverted-order\nstructures across partitions (e.g., flashbacks). We perform a comprehensive\nevaluation of XpandA on multiple long-context benchmarks with length varying\nfrom 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long\nsequences and its significant effectiveness in enhancing the long-context\ncapabilities of various LLMs by achieving 20\\% improvements and 1.5x inference\nspeedup over baselines of full-context, RAG and previous agent-based methods.", "categories": ["cs.CL"], "published": "2025-05-27 02:05:42", "updated": "2025-05-27 02:05:42", "pdf_url": "http://arxiv.org/pdf/2505.20625v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20630v1", "title": "SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis", "authors": ["Yansong Li", "Paula Branco", "Alexander M. Hoole", "Manish Marwah", "Hari Manassery Koduvely", "Guy-Vincent Jourdan", "Stephan Jou"], "abstract": "As Large Language Models (LLMs) evolve in understanding and generating code,\naccurately evaluating their reliability in analyzing source code\nvulnerabilities becomes increasingly vital. While studies have examined LLM\ncapabilities in tasks like vulnerability detection and repair, they often\noverlook the importance of both structure and semantic reasoning crucial for\ntrustworthy vulnerability analysis. To address this gap, we introduce\nSV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities for\nvulnerability analysis of code written in the C programming language through\ntwo key dimensions: structure reasoning - assessing how models identify\nrelationships between code elements under varying data and control flow\ncomplexities; and semantic reasoning - examining their logical consistency in\nscenarios where code is structurally and semantically perturbed. Our results\nshow that current LLMs are far from satisfactory in understanding complex code\nrelationships and that their vulnerability analyses rely more on pattern\nmatching than on robust logical reasoning. These findings underscore the\neffectiveness of the SV-TrustEval-C benchmark and highlight critical areas for\nenhancing the reasoning capabilities and trustworthiness of LLMs in real-world\nvulnerability analysis tasks. Our initial benchmark dataset is publicly\navailable.", "categories": ["cs.SE", "cs.CL"], "published": "2025-05-27 02:16:27", "updated": "2025-05-27 02:16:27", "pdf_url": "http://arxiv.org/pdf/2505.20630v1", "comment": null, "doi": null, "journal_ref": "2025 IEEE Symposium on Security and Privacy (SP), 2025, pp.\n  2791-2809"}
{"arxiv_id": "2505.20633v1", "title": "Test-Time Learning for Large Language Models", "authors": ["Jinwu Hu", "Zhitian Zhang", "Guohao Chen", "Xutao Wen", "Chao Shuai", "Wei Luo", "Bin Xiao", "Yuanqing Li", "Mingkui Tan"], "abstract": "While Large Language Models (LLMs) have exhibited remarkable emergent\ncapabilities through extensive pre-training, they still face critical\nlimitations in generalizing to specialized domains and handling diverse\nlinguistic variations, known as distribution shifts. In this paper, we propose\na Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically\nadapts LLMs to target domains using only unlabeled test data during testing.\nSpecifically, we first provide empirical evidence and theoretical insights to\nreveal that more accurate predictions from LLMs can be achieved by minimizing\nthe input perplexity of the unlabeled test data. Based on this insight, we\nformulate the Test-Time Learning process of LLMs as input perplexity\nminimization, enabling self-supervised enhancement of LLM performance.\nFurthermore, we observe that high-perplexity samples tend to be more\ninformative for model optimization. Accordingly, we introduce a Sample\nEfficient Learning Strategy that actively selects and emphasizes these\nhigh-perplexity samples for test-time updates. Lastly, to mitigate catastrophic\nforgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)\ninstead of full-parameter optimization, which allows lightweight model updates\nwhile preserving more original knowledge from the model. We introduce the\nAdaptEval benchmark for TTL and demonstrate through experiments that TLM\nimproves performance by at least 20% compared to original LLMs on domain\nknowledge adaptation.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-27 02:18:59", "updated": "2025-05-27 02:18:59", "pdf_url": "http://arxiv.org/pdf/2505.20633v1", "comment": "Accepted by ICML2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20645v1", "title": "STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models", "authors": ["Kai Chen", "Zihao He", "Taiwei Shi", "Kristina Lerman"], "abstract": "Steerability, or the ability of large language models (LLMs) to adapt outputs\nto align with diverse community-specific norms, perspectives, and communication\nstyles, is critical for real-world applications but remains under-evaluated. We\nintroduce Steer-Bench, a benchmark for assessing population-specific steering\nusing contrasting Reddit communities. Covering 30 contrasting subreddit pairs\nacross 19 domains, Steer-Bench includes over 10,000 instruction-response pairs\nand validated 5,500 multiple-choice question with corresponding silver labels\nto test alignment with diverse community norms. Our evaluation of 13 popular\nLLMs using Steer-Bench reveals that while human experts achieve an accuracy of\n81% with silver labels, the best-performing models reach only around 65%\naccuracy depending on the domain and configuration. Some models lag behind\nhuman-level alignment by over 15 percentage points, highlighting significant\ngaps in community-sensitive steerability. Steer-Bench is a benchmark to\nsystematically assess how effectively LLMs understand community-specific\ninstructions, their resilience to adversarial steering attempts, and their\nability to accurately represent diverse cultural and ideological perspectives.", "categories": ["cs.CL"], "published": "2025-05-27 02:47:56", "updated": "2025-05-27 02:47:56", "pdf_url": "http://arxiv.org/pdf/2505.20645v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20650v1", "title": "FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information", "authors": ["Yan Wang", "Yang Ren", "Lingfei Qian", "Xueqing Peng", "Keyi Wang", "Yi Han", "Dongji Feng", "Xiao-Yang Liu", "Jimin Huang", "Qianqian Xie"], "abstract": "We introduce FinTagging, the first full-scope, table-aware XBRL benchmark\ndesigned to evaluate the structured information extraction and semantic\nalignment capabilities of large language models (LLMs) in the context of\nXBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL\ntagging as flat multi-class classification and focus solely on narrative text,\nFinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for\nfinancial entity extraction and FinCL for taxonomy-driven concept alignment. It\nrequires models to jointly extract facts and align them with the full 10k+\nUS-GAAP taxonomy across both unstructured text and structured tables, enabling\nrealistic, fine-grained evaluation. We assess a diverse set of LLMs under\nzero-shot settings, systematically analyzing their performance on both subtasks\nand overall tagging accuracy. Our results reveal that, while LLMs demonstrate\nstrong generalization in information extraction, they struggle with\nfine-grained concept alignment, particularly in disambiguating closely related\ntaxonomy entries. These findings highlight the limitations of existing LLMs in\nfully automating XBRL tagging and underscore the need for improved semantic\nreasoning and schema-aware modeling to meet the demands of accurate financial\ndisclosure. Code is available at our GitHub repository and data is at our\nHugging Face repository.", "categories": ["cs.CL", "cs.AI", "cs.CE"], "published": "2025-05-27 02:55:53", "updated": "2025-05-27 02:55:53", "pdf_url": "http://arxiv.org/pdf/2505.20650v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20654v1", "title": "Chinese Cyberbullying Detection: Dataset, Method, and Validation", "authors": ["Yi Zhu", "Xin Zou", "Xindong Wu"], "abstract": "Existing cyberbullying detection benchmarks were organized by the polarity of\nspeech, such as \"offensive\" and \"non-offensive\", which were essentially hate\nspeech detection. However, in the real world, cyberbullying often attracted\nwidespread social attention through incidents. To address this problem, we\npropose a novel annotation method to construct a cyberbullying dataset that\norganized by incidents. The constructed CHNCI is the first Chinese\ncyberbullying incident detection dataset, which consists of 220,676 comments in\n91 incidents. Specifically, we first combine three cyberbullying detection\nmethods based on explanations generation as an ensemble method to generate the\npseudo labels, and then let human annotators judge these labels. Then we\npropose the evaluation criteria for validating whether it constitutes a\ncyberbullying incident. Experimental results demonstrate that the constructed\ndataset can be a benchmark for the tasks of cyberbullying detection and\nincident prediction. To the best of our knowledge, this is the first study for\nthe Chinese cyberbullying incident detection task.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 03:03:55", "updated": "2025-05-27 03:03:55", "pdf_url": "http://arxiv.org/pdf/2505.20654v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20658v1", "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge", "authors": ["Yue Fang", "Zhi Jin", "Jie An", "Hongshen Chen", "Xiaohong Chen", "Naijun Zhan"], "abstract": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise\nformal specification, making it widely used in cyber-physical systems such as\nautonomous driving and robotics. Automatically transforming NL into STL is an\nattractive approach to overcome the limitations of manual transformation, which\nis time-consuming and error-prone. However, due to the lack of datasets,\nautomatic transformation currently faces significant challenges and has not\nbeen fully explored. In this paper, we propose an NL-STL dataset named\nSTL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched\nwith diverse patterns. To develop the dataset, we first manually create a\nsmall-scale seed set of NL-STL pairs. Next, representative examples are\nidentified through clustering and used to guide large language models (LLMs) in\ngenerating additional NL-STL pairs. Finally, diversity and accuracy are ensured\nthrough rigorous rule-based filters and human validation. Furthermore, we\nintroduce the Knowledge-Guided STL Transformation (KGST) framework, a novel\napproach for transforming natural language into STL, involving a\ngenerate-then-refine process based on external knowledge. Statistical analysis\nshows that the STL-DivEn dataset exhibits more diversity than the existing\nNL-STL dataset. Moreover, both metric-based and human evaluations indicate that\nour KGST approach outperforms baseline models in transformation accuracy on\nSTL-DivEn and DeepSTL datasets.", "categories": ["cs.CL"], "published": "2025-05-27 03:07:25", "updated": "2025-05-27 03:07:25", "pdf_url": "http://arxiv.org/pdf/2505.20658v1", "comment": "13 pages, 5 figures, published to ACL", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20660v1", "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism", "authors": ["Qinzhuo Wu", "Pengzhi Gao", "Wei Liu", "Jian Luan"], "abstract": "Graphical User Interface (GUI) agents have gained substantial attention due\nto their impressive capabilities to complete tasks through multiple\ninteractions within GUI environments. However, existing agents primarily focus\non enhancing the accuracy of individual actions and often lack effective\nmechanisms for detecting and recovering from errors. To address these\nshortcomings, we propose the BacktrackAgent, a robust framework that\nincorporates a backtracking mechanism to improve task completion efficiency.\nBacktrackAgent includes verifier, judger, and reflector components as modules\nfor error detection and recovery, while also applying judgment rewards to\nfurther enhance the agent's performance. Additionally, we develop a training\ndataset specifically designed for the backtracking mechanism, which considers\nthe outcome pages after action executions. Experimental results show that\nBacktrackAgent has achieved performance improvements in both task success rate\nand step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be\nreleased upon acceptance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 03:09:06", "updated": "2025-05-27 03:09:06", "pdf_url": "http://arxiv.org/pdf/2505.20660v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20663v1", "title": "TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research", "authors": ["Xu Kang", "Siqi Jiang", "Kangwei Xu", "Jiahao Li", "Ruibo Wu"], "abstract": "Terpenoids are a crucial class of natural products that have been studied for\nover 150 years, but their interdisciplinary nature (spanning chemistry,\npharmacology, and biology) complicates knowledge integration. To address this,\nthe authors developed TeroSeek, a curated knowledge base (KB) built from two\ndecades of terpenoid literature, coupled with an AI-powered question-answering\nchatbot and web service. Leveraging a retrieval-augmented generation (RAG)\nframework, TeroSeek provides structured, high-quality information and\noutperforms general-purpose large language models (LLMs) in terpenoid-related\nqueries. It serves as a domain-specific expert tool for multidisciplinary\nresearch and is publicly available at http://teroseek.qmclab.com.", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3; I.2"], "published": "2025-05-27 03:17:30", "updated": "2025-05-27 03:17:30", "pdf_url": "http://arxiv.org/pdf/2505.20663v1", "comment": "18 pages, 4 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20664v1", "title": "Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning", "authors": ["Yang He", "Xiao Ding", "Bibo Cai", "Yufei Zhang", "Kai Xiong", "Zhouhao Sun", "Bing Qin", "Ting Liu"], "abstract": "While reasoning-augmented large language models (RLLMs) significantly enhance\ncomplex task performance through extended reasoning chains, they inevitably\nintroduce substantial unnecessary token consumption, particularly for simpler\nproblems where Short Chain-of-Thought (Short CoT) suffices. This overthinking\nphenomenon leads to inefficient resource usage without proportional accuracy\ngains. To address this issue, we propose Self-Route, a dynamic reasoning\nframework that automatically selects between general and reasoning modes based\non model capability estimation. Our approach introduces a lightweight\npre-inference stage to extract capability-aware embeddings from hidden layer\nrepresentations, enabling real-time evaluation of the model's ability to solve\nproblems. We further construct Gradient-10K, a model difficulty\nestimation-based dataset with dense complexity sampling, to train the router\nfor precise capability boundary detection. Extensive experiments demonstrate\nthat Self-Route achieves comparable accuracy to reasoning models while reducing\ntoken consumption by 30-55\\% across diverse benchmarks. The proposed framework\ndemonstrates consistent effectiveness across models with different parameter\nscales and reasoning paradigms, highlighting its general applicability and\npractical value.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 03:18:31", "updated": "2025-05-27 03:18:31", "pdf_url": "http://arxiv.org/pdf/2505.20664v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20674v1", "title": "Pretraining Language Models to Ponder in Continuous Space", "authors": ["Boyi Zeng", "Shixiang Song", "Siyuan Huang", "Yixuan Wang", "He Li", "Ziwei He", "Xinbing Wang", "Zhiyu Li", "Zhouhan Lin"], "abstract": "Humans ponder before articulating complex sentence elements, enabling deeper\ncognitive processing through focused effort. In this work, we introduce this\npondering process into language models by repeatedly invoking the forward\nprocess within a single token generation step. During pondering, instead of\ngenerating an actual token sampled from the prediction distribution, the model\nponders by yielding a weighted sum of all token embeddings according to the\npredicted token distribution. The generated embedding is then fed back as input\nfor another forward pass. We show that the model can learn to ponder in this\nway through self-supervised learning, without any human annotations. Our method\nis straightforward and can be seamlessly integrated with various existing\nlanguage models. Experiments across three widely used open-source\narchitectures-GPT-2, Pythia, and LLaMA-and extensive downstream task\nevaluations demonstrate the effectiveness and generality of our method. For\nlanguage modeling tasks, pondering language models achieve performance\ncomparable to vanilla models with twice the number of parameters. On 9\ndownstream benchmarks, our pondering-enhanced Pythia models significantly\noutperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is\ncomparable to TinyLlama-1.1B, which is trained on 10 times more data. The code\nis available at https://github.com/LUMIA-Group/PonderingLM.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 03:47:33", "updated": "2025-05-27 03:47:33", "pdf_url": "http://arxiv.org/pdf/2505.20674v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20679v1", "title": "SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations", "authors": ["Danush Khanna", "Pratinav Seth", "Sidhaarth Sredharan Murali", "Aditya Kumar Guru", "Siddharth Shukla", "Tanuj Tyagi", "Sandeep Chaurasia", "Kripabandhu Ghosh"], "abstract": "Mental manipulation is a subtle yet pervasive form of abuse in interpersonal\ncommunication, making its detection critical for safeguarding potential\nvictims. However, due to manipulation's nuanced and context-specific nature,\nidentifying manipulative language in complex, multi-turn, and multi-person\nconversations remains a significant challenge for large language models (LLMs).\nTo address this gap, we introduce the MultiManip dataset, comprising 220\nmulti-turn, multi-person dialogues balanced between manipulative and\nnon-manipulative interactions, all drawn from reality shows that mimic\nreal-world scenarios. For manipulative interactions, it includes 11 distinct\nmanipulations depicting real-life scenarios. We conduct extensive evaluations\nof state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various\nprompting strategies. Despite their capabilities, these models often struggle\nto detect manipulation effectively. To overcome this limitation, we propose\nSELF-PERCEPT, a novel, two-stage prompting framework inspired by\nSelf-Perception Theory, demonstrating strong performance in detecting\nmulti-person, multi-turn mental manipulation. Our code and data are publicly\navailable at https://github.com/danushkhanna/self-percept .", "categories": ["cs.CL", "cs.HC", "cs.LG"], "published": "2025-05-27 03:51:25", "updated": "2025-05-27 03:51:25", "pdf_url": "http://arxiv.org/pdf/2505.20679v1", "comment": "Accepted to ACL 2025 (Main)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20692v1", "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions", "authors": ["Saharsh Barve", "Andy Mao", "Jiayue Melissa Shi", "Prerna Juneja", "Koustuv Saha"], "abstract": "Recent advances in generative AI have enabled visual content creation through\ntext-to-image (T2I) generation. However, despite their creative potential, T2I\nmodels often replicate and amplify societal stereotypes -- particularly those\nrelated to gender, race, and culture -- raising important ethical concerns.\nThis paper proposes a theory-driven bias detection rubric and a Social\nStereotype Index (SSI) to systematically evaluate social biases in T2I outputs.\nWe audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and\nStability AI Core -- using 100 queries across three categories -- geocultural,\noccupational, and adjectival. Our analysis reveals that initial outputs are\nprone to include stereotypical visual cues, including gendered professions,\ncultural markers, and western beauty norms. To address this, we adopted our\nrubric to conduct targeted prompt refinement using LLMs, which significantly\nreduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and\n51% for adjectival queries. We complemented our quantitative analysis through a\nuser study examining perceptions, awareness, and preferences around\nAI-generated biased imagery. Our findings reveal a key tension -- although\nprompt refinement can mitigate stereotypes, it can limit contextual alignment.\nInterestingly, users often perceived stereotypical images to be more aligned\nwith their expectations. We discuss the need to balance ethical debiasing with\ncontextual relevance and call for T2I systems that support global diversity and\ninclusivity while not compromising the reflection of real-world social\ncomplexity.", "categories": ["cs.HC", "cs.AI", "cs.CL"], "published": "2025-05-27 04:01:03", "updated": "2025-05-27 04:01:03", "pdf_url": "http://arxiv.org/pdf/2505.20692v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20693v1", "title": "Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages", "authors": ["Praveen Srinivasa Varadhan", "Srija Anand", "Soma Siddhartha", "Mitesh M. Khapra"], "abstract": "What happens when an English Fairytaler is fine-tuned on Indian languages? We\nevaluate how the English F5-TTS model adapts to 11 Indian languages, measuring\npolyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:\n(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and\n(iii) fine-tuning on both Indian and English data to prevent forgetting.\nFine-tuning with only Indian data proves most effective and the resultant IN-F5\nis a near-human polyglot; that enables speakers of one language (e.g., Odia) to\nfluently speak in another (e.g., Hindi). Our results show English pretraining\naids low-resource TTS in reaching human parity. To aid progress in other\nlow-resource languages, we study data-constrained setups and arrive at a\ncompute optimal strategy. Finally, we show IN-F5 can synthesize unseen\nlanguages like Bhojpuri and Tulu using a human-in-the-loop approach for\nzero-resource TTS via synthetic data generation.", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "published": "2025-05-27 04:02:01", "updated": "2025-05-27 04:02:01", "pdf_url": "http://arxiv.org/pdf/2505.20693v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20700v1", "title": "Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration", "authors": ["Yong Wu", "Weihang Pan", "Ke Li", "Chen Binhui", "Ping Li", "Binbin Lin"], "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nyet aligning such abilities to small language models (SLMs) remains a challenge\ndue to distributional mismatches and limited model capacity. Existing reasoning\ndatasets, typically designed for powerful LLMs, often lead to degraded\nperformance when directly applied to weaker models. In this work, we introduce\nDynamic Adaptation of Reasoning Trajectories (DART), a novel data adaptation\nframework that bridges the capability gap between expert reasoning trajectories\nand diverse SLMs. Instead of uniformly imitating expert steps, DART employs a\nselective imitation strategy guided by step-wise adaptability estimation via\nsolution simulation. When expert steps surpass the student's capacity --\nsignaled by an Imitation Gap -- the student autonomously explores alternative\nreasoning paths, constrained by outcome consistency. We validate DART across\nmultiple reasoning benchmarks and model scales, demonstrating that it\nsignificantly improves generalization and data efficiency over static\nfine-tuning. Our method enhances supervision quality by aligning training\nsignals with the student's reasoning capabilities, offering a scalable solution\nfor reasoning alignment in resource-constrained models.", "categories": ["cs.CL"], "published": "2025-05-27 04:08:11", "updated": "2025-05-27 04:08:11", "pdf_url": "http://arxiv.org/pdf/2505.20700v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20707v1", "title": "Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Deepak Subramani"], "abstract": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, making them promising for educational applications. However,\ntheir capacity for complex reasoning, particularly in domains such as physics,\nremains underexplored. This study investigates the high school physics\nreasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),\nincluding instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.\nWe developed a comprehensive physics dataset from the OpenStax High School\nPhysics textbook, annotated according to Bloom's Taxonomy, with LaTeX and\nplaintext mathematical notations. A novel cultural contextualization approach\nwas applied to a subset, creating culturally adapted problems for Asian,\nAfrican, and South American/Australian contexts while preserving core physics\nprinciples. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,\nwe evaluated answer and reasoning chain correctness, along with calculation\naccuracy. The results reveal significant differences between the SLMs. Qwen 3\n1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was\nsubstantially low (38%). The format of the mathematical notation had a\nnegligible impact on performance. SLMs exhibited varied performance across the\nphysics topics and showed a decline in reasoning quality with increasing\ncognitive and knowledge complexity. In particular, the consistency of reasoning\nwas largely maintained in diverse cultural contexts, especially by better\nperforming models. These findings indicate that, while SLMs can often find\ncorrect answers, their underlying reasoning is frequently flawed, suggesting an\noverreliance on pattern recognition. For SLMs to become reliable educational\ntools in physics, future development must prioritize enhancing genuine\nunderstanding and the generation of sound, verifiable reasoning chains over\nmere answer accuracy.", "categories": ["cs.CL", "cs.AI", "physics.ed-ph"], "published": "2025-05-27 04:33:13", "updated": "2025-05-27 04:33:13", "pdf_url": "http://arxiv.org/pdf/2505.20707v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20715v1", "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding", "authors": ["Fuwen Luo", "Shengfeng Lou", "Chi Chen", "Ziyue Wang", "Chenliang Li", "Weizhou Shen", "Jiyue Guo", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "abstract": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-27 04:50:07", "updated": "2025-05-27 04:50:07", "pdf_url": "http://arxiv.org/pdf/2505.20715v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20730v1", "title": "What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals", "authors": ["Shahrooz Pouryousef"], "abstract": "User-item interactions contain rich collaborative signals that form the\nbackbone of many successful recommender systems. While recent work has explored\nthe use of large language models (LLMs) for recommendation, it remains unclear\nwhether LLMs can effectively reason over this type of collaborative\ninformation. In this paper, we conduct a systematic comparison between LLMs and\nclassical matrix factorization (MF) models to assess LLMs' ability to leverage\nuser-item interaction data. We further introduce a simple retrieval-augmented\ngeneration (RAG) method that enhances LLMs by grounding their predictions in\nstructured interaction data. Our experiments reveal that current LLMs often\nfall short in capturing collaborative patterns inherent to MF models, but that\nour RAG-based approach substantially improves recommendation\nquality-highlighting a promising direction for future LLM-based recommenders.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-05-27 05:18:57", "updated": "2025-05-27 05:18:57", "pdf_url": "http://arxiv.org/pdf/2505.20730v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20732v1", "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "abstract": "Reinforcement learning (RL) holds significant promise for training LLM agents\nto handle complex, goal-oriented tasks that require multi-step interactions\nwith external environments. However, a critical challenge when applying RL to\nthese agentic tasks arises from delayed rewards: feedback signals are typically\navailable only after the entire task is completed. This makes it non-trivial to\nassign delayed rewards to earlier actions, providing insufficient guidance\nregarding environmental constraints and hindering agent training. In this work,\nwe draw on the insight that the ultimate completion of a task emerges from the\ncumulative progress an agent makes across individual steps. We propose Stepwise\nProgress Attribution (SPA), a general reward redistribution framework that\ndecomposes the final reward into stepwise contributions, each reflecting its\nincremental progress toward overall task completion. To achieve this, we train\na progress estimator that accumulates stepwise contributions over a trajectory\nto match the task completion. During policy optimization, we combine the\nestimated per-step contribution with a grounding signal for actions executed in\nthe environment as the fine-grained, intermediate reward for effective agent\ntraining. Extensive experiments on common agent benchmarks (including Webshop,\nALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the\nstate-of-the-art method in both success rate (+2.5\\% on average) and grounding\naccuracy (+1.9\\% on average). Further analyses demonstrate that our method\nremarkably provides more effective intermediate rewards for RL training. Our\ncode is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-27 05:21:04", "updated": "2025-05-27 05:21:04", "pdf_url": "http://arxiv.org/pdf/2505.20732v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20738v1", "title": "Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator", "authors": ["Peiwen Yuan", "Yiwei Li", "Shaoxiong Feng", "Xinglin Wang", "Yueqi Zhang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "abstract": "LLM-as-Benchmark-Generator methods have been widely studied as a supplement\nto human annotators for scalable evaluation, while the potential biases within\nthis paradigm remain underexplored. In this work, we systematically define and\nvalidate the phenomenon of inflated performance in models evaluated on their\nself-generated benchmarks, referred to as self-bias, and attribute it to\nsub-biases arising from question domain, language style, and wrong labels. On\nthis basis, we propose Silencer, a general framework that leverages the\nheterogeneity between multiple generators at both the sample and benchmark\nlevels to neutralize bias and generate high-quality, self-bias-silenced\nbenchmark. Experimental results across various settings demonstrate that\nSilencer can suppress self-bias to near zero, significantly improve evaluation\neffectiveness of the generated benchmark (with an average improvement from\n0.655 to 0.833 in Pearson correlation with high-quality human-annotated\nbenchmark), while also exhibiting strong generalizability.", "categories": ["cs.CL"], "published": "2025-05-27 05:28:45", "updated": "2025-05-27 05:28:45", "pdf_url": "http://arxiv.org/pdf/2505.20738v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20767v1", "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models", "authors": ["Xiaqiang Tang", "Jian Li", "Keyu Hu", "Du Nan", "Xiaolong Li", "Xi Zhang", "Weigao Sun", "Sihong Xie"], "abstract": "Faithfulness hallucination are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandard, existing benchmarks only contain \"factual statements\" that rephrase\nsource materials without marking \"cognitive statements\" that make inference\nfrom the given context, making the consistency evaluation and optimization of\ncognitive statements difficult. Inspired by how an evidence is assessed in the\nlegislative domain, we design a rigorous framework to assess different levels\nof faithfulness of cognitive statements and create a benchmark dataset where we\nreveal insightful statistics. We design an annotation pipeline to create larger\nbenchmarks for different LLMs automatically, and the resulting larger-scale\nCogniBench-L dataset can be used to train accurate cognitive hallucination\ndetection model. We release our model and dataset at:\nhttps://github.com/FUTUREEEEEE/CogniBench", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 06:16:27", "updated": "2025-05-27 06:16:27", "pdf_url": "http://arxiv.org/pdf/2505.20767v1", "comment": "ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20776v1", "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences", "authors": ["Jungyoub Cha", "Hyunjong Kim", "Sungzoon Cho"], "abstract": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; C.4"], "published": "2025-05-27 06:30:00", "updated": "2025-05-27 06:30:00", "pdf_url": "http://arxiv.org/pdf/2505.20776v1", "comment": "8 pages, 3 figures. Under review at EMNLP 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20779v1", "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature", "authors": ["Noy Sternlicht", "Tom Hope"], "abstract": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA", "categories": ["cs.CL"], "published": "2025-05-27 06:36:04", "updated": "2025-05-27 06:36:04", "pdf_url": "http://arxiv.org/pdf/2505.20779v1", "comment": "Project page: https://noy-sternlicht.github.io/CHIMERA-Web", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20809v1", "title": "Improved Representation Steering for Language Models", "authors": ["Zhengxuan Wu", "Qinan Yu", "Aryaman Arora", "Christopher D. Manning", "Christopher Potts"], "abstract": "Steering methods for language models (LMs) seek to provide fine-grained and\ninterpretable control over model generations by variously changing model\ninputs, weights, or representations to adjust behavior. Recent work has shown\nthat adjusting weights or representations is often less effective than steering\nby prompting, for instance when wanting to introduce or suppress a particular\nconcept. We demonstrate how to improve representation steering via our new\nReference-free Preference Steering (RePS), a bidirectional\npreference-optimization objective that jointly does concept steering and\nsuppression. We train three parameterizations of RePS and evaluate them on\nAxBench, a large-scale model steering benchmark. On Gemma models with sizes\nranging from 2B to 27B, RePS outperforms all existing steering methods trained\nwith a language modeling objective and substantially narrows the gap with\nprompting -- while promoting interpretability and minimizing parameter count.\nIn suppression, RePS matches the language-modeling objective on Gemma-2 and\noutperforms it on the larger Gemma-3 variants while remaining resilient to\nprompt-based jailbreaking attacks that defeat prompting. Overall, our results\nsuggest that RePS provides an interpretable and robust alternative to prompting\nfor both steering and suppression.", "categories": ["cs.CL"], "published": "2025-05-27 07:16:40", "updated": "2025-05-27 07:16:40", "pdf_url": "http://arxiv.org/pdf/2505.20809v1", "comment": "46 pages, 23 figures, preprint", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20813v1", "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph", "authors": ["Junsik Kim", "Jinwook Park", "Kangil Kim"], "abstract": "In knowledge graph embedding, leveraging relation-specific\nentity-transformation has markedly enhanced performance. However, the\nconsistency of embedding differences before and after transformation remains\nunaddressed, risking the loss of valuable inductive bias inherent in the\nembeddings. This inconsistency stems from two problems. First, transformation\nrepresentations are specified for relations in a disconnected manner, allowing\ndissimilar transformations and corresponding entity-embeddings for similar\nrelations. Second, a generalized plug-in approach as a SFBR (Semantic Filter\nBased on Relations) disrupts this consistency through excessive concentration\nof entity embeddings under entity-based regularization, generating\nindistinguishable score distributions among relations. In this paper, we\nintroduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),\ncontaining more consistent entity-transformation characterized by three\nfeatures: 1) shared affine transformation of relation embeddings across all\nrelations, 2) rooted entity-transformation that adds an entity embedding to its\nchange represented by the transformed vector, and 3) normalization of the\nchange to prevent scale reduction. To amplify the advantages of consistency\nthat preserve semantics on embeddings, RSCF adds relation transformation and\nprediction modules for enhancing the semantics. In knowledge graph completion\ntasks with distance-based and tensor decomposition models, RSCF significantly\noutperforms state-of-the-art KGE methods, showing robustness across all\nrelations and their frequencies.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 07:22:00", "updated": "2025-05-27 07:22:00", "pdf_url": "http://arxiv.org/pdf/2505.20813v1", "comment": "Accepted to ACL 2025, 17 pages, 10 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20816v1", "title": "Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective", "authors": ["Krishna Singh Rajput", "Tejas Anvekar", "Chitta Baral", "Vivek Gupta"], "abstract": "Recent advances in multimodal question answering have primarily focused on\ncombining heterogeneous modalities or fine-tuning multimodal large language\nmodels. While these approaches have shown strong performance, they often rely\non a single, generalized reasoning strategy, overlooking the unique\ncharacteristics of each modality ultimately limiting both accuracy and\ninterpretability. To address these limitations, we propose MAMMQA, a\nmulti-agent QA framework for multimodal inputs spanning text, tables, and\nimages. Our system includes two Visual Language Model (VLM) agents and one\ntext-based Large Language Model (LLM) agent. The first VLM decomposes the user\nquery into sub-questions and sequentially retrieves partial answers from each\nmodality. The second VLM synthesizes and refines these results through\ncross-modal reasoning. Finally, the LLM integrates the insights into a cohesive\nanswer. This modular design enhances interpretability by making the reasoning\nprocess transparent and allows each agent to operate within its domain of\nexpertise. Experiments on diverse multimodal QA benchmarks demonstrate that our\ncooperative, multi-agent framework consistently outperforms existing baselines\nin both accuracy and robustness.", "categories": ["cs.CL"], "published": "2025-05-27 07:23:38", "updated": "2025-05-27 07:23:38", "pdf_url": "http://arxiv.org/pdf/2505.20816v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20819v1", "title": "Tracing and Reversing Rank-One Model Edits", "authors": ["Paul Youssef", "Zhixue Zhao", "Christin Seifert", "J\u00f6rg Schl\u00f6tterer"], "abstract": "Knowledge editing methods (KEs) are a cost-effective way to update the\nfactual content of large language models (LLMs), but they pose a dual-use risk.\nWhile KEs are beneficial for updating outdated or incorrect information, they\ncan be exploited maliciously to implant misinformation or bias. In order to\ndefend against these types of malicious manipulation, we need robust techniques\nthat can reliably detect, interpret, and mitigate adversarial edits. This work\ninvestigates the traceability and reversibility of knowledge edits, focusing on\nthe widely used Rank-One Model Editing (ROME) method. We first show that ROME\nintroduces distinctive distributional patterns in the edited weight matrices,\nwhich can serve as effective signals for locating the edited weights. Second,\nwe show that these altered weights can reliably be used to predict the edited\nfactual relation, enabling partial reconstruction of the modified fact.\nBuilding on this, we propose a method to infer the edited object entity\ndirectly from the modified weights, without access to the editing prompt,\nachieving over 95% accuracy. Finally, we demonstrate that ROME edits can be\nreversed, recovering the model's original outputs with $\\geq$ 80% accuracy. Our\nfindings highlight the feasibility of detecting, tracing, and reversing edits\nbased on the edited weights, offering a robust framework for safeguarding LLMs\nagainst adversarial manipulations.", "categories": ["cs.CL"], "published": "2025-05-27 07:27:01", "updated": "2025-05-27 07:27:01", "pdf_url": "http://arxiv.org/pdf/2505.20819v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20825v1", "title": "Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "abstract": "Long-form question answering (LFQA) presents unique challenges for large\nlanguage models, requiring the synthesis of coherent, paragraph-length answers.\nWhile retrieval-augmented generation (RAG) systems have emerged as a promising\nsolution, existing research struggles with key limitations: the scarcity of\nhigh-quality training data for long-form generation, the compounding risk of\nhallucination in extended outputs, and the absence of reliable evaluation\nmetrics for factual completeness. In this paper, we propose RioRAG, a novel\nreinforcement learning (RL) framework that advances long-form RAG through\nreinforced informativeness optimization. Our approach introduces two\nfundamental innovations to address the core challenges. First, we develop an RL\ntraining paradigm of reinforced informativeness optimization that directly\noptimizes informativeness and effectively addresses the slow-thinking deficit\nin conventional RAG systems, bypassing the need for expensive supervised data.\nSecond, we propose a nugget-centric hierarchical reward modeling approach that\nenables precise assessment of long-form answers through a three-stage process:\nextracting the nugget from every source webpage, constructing a nugget claim\nchecklist, and computing rewards based on factual alignment. Extensive\nexperiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the\neffectiveness of the proposed method. Our codes are available at\nhttps://github.com/RUCAIBox/RioRAG.", "categories": ["cs.CL"], "published": "2025-05-27 07:34:41", "updated": "2025-05-27 07:34:41", "pdf_url": "http://arxiv.org/pdf/2505.20825v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20826v1", "title": "AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset", "authors": ["Soichiro Murakami", "Peinan Zhang", "Hidetaka Kamigaito", "Hiroya Takamura", "Manabu Okumura"], "abstract": "Identifying factors that make ad text attractive is essential for advertising\nsuccess. This study proposes AdParaphrase v2.0, a dataset for ad text\nparaphrasing, containing human preference data, to enable the analysis of the\nlinguistic factors and to support the development of methods for generating\nattractive ad texts. Compared with v1.0, this dataset is 20 times larger,\ncomprising 16,460 ad text paraphrase pairs, each annotated with preference data\nfrom ten evaluators, thereby enabling a more comprehensive and reliable\nanalysis. Through the experiments, we identified multiple linguistic features\nof engaging ad texts that were not observed in v1.0 and explored various\nmethods for generating attractive ad texts. Furthermore, our analysis\ndemonstrated the relationships between human preference and ad performance, and\nhighlighted the potential of reference-free metrics based on large language\nmodels for evaluating ad text attractiveness. The dataset is publicly available\nat: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.", "categories": ["cs.CL"], "published": "2025-05-27 07:34:44", "updated": "2025-05-27 07:34:44", "pdf_url": "http://arxiv.org/pdf/2505.20826v1", "comment": "Accepted to ACL2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20841v1", "title": "Concealment of Intent: A Game-Theoretic Analysis", "authors": ["Xinbo Wu", "Abhishek Umrawal", "Lav R. Varshney"], "abstract": "As large language models (LLMs) grow more capable, concerns about their safe\ndeployment have also grown. Although alignment mechanisms have been introduced\nto deter misuse, they remain vulnerable to carefully designed adversarial\nprompts. In this work, we present a scalable attack strategy: intent-hiding\nadversarial prompting, which conceals malicious intent through the composition\nof skills. We develop a game-theoretic framework to model the interaction\nbetween such attacks and defense systems that apply both prompt and response\nfiltering. Our analysis identifies equilibrium points and reveals structural\nadvantages for the attacker. To counter these threats, we propose and analyze a\ndefense mechanism tailored to intent-hiding attacks. Empirically, we validate\nthe attack's effectiveness on multiple real-world LLMs across a range of\nmalicious behaviors, demonstrating clear advantages over existing adversarial\nprompting techniques.", "categories": ["cs.CL"], "published": "2025-05-27 07:59:56", "updated": "2025-05-27 07:59:56", "pdf_url": "http://arxiv.org/pdf/2505.20841v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20854v1", "title": "An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks", "authors": ["Xin Zhou", "Kisub Kim", "Ting Zhang", "Martin Weyssow", "Luis F. Gomes", "Guang Yang", "David Lo"], "abstract": "Large Language Models (LLMs) and other automated techniques have been\nincreasingly used to support software developers by generating software\nartifacts such as code snippets, patches, and comments. However, accurately\nassessing the correctness of these generated artifacts remains a significant\nchallenge. On one hand, human evaluation provides high accuracy but is\nlabor-intensive and lacks scalability. On the other hand, other existing\nautomatic evaluation metrics are scalable and require minimal human effort, but\nthey often fail to accurately reflect the actual correctness of generated\nsoftware artifacts.\n  In this paper, we present SWE-Judge, the first evaluation metric for\nLLM-as-Ensemble-Judge specifically designed to accurately assess the\ncorrectness of generated software artifacts. SWE-Judge first defines five\ndistinct evaluation strategies, each implemented as an independent judge. A\ndynamic team selection mechanism then identifies the most appropriate subset of\njudges to produce a final correctness score through ensembling. We evaluate\nSWE-Judge across a diverse set of software engineering (SE) benchmarks,\nincluding CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess.\nThese benchmarks span three SE tasks: code generation, automated program\nrepair, and code summarization. Experimental results demonstrate that SWE-Judge\nconsistently achieves a higher correlation with human judgments, with\nimprovements ranging from 5.9% to 183.8% over existing automatic metrics.\nFurthermore, SWE-Judge reaches agreement levels with human annotators that are\ncomparable to inter-annotator agreement in code generation and program repair\ntasks. These findings underscore SWE-Judge's potential as a scalable and\nreliable alternative to human evaluation.", "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2025-05-27 08:04:34", "updated": "2025-05-27 08:04:34", "pdf_url": "http://arxiv.org/pdf/2505.20854v1", "comment": "20 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20871v1", "title": "Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG", "authors": ["Xin Sun", "Jianan Xie", "Zhongqi Chen", "Qiang Liu", "Shu Wu", "Yuehe Chen", "Bowen Song", "Weiqiang Wang", "Zilei Wang", "Liang Wang"], "abstract": "Large language models (LLMs) augmented with retrieval systems have\nsignificantly advanced natural language processing tasks by integrating\nexternal knowledge sources, enabling more accurate and contextually rich\nresponses. To improve the robustness of such systems against noisy retrievals,\nRetrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method.\nHowever, RAFT conditions models to generate answers even in the absence of\nreliable knowledge. This behavior undermines their reliability in high-stakes\ndomains, where acknowledging uncertainty is critical. To address this issue, we\npropose Divide-Then-Align (DTA), a post-training approach designed to endow RAG\nsystems with the ability to respond with \"I don't know\" when the query is out\nof the knowledge boundary of both the retrieved passages and the model's\ninternal knowledge. DTA divides data samples into four knowledge quadrants and\nconstructs tailored preference data for each quadrant, resulting in a curated\ndataset for Direct Preference Optimization (DPO). Experimental results on three\nbenchmark datasets demonstrate that DTA effectively balances accuracy with\nappropriate abstention, enhancing the reliability and trustworthiness of\nretrieval-augmented systems.", "categories": ["cs.CL"], "published": "2025-05-27 08:21:21", "updated": "2025-05-27 08:21:21", "pdf_url": "http://arxiv.org/pdf/2505.20871v1", "comment": "ACL 2025 main", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20874v1", "title": "Can LLMs Learn to Map the World from Local Descriptions?", "authors": ["Sirui Xia", "Aili Chen", "Xintao Wang", "Tinghui Zhu", "Yikai Zhang", "Jiangjie Chen", "Yanghua Xiao"], "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated strong\ncapabilities in tasks such as code and mathematics. However, their potential to\ninternalize structured spatial knowledge remains underexplored. This study\ninvestigates whether LLMs, grounded in locally relative human observations, can\nconstruct coherent global spatial cognition by integrating fragmented\nrelational descriptions. We focus on two core aspects of spatial cognition:\nspatial perception, where models infer consistent global layouts from local\npositional relationships, and spatial navigation, where models learn road\nconnectivity from trajectory data and plan optimal paths between unconnected\nlocations. Experiments conducted in a simulated urban environment demonstrate\nthat LLMs not only generalize to unseen spatial relationships between points of\ninterest (POIs) but also exhibit latent representations aligned with real-world\nspatial distributions. Furthermore, LLMs can learn road connectivity from\ntrajectory descriptions, enabling accurate path planning and dynamic spatial\nawareness during navigation.", "categories": ["cs.CL"], "published": "2025-05-27 08:22:58", "updated": "2025-05-27 08:22:58", "pdf_url": "http://arxiv.org/pdf/2505.20874v1", "comment": "19 pages, 11 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20875v1", "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties", "authors": ["Jiyoung Lee", "Seungho Kim", "Jieun Han", "Jun-Min Lee", "Kitaek Kim", "Alice Oh", "Edward Choi"], "abstract": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our\n\\href{https://github.com/jiyounglee-0523/TransEnV}{code} and\n\\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}\nare publicly available.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 08:23:27", "updated": "2025-05-27 08:23:27", "pdf_url": "http://arxiv.org/pdf/2505.20875v1", "comment": "27 pages, 6 figures, 16 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20880v1", "title": "MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection", "authors": ["Baraa Hikal", "Ahmed Nasreldin", "Ali Hamdi"], "abstract": "This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes. The task involves detecting hallucinated spans in text\ngenerated by instruction-tuned Large Language Models (LLMs) across multiple\nlanguages. Our approach combines task-specific prompt engineering with an LLM\nensemble verification mechanism, where a primary model extracts hallucination\nspans and three independent LLMs adjudicate their validity through\nprobability-based voting. This framework simulates the human annotation\nworkflow used in the shared task validation and test data. Additionally, fuzzy\nmatching refines span alignment. Our system ranked 1st in Arabic and Basque,\n2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.", "categories": ["cs.CL"], "published": "2025-05-27 08:26:17", "updated": "2025-05-27 08:26:17", "pdf_url": "http://arxiv.org/pdf/2505.20880v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20888v1", "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models", "authors": ["Chengyu Wang", "Junbing Yan", "Wenrui Cai", "Yuanhao Yue", "Jun Huang"], "abstract": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 08:32:51", "updated": "2025-05-27 08:32:51", "pdf_url": "http://arxiv.org/pdf/2505.20888v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20896v1", "title": "How Do Transformers Learn Variable Binding in Symbolic Programs?", "authors": ["Yiwei Wu", "Atticus Geiger", "Rapha\u00ebl Milli\u00e8re"], "abstract": "Variable binding -- the ability to associate variables with values -- is\nfundamental to symbolic computation and cognition. Although classical\narchitectures typically implement variable binding via addressable memory, it\nis not well understood how modern neural networks lacking built-in binding\noperations may acquire this capacity. We investigate this by training a\nTransformer to dereference queried variables in symbolic programs where\nvariables are assigned either numerical constants or other variables. Each\nprogram requires following chains of variable assignments up to four steps deep\nto find the queried value, and also contains irrelevant chains of assignments\nacting as distractors. Our analysis reveals a developmental trajectory with\nthree distinct phases during training: (1) random prediction of numerical\nconstants, (2) a shallow heuristic prioritizing early variable assignments, and\n(3) the emergence of a systematic mechanism for dereferencing assignment\nchains. Using causal interventions, we find that the model learns to exploit\nthe residual stream as an addressable memory space, with specialized attention\nheads routing information across token positions. This mechanism allows the\nmodel to dynamically track variable bindings across layers, resulting in\naccurate dereferencing. Our results show how Transformer models can learn to\nimplement systematic variable binding without explicit architectural support,\nbridging connectionist and symbolic approaches.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-27 08:39:20", "updated": "2025-05-27 08:39:20", "pdf_url": "http://arxiv.org/pdf/2505.20896v1", "comment": "16 pages, 10 figures, 1 table. To appear in the Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20897v1", "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation", "authors": ["Pingrui Zhang", "Yifei Su", "Pengyuan Wu", "Dong An", "Li Zhang", "Zhigang Wang", "Dong Wang", "Yan Ding", "Bin Zhao", "Xuelong Li"], "abstract": "Vision-and-Language Navigation (VLN) requires the agent to navigate by\nfollowing natural instructions under partial observability, making it difficult\nto align perception with language. Recent methods mitigate this by imagining\nfuture scenes, yet they rely on vision-based synthesis, leading to high\ncomputational cost and redundant details. To this end, we propose to adaptively\nimagine key environmental semantics via \\textit{language} form, enabling a more\nreliable and efficient strategy. Specifically, we introduce a novel Adaptive\nText Dreamer (ATD), a dual-branch self-guided imagination policy built upon a\nlarge language model (LLM). ATD is designed with a human-like left-right brain\narchitecture, where the left brain focuses on logical integration, and the\nright brain is responsible for imaginative prediction of future scenes. To\nachieve this, we fine-tune only the Q-former within both brains to efficiently\nactivate domain-specific knowledge in the LLM, enabling dynamic updates of\nlogical reasoning and imagination during navigation. Furthermore, we introduce\na cross-interaction mechanism to regularize the imagined outputs and inject\nthem into a navigation expert module, allowing ATD to jointly exploit both the\nreasoning capacity of the LLM and the expertise of the navigation model. We\nconduct extensive experiments on the R2R benchmark, where ATD achieves\nstate-of-the-art performance with fewer parameters. The code is\n\\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "published": "2025-05-27 08:40:20", "updated": "2025-05-27 08:40:20", "pdf_url": "http://arxiv.org/pdf/2505.20897v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20899v1", "title": "Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing", "authors": ["Jeongsoo Choi", "Jaehun Kim", "Joon Son Chung"], "abstract": "This paper introduces a cross-lingual dubbing system that translates speech\nfrom one language to another while preserving key characteristics such as\nduration, speaker identity, and speaking speed. Despite the strong translation\nquality of existing speech translation approaches, they often overlook the\ntransfer of speech patterns, leading to mismatches with source speech and\nlimiting their suitability for dubbing applications. To address this, we\npropose a discrete diffusion-based speech-to-unit translation model with\nexplicit duration control, enabling time-aligned translation. We then\nsynthesize speech based on the predicted units and source identity with a\nconditional flow matching model. Additionally, we introduce a unit-based speed\nadaptation mechanism that guides the translation model to produce speech at a\nrate consistent with the source, without relying on any text. Extensive\nexperiments demonstrate that our framework generates natural and fluent\ntranslations that align with the original speech's duration and speaking pace,\nwhile achieving competitive translation performance.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-27 08:43:28", "updated": "2025-05-27 08:43:28", "pdf_url": "http://arxiv.org/pdf/2505.20899v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20901v1", "title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models", "authors": ["Junhyuk Choi", "Minju Kim", "Yeseon Hong", "Bugeun Kim"], "abstract": "As large vision language models(LVLMs) rapidly advance, concerns about their\npotential to learn and generate social biases and stereotypes are increasing.\nPrevious studies on LVLM's stereotypes face two primary limitations: metrics\nthat overlooked the importance of content words, and datasets that overlooked\nthe effect of color. To address these limitations, this study introduces new\nevaluation metrics based on the Stereotype Content Model (SCM). We also propose\nBASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM\nmetrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.\nAs a result, we found three findings. (1) The SCM-based evaluation is effective\nin capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output\nalong with gender and race ones. (3) Interaction between model architecture and\nparameter sizes seems to affect stereotypes. We release BASIC publicly on\n[anonymized for review].", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 08:44:05", "updated": "2025-05-27 08:44:05", "pdf_url": "http://arxiv.org/pdf/2505.20901v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20903v1", "title": "Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?", "authors": ["Ziming Wang", "Zeyu Shi", "Haoyi Zhou", "Shiqi Gao", "Qingyun Sun", "Jianxin Li"], "abstract": "Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration,\nwith their confidence scores misaligned with actual performance. While\ncalibration has been extensively studied in models trained from scratch, the\nimpact of LLMs' prior knowledge on calibration during fine-tuning remains\nunderstudied. Our research reveals that LLMs' prior knowledge causes potential\npoor calibration due to the ubiquitous presence of known data in real-world\nfine-tuning, which appears harmful for calibration. Specifically, data aligned\nwith LLMs' prior knowledge would induce overconfidence, while new knowledge\nimproves calibration. Our findings expose a tension: LLMs' encyclopedic\nknowledge, while enabling task versatility, undermines calibration through\nunavoidable knowledge overlaps. To address this, we propose CogCalib, a\ncognition-aware framework that applies targeted learning strategies according\nto the model's prior knowledge. Experiments across 7 tasks using 3 LLM families\nprove that CogCalib significantly improves calibration while maintaining\nperformance, achieving an average 57\\% reduction in ECE compared to standard\nfine-tuning in Llama3-8B. These improvements generalize well to out-of-domain\ntasks, enhancing the objectivity and reliability of domain-specific LLMs, and\nmaking them more trustworthy for critical human-AI interaction applications.", "categories": ["cs.CL"], "published": "2025-05-27 08:51:31", "updated": "2025-05-27 08:51:31", "pdf_url": "http://arxiv.org/pdf/2505.20903v1", "comment": "Accepted to ACL2025 Main; The code will be released soon", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20910v1", "title": "Automated Privacy Information Annotation in Large Language Model Interactions", "authors": ["Hang Zeng", "Xiangyu Liu", "Yong Hu", "Chaoyue Niu", "Fan Wu", "Shaojie Tang", "Guihai Chen"], "abstract": "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application scenarios, typically tagging personally identifiable\ninformation (PII) in anonymous content. In this work, to support the\ndevelopment and evaluation of privacy detection models for LLM interactions\nthat are deployable on local user devices, we construct a large-scale\nmultilingual dataset with 249K user queries and 154K annotated privacy phrases.\nIn particular, we build an automated privacy annotation pipeline with\ncloud-based strong LLMs to automatically extract privacy phrases from dialogue\ndatasets and annotate leaked information. We also design evaluation metrics at\nthe levels of privacy leakage, extracted privacy phrase, and privacy\ninformation. We further establish baseline methods using light-weight LLMs with\nboth tuning-free and tuning-based methods, and report a comprehensive\nevaluation of their performance. Evaluation results reveal a gap between\ncurrent performance and the requirements of real-world LLM applications,\nmotivating future research into more effective local privacy detection methods\ngrounded in our dataset.", "categories": ["cs.CL"], "published": "2025-05-27 09:00:12", "updated": "2025-05-27 09:00:12", "pdf_url": "http://arxiv.org/pdf/2505.20910v1", "comment": "9 content pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20921v1", "title": "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models", "authors": ["Injae Na", "Keonwoong Noh", "Woohwan Jung"], "abstract": "LLM providers typically offer multiple LLM tiers, varying in performance and\nprice. As NLP tasks become more complex and modularized, selecting the suitable\nLLM tier for each subtask is a key challenge to balance between cost and\nperformance. To address the problem, we introduce LLM Automatic Transmission\n(LLM-AT) framework that automatically selects LLM tiers without training.\nLLM-AT consists of Starter, Generator, and Judge. The starter selects the\ninitial LLM tier expected to solve the given question, the generator produces a\nresponse using the LLM of the selected tier, and the judge evaluates the\nvalidity of the response. If the response is invalid, LLM-AT iteratively\nupgrades to a higher-tier model, generates a new response, and re-evaluates\nuntil a valid response is obtained. Additionally, we propose accuracy\nestimator, which enables the suitable initial LLM tier selection without\ntraining. Given an input question, accuracy estimator estimates the expected\naccuracy of each LLM tier by computing the valid response rate across top-k\nsimilar queries from past inference records. Experiments demonstrate that\nLLM-AT achieves superior performance while reducing costs, making it a\npractical solution for real-world applications.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 09:11:00", "updated": "2025-05-27 09:11:00", "pdf_url": "http://arxiv.org/pdf/2505.20921v1", "comment": "ACL 2025 (Findings)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20925v1", "title": "Multi-objective Large Language Model Alignment with Hierarchical Experts", "authors": ["Zhuo Li", "Guodong Du", "Weiyang Guo", "Yigeng Zhou", "Xiucheng Li", "Wenya Wang", "Fangming Liu", "Yequan Wang", "Deheng Ye", "Min Zhang", "Jing Li"], "abstract": "Aligning large language models (LLMs) to simultaneously satisfy multiple\nobjectives remains a significant challenge, especially given the diverse and\noften conflicting nature of human preferences. Existing alignment methods\nstruggle to balance trade-offs effectively, often requiring costly retraining\nor yielding suboptimal results across the Pareto frontier of preferences. In\nthis paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a\n\\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play}\napproach that eliminates the need for model training, while enabling LLMs to\nadapt across the entire Pareto frontier and accommodate diverse user\npreferences. In particular, \\textit{HoE} consists of three hierarchical\ncomponents: LoRA Experts, Router Experts and Preference Routing, reaching\noptimal Pareto frontiers and achieving a trade-off between parameter size,\ntraining cost, and performance. We evaluate \\textit{HoE} across various tasks\non 14 objectives and 200 different preferences among 6 benchmarks,\ndemonstrating superior performance over 15 recent baselines. Code is available\nin the supplementary materials.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 09:15:03", "updated": "2025-05-27 09:15:03", "pdf_url": "http://arxiv.org/pdf/2505.20925v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20933v1", "title": "Information-Theoretic Complementary Prompts for Improved Continual Text Classification", "authors": ["Duzhen Zhang", "Yong Ren", "Chenxing Li", "Dong Yu", "Tielin Zhang"], "abstract": "Continual Text Classification (CTC) aims to continuously classify new text\ndata over time while minimizing catastrophic forgetting of previously acquired\nknowledge. However, existing methods often focus on task-specific knowledge,\noverlooking the importance of shared, task-agnostic knowledge. Inspired by the\ncomplementary learning systems theory, which posits that humans learn\ncontinually through the interaction of two systems -- the hippocampus,\nresponsible for forming distinct representations of specific experiences, and\nthe neocortex, which extracts more general and transferable representations\nfrom past experiences -- we introduce Information-Theoretic Complementary\nPrompts (InfoComp), a novel approach for CTC. InfoComp explicitly learns two\ndistinct prompt spaces: P(rivate)-Prompt and S(hared)-Prompt. These\nrespectively encode task-specific and task-invariant knowledge, enabling models\nto sequentially learn classification tasks without relying on data replay. To\npromote more informative prompt learning, InfoComp uses an\ninformation-theoretic framework that maximizes mutual information between\ndifferent parameters (or encoded representations). Within this framework, we\ndesign two novel loss functions: (1) to strengthen the accumulation of\ntask-specific knowledge in P-Prompt, effectively mitigating catastrophic\nforgetting, and (2) to enhance the retention of task-invariant knowledge in\nS-Prompt, improving forward knowledge transfer. Extensive experiments on\ndiverse CTC benchmarks show that our approach outperforms previous\nstate-of-the-art methods.", "categories": ["cs.CL"], "published": "2025-05-27 09:22:14", "updated": "2025-05-27 09:22:14", "pdf_url": "http://arxiv.org/pdf/2505.20933v1", "comment": "Accepted by Neural Networks", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20937v1", "title": "On VLMs for Diverse Tasks in Multimodal Meme Classification", "authors": ["Deepesh Gavit", "Debajyoti Mazumder", "Samiran Das", "Jasabanta Patro"], "abstract": "In this paper, we present a comprehensive and systematic analysis of\nvision-language models (VLMs) for disparate meme classification tasks. We\nintroduced a novel approach that generates a VLM-based understanding of meme\nimages and fine-tunes the LLMs on textual understanding of the embedded meme\ntext for improving the performance. Our contributions are threefold: (1)\nBenchmarking VLMs with diverse prompting strategies purposely to each sub-task;\n(2) Evaluating LoRA fine-tuning across all VLM components to assess performance\ngains; and (3) Proposing a novel approach where detailed meme interpretations\ngenerated by VLMs are used to train smaller language models (LLMs),\nsignificantly improving classification. The strategy of combining VLMs with\nLLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm,\noffensive and sentiment classification, respectively. Our results reveal the\nstrengths and limitations of VLMs and present a novel strategy for meme\nunderstanding.", "categories": ["cs.CL"], "published": "2025-05-27 09:25:46", "updated": "2025-05-27 09:25:46", "pdf_url": "http://arxiv.org/pdf/2505.20937v1", "comment": "16 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20959v1", "title": "Research Community Perspectives on \"Intelligence\" and Large Language Models", "authors": ["Bertram H\u00f8jer", "Terne Sasha Thorn Jakobsen", "Anna Rogers", "Stefan Heinrich"], "abstract": "Despite the widespread use of ''artificial intelligence'' (AI) framing in\nNatural Language Processing (NLP) research, it is not clear what researchers\nmean by ''intelligence''. To that end, we present the results of a survey on\nthe notion of ''intelligence'' among researchers and its role in the research\nagenda. The survey elicited complete responses from 303 researchers from a\nvariety of fields including NLP, Machine Learning (ML), Cognitive Science,\nLinguistics, and Neuroscience. We identify 3 criteria of intelligence that the\ncommunity agrees on the most: generalization, adaptability, & reasoning. Our\nresults suggests that the perception of the current NLP systems as\n''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the\nrespondents see developing intelligent systems as a research goal, and these\nrespondents are more likely to consider the current systems intelligent.", "categories": ["cs.CL", "cs.CY"], "published": "2025-05-27 09:53:27", "updated": "2025-05-27 09:53:27", "pdf_url": "http://arxiv.org/pdf/2505.20959v1", "comment": "ACL Findings 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20963v1", "title": "Context-Aware Content Moderation for German Newspaper Comments", "authors": ["Felix Krejca", "Tobias Kietreiber", "Alexander Buchelt", "Sebastian Neumaier"], "abstract": "The increasing volume of online discussions requires advanced automatic\ncontent moderation to maintain responsible discourse. While hate speech\ndetection on social media is well-studied, research on German-language\nnewspaper forums remains limited. Existing studies often neglect\nplatform-specific context, such as user history and article themes. This paper\naddresses this gap by developing and evaluating binary classification models\nfor automatic content moderation in German newspaper forums, incorporating\ncontextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging\nthe One Million Posts Corpus from the Austrian newspaper Der Standard, we\nassess the impact of context-aware models. Results show that CNN and LSTM\nmodels benefit from contextual information and perform competitively with\nstate-of-the-art approaches. In contrast, ChatGPT's zero-shot classification\ndoes not improve with added context and underperforms.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 09:57:02", "updated": "2025-05-27 09:57:02", "pdf_url": "http://arxiv.org/pdf/2505.20963v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20966v1", "title": "Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation", "authors": ["Zhibo Wang", "Xiaoze Jiang", "Zhiheng Qin", "Enyun Yu", "Han Li"], "abstract": "Query auto-completion (QAC) plays a crucial role in modern search systems.\nHowever, in real-world applications, there are two pressing challenges that\nstill need to be addressed. First, there is a need for hierarchical\npersonalized representations for users. Previous approaches have typically used\nusers' search behavior as a single, overall representation, which proves\ninadequate in more nuanced generative scenarios. Additionally, query prefixes\nare typically short and may contain typos or sensitive information, increasing\nthe likelihood of generating toxic content compared to traditional text\ngeneration tasks. Such toxic content can degrade user experience and lead to\npublic relations issues. Therefore, the second critical challenge is\ndetoxifying QAC systems.\n  To address these two limitations, we propose a novel model (LaD) that\ncaptures personalized information from both long-term and short-term interests,\nincorporating adaptive detoxification. In LaD, personalized information is\ncaptured hierarchically at both coarse-grained and fine-grained levels. This\napproach preserves as much personalized information as possible while enabling\nonline generation within time constraints. To move a futher step, we propose an\nonline training method based on Reject Preference Optimization (RPO). By\nincorporating a special token [Reject] during both the training and inference\nprocesses, the model achieves adaptive detoxification. Consequently, the\ngenerated text presented to users is both non-toxic and relevant to the given\nprefix. We conduct comprehensive experiments on industrial-scale datasets and\nperform online A/B tests, delivering the largest single-experiment metric\nimprovement in nearly two years of our product. Our model has been deployed on\nKuaishou search, driving the primary traffic for hundreds of millions of active\nusers. The code is available at https://github.com/JXZe/LaD.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-27 09:58:42", "updated": "2025-05-27 09:58:42", "pdf_url": "http://arxiv.org/pdf/2505.20966v1", "comment": "KDD 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20971v1", "title": "Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA", "authors": ["Xiangqing Shen", "Fanfan Wang", "Rui Xia"], "abstract": "LLMs have demonstrated remarkable capabilities in complex reasoning tasks,\nyet they often suffer from hallucinations and lack reliable factual grounding.\nMeanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack\nthe flexible reasoning abilities of LLMs. In this paper, we present\nReason-Align-Respond (RAR), a novel framework that systematically integrates\nLLM reasoning with knowledge graphs for KGQA. Our approach consists of three\nkey components: a Reasoner that generates human-like reasoning chains, an\nAligner that maps these chains to valid KG paths, and a Responser that\nsynthesizes the final answer. We formulate this process as a probabilistic\nmodel and optimize it using the Expectation-Maximization algorithm, which\niteratively refines the reasoning chains and knowledge paths. Extensive\nexperiments on multiple benchmarks demonstrate the effectiveness of RAR,\nachieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on\nWebQSP and CWQ respectively. Human evaluation confirms that RAR generates\nhigh-quality, interpretable reasoning chains well-aligned with KG paths.\nFurthermore, RAR exhibits strong zero-shot generalization capabilities and\nmaintains computational efficiency during inference.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 10:04:53", "updated": "2025-05-27 10:04:53", "pdf_url": "http://arxiv.org/pdf/2505.20971v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20976v1", "title": "Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing", "authors": ["Peiming Guo", "Meishan Zhang", "Jianling Li", "Min Zhang", "Yue Zhang"], "abstract": "Cross-domain constituency parsing is still an unsolved challenge in\ncomputational linguistics since the available multi-domain constituency\ntreebank is limited. We investigate automatic treebank generation by large\nlanguage models (LLMs) in this paper. The performance of LLMs on constituency\nparsing is poor, therefore we propose a novel treebank generation method, LLM\nback generation, which is similar to the reverse process of constituency\nparsing. LLM back generation takes the incomplete cross-domain constituency\ntree with only domain keyword leaf nodes as input and fills the missing words\nto generate the cross-domain constituency treebank. Besides, we also introduce\na span-level contrastive learning pre-training strategy to make full use of the\nLLM back generation treebank for cross-domain constituency parsing. We verify\nthe effectiveness of our LLM back generation treebank coupled with contrastive\nlearning pre-training on five target domains of MCTB. Experimental results show\nthat our approach achieves state-of-the-art performance on average results\ncompared with various baselines.", "categories": ["cs.CL"], "published": "2025-05-27 10:07:54", "updated": "2025-05-27 10:07:54", "pdf_url": "http://arxiv.org/pdf/2505.20976v1", "comment": "Accepted by ACL 2025 main conference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20977v1", "title": "Evaluating and Steering Modality Preferences in Multimodal Large Language Model", "authors": ["Yu Zhang", "Jinlong Ma", "Yongshuai Hou", "Xuefeng Bai", "Kehai Chen", "Yang Xiang", "Jun Yu", "Min Zhang"], "abstract": "Multimodal large language models (MLLMs) have achieved remarkable performance\non complex tasks with multimodal context. However, it is still understudied\nwhether they exhibit modality preference when processing multimodal contexts.\nTo study this question, we first build a \\textbf{MC\\textsuperscript{2}}\nbenchmark under controlled evidence conflict scenarios to systematically\nevaluate modality preference, which is the tendency to favor one modality over\nanother when making decisions based on multimodal conflicting evidence. Our\nextensive evaluation reveals that all 18 tested MLLMs generally demonstrate\nclear modality bias, and modality preference can be influenced by external\ninterventions. An in-depth analysis reveals that the preference direction can\nbe captured within the latent representations of MLLMs. Built on this, we\npropose a probing and steering method based on representation engineering to\nexplicitly control modality preference without additional fine-tuning or\ncarefully crafted prompts. Our method effectively amplifies modality preference\ntoward a desired direction and applies to downstream tasks such as\nhallucination mitigation and multimodal machine translation, yielding promising\nimprovements.", "categories": ["cs.CL"], "published": "2025-05-27 10:07:59", "updated": "2025-05-27 10:07:59", "pdf_url": "http://arxiv.org/pdf/2505.20977v1", "comment": "Modality Preference", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20981v1", "title": "RefAV: Towards Planning-Centric Scenario Mining", "authors": ["Cainan Davidson", "Deva Ramanan", "Neehar Peri"], "abstract": "Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal\ndata localized to HD maps during normal fleet testing. However, identifying\ninteresting and safety-critical scenarios from uncurated driving logs remains a\nsignificant challenge. Traditional scenario mining techniques are error-prone\nand prohibitively time-consuming, often relying on hand-crafted structured\nqueries. In this work, we revisit spatio-temporal scenario mining through the\nlens of recent vision-language models (VLMs) to detect whether a described\nscenario occurs in a driving log and, if so, precisely localize it in both time\nand space. To address this problem, we introduce RefAV, a large-scale dataset\nof 10,000 diverse natural language queries that describe complex multi-agent\ninteractions relevant to motion planning derived from 1000 driving logs in the\nArgoverse 2 Sensor dataset. We evaluate several referential multi-object\ntrackers and present an empirical analysis of our baselines. Notably, we find\nthat naively repurposing off-the-shelf VLMs yields poor performance, suggesting\nthat scenario mining presents unique challenges. Our code and dataset are\navailable at https://github.com/CainanD/RefAV/ and\nhttps://argoverse.github.io/user-guide/tasks/scenario_mining.html", "categories": ["cs.CV", "cs.CL", "cs.RO"], "published": "2025-05-27 10:14:35", "updated": "2025-05-27 10:14:35", "pdf_url": "http://arxiv.org/pdf/2505.20981v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20993v1", "title": "Who Reasons in the Large Language Models?", "authors": ["Jie Shao", "Jianxin Wu"], "abstract": "Despite the impressive performance of large language models (LLMs), the\nprocess of endowing them with new capabilities--such as mathematical\nreasoning--remains largely empirical and opaque. A critical open question is\nwhether reasoning abilities stem from the entire model, specific modules, or\nare merely artifacts of overfitting. In this work, we hypothesize that the\nreasoning capabilities in well-trained LLMs are primarily attributed to the\noutput projection module (oproj) in the Transformer's multi-head self-attention\n(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for\nNetworks (SfN), a suite of diagnostic tools designed to probe and analyze the\ninternal behaviors of LLMs. Using SfN, we provide both circumstantial and\nempirical evidence suggesting that oproj plays a central role in enabling\nreasoning, whereas other modules contribute more to fluent dialogue. These\nfindings offer a new perspective on LLM interpretability and open avenues for\nmore targeted training strategies, potentially enabling more efficient and\nspecialized LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 10:26:47", "updated": "2025-05-27 10:26:47", "pdf_url": "http://arxiv.org/pdf/2505.20993v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.20995v1", "title": "Articulatory strategy in vowel production as a basis for speaker discrimination", "authors": ["Justin J. H. Lo", "Patrycja Strycharczuk", "Sam Kirkham"], "abstract": "The way speakers articulate is well known to be variable across individuals\nwhile at the same time subject to anatomical and biomechanical constraints. In\nthis study, we ask whether articulatory strategy in vowel production can be\nsufficiently speaker-specific to form the basis for speaker discrimination. We\nconducted Generalised Procrustes Analyses of tongue shape data from 40 English\nspeakers from the North West of England, and assessed the\nspeaker-discriminatory potential of orthogonal tongue shape features within the\nframework of likelihood ratios. Tongue size emerged as the individual dimension\nwith the strongest discriminatory power, while tongue shape variation in the\nmore anterior part of the tongue generally outperformed tongue shape variation\nin the posterior part. When considered in combination, shape-only information\nmay offer comparable levels of speaker specificity to size-and-shape\ninformation, but only when features do not exhibit speaker-level co-variation.", "categories": ["cs.CL"], "published": "2025-05-27 10:29:05", "updated": "2025-05-27 10:29:05", "pdf_url": "http://arxiv.org/pdf/2505.20995v1", "comment": "Accepted to Interspeech 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21003v1", "title": "Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?", "authors": ["Yifei Wang", "Yu Sheng", "Linjing Li", "Daniel Zeng"], "abstract": "Recent advances in handling long sequences have facilitated the exploration\nof long-context in-context learning (ICL). While much of the existing research\nemphasizes performance improvements driven by additional in-context examples,\nthe influence on the trustworthiness of generated responses remains\nunderexplored. This paper addresses this gap by investigating how increased\nexamples influence predictive uncertainty, an essential aspect in\ntrustworthiness. We begin by systematically quantifying the uncertainty of ICL\nwith varying shot counts, analyzing the impact of example quantity. Through\nuncertainty decomposition, we introduce a novel perspective on performance\nenhancement, with a focus on epistemic uncertainty (EU). Our results reveal\nthat additional examples reduce total uncertainty in both simple and complex\ntasks by injecting task-specific knowledge, thereby diminishing EU and\nenhancing performance. For complex tasks, these advantages emerge only after\naddressing the increased noise and uncertainty associated with longer inputs.\nFinally, we explore the evolution of internal confidence across layers,\nunveiling the mechanisms driving the reduction in uncertainty.", "categories": ["cs.CL"], "published": "2025-05-27 10:36:39", "updated": "2025-05-27 10:36:39", "pdf_url": "http://arxiv.org/pdf/2505.21003v1", "comment": "Camera-ready versions for ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21011v1", "title": "LLMs are Frequency Pattern Learners in Natural Language Inference", "authors": ["Liang Cheng", "Zhaowei Wang", "Mark Steedman"], "abstract": "While fine-tuning LLMs on NLI corpora improves their inferential performance,\nthe underlying mechanisms driving this improvement remain largely opaque. In\nthis work, we conduct a series of experiments to investigate what LLMs actually\nlearn during fine-tuning. We begin by analyzing predicate frequencies in\npremises and hypotheses across NLI datasets and identify a consistent frequency\nbias, where predicates in hypotheses occur more frequently than those in\npremises for positive instances. To assess the impact of this bias, we evaluate\nboth standard and NLI fine-tuned LLMs on bias-consistent and bias-adversarial\ncases. We find that LLMs exploit frequency bias for inference and perform\npoorly on adversarial instances. Furthermore, fine-tuned LLMs exhibit\nsignificantly increased reliance on this bias, suggesting that they are\nlearning these frequency patterns from datasets. Finally, we compute the\nfrequencies of hyponyms and their corresponding hypernyms from WordNet,\nrevealing a correlation between frequency bias and textual entailment. These\nfindings help explain why learning frequency patterns can enhance model\nperformance on inference tasks.", "categories": ["cs.CL"], "published": "2025-05-27 10:45:29", "updated": "2025-05-27 10:45:29", "pdf_url": "http://arxiv.org/pdf/2505.21011v1", "comment": "9 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21024v1", "title": "Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers", "authors": ["Charles London", "Varun Kanade"], "abstract": "Pause tokens, simple filler symbols such as \"...\", consistently improve\nTransformer performance on both language and mathematical tasks, yet their\ntheoretical effect remains unexplained. We provide the first formal separation\nresult, proving that adding pause tokens to constant-depth, logarithmic-width\nTransformers strictly increases their computational expressivity. With\nbounded-precision activations, Transformers without pause tokens compute only a\nstrict subset of $\\mathsf{AC}^0$ functions, while adding a polynomial number of\npause tokens allows them to express the entire class. For logarithmic-precision\nTransformers, we show that adding pause tokens achieves expressivity equivalent\nto $\\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate\nthat two-layer causally masked Transformers can learn parity when supplied with\npause tokens, a function that they appear unable to learn without them. Our\nresults provide a rigorous theoretical explanation for prior empirical\nfindings, clarify how pause tokens interact with width, depth, and numeric\nprecision, and position them as a distinct mechanism, complementary to\nchain-of-thought prompting, for enhancing Transformer reasoning.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-27 10:59:27", "updated": "2025-05-27 10:59:27", "pdf_url": "http://arxiv.org/pdf/2505.21024v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21033v1", "title": "Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation", "authors": ["Seungmin Lee", "Yongsang Yoo", "Minhwa Jung", "Min Song"], "abstract": "Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent\nsegments. DTS plays a crucial role in various NLP downstream tasks, but suffers\nfrom chronic problems: data shortage, labeling ambiguity, and incremental\ncomplexity of recently proposed solutions. On the other hand, Despite advances\nin Large Language Models (LLMs) and reasoning strategies, these have rarely\nbeen applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for\nOpen-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step\ndeductive reasoning to enhance DTS performance and enable case study using\nintermediate result. Our method employs a structured prompting approach for\nbidirectional context summarization, utterance intent classification, and\ndeductive topic shift detection. In the intent classification process, we\npropose the generalizable intent list for domain-agnostic dialogue intent\nclassification. Experiments in various dialogue settings demonstrate that\nDef-DTS consistently outperforms traditional and state-of-the-art approaches,\nwith each subtask contributing to improved performance, particularly in\nreducing type 2 error. We also explore the potential for autolabeling,\nemphasizing the importance of LLM reasoning techniques in DTS.", "categories": ["cs.CL"], "published": "2025-05-27 11:07:53", "updated": "2025-05-27 11:07:53", "pdf_url": "http://arxiv.org/pdf/2505.21033v1", "comment": "19 pages, 3 figures, Accepted to Findings of the ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21040v1", "title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis", "authors": ["Wei Chen", "Zhao Zhang", "Meng Yuan", "Kepeng Xu", "Fuzhen Zhuang"], "abstract": "In this paper, we address the task of targeted sentiment analysis (TSA),\nwhich involves two sub-tasks, i.e., identifying specific aspects from reviews\nand determining their corresponding sentiments. Aspect extraction forms the\nfoundation for sentiment prediction, highlighting the critical dependency\nbetween these two tasks for effective cross-task knowledge transfer. While most\nexisting studies adopt a multi-task learning paradigm to align task-specific\nfeatures in the latent space, they predominantly rely on coarse-grained\nknowledge transfer. Such approaches lack fine-grained control over\naspect-sentiment relationships, often assuming uniform sentiment polarity\nwithin related aspects. This oversimplification neglects contextual cues that\ndifferentiate sentiments, leading to negative transfer. To overcome these\nlimitations, we propose FCKT, a fine-grained cross-task knowledge transfer\nframework tailored for TSA. By explicitly incorporating aspect-level\ninformation into sentiment prediction, FCKT achieves fine-grained knowledge\ntransfer, effectively mitigating negative transfer and enhancing task\nperformance. Experiments on three datasets, including comparisons with various\nbaselines and large language models (LLMs), demonstrate the effectiveness of\nFCKT. The source code is available on https://github.com/cwei01/FCKT.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 11:23:53", "updated": "2025-05-27 11:23:53", "pdf_url": "http://arxiv.org/pdf/2505.21040v1", "comment": "11 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21043v1", "title": "Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "abstract": "Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs)\nfacilitate naturalistic human-robot interaction, yet most rely solely on\nspeech. We introduce MM-VAP, a multimodal PTTM which combines speech with\nvisual cues including facial expression, head pose and gaze. We find that it\noutperforms the state-of-the-art audio-only in videoconferencing interactions\n(84% vs. 79% hold/shift prediction accuracy). Unlike prior work which\naggregates all holds and shifts, we group by duration of silence between turns.\nThis reveals that through the inclusion of visual features, MM-VAP outperforms\na state-of-the-art audio-only turn-taking model across all durations of speaker\ntransitions. We conduct a detailed ablation study, which reveals that facial\nexpression features contribute the most to model performance. Thus, our working\nhypothesis is that when interlocutors can see one another, visual cues are\nvital for turn-taking and must therefore be included for accurate turn-taking\nprediction. We additionally validate the suitability of automatic speech\nalignment for PTTM training using telephone speech. This work represents the\nfirst comprehensive analysis of multimodal PTTMs. We discuss implications for\nfuture work and make all code publicly available.", "categories": ["cs.CL", "cs.RO"], "published": "2025-05-27 11:24:38", "updated": "2025-05-27 11:24:38", "pdf_url": "http://arxiv.org/pdf/2505.21043v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21068v1", "title": "Predicting Implicit Arguments in Procedural Video Instructions", "authors": ["Anil Batra", "Laura Sevilla-Lara", "Marcus Rohrbach", "Frank Keller"], "abstract": "Procedural texts help AI enhance reasoning about context and action\nsequences. Transforming these into Semantic Role Labeling (SRL) improves\nunderstanding of individual steps by identifying predicate-argument structure\nlike {verb,what,where/with}. Procedural instructions are highly elliptic, for\ninstance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second\nstep's where argument is inferred from the context, referring to where the\ncucumber was placed. Prior SRL benchmarks often miss implicit arguments,\nleading to incomplete understanding. To address this, we introduce\nImplicit-VidSRL, a dataset that necessitates inferring implicit and explicit\narguments from contextual information in multimodal cooking procedures. Our\nproposed dataset benchmarks multimodal models' contextual reasoning, requiring\nentity tracking through visual changes in recipes. We study recent multimodal\nLLMs and reveal that they struggle to predict implicit arguments of what and\nwhere/with from multi-modal procedural data given the verb. Lastly, we propose\niSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for\nwhat-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.", "categories": ["cs.CL", "cs.CV"], "published": "2025-05-27 11:53:06", "updated": "2025-05-27 11:53:06", "pdf_url": "http://arxiv.org/pdf/2505.21068v1", "comment": "ACL 2025 Main", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21072v1", "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation", "authors": ["Ekaterina Fadeeva", "Aleksandr Rubashevskii", "Roman Vashurin", "Shehzaad Dhuliawala", "Artem Shelmanov", "Timothy Baldwin", "Preslav Nakov", "Mrinmaya Sachan", "Maxim Panov"], "abstract": "Large Language Models (LLMs) enhanced with external knowledge retrieval, an\napproach known as Retrieval-Augmented Generation (RAG), have shown strong\nperformance in open-domain question answering. However, RAG systems remain\nsusceptible to hallucinations: factually incorrect outputs that may arise\neither from inconsistencies in the model's internal knowledge or incorrect use\nof the retrieved context. Existing approaches often conflate factuality with\nfaithfulness to the retrieved context, misclassifying factually correct\nstatements as hallucinations if they are not directly supported by the\nretrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval\nAugmented UNcertainty Quantification), a novel method for hallucination\ndetection in RAG outputs. FRANQ applies different Uncertainty Quantification\n(UQ) techniques to estimate factuality based on whether a statement is faithful\nto the retrieved context or not. To evaluate FRANQ and other UQ techniques for\nRAG, we present a new long-form Question Answering (QA) dataset annotated for\nboth factuality and faithfulness, combining automated labeling with manual\nvalidation of challenging examples. Extensive experiments on long- and\nshort-form QA across multiple datasets and LLMs show that FRANQ achieves more\naccurate detection of factual errors in RAG-generated responses compared to\nexisting methods.", "categories": ["cs.CL"], "published": "2025-05-27 11:56:59", "updated": "2025-05-27 11:56:59", "pdf_url": "http://arxiv.org/pdf/2505.21072v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21082v1", "title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models", "authors": ["Jieyong Kim", "Tongyoung Kim", "Soonjin Yoon", "Jaehyung Kim", "Dongha Lee"], "abstract": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs.", "categories": ["cs.CL"], "published": "2025-05-27 12:06:16", "updated": "2025-05-27 12:06:16", "pdf_url": "http://arxiv.org/pdf/2505.21082v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21091v1", "title": "Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)", "authors": ["Anna Neumann", "Elisabeth Kirsten", "Muhammad Bilal Zafar", "Jatinder Singh"], "abstract": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments.", "categories": ["cs.CY", "cs.AI", "cs.CL"], "published": "2025-05-27 12:19:08", "updated": "2025-05-27 12:19:08", "pdf_url": "http://arxiv.org/pdf/2505.21091v1", "comment": "Forthcoming in Proceedings of ACM FAccT 2025", "doi": "10.1145/3715275.3732038", "journal_ref": null}
{"arxiv_id": "2505.21092v1", "title": "BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge", "authors": ["Daeen Kabir", "Minhajur Rahman Chowdhury Mahim", "Sheikh Shafayat", "Adnan Sadik", "Arian Ahmed", "Eunsu Kim", "Alice Oh"], "abstract": "In this work, we introduce BLUCK, a new dataset designed to measure the\nperformance of Large Language Models (LLMs) in Bengali linguistic understanding\nand cultural knowledge. Our dataset comprises 2366 multiple-choice questions\n(MCQs) carefully curated from compiled collections of several college and job\nlevel examinations and spans 23 categories covering knowledge on Bangladesh's\nculture and history and Bengali linguistics. We benchmarked BLUCK using 6\nproprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,\nGemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that\nwhile these models perform reasonably well overall, they, however, struggles in\nsome areas of Bengali phonetics. Although current LLMs' performance on Bengali\ncultural and linguistic contexts is still not comparable to that of mainstream\nlanguages like English, our results indicate Bengali's status as a mid-resource\nlanguage. Importantly, BLUCK is also the first MCQ-based evaluation benchmark\nthat is centered around native Bengali culture, history, and linguistics.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 12:19:12", "updated": "2025-05-27 12:19:12", "pdf_url": "http://arxiv.org/pdf/2505.21092v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21097v1", "title": "Thinker: Learning to Think Fast and Slow", "authors": ["Stephen Chung", "Wenyu Du", "Jie Fu"], "abstract": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training.", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.8; I.5.1"], "published": "2025-05-27 12:22:46", "updated": "2025-05-27 12:22:46", "pdf_url": "http://arxiv.org/pdf/2505.21097v1", "comment": "21 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21109v1", "title": "A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction", "authors": ["Bogdan Bogachov", "Yaoyao Fiona Zhao"], "abstract": "Despite recent advancements in domain adaptation techniques for large\nlanguage models, these methods remain computationally intensive, and the\nresulting models can still exhibit hallucination issues. Most existing\nadaptation methods do not prioritize reducing the computational resources\nrequired for fine-tuning and inference of language models. Hallucination issues\nhave gradually decreased with each new model release. However, they remain\nprevalent in engineering contexts, where generating well-structured text with\nminimal errors and inconsistencies is critical. This work introduces a novel\napproach called the Small Language Graph (SLG), which is a lightweight\nadaptation solution designed to address the two key challenges outlined above.\nThe system is structured in the form of a graph, where each node represents a\nlightweight expert - a small language model fine-tuned on specific and concise\ntexts. The results of this study have shown that SLG was able to surpass\nconventional fine-tuning methods on the Exact Match metric by 3 times.\nAdditionally, the fine-tuning process was 1.7 times faster compared to that of\na larger stand-alone language model. These findings introduce a potential for\nsmall to medium-sized engineering companies to confidently use generative AI\ntechnologies, such as LLMs, without the necessity to invest in expensive\ncomputational resources. Also, the graph architecture and the small size of\nexpert nodes offer a possible opportunity for distributed AI systems, thus\npotentially diverting the global need for expensive centralized compute\nclusters.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.LG", "I.2.7; I.2.1; I.5.1; I.2.6; H.3.1"], "published": "2025-05-27 12:31:24", "updated": "2025-05-27 12:31:24", "pdf_url": "http://arxiv.org/pdf/2505.21109v1", "comment": "10 pages, 4 Figures, 6 Tables. This paper has been accepted to be\n  published in the proceedings of IDETC-CIE 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21115v1", "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA", "authors": ["Sergey Pletenev", "Maria Marina", "Nikolay Ivanov", "Daria Galimzianova", "Nikita Krayko", "Mikhail Salnikov", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "abstract": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.", "categories": ["cs.CL"], "published": "2025-05-27 12:35:13", "updated": "2025-05-27 12:35:13", "pdf_url": "http://arxiv.org/pdf/2505.21115v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21116v1", "title": "Creativity in LLM-based Multi-Agent Systems: A Survey", "authors": ["Yi-Cheng Lin", "Kang-Chieh Chen", "Zhe-Yan Li", "Tzu-Heng Wu", "Tzu-Hsuan Wu", "Kuan-Yu Chen", "Hung-yi Lee", "Yun-Nung Chen"], "abstract": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS.", "categories": ["cs.HC", "cs.AI", "cs.CL"], "published": "2025-05-27 12:36:14", "updated": "2025-05-27 12:36:14", "pdf_url": "http://arxiv.org/pdf/2505.21116v1", "comment": "23 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21137v1", "title": "Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction", "authors": ["Mengjie Qian", "Rao Ma", "Stefano Bann\u00f2", "Kate M. Knill", "Mark J. F. Gales"], "abstract": "Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial\nfor second language learners, teachers and test takers. Traditional SGEC\nsystems rely on a cascaded pipeline consisting of an ASR, a module for\ndisfluency detection (DD) and removal and one for GEC. With the rise of\nend-to-end (E2E) speech foundation models, we investigate their effectiveness\nin SGEC and feedback generation. This work introduces a pseudo-labelling\nprocess to address the challenge of limited labelled data, expanding the\ntraining data size from 77 hours to approximately 2500 hours, leading to\nimproved performance. Additionally, we prompt an E2E Whisper-based SGEC model\nwith fluent transcriptions, showing a slight improvement in SGEC performance,\nwith more significant gains in feedback generation. Finally, we assess the\nimpact of increasing model size, revealing that while pseudo-labelled data does\nnot yield performance gain for a larger Whisper model, training with prompts\nproves beneficial.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-27 12:50:53", "updated": "2025-05-27 12:50:53", "pdf_url": "http://arxiv.org/pdf/2505.21137v1", "comment": "submitted to Interspeech", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21138v1", "title": "Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis", "authors": ["Tianyi Xu", "Hongjie Chen", "Wang Qing", "Lv Hang", "Jian Kang", "Li Jie", "Zhennan Lin", "Yongxiang Li", "Xie Lei"], "abstract": "Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised pre-\ntraining, combined with large language models (LLM), can effectively enhance\nASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-27 12:50:55", "updated": "2025-05-27 12:50:55", "pdf_url": "http://arxiv.org/pdf/2505.21138v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21148v1", "title": "Assessment of L2 Oral Proficiency using Speech Large Language Models", "authors": ["Rao Ma", "Mengjie Qian", "Siyuan Tang", "Stefano Bann\u00f2", "Kate M. Knill", "Mark J. F. Gales"], "abstract": "The growing population of L2 English speakers has increased the demand for\ndeveloping automatic graders for spoken language assessment (SLA).\nHistorically, statistical models, text encoders, and self-supervised speech\nmodels have been utilised for this task. However, cascaded systems suffer from\nthe loss of information, while E2E graders also have limitations. With the\nrecent advancements of multi-modal large language models (LLMs), we aim to\nexplore their potential as L2 oral proficiency graders and overcome these\nissues. In this work, we compare various training strategies using regression\nand classification targets. Our results show that speech LLMs outperform all\nprevious competitive baselines, achieving superior performance on two datasets.\nFurthermore, the trained grader demonstrates strong generalisation capabilities\nin the cross-part or cross-task evaluation, facilitated by the audio\nunderstanding knowledge acquired during LLM pre-training.", "categories": ["cs.CL", "cs.SD", "eess.AS"], "published": "2025-05-27 12:58:21", "updated": "2025-05-27 12:58:21", "pdf_url": "http://arxiv.org/pdf/2505.21148v1", "comment": "submitted to Interspeech", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21162v1", "title": "Leveraging GANs for citation intent classification and its impact on citation network analysis", "authors": ["Davi A. Bezerra", "Filipi N. Silva", "Diego R. Amancio"], "abstract": "Citations play a fundamental role in the scientific ecosystem, serving as a\nfoundation for tracking the flow of knowledge, acknowledging prior work, and\nassessing scholarly influence. In scientometrics, they are also central to the\nconstruction of quantitative indicators. Not all citations, however, serve the\nsame function: some provide background, others introduce methods, or compare\nresults. Therefore, understanding citation intent allows for a more nuanced\ninterpretation of scientific impact. In this paper, we adopted a GAN-based\nmethod to classify citation intents. Our results revealed that the proposed\nmethod achieves competitive classification performance, closely matching\nstate-of-the-art results with substantially fewer parameters. This demonstrates\nthe effectiveness and efficiency of leveraging GAN architectures combined with\ncontextual embeddings in intent classification task. We also investigated\nwhether filtering citation intents affects the centrality of papers in citation\nnetworks. Analyzing the network constructed from the unArXiv dataset, we found\nthat paper rankings can be significantly influenced by citation intent. All\nfour centrality metrics examined- degree, PageRank, closeness, and betweenness\n- were sensitive to the filtering of citation types. The betweenness centrality\ndisplayed the greatest sensitivity, showing substantial changes in ranking when\nspecific citation intents were removed.", "categories": ["cs.DL", "cs.CL", "cs.SI"], "published": "2025-05-27 13:16:09", "updated": "2025-05-27 13:16:09", "pdf_url": "http://arxiv.org/pdf/2505.21162v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21171v1", "title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs", "authors": ["Rochelle Choenni", "Ivan Titov"], "abstract": "Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 13:24:38", "updated": "2025-05-27 13:24:38", "pdf_url": "http://arxiv.org/pdf/2505.21171v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21172v1", "title": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment", "authors": ["Zheng Li", "Mao Zheng", "Mingyang Song", "Wenjie Yang"], "abstract": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have\nmade significant progress in tasks such as mathematics and coding. Inspired by\nthis, several studies have employed reinforcement learning(RL) to enhance\nmodels' deep reasoning capabilities and improve machine translation(MT)\nquality. However, the terminology translation, an essential task in MT, remains\nunexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a\nterminology-aware translation model trained with reinforcement learning and\nword alignment. Specifically, we first extract the keyword translation pairs\nusing a word alignment model. Then we carefully design three types of\nrule-based alignment rewards with the extracted alignment relationships. With\nthose alignment rewards, the RL-trained translation model can learn to focus on\nthe accurate translation of key information, including terminology in the\nsource text. Experimental results show the effectiveness of TAT-R1. Our model\nsignificantly improves terminology translation accuracy compared to the\nbaseline models while maintaining comparable performance on general translation\ntasks. In addition, we conduct detailed ablation studies of the\nDeepSeek-R1-like training paradigm for machine translation and reveal several\nkey findings.", "categories": ["cs.CL"], "published": "2025-05-27 13:26:02", "updated": "2025-05-27 13:26:02", "pdf_url": "http://arxiv.org/pdf/2505.21172v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21178v1", "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning", "authors": ["Mingyang Song", "Mao Zheng"], "abstract": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks.", "categories": ["cs.CL"], "published": "2025-05-27 13:29:51", "updated": "2025-05-27 13:29:51", "pdf_url": "http://arxiv.org/pdf/2505.21178v1", "comment": "Ongoing Work", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21184v1", "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing", "authors": ["Yu Yan", "Sheng Sun", "Zhifei Zheng", "Ziji Hao", "Teli Liu", "Min Liu"], "abstract": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-05-27 13:33:57", "updated": "2025-05-27 13:33:57", "pdf_url": "http://arxiv.org/pdf/2505.21184v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21189v1", "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation", "authors": ["Gleb Mezentsev", "Ivan Oseledets"], "abstract": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-27 13:39:24", "updated": "2025-05-27 13:39:24", "pdf_url": "http://arxiv.org/pdf/2505.21189v1", "comment": "under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21190v1", "title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation", "authors": ["Jong Hak Moon", "Geon Choi", "Paloma Rabaey", "Min Gwan Kim", "Hyuk Gi Hong", "Jung-Oh Lee", "Hangyul Yoon", "Eun Woo Doe", "Jiyoun Kim", "Harshita Sharma", "Daniel C. Castro", "Javier Alvarez-Valle", "Edward Choi"], "abstract": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 13:40:00", "updated": "2025-05-27 13:40:00", "pdf_url": "http://arxiv.org/pdf/2505.21190v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21191v1", "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities", "authors": ["Junyan Zhang", "Yubo Gao", "Yibo Yan", "Jungang Li", "Zhaorui Hou", "Sicheng Tao", "Shuliang Liu", "Song Dai", "Yonghua Hei", "Junzhuo Li", "Xuming Hu"], "abstract": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-27 13:40:28", "updated": "2025-05-27 13:40:28", "pdf_url": "http://arxiv.org/pdf/2505.21191v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21218v1", "title": "Pretrained LLMs Learn Multiple Types of Uncertainty", "authors": ["Roi Cohen", "Omri Fahn", "Gerard de Melo"], "abstract": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 14:06:15", "updated": "2025-05-27 14:06:15", "pdf_url": "http://arxiv.org/pdf/2505.21218v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21224v1", "title": "A Representation Level Analysis of NMT Model Robustness to Grammatical Errors", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "abstract": "Understanding robustness is essential for building reliable NLP systems.\nUnfortunately, in the context of machine translation, previous work mainly\nfocused on documenting robustness failures or improving robustness. In\ncontrast, we study robustness from a model representation perspective by\nlooking at internal model representations of ungrammatical inputs and how they\nevolve through model layers. For this purpose, we perform Grammatical Error\nDetection (GED) probing and representational similarity analysis. Our findings\nindicate that the encoder first detects the grammatical error, then corrects it\nby moving its representation toward the correct form. To understand what\ncontributes to this process, we turn to the attention mechanism where we\nidentify what we term Robustness Heads. We find that Robustness Heads attend to\ninterpretable linguistic units when responding to grammatical errors, and that\nwhen we fine-tune models for robustness, they tend to rely more on Robustness\nHeads for updating the ungrammatical word representation.", "categories": ["cs.CL"], "published": "2025-05-27 14:10:30", "updated": "2025-05-27 14:10:30", "pdf_url": "http://arxiv.org/pdf/2505.21224v1", "comment": "ACL 2025 Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21230v1", "title": "PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems", "authors": ["Nima Sedghiyeh", "Sara Sadeghi", "Reza Khodadadi", "Farzin Kashani", "Omid Aghdaei", "Somayeh Rahimi", "Mohammad Sadegh Safari"], "abstract": "Although Automatic Speech Recognition (ASR) systems have become an integral\npart of modern technology, their evaluation remains challenging, particularly\nfor low-resource languages such as Persian. This paper introduces Persian\nSpeech Recognition Benchmark(PSRB), a comprehensive benchmark designed to\naddress this gap by incorporating diverse linguistic and acoustic conditions.\nWe evaluate ten ASR systems, including state-of-the-art commercial and\nopen-source models, to examine performance variations and inherent biases.\nAdditionally, we conduct an in-depth analysis of Persian ASR transcriptions,\nidentifying key error types and proposing a novel metric that weights\nsubstitution errors. This metric enhances evaluation robustness by reducing the\nimpact of minor and partial errors, thereby improving the precision of\nperformance assessment. Our findings indicate that while ASR models generally\nperform well on standard Persian, they struggle with regional accents,\nchildren's speech, and specific linguistic challenges. These results highlight\nthe necessity of fine-tuning and incorporating diverse, representative training\ndatasets to mitigate biases and enhance overall ASR performance. PSRB provides\na valuable resource for advancing ASR research in Persian and serves as a\nframework for developing benchmarks in other low-resource languages. A subset\nof the PSRB dataset is publicly available at\nhttps://huggingface.co/datasets/PartAI/PSRB.", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "published": "2025-05-27 14:14:55", "updated": "2025-05-27 14:14:55", "pdf_url": "http://arxiv.org/pdf/2505.21230v1", "comment": "25 pages, 7 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21239v1", "title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners", "authors": ["Yu He", "Zihan Yao", "Chentao Song", "Tianyu Qi", "Jun Liu", "Ming Li", "Qing Huang"], "abstract": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD", "categories": ["cs.CL"], "published": "2025-05-27 14:19:35", "updated": "2025-05-27 14:19:35", "pdf_url": "http://arxiv.org/pdf/2505.21239v1", "comment": "work in progress", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21242v1", "title": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings", "authors": ["Gunjan Balde", "Soumyadeep Roy", "Mainack Mondal", "Niloy Ganguly"], "abstract": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.", "categories": ["cs.CL"], "published": "2025-05-27 14:23:03", "updated": "2025-05-27 14:23:03", "pdf_url": "http://arxiv.org/pdf/2505.21242v1", "comment": "16 pages. Accepted for publication in the Findings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21250v1", "title": "ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision", "authors": ["Dosung Lee", "Wonjun Oh", "Boyoung Kim", "Minyoung Kim", "Joonsuk Park", "Paul Hongsuck Seo"], "abstract": "Multi-hop question answering (MHQA) involves reasoning across multiple\ndocuments to answer complex questions. Dense retrievers typically outperform\nsparse methods like BM25 by leveraging semantic embeddings; however, they\nrequire labeled query-document pairs for fine-tuning. This poses a significant\nchallenge in MHQA due to the high variability of queries (reformulated)\nquestions throughout the reasoning steps. To overcome this limitation, we\nintroduce Retriever Supervision with Consistency and Relevance (ReSCORE), a\nnovel method for training dense retrievers for MHQA without labeled documents.\nReSCORE leverages large language models to capture each documents relevance to\nthe question and consistency with the correct answer and use them to train a\nretriever within an iterative question-answering framework. Experiments on\nthree MHQA benchmarks demonstrate the effectiveness of ReSCORE, with\nsignificant improvements in retrieval, and in turn, the state-of-the-art MHQA\nperformance. Our implementation is available at:\nhttps://leeds1219.github.io/ReSCORE.", "categories": ["cs.CL"], "published": "2025-05-27 14:28:24", "updated": "2025-05-27 14:28:24", "pdf_url": "http://arxiv.org/pdf/2505.21250v1", "comment": "9 pages, 3 figures, ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21265v1", "title": "Multilingual Pretraining for Pixel Language Models", "authors": ["Ilker Kesen", "Jonas F. Lotz", "Ingo Ziegler", "Phillip Rust", "Desmond Elliott"], "abstract": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 14:40:47", "updated": "2025-05-27 14:40:47", "pdf_url": "http://arxiv.org/pdf/2505.21265v1", "comment": "17 pages, 19 figures, 7 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21277v1", "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space", "authors": ["Yao Huang", "Yitong Sun", "Shouwei Ruan", "Yichi Zhang", "Yinpeng Dong", "Xingxing Wei"], "abstract": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "published": "2025-05-27 14:48:44", "updated": "2025-05-27 14:48:44", "pdf_url": "http://arxiv.org/pdf/2505.21277v1", "comment": "19 pages, 20 figures, accepted by ACL 2025, Findings", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21297v1", "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset", "authors": ["Yifei Liu", "Li Lyna Zhang", "Yi Zhu", "Bingcheng Dong", "Xudong Zhou", "Ning Shang", "Fan Yang", "Mao Yang"], "abstract": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar.", "categories": ["cs.CL"], "published": "2025-05-27 15:00:57", "updated": "2025-05-27 15:00:57", "pdf_url": "http://arxiv.org/pdf/2505.21297v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21301v1", "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian", "authors": ["Andrea Pedrotti", "Giulia Rambelli", "Caterina Villani", "Marianna Bolognesi"], "abstract": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 15:04:52", "updated": "2025-05-27 15:04:52", "pdf_url": "http://arxiv.org/pdf/2505.21301v1", "comment": "Accepted at ACL 2025", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21304v1", "title": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants", "authors": ["Louis Jalouzot", "Alexis Thual", "Yair Lakretz", "Christophe Pallier", "Bertrand Thirion"], "abstract": "We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort.", "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "published": "2025-05-27 15:06:04", "updated": "2025-05-27 15:06:04", "pdf_url": "http://arxiv.org/pdf/2505.21304v1", "comment": "13 pages, 6 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21315v1", "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead", "authors": ["Jesujoba O. Alabi", "Michael A. Hedderich", "David Ifeoluwa Adelani", "Dietrich Klakow"], "abstract": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages.", "categories": ["cs.CL"], "published": "2025-05-27 15:13:08", "updated": "2025-05-27 15:13:08", "pdf_url": "http://arxiv.org/pdf/2505.21315v1", "comment": "Working paper", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21324v1", "title": "Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts", "authors": ["Yuxin Zhu", "Yuting Guo", "Noah Marchuck", "Abeed Sarker", "Yun Wang"], "abstract": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification.", "categories": ["cs.CL"], "published": "2025-05-27 15:22:01", "updated": "2025-05-27 15:22:01", "pdf_url": "http://arxiv.org/pdf/2505.21324v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21329v1", "title": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks", "authors": ["Allaa Boutaleb", "Bernd Amann", "Hubert Naacke", "Rafael Angarita"], "abstract": "Recent table representation learning and data discovery methods tackle table\nunion search (TUS) within data lakes, which involves identifying tables that\ncan be unioned with a given query table to enrich its content. These methods\nare commonly evaluated using benchmarks that aim to assess semantic\nunderstanding in real-world TUS tasks. However, our analysis of prominent TUS\nbenchmarks reveals several limitations that allow simple baselines to perform\nsurprisingly well, often outperforming more sophisticated approaches. This\nsuggests that current benchmark scores are heavily influenced by\ndataset-specific characteristics and fail to effectively isolate the gains from\nsemantic understanding. To address this, we propose essential criteria for\nfuture benchmarks to enable a more realistic and reliable evaluation of\nprogress in semantic table union search.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "published": "2025-05-27 15:23:52", "updated": "2025-05-27 15:23:52", "pdf_url": "http://arxiv.org/pdf/2505.21329v1", "comment": "Accepted @ ACL 2025's Table Representation Learning Workshop (TRL)", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21342v1", "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims", "authors": ["Valentin Knappich", "Annemarie Friedrich", "Anna H\u00e4tty", "Simon Razniewski"], "abstract": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date.\n  We introduce PEDANTIC (\\underline{P}at\\underline{e}nt\n\\underline{D}efiniteness Ex\\underline{a}mi\\underline{n}a\\underline{ti}on\n\\underline{C}orpus), a novel dataset of 14k US patent claims from patent\napplications relating to Natural Language Processing (NLP), annotated with\nreasons for indefiniteness. We construct PEDANTIC using a fully automatic\npipeline that retrieves office action documents from the USPTO and uses Large\nLanguage Models (LLMs) to extract the reasons for indefiniteness. A human\nvalidation study confirms the pipeline's accuracy in generating high-quality\nannotations. To gain insight beyond binary classification metrics, we implement\nan LLM-as-Judge evaluation that compares the free-form reasoning of every\nmodel-cited reason with every examiner-cited reason. We show that LLM agents\nbased on Qwen 2.5 32B and 72B struggle to outperform logistic regression\nbaselines on definiteness prediction, even though they often correctly identify\nthe underlying reasons. PEDANTIC provides a valuable resource for patent AI\nresearchers, enabling the development of advanced examination models. We will\npublicly release the dataset and code.", "categories": ["cs.CL"], "published": "2025-05-27 15:34:39", "updated": "2025-05-27 15:34:39", "pdf_url": "http://arxiv.org/pdf/2505.21342v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21344v1", "title": "The Multilingual Divide and Its Impact on Global AI Safety", "authors": ["Aidan Peppin", "Julia Kreutzer", "Alice Schoenauer Sebag", "Kelly Marchisio", "Beyza Ermis", "John Dang", "Samuel Cahyawijaya", "Shivalika Singh", "Seraphina Goldfarb-Tarrant", "Viraat Aryabumi", "Aakanksha", "Wei-Yin Ko", "Ahmet \u00dcst\u00fcn", "Matthias Gall\u00e9", "Marzieh Fadaee", "Sara Hooker"], "abstract": "Despite advances in large language model capabilities in recent years, a\nlarge gap remains in their capabilities and safety performance for many\nlanguages beyond a relatively small handful of globally dominant languages.\nThis paper provides researchers, policymakers and governance experts with an\noverview of key challenges to bridging the \"language gap\" in AI and minimizing\nsafety risks across languages. We provide an analysis of why the language gap\nin AI exists and grows, and how it creates disparities in global AI safety. We\nidentify barriers to address these challenges, and recommend how those working\nin policy and governance can help address safety concerns associated with the\nlanguage gap by supporting multilingual dataset creation, transparency, and\nresearch.", "categories": ["cs.AI", "cs.CL"], "published": "2025-05-27 15:37:32", "updated": "2025-05-27 15:37:32", "pdf_url": "http://arxiv.org/pdf/2505.21344v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21354v1", "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning", "authors": ["Bidyarthi Paul", "Jalisha Jashim Era", "Mirazur Rahman Zim", "Tahmid Sattar Aothoi", "Faisal Muhammad Shah"], "abstract": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.", "categories": ["cs.CL", "cs.LG"], "published": "2025-05-27 15:47:10", "updated": "2025-05-27 15:47:10", "pdf_url": "http://arxiv.org/pdf/2505.21354v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21362v1", "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "authors": ["Qishuai Zhong", "Zongmin Li", "Siqi Fan", "Aixin Sun"], "abstract": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.", "categories": ["cs.CL", "cs.AI", "cs.HC"], "published": "2025-05-27 15:52:39", "updated": "2025-05-27 15:52:39", "pdf_url": "http://arxiv.org/pdf/2505.21362v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21378v1", "title": "Analyzing values about gendered language reform in LLMs' revisions", "authors": ["Jules Watson", "Xi Wang", "Raymond Liu", "Suzanne Stevenson", "Barend Beekhuizen"], "abstract": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment.", "categories": ["cs.CL"], "published": "2025-05-27 16:07:33", "updated": "2025-05-27 16:07:33", "pdf_url": "http://arxiv.org/pdf/2505.21378v1", "comment": "15 pages", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21380v1", "title": "PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense", "authors": ["Byungjun Kim", "Minju Kim", "Hyeonchu Park", "Bugeun Kim"], "abstract": "As malicious users increasingly employ phonetic substitution to evade hate\nspeech detection, researchers have investigated such strategies. However, two\nkey challenges remain. First, existing studies have overlooked the Korean\nlanguage, despite its vulnerability to phonetic perturbations due to its\nphonographic nature. Second, prior work has primarily focused on constructing\ndatasets rather than developing architectural defenses. To address these\nchallenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH)\nthat exploits the phonological characteristics of the Korean writing system,\nand (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the\ndetector's robustness by incorporating phonetic information at the\narchitectural level. Our experimental results demonstrate the effectiveness of\nour proposed methods on both perturbed and unperturbed datasets, suggesting\nthat they not only improve detection performance but also reflect realistic\nadversarial behaviors employed by malicious users.", "categories": ["cs.CL"], "published": "2025-05-27 16:09:02", "updated": "2025-05-27 16:09:02", "pdf_url": "http://arxiv.org/pdf/2505.21380v1", "comment": "Under review", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21389v1", "title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs", "authors": ["Xuanwen Ding", "Chengjun Pan", "Zejun Li", "Jiwen Zhang", "Siyuan Wang", "Zhongyu Wei"], "abstract": "Evaluating multimodal large language models (MLLMs) is increasingly\nexpensive, as the growing size and cross-modality complexity of benchmarks\ndemand significant scoring efforts. To tackle with this difficulty, we\nintroduce AutoJudger, an agent-driven framework for efficient and adaptive\nbenchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the\nItem Response Theory (IRT) to estimate the question difficulty and an\nautonomous evaluation agent to dynamically select the most informative test\nquestions based on the model's real-time performance. Specifically, AutoJudger\nincorporates two pivotal components: a semantic-aware retrieval mechanism to\nensure that selected questions cover diverse and challenging scenarios across\nboth vision and language modalities, and a dynamic memory that maintains\ncontextual statistics of previously evaluated questions to guide coherent and\nglobally informed question selection throughout the evaluation process.\nExtensive experiments on four representative multimodal benchmarks demonstrate\nthat our adaptive framework dramatically reduces evaluation expenses, i.e.\nAutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with\nthe full benchmark evaluation on MMT-Bench.", "categories": ["cs.CL"], "published": "2025-05-27 16:17:15", "updated": "2025-05-27 16:17:15", "pdf_url": "http://arxiv.org/pdf/2505.21389v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21396v1", "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science", "authors": ["Xiao Liu", "Xinyi Dong", "Xinyang Gao", "Yansong Feng", "Xun Pang"], "abstract": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "published": "2025-05-27 16:23:42", "updated": "2025-05-27 16:23:42", "pdf_url": "http://arxiv.org/pdf/2505.21396v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21397v1", "title": "DecisionFlow: Advancing Large Language Model as Principled Decision Maker", "authors": ["Xiusi Chen", "Shanyong Wang", "Cheng Qian", "Hongru Wang", "Peixuan Han", "Heng Ji"], "abstract": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow.", "categories": ["cs.CL"], "published": "2025-05-27 16:23:53", "updated": "2025-05-27 16:23:53", "pdf_url": "http://arxiv.org/pdf/2505.21397v1", "comment": "24 pages, 13 figures", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21399v1", "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling", "authors": ["Hovhannes Tamoyan", "Subhabrata Dutta", "Iryna Gurevych"], "abstract": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-05-27 16:24:02", "updated": "2025-05-27 16:24:02", "pdf_url": "http://arxiv.org/pdf/2505.21399v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21409v1", "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models", "authors": ["Dario Satriani", "Enzo Veltri", "Donatello Santoro", "Paolo Papotti"], "abstract": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.", "categories": ["cs.CL", "cs.AI", "cs.DB"], "published": "2025-05-27 16:33:38", "updated": "2025-05-27 16:33:38", "pdf_url": "http://arxiv.org/pdf/2505.21409v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21411v1", "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity", "authors": ["Yehui Tang", "Xiaosong Li", "Fangcheng Liu", "Wei Guo", "Hang Zhou", "Yaoyuan Wang", "Kai Han", "Xianzhi Yu", "Jinpeng Li", "Hui Zang", "Fei Mi", "Xiaojun Meng", "Zhicheng Liu", "Hanting Chen", "Binfan Zheng", "Can Chen", "Youliang Yan", "Ruiming Tang", "Peifeng Qin", "Xinghao Chen", "Dacheng Tao", "Yunhe Wang"], "abstract": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo.Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.", "categories": ["cs.CL"], "published": "2025-05-27 16:40:21", "updated": "2025-05-27 16:40:21", "pdf_url": "http://arxiv.org/pdf/2505.21411v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21413v1", "title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation", "authors": ["Xiao Liu", "Da Yin", "Zirui Wu", "Yansong Feng"], "abstract": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 16:41:19", "updated": "2025-05-27 16:41:19", "pdf_url": "http://arxiv.org/pdf/2505.21413v1", "comment": "Code is available at https://github.com/xxxiaol/RefTool", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21439v1", "title": "Towards Better Instruction Following Retrieval Models", "authors": ["Yuchen Zhuang", "Aaron Trinh", "Rushi Qiang", "Haotian Sun", "Chao Zhang", "Hanjun Dai", "Bo Dai"], "abstract": "Modern information retrieval (IR) models, trained exclusively on standard\n<query, passage> pairs, struggle to effectively interpret and follow explicit\nuser instructions. We introduce InF-IR, a large-scale, high-quality training\ncorpus tailored for enhancing retrieval models in Instruction-Following IR.\nInF-IR expands traditional training pairs into over 38,000 expressive\n<instruction, query, passage> triplets as positive samples. In particular, for\neach positive triplet, we generate two additional hard negative examples by\npoisoning both instructions and queries, then rigorously validated by an\nadvanced reasoning model (o3-mini) to ensure semantic plausibility while\nmaintaining instructional incorrectness. Unlike existing corpora that primarily\nsupport computationally intensive reranking tasks for decoder-only language\nmodels, the highly contrastive positive-negative triplets in InF-IR further\nenable efficient representation learning for smaller encoder-only models,\nfacilitating direct embedding-based retrieval. Using this corpus, we train\nInF-Embed, an instruction-aware Embedding model optimized through contrastive\nlearning and instruction-query attention mechanisms to align retrieval outcomes\nprecisely with user intents. Extensive experiments across five\ninstruction-based retrieval benchmarks demonstrate that InF-Embed significantly\nsurpasses competitive baselines by 8.1% in p-MRR, measuring the\ninstruction-following capabilities.", "categories": ["cs.CL", "cs.IR"], "published": "2025-05-27 17:14:37", "updated": "2025-05-27 17:14:37", "pdf_url": "http://arxiv.org/pdf/2505.21439v1", "comment": "Retrieval Models, Embedding, Retrieval with Instructions", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21451v1", "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication", "authors": ["Jocelyn Shen", "Akhila Yerukola", "Xuhui Zhou", "Cynthia Breazeal", "Maarten Sap", "Hae Won Park"], "abstract": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection.", "categories": ["cs.CL"], "published": "2025-05-27 17:23:57", "updated": "2025-05-27 17:23:57", "pdf_url": "http://arxiv.org/pdf/2505.21451v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21458v1", "title": "Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance", "authors": ["Shintaro Ozaki", "Tatsuya Hiraoka", "Hiroto Otake", "Hiroki Ouchi", "Masaru Isonuma", "Benjamin Heinzerling", "Kentaro Inui", "Taro Watanabe", "Yusuke Miyao", "Yohei Oseki", "Yu Takagi"], "abstract": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance.", "categories": ["cs.CL"], "published": "2025-05-27 17:30:57", "updated": "2025-05-27 17:30:57", "pdf_url": "http://arxiv.org/pdf/2505.21458v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21465v1", "title": "ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models", "authors": ["Bozhou Li", "Wentao Zhang"], "abstract": "Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)\nperformance is to encode both the high-resolution version and the thumbnail of\nan image simultaneously. While effective, this method generates a large number\nof image tokens. When combined with the widely used Rotary Position Embedding\n(RoPE), its long-term decay property hinders the interaction between\nhigh-resolution tokens and thumbnail tokens, as well as between text and image.\nTo address these issues, we propose ID-Align, which alleviates these problems\nby reordering position IDs. In this method, high-resolution tokens inherit IDs\nfrom their corresponding thumbnail token while constraining the overexpansion\nof positional indices. Our experiments conducted within the LLaVA-Next\nframework demonstrate that ID-Align achieves significant improvements,\nincluding a 6.09% enhancement on MMBench's relation reasoning tasks and notable\ngains across multiple benchmarks. Our code is available at the following link:\nhttps://github.com/zooblastlbz/ID-Align.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-27 17:36:23", "updated": "2025-05-27 17:36:23", "pdf_url": "http://arxiv.org/pdf/2505.21465v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21467v1", "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion", "authors": ["Zhanqiu Hu", "Jian Meng", "Yash Akhauri", "Mohamed S. Abdelfattah", "Jae-sun Seo", "Zhiru Zhang", "Udit Gupta"], "abstract": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.", "categories": ["cs.CL"], "published": "2025-05-27 17:39:39", "updated": "2025-05-27 17:39:39", "pdf_url": "http://arxiv.org/pdf/2505.21467v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21471v1", "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration", "authors": ["Zijun Liu", "Zhennan Wan", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "abstract": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.", "categories": ["cs.CL"], "published": "2025-05-27 17:45:04", "updated": "2025-05-27 17:45:04", "pdf_url": "http://arxiv.org/pdf/2505.21471v1", "comment": "30 pages, 9 figures. Code and data are available at\n  https://github.com/THUNLP-MT/ExtAgents", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21472v1", "title": "Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration", "authors": ["Mehrdad Fazli", "Bowen Wei", "Ziwei Zhu"], "abstract": "Large vision-language models (LVLMs) achieve impressive performance on\nmultimodal tasks but often suffer from hallucination, and confidently describe\nobjects or attributes not present in the image. Current inference-time\ninterventions, while training-free, struggle to maintain accuracy in open-ended\nand long-form generation scenarios. We introduce the Confidence-Aware Attention\nCalibration (CAAC) framework to address this challenge by targeting two key\nbiases: spatial perception bias, which distributes attention disproportionately\nacross image tokens, and modality bias, which shifts focus from visual to\ntextual inputs over time. CAAC employs a two-step approach: Visual-Token\nCalibration (VTC) to balance attention across visual tokens, and Adaptive\nAttention Re-Scaling (AAR) to reinforce visual grounding based on the model's\nconfidence. This confidence-driven adjustment ensures consistent visual\nalignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks\ndemonstrate that CAAC outperforms baselines, particularly in long-form\ngenerations, effectively reducing hallucination.", "categories": ["cs.CV", "cs.CL"], "published": "2025-05-27 17:45:21", "updated": "2025-05-27 17:45:21", "pdf_url": "http://arxiv.org/pdf/2505.21472v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21479v1", "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?", "authors": ["Keenan Samway", "Max Kleiman-Weiner", "David Guzman Piedrahita", "Rada Mihalcea", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "abstract": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .", "categories": ["cs.CL"], "published": "2025-05-27 17:51:18", "updated": "2025-05-27 17:51:18", "pdf_url": "http://arxiv.org/pdf/2505.21479v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21487v1", "title": "Hardware-Efficient Attention for Fast Decoding", "authors": ["Ted Zadouri", "Hubert Strauss", "Tri Dao"], "abstract": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-27 17:54:07", "updated": "2025-05-27 17:54:07", "pdf_url": "http://arxiv.org/pdf/2505.21487v1", "comment": "37 pages, 15 figures, 45 tables", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21493v1", "title": "Reinforcing General Reasoning without Verifiers", "authors": ["Xiangxin Zhou", "Zichen Liu", "Anya Sims", "Haonan Wang", "Tianyu Pang", "Chongxuan Li", "Liang Wang", "Min Lin", "Chao Du"], "abstract": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.", "categories": ["cs.LG", "cs.CL"], "published": "2025-05-27 17:56:27", "updated": "2025-05-27 17:56:27", "pdf_url": "http://arxiv.org/pdf/2505.21493v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21496v1", "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents", "authors": ["Han Xiao", "Guozhi Wang", "Yuxiang Chai", "Zimu Lu", "Weifeng Lin", "Hao He", "Lue Fan", "Liuyang Bian", "Rui Hu", "Liang Liu", "Shuai Ren", "Yafei Wen", "Xiaoxin Chen", "Aojun Zhou", "Hongsheng Li"], "abstract": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie.", "categories": ["cs.CL", "cs.CV", "cs.LG"], "published": "2025-05-27 17:58:06", "updated": "2025-05-27 17:58:06", "pdf_url": "http://arxiv.org/pdf/2505.21496v1", "comment": "https://github.com/Euphoria16/UI-Genie", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21497v1", "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers", "authors": ["Wei Pang", "Kevin Qinghong Lin", "Xiangru Jian", "Xi He", "Philip Torr"], "abstract": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA"], "published": "2025-05-27 17:58:49", "updated": "2025-05-27 17:58:49", "pdf_url": "http://arxiv.org/pdf/2505.21497v1", "comment": "Project Page: https://github.com/Paper2Poster/Paper2Poster", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-05-27 17:59:26", "updated": "2025-05-27 17:59:26", "pdf_url": "http://arxiv.org/pdf/2505.21500v1", "comment": "Project: https://zju-real.github.io/ViewSpatial-Page/", "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21503v1", "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making", "authors": ["Yihan Wang", "Qiao Yan", "Zhenghao Xing", "Lihao Liu", "Junjun He", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "abstract": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1.", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.OT"], "published": "2025-05-27 17:59:50", "updated": "2025-05-27 17:59:50", "pdf_url": "http://arxiv.org/pdf/2505.21503v1", "comment": null, "doi": null, "journal_ref": null}
{"arxiv_id": "2505.21505v1", "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective", "authors": ["Shimao Zhang", "Zhejian Lai", "Xiang Liu", "Shuaijie She", "Xiao Liu", "Yeyun Gong", "Shujian Huang", "Jiajun Chen"], "abstract": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-05-27 17:59:52", "updated": "2025-05-27 17:59:52", "pdf_url": "http://arxiv.org/pdf/2505.21505v1", "comment": null, "doi": null, "journal_ref": null}
